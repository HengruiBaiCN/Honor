{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLinear(nn.Linear):\n",
    "    r\"\"\"Applies a linear transformation to the input data as follows\n",
    "    :math:`y = naxA^T + b`.\n",
    "    More details available in Jagtap, A. D. et al. Locally adaptive\n",
    "    activation functions with slope recovery for deep and\n",
    "    physics-informed neural networks, Proc. R. Soc. 2020.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        The size of each input sample\n",
    "    out_features : int \n",
    "        The size of each output sample\n",
    "    bias : bool, optional\n",
    "        If set to ``False``, the layer will not learn an additive bias\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice \n",
    "        (self.A is the learnable parameter which is initialized by the \n",
    "        self.adaptive rate. To create a learnable parameter for each neuron, \n",
    "        it multiplies the self.adaptive rate to the number of input features.)\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(AdaptiveLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.adaptive_rate = adaptive_rate\n",
    "        self.adaptive_rate_scaler = adaptive_rate_scaler\n",
    "        if self.adaptive_rate:\n",
    "            self.A = nn.Parameter(self.adaptive_rate * torch.ones(self.in_features))\n",
    "            if not self.adaptive_rate_scaler:\n",
    "                self.adaptive_rate_scaler = 10.0\n",
    "            \n",
    "    def forward(self, input):\n",
    "        if self.adaptive_rate:\n",
    "            return nn.functional.linear(self.adaptive_rate_scaler * self.A * input, self.weight, self.bias)\n",
    "        return nn.functional.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, '\n",
    "            f'adaptive_rate={self.adaptive_rate is not None}, adaptive_rate_scaler={self.adaptive_rate_scaler is not None}'\n",
    "        )\n",
    "\n",
    "class Net(nn.Module):\n",
    "    r\"\"\"Neural approximator for the unknown function that is supposed\n",
    "    to be solved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sizes : list\n",
    "        Each element represents the number of neuron per layer\n",
    "    activation : callable \n",
    "        Activation function\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate for regulrization during training process and\n",
    "        uncertainty quantification by means of Monte Carlo dropout\n",
    "        procedure while performing evaluation\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, activation, dropout_rate=0.0, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            *[\n",
    "                Net.linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler)\n",
    "                for in_features, out_features in zip(sizes[:-1], sizes[1:-1])\n",
    "            ],\n",
    "            # because the output is the price of the option, we use regular linear transformation     \n",
    "            AdaptiveLinear(sizes[-2], sizes[-1]) # output layer is regular linear transformation\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler):\n",
    "        activation_dispatcher = nn.ModuleDict([\n",
    "            ['lrelu', nn.LeakyReLU()],\n",
    "            ['relu', nn.ReLU()],\n",
    "            ['tanh', nn.Tanh()],\n",
    "            ['sigmoid', nn.Sigmoid()],\n",
    "            # ['swish', Swish()]\n",
    "        ])\n",
    "        return nn.Sequential(\n",
    "            AdaptiveLinear(in_features, out_features, adaptive_rate=adaptive_rate, adaptive_rate_scaler=adaptive_rate_scaler),\n",
    "            activation_dispatcher[activation],\n",
    "            nn.Dropout(dropout_rate),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "IPINN = Net(\n",
    "    sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1], activation='relu', dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0\n",
    "    )\n",
    "IPINN.cuda()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3000\n",
    "lossFunction = nn.MSELoss()\n",
    "lr = 3e-5\n",
    "optimizer = optim.Adam(IPINN.parameters(), lr=lr)\n",
    "\n",
    "x_f_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_label_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_data_s = torch.tensor(-1.).float().to(device).requires_grad_(True)\n",
    "optimizer_adam_weight = optim.Adam([x_f_s] + [x_label_s] + [x_data_s], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3000 PDE Loss: 0.00001, BC Loss: 677.78241, Data Loss: 426.71207, reg_loss:  0.904837, total loss: 1104.494507, minimum loss: 1104.49451\n",
      "the weight is 1.00100, 0.99900. 2.71556\n",
      "500/3000 PDE Loss: 0.00645, BC Loss: 9.21340, Data Loss: 13.64884, reg_loss:  0.895256, total loss: 22.868685, minimum loss: 22.86868\n",
      "the weight is 1.64980, 0.72257. 1.96529\n",
      "1000/3000 PDE Loss: 0.46396, BC Loss: 0.31043, Data Loss: 0.45966, reg_loss:  0.890655, total loss: 1.234053, minimum loss: 1.23405\n",
      "the weight is 1.72870, 0.71966. 1.94094\n",
      "1500/3000 PDE Loss: 0.00035, BC Loss: 0.00171, Data Loss: 0.00191, reg_loss:  0.890149, total loss: 0.003971, minimum loss: 0.00376\n",
      "the weight is 2.93295, 0.72126. 1.94334\n",
      "2000/3000 PDE Loss: 0.00027, BC Loss: 0.00164, Data Loss: 0.00149, reg_loss:  0.890085, total loss: 0.003402, minimum loss: 0.00317\n",
      "the weight is 5.02876, 0.72350. 1.94689\n",
      "2500/3000 PDE Loss: 0.00007, BC Loss: 0.00146, Data Loss: 0.00140, reg_loss:  0.890015, total loss: 0.002930, minimum loss: 0.00293\n",
      "the weight is 8.46466, 0.72652. 1.95164\n",
      "run time: 74.56872272491455\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.autograd as tgrad\n",
    "\n",
    "loss_hist = []\n",
    "min_train_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = time.time()\n",
    "\n",
    "# tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs)\n",
    "for _ in range(n_epochs):\n",
    "    \n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, r, sigma, T, S_range[-1], S_range, t_range, gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)   \n",
    "    \n",
    "    # pde residual loss\n",
    "    y1_hat = IPINN(n_st_train)\n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), \n",
    "                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), \n",
    "                    create_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    # boudary condition loss\n",
    "    y21_hat = IPINN(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y21_hat)\n",
    "    \n",
    "    # data Round\n",
    "    y3_hat = IPINN(X_train_tensor)\n",
    "    data_loss = lossFunction(y_train_tensor, y3_hat)\n",
    "    \n",
    "    # slope recovery term\n",
    "    local_recovery_terms = torch.tensor([torch.mean(IPINN.regressor[layer][0].A.data) for layer in range(len(IPINN.regressor) - 1)])\n",
    "    slope_recovery_term = 1 / torch.mean(torch.exp(local_recovery_terms))\n",
    "    \n",
    "    # update the neural network parameters\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = torch.exp(-x_f_s.detach()) * pde_loss + torch.exp(-x_label_s.detach()) * bc_loss + torch.exp(-x_data_s.detach()) * data_loss + slope_recovery_term + x_f_s + x_label_s + x_data_s\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update the weight\n",
    "    optimizer_adam_weight.zero_grad()\n",
    "    loss = torch.exp(-x_f_s) * pde_loss.detach() + x_f_s + torch.exp(-x_label_s) * bc_loss.detach() + x_label_s + torch.exp(-x_data_s) * data_loss.detach() + x_data_s\n",
    "    loss.backward()\n",
    "    optimizer_adam_weight.step()\n",
    "    \n",
    "    # print loss\n",
    "    mse_loss = pde_loss + bc_loss + data_loss\n",
    "    loss_hist.append(mse_loss.item())\n",
    "    if _ % 500 == 0:\n",
    "        print(f'{_}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, Data Loss: {data_loss.item():.5f}, reg_loss: {slope_recovery_term.item(): 5f}, total loss: {mse_loss.item():5f}, minimum loss: {min(loss_hist):.5f}')\n",
    "        print(f'the weight is {torch.exp(-x_f_s.detach()).item():.5f}, {torch.exp(-x_label_s.detach()).item():.5f}. {torch.exp(-x_data_s.detach()).item():.5f}')\n",
    "    \n",
    "    # save the best model\n",
    "    if mse_loss.item() < min_train_loss:\n",
    "        min_train_loss = mse_loss.item()\n",
    "        final_model = IPINN.state_dict()\n",
    "    pass\n",
    "elapsed = timer() - start_time\n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "logging.info(f'Training finished. Elapsed time: {elapsed} s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "IPINN.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = IPINN(X_test_tensor)\n",
    "    test_loss = lossFunction(test_outputs, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104.4945068359375\n",
      "22.868684768676758\n",
      "1.2340525388717651\n",
      "0.003970734775066376\n",
      "0.0034018708392977715\n",
      "0.002930071670562029\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    if i % 500 == 0:\n",
    "        print(loss_hist[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
