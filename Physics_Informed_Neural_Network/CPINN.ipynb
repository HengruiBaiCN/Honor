{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical loss samples\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-7): 7 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "# Create the model\n",
    "PINNCGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "PINNCGD.to(device)\n",
    "print(PINNCGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_CGD = networks.Discriminator(2, 25, 3)\n",
    "D_CGD.to(device)\n",
    "D_CGD.load_state_dict(D_CGD.state_dict()) # copy weights and stuff\n",
    "print(D_CGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "tol = 1e-7\n",
    "atol = 1e-20\n",
    "g_iter = 1000\n",
    "lr = 0.01\n",
    "track_cond = lambda x, y: True\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# optimizer = CGDs.BCGD(max_params=D_CGD.parameters(), min_params=PINNCGD.parameters(), device = device,\n",
    "#                  lr_max=lr, lr_min=lr, tol=1e-10, collect_info=True)\n",
    "# optimizer = CGDs.ACGD(max_params=D_CGD.parameters(), min_params=PINNCGD.parameters(),\n",
    "#                  lr_max=lr, lr_min=lr, tol=1e-10, beta=0.99, eps=1e-8, collect_info=True)\n",
    "optimizer = CGDs.GACGD(x_params=D_CGD.parameters(), y_params = PINNCGD.parameters(), max_iter = g_iter,\n",
    "            lr_x=0.001, lr_y=0.001, tol=tol, atol = atol, eps=1e-8, beta=0.99, track_cond = track_cond)\n",
    "lossFunction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:124: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2197.)\n",
      "  y, _ = torch.triangular_solve(beta[0:j + 1].unsqueeze(-1), H[0:j + 1, 0:j + 1])  # j x j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000 mse loss: 1099.073486, minloss: 1099.073486\n",
      "              PDE Loss: 0.000000, BC Loss: 674.826599, Data Loss: 424.246826,  \n",
      "              nn loss: -0.37645411, \n",
      "              D1: -0.03633518, D2: 0.04767470, D3: -0.15273146,\n",
      "              nn loss1: -0.00000981, nn loss2: -3.05781603, nn loss3: 2.68137169\n",
      "              \n",
      "50/10000 mse loss: 31.948727, minloss: 12.729399\n",
      "              PDE Loss: 1.542150, BC Loss: 15.280569, Data Loss: 15.126006,  \n",
      "              nn loss: 7.32657719, \n",
      "              D1: 2.88267684, D2: 1.41807616, D3: -1.80625618,\n",
      "              nn loss1: 3.92576551, nn loss2: 1.43562663, nn loss3: 1.96518517\n",
      "              \n",
      "100/10000 mse loss: 5.094777, minloss: 2.053089\n",
      "              PDE Loss: 0.266004, BC Loss: 1.467901, Data Loss: 3.360872,  \n",
      "              nn loss: -0.07670848, \n",
      "              D1: -0.09067609, D2: 0.07486301, D3: 0.19777460,\n",
      "              nn loss1: 0.03134224, nn loss2: -0.07057397, nn loss3: -0.03747674\n",
      "              \n",
      "150/10000 mse loss: 11.015051, minloss: 1.335016\n",
      "              PDE Loss: 0.184708, BC Loss: 9.161817, Data Loss: 1.668526,  \n",
      "              nn loss: -0.17132993, \n",
      "              D1: 0.01516554, D2: -0.15360190, D3: 0.12840611,\n",
      "              nn loss1: 0.00438497, nn loss2: -0.17621283, nn loss3: 0.00049794\n",
      "              \n",
      "200/10000 mse loss: 1.772645, minloss: 1.335016\n",
      "              PDE Loss: 0.081691, BC Loss: 0.507492, Data Loss: 1.183462,  \n",
      "              nn loss: -0.16420470, \n",
      "              D1: -0.03995377, D2: 0.36171377, D3: -0.31258744,\n",
      "              nn loss1: 0.00399000, nn loss2: -0.05662033, nn loss3: -0.11157437\n",
      "              \n",
      "250/10000 mse loss: 2.277482, minloss: 0.735163\n",
      "              PDE Loss: 0.028586, BC Loss: 1.336312, Data Loss: 0.912584,  \n",
      "              nn loss: -0.02618843, \n",
      "              D1: -0.02717708, D2: -0.00947020, D3: 0.02717118,\n",
      "              nn loss1: -0.00028093, nn loss2: 0.00484545, nn loss3: -0.03075295\n",
      "              \n",
      "300/10000 mse loss: 0.172718, minloss: 0.172718\n",
      "              PDE Loss: 0.041356, BC Loss: 0.069385, Data Loss: 0.061977,  \n",
      "              nn loss: -0.01037147, \n",
      "              D1: 0.02184549, D2: -0.08228405, D3: 0.07474238,\n",
      "              nn loss1: -0.00117047, nn loss2: -0.00185493, nn loss3: -0.00734606\n",
      "              \n",
      "350/10000 mse loss: 0.412160, minloss: 0.127246\n",
      "              PDE Loss: 0.144390, BC Loss: 0.107170, Data Loss: 0.160600,  \n",
      "              nn loss: -0.00588047, \n",
      "              D1: 0.06341225, D2: -0.05070589, D3: 0.05045328,\n",
      "              nn loss1: -0.00952929, nn loss2: 0.00231360, nn loss3: 0.00133522\n",
      "              \n",
      "400/10000 mse loss: 0.104064, minloss: 0.103912\n",
      "              PDE Loss: 0.025374, BC Loss: 0.027156, Data Loss: 0.051534,  \n",
      "              nn loss: 0.00073118, \n",
      "              D1: 0.02024884, D2: -0.04738766, D3: 0.04732958,\n",
      "              nn loss1: 0.00040936, nn loss2: 0.00063383, nn loss3: -0.00031200\n",
      "              \n",
      "450/10000 mse loss: 0.111426, minloss: 0.048175\n",
      "              PDE Loss: 0.007093, BC Loss: 0.044640, Data Loss: 0.059693,  \n",
      "              nn loss: 0.00217514, \n",
      "              D1: 0.00283071, D2: -0.03371219, D3: 0.03634053,\n",
      "              nn loss1: -0.00031133, nn loss2: -0.00049637, nn loss3: 0.00298284\n",
      "              \n",
      "500/10000 mse loss: 0.305108, minloss: 0.048175\n",
      "              PDE Loss: 0.006631, BC Loss: 0.078803, Data Loss: 0.219674,  \n",
      "              nn loss: 0.00106032, \n",
      "              D1: 0.01528170, D2: -0.03473052, D3: 0.03592725,\n",
      "              nn loss1: 0.00071142, nn loss2: 0.00155106, nn loss3: -0.00120216\n",
      "              \n",
      "550/10000 mse loss: 0.123223, minloss: 0.048175\n",
      "              PDE Loss: 0.006929, BC Loss: 0.030980, Data Loss: 0.085315,  \n",
      "              nn loss: -0.00303728, \n",
      "              D1: 0.01617724, D2: -0.03105584, D3: 0.03167966,\n",
      "              nn loss1: 0.00102268, nn loss2: -0.00053293, nn loss3: -0.00352703\n",
      "              \n",
      "600/10000 mse loss: 0.089305, minloss: 0.047439\n",
      "              PDE Loss: 0.003614, BC Loss: 0.029507, Data Loss: 0.056184,  \n",
      "              nn loss: -0.00052496, \n",
      "              D1: 0.02144745, D2: -0.03112327, D3: 0.03134097,\n",
      "              nn loss1: 0.00037920, nn loss2: -0.00089835, nn loss3: -0.00000582\n",
      "              \n",
      "650/10000 mse loss: 0.067494, minloss: 0.047439\n",
      "              PDE Loss: 0.006049, BC Loss: 0.022652, Data Loss: 0.038793,  \n",
      "              nn loss: -0.00101136, \n",
      "              D1: 0.01827125, D2: -0.03137395, D3: 0.03214680,\n",
      "              nn loss1: -0.00001438, nn loss2: -0.00087931, nn loss3: -0.00011766\n",
      "              \n",
      "700/10000 mse loss: 0.018662, minloss: 0.018662\n",
      "              PDE Loss: 0.005146, BC Loss: 0.006063, Data Loss: 0.007452,  \n",
      "              nn loss: -0.00130321, \n",
      "              D1: 0.01953481, D2: -0.03647412, D3: 0.03712235,\n",
      "              nn loss1: -0.00012818, nn loss2: -0.00094372, nn loss3: -0.00023131\n",
      "              \n",
      "750/10000 mse loss: 0.019598, minloss: 0.015650\n",
      "              PDE Loss: 0.004953, BC Loss: 0.007258, Data Loss: 0.007387,  \n",
      "              nn loss: -0.00163136, \n",
      "              D1: 0.02375090, D2: -0.04012172, D3: 0.04050672,\n",
      "              nn loss1: -0.00043354, nn loss2: -0.00117878, nn loss3: -0.00001904\n",
      "              \n",
      "800/10000 mse loss: 0.011143, minloss: 0.010685\n",
      "              PDE Loss: 0.001633, BC Loss: 0.004794, Data Loss: 0.004715,  \n",
      "              nn loss: -0.00192985, \n",
      "              D1: 0.02919148, D2: -0.05378101, D3: 0.05501850,\n",
      "              nn loss1: -0.00031810, nn loss2: -0.00133330, nn loss3: -0.00027845\n",
      "              \n",
      "850/10000 mse loss: 0.015442, minloss: 0.009348\n",
      "              PDE Loss: 0.003262, BC Loss: 0.006034, Data Loss: 0.006147,  \n",
      "              nn loss: -0.00109923, \n",
      "              D1: 0.02687967, D2: -0.05895983, D3: 0.06101910,\n",
      "              nn loss1: -0.00004821, nn loss2: -0.00041124, nn loss3: -0.00063978\n",
      "              \n",
      "900/10000 mse loss: 0.010531, minloss: 0.004912\n",
      "              PDE Loss: 0.000752, BC Loss: 0.004841, Data Loss: 0.004938,  \n",
      "              nn loss: -0.00216194, \n",
      "              D1: 0.03839709, D2: -0.06836347, D3: 0.06960932,\n",
      "              nn loss1: 0.00041012, nn loss2: -0.00290184, nn loss3: 0.00032979\n",
      "              \n",
      "950/10000 mse loss: 0.011519, minloss: 0.004912\n",
      "              PDE Loss: 0.000318, BC Loss: 0.005472, Data Loss: 0.005729,  \n",
      "              nn loss: -0.00265819, \n",
      "              D1: 0.04451461, D2: -0.07865803, D3: 0.08012497,\n",
      "              nn loss1: 0.00004779, nn loss2: -0.00259573, nn loss3: -0.00011025\n",
      "              \n",
      "1000/10000 mse loss: 0.007118, minloss: 0.004736\n",
      "              PDE Loss: 0.000361, BC Loss: 0.003008, Data Loss: 0.003749,  \n",
      "              nn loss: -0.00321316, \n",
      "              D1: 0.05605249, D2: -0.10119737, D3: 0.10341023,\n",
      "              nn loss1: 0.00006483, nn loss2: -0.00245591, nn loss3: -0.00082208\n",
      "              \n",
      "1050/10000 mse loss: 0.008611, minloss: 0.004736\n",
      "              PDE Loss: 0.000284, BC Loss: 0.003546, Data Loss: 0.004781,  \n",
      "              nn loss: -0.00432004, \n",
      "              D1: 0.07498004, D2: -0.13885243, D3: 0.14182034,\n",
      "              nn loss1: -0.00004597, nn loss2: -0.00300904, nn loss3: -0.00126504\n",
      "              \n",
      "1100/10000 mse loss: 0.008809, minloss: 0.004736\n",
      "              PDE Loss: 0.000976, BC Loss: 0.003158, Data Loss: 0.004675,  \n",
      "              nn loss: -0.00575363, \n",
      "              D1: 0.10713068, D2: -0.20082389, D3: 0.20427991,\n",
      "              nn loss1: -0.00032746, nn loss2: -0.00485739, nn loss3: -0.00056877\n",
      "              \n",
      "1150/10000 mse loss: 0.009359, minloss: 0.004699\n",
      "              PDE Loss: 0.000835, BC Loss: 0.004101, Data Loss: 0.004422,  \n",
      "              nn loss: -0.00707539, \n",
      "              D1: 0.13554204, D2: -0.26094952, D3: 0.26536277,\n",
      "              nn loss1: 0.00052087, nn loss2: -0.01099308, nn loss3: 0.00339682\n",
      "              \n",
      "1200/10000 mse loss: 0.014188, minloss: 0.004699\n",
      "              PDE Loss: 0.000432, BC Loss: 0.003930, Data Loss: 0.009825,  \n",
      "              nn loss: -0.00921969, \n",
      "              D1: 0.15986094, D2: -0.30497411, D3: 0.31070504,\n",
      "              nn loss1: 0.00006020, nn loss2: 0.00036728, nn loss3: -0.00964717\n",
      "              \n",
      "1250/10000 mse loss: 0.003805, minloss: 0.003641\n",
      "              PDE Loss: 0.000194, BC Loss: 0.001521, Data Loss: 0.002090,  \n",
      "              nn loss: -0.01105867, \n",
      "              D1: 0.20394237, D2: -0.40204671, D3: 0.40887547,\n",
      "              nn loss1: -0.00007122, nn loss2: -0.01064699, nn loss3: -0.00034046\n",
      "              \n",
      "1300/10000 mse loss: 0.003941, minloss: 0.003641\n",
      "              PDE Loss: 0.000134, BC Loss: 0.001566, Data Loss: 0.002241,  \n",
      "              nn loss: -0.01476413, \n",
      "              D1: 0.27931330, D2: -0.56198215, D3: 0.57162875,\n",
      "              nn loss1: 0.00016951, nn loss2: -0.01398144, nn loss3: -0.00095220\n",
      "              \n",
      "1350/10000 mse loss: 0.003791, minloss: 0.003455\n",
      "              PDE Loss: 0.000158, BC Loss: 0.001235, Data Loss: 0.002399,  \n",
      "              nn loss: -0.02358409, \n",
      "              D1: 0.45785668, D2: -0.93763900, D3: 0.95357966,\n",
      "              nn loss1: -0.00009967, nn loss2: -0.01963272, nn loss3: -0.00385171\n",
      "              \n",
      "1400/10000 mse loss: 0.113479, minloss: 0.003455\n",
      "              PDE Loss: 0.000117, BC Loss: 0.056475, Data Loss: 0.056887,  \n",
      "              nn loss: -0.04233262, \n",
      "              D1: 0.72343242, D2: -1.48036277, D3: 1.49727023,\n",
      "              nn loss1: 0.00572668, nn loss2: -0.14481124, nn loss3: 0.09675194\n",
      "              \n",
      "1450/10000 mse loss: 0.006289, minloss: 0.003455\n",
      "              PDE Loss: 0.000090, BC Loss: 0.002448, Data Loss: 0.003751,  \n",
      "              nn loss: -0.04643115, \n",
      "              D1: 0.81190395, D2: -1.73176825, D3: 1.76341319,\n",
      "              nn loss1: -0.00048856, nn loss2: -0.04167880, nn loss3: -0.00426379\n",
      "              \n",
      "1500/10000 mse loss: 0.008808, minloss: 0.003348\n",
      "              PDE Loss: 0.000089, BC Loss: 0.002168, Data Loss: 0.006551,  \n",
      "              nn loss: -0.05126910, \n",
      "              D1: 0.93138027, D2: -1.96966040, D3: 2.00309372,\n",
      "              nn loss1: 0.00109459, nn loss2: -0.02436268, nn loss3: -0.02800101\n",
      "              \n",
      "1550/10000 mse loss: 0.006739, minloss: 0.003348\n",
      "              PDE Loss: 0.000062, BC Loss: 0.003032, Data Loss: 0.003645,  \n",
      "              nn loss: -0.05897391, \n",
      "              D1: 1.08899844, D2: -2.31659865, D3: 2.35512471,\n",
      "              nn loss1: -0.00022523, nn loss2: -0.08512009, nn loss3: 0.02637140\n",
      "              \n",
      "1600/10000 mse loss: 0.357976, minloss: 0.003348\n",
      "              PDE Loss: 0.000235, BC Loss: 0.186333, Data Loss: 0.171408,  \n",
      "              nn loss: -0.03597462, \n",
      "              D1: 1.25438416, D2: -2.42076278, D3: 2.40669179,\n",
      "              nn loss1: -0.00920411, nn loss2: 0.32438713, nn loss3: -0.35115764\n",
      "              \n",
      "1650/10000 mse loss: 0.063997, minloss: 0.003348\n",
      "              PDE Loss: 0.000185, BC Loss: 0.019353, Data Loss: 0.044458,  \n",
      "              nn loss: -0.05358848, \n",
      "              D1: 1.23930907, D2: -2.52768636, D3: 2.56727552,\n",
      "              nn loss1: -0.00623157, nn loss2: 0.00060052, nn loss3: -0.04795743\n",
      "              \n",
      "1700/10000 mse loss: 0.008287, minloss: 0.003348\n",
      "              PDE Loss: 0.000178, BC Loss: 0.003028, Data Loss: 0.005081,  \n",
      "              nn loss: -0.09036200, \n",
      "              D1: 1.68185663, D2: -3.61641264, D3: 3.66899920,\n",
      "              nn loss1: -0.00335519, nn loss2: -0.07869537, nn loss3: -0.00831143\n",
      "              \n",
      "1750/10000 mse loss: 0.005215, minloss: 0.003348\n",
      "              PDE Loss: 0.000152, BC Loss: 0.001796, Data Loss: 0.003268,  \n",
      "              nn loss: -0.12083189, \n",
      "              D1: 2.32551527, D2: -4.95386171, D3: 5.03284168,\n",
      "              nn loss1: -0.00098797, nn loss2: -0.10477254, nn loss3: -0.01507139\n",
      "              \n",
      "1800/10000 mse loss: 0.006975, minloss: 0.003348\n",
      "              PDE Loss: 0.000270, BC Loss: 0.002340, Data Loss: 0.004365,  \n",
      "              nn loss: -0.19081670, \n",
      "              D1: 3.63741636, D2: -7.70518398, D3: 7.83526373,\n",
      "              nn loss1: -0.00305133, nn loss2: -0.15565820, nn loss3: -0.03210717\n",
      "              \n",
      "1850/10000 mse loss: 0.007707, minloss: 0.003348\n",
      "              PDE Loss: 0.000285, BC Loss: 0.002775, Data Loss: 0.004647,  \n",
      "              nn loss: -0.33827472, \n",
      "              D1: 6.56026077, D2: -13.96440887, D3: 14.19885349,\n",
      "              nn loss1: 0.00105194, nn loss2: -0.31035852, nn loss3: -0.02896811\n",
      "              \n",
      "1900/10000 mse loss: 0.017655, minloss: 0.003348\n",
      "              PDE Loss: 0.000220, BC Loss: 0.006839, Data Loss: 0.010595,  \n",
      "              nn loss: -0.64465427, \n",
      "              D1: 13.12725258, D2: -27.96188545, D3: 28.38210869,\n",
      "              nn loss1: -0.02180374, nn loss2: -1.08416057, nn loss3: 0.46131000\n",
      "              \n",
      "1950/10000 mse loss: 0.020425, minloss: 0.003348\n",
      "              PDE Loss: 0.000191, BC Loss: 0.007238, Data Loss: 0.012996,  \n",
      "              nn loss: -1.22581923, \n",
      "              D1: 23.88163757, D2: -50.86355209, D3: 51.67240524,\n",
      "              nn loss1: -0.00569696, nn loss2: -0.88308901, nn loss3: -0.33703327\n",
      "              \n",
      "2000/10000 mse loss: 0.084044, minloss: 0.003348\n",
      "              PDE Loss: 0.000312, BC Loss: 0.024481, Data Loss: 0.059252,  \n",
      "              nn loss: -1.41802347, \n",
      "              D1: 25.24985886, D2: -54.74948883, D3: 55.69865036,\n",
      "              nn loss1: 0.01997281, nn loss2: -0.72826660, nn loss3: -0.70972967\n",
      "              \n",
      "2050/10000 mse loss: 0.118219, minloss: 0.003348\n",
      "              PDE Loss: 0.006863, BC Loss: 0.038931, Data Loss: 0.072425,  \n",
      "              nn loss: -1.00299978, \n",
      "              D1: 21.21914864, D2: -47.86988449, D3: 48.85269928,\n",
      "              nn loss1: 0.59505582, nn loss2: -0.47171849, nn loss3: -1.12633705\n",
      "              \n",
      "2100/10000 mse loss: 0.013211, minloss: 0.003348\n",
      "              PDE Loss: 0.003891, BC Loss: 0.002754, Data Loss: 0.006566,  \n",
      "              nn loss: -1.12239492, \n",
      "              D1: 20.54094887, D2: -47.54800034, D3: 48.40942001,\n",
      "              nn loss1: -0.04573756, nn loss2: 0.92776698, nn loss3: -2.00442433\n",
      "              \n",
      "2150/10000 mse loss: 3.268194, minloss: 0.003348\n",
      "              PDE Loss: 0.029089, BC Loss: 1.500348, Data Loss: 1.738757,  \n",
      "              nn loss: -1.60404205, \n",
      "              D1: 18.42547226, D2: -45.32265472, D3: 46.09972763,\n",
      "              nn loss1: 2.71655703, nn loss2: 41.77813339, nn loss3: -46.09873199\n",
      "              \n",
      "2200/10000 mse loss: 21.290485, minloss: 0.003348\n",
      "              PDE Loss: 0.060057, BC Loss: 10.304005, Data Loss: 10.926422,  \n",
      "              nn loss: 0.08000946, \n",
      "              D1: 21.07532120, D2: -40.53331375, D3: 41.89632416,\n",
      "              nn loss1: -4.63891459, nn loss2: 128.77786255, nn loss3: -124.05893707\n",
      "              \n",
      "2250/10000 mse loss: 0.118906, minloss: 0.003348\n",
      "              PDE Loss: 0.005069, BC Loss: 0.037872, Data Loss: 0.075965,  \n",
      "              nn loss: -1.16392946, \n",
      "              D1: 19.86270905, D2: -44.93682098, D3: 46.14440536,\n",
      "              nn loss1: 0.65201288, nn loss2: 6.67898703, nn loss3: -8.49492931\n",
      "              \n",
      "2300/10000 mse loss: 0.036294, minloss: 0.003348\n",
      "              PDE Loss: 0.000449, BC Loss: 0.013840, Data Loss: 0.022004,  \n",
      "              nn loss: -1.18261337, \n",
      "              D1: 19.87106895, D2: -45.04526520, D3: 45.90547943,\n",
      "              nn loss1: -0.00147962, nn loss2: -1.76554608, nn loss3: 0.58441240\n",
      "              \n",
      "2350/10000 mse loss: 0.014299, minloss: 0.003348\n",
      "              PDE Loss: 0.001581, BC Loss: 0.005580, Data Loss: 0.007138,  \n",
      "              nn loss: -1.13615894, \n",
      "              D1: 19.81369972, D2: -45.21675873, D3: 45.96678543,\n",
      "              nn loss1: 0.13773164, nn loss2: -0.88435805, nn loss3: -0.38953257\n",
      "              \n",
      "2400/10000 mse loss: 0.016838, minloss: 0.003348\n",
      "              PDE Loss: 0.001079, BC Loss: 0.005683, Data Loss: 0.010075,  \n",
      "              nn loss: -1.10143065, \n",
      "              D1: 20.02653694, D2: -46.31602097, D3: 47.10126877,\n",
      "              nn loss1: -0.10789949, nn loss2: 0.57073659, nn loss3: -1.56426775\n",
      "              \n",
      "2450/10000 mse loss: 2.212886, minloss: 0.003348\n",
      "              PDE Loss: 0.003531, BC Loss: 1.349776, Data Loss: 0.859579,  \n",
      "              nn loss: -1.27312088, \n",
      "              D1: 21.84007835, D2: -46.38698578, D3: 47.11499023,\n",
      "              nn loss1: -0.43653193, nn loss2: 30.81658363, nn loss3: -31.65317345\n",
      "              \n",
      "2500/10000 mse loss: 0.048440, minloss: 0.003348\n",
      "              PDE Loss: 0.001955, BC Loss: 0.016906, Data Loss: 0.029579,  \n",
      "              nn loss: -0.88701987, \n",
      "              D1: 19.75363731, D2: -44.63071823, D3: 45.22663879,\n",
      "              nn loss1: 0.43685332, nn loss2: -3.48371959, nn loss3: 2.15984631\n",
      "              \n",
      "2550/10000 mse loss: 0.055740, minloss: 0.003348\n",
      "              PDE Loss: 0.000901, BC Loss: 0.019285, Data Loss: 0.035554,  \n",
      "              nn loss: -0.85259640, \n",
      "              D1: 19.42295647, D2: -46.41189575, D3: 47.01836777,\n",
      "              nn loss1: 0.11759824, nn loss2: -2.93208337, nn loss3: 1.96188867\n",
      "              \n",
      "2600/10000 mse loss: 0.059810, minloss: 0.003348\n",
      "              PDE Loss: 0.000364, BC Loss: 0.020194, Data Loss: 0.039252,  \n",
      "              nn loss: -0.66894054, \n",
      "              D1: 18.67306900, D2: -45.87979507, D3: 46.73492432,\n",
      "              nn loss1: -0.13820122, nn loss2: 2.10315776, nn loss3: -2.63389707\n",
      "              \n",
      "2650/10000 mse loss: 0.065611, minloss: 0.003348\n",
      "              PDE Loss: 0.003168, BC Loss: 0.021652, Data Loss: 0.040790,  \n",
      "              nn loss: -0.92851162, \n",
      "              D1: 19.31022644, D2: -44.36766434, D3: 44.79898071,\n",
      "              nn loss1: 0.18511100, nn loss2: -6.26698303, nn loss3: 5.15336037\n",
      "              \n",
      "2700/10000 mse loss: 0.081119, minloss: 0.003348\n",
      "              PDE Loss: 0.001101, BC Loss: 0.024009, Data Loss: 0.056008,  \n",
      "              nn loss: -0.36160940, \n",
      "              D1: 17.10899925, D2: -38.88594055, D3: 39.56691360,\n",
      "              nn loss1: 0.18552493, nn loss2: -0.45153701, nn loss3: -0.09559731\n",
      "              \n",
      "2750/10000 mse loss: 0.108615, minloss: 0.003348\n",
      "              PDE Loss: 0.001566, BC Loss: 0.050271, Data Loss: 0.056778,  \n",
      "              nn loss: -0.76666951, \n",
      "              D1: 12.69962025, D2: -32.15596390, D3: 32.23281479,\n",
      "              nn loss1: 0.23055851, nn loss2: -3.25631571, nn loss3: 2.25908780\n",
      "              \n",
      "2800/10000 mse loss: 0.084018, minloss: 0.003348\n",
      "              PDE Loss: 0.000835, BC Loss: 0.024709, Data Loss: 0.058473,  \n",
      "              nn loss: -0.42684221, \n",
      "              D1: 10.99276352, D2: -24.66330147, D3: 24.73845482,\n",
      "              nn loss1: 0.04428517, nn loss2: -1.47018695, nn loss3: 0.99905956\n",
      "              \n",
      "2850/10000 mse loss: 0.124987, minloss: 0.003348\n",
      "              PDE Loss: 0.001417, BC Loss: 0.036568, Data Loss: 0.087003,  \n",
      "              nn loss: -0.26269835, \n",
      "              D1: 8.42253208, D2: -19.02160835, D3: 18.98343658,\n",
      "              nn loss1: 0.07853813, nn loss2: -0.90934849, nn loss3: 0.56811202\n",
      "              \n",
      "2900/10000 mse loss: 0.155889, minloss: 0.003348\n",
      "              PDE Loss: 0.001225, BC Loss: 0.043525, Data Loss: 0.111140,  \n",
      "              nn loss: -0.22695446, \n",
      "              D1: 6.94694376, D2: -16.54298401, D3: 16.62491608,\n",
      "              nn loss1: 0.02318656, nn loss2: -1.04371536, nn loss3: 0.79357433\n",
      "              \n",
      "2950/10000 mse loss: 0.239932, minloss: 0.003348\n",
      "              PDE Loss: 0.001763, BC Loss: 0.064514, Data Loss: 0.173655,  \n",
      "              nn loss: -0.21953100, \n",
      "              D1: 7.16857576, D2: -15.02033806, D3: 15.52064419,\n",
      "              nn loss1: -0.05599996, nn loss2: 0.57140779, nn loss3: -0.73493886\n",
      "              \n",
      "3000/10000 mse loss: 0.251805, minloss: 0.003348\n",
      "              PDE Loss: 0.001631, BC Loss: 0.064175, Data Loss: 0.186000,  \n",
      "              nn loss: 0.01007581, \n",
      "              D1: 5.60896683, D2: -12.45614719, D3: 12.68461704,\n",
      "              nn loss1: 0.00040467, nn loss2: -0.11825019, nn loss3: 0.12792133\n",
      "              \n",
      "3050/10000 mse loss: 0.403227, minloss: 0.003348\n",
      "              PDE Loss: 0.002044, BC Loss: 0.131402, Data Loss: 0.269781,  \n",
      "              nn loss: -0.14884031, \n",
      "              D1: 5.10674667, D2: -10.03739262, D3: 10.40706158,\n",
      "              nn loss1: 0.00083211, nn loss2: 0.80814469, nn loss3: -0.95781714\n",
      "              \n",
      "3100/10000 mse loss: 0.514864, minloss: 0.003348\n",
      "              PDE Loss: 0.001958, BC Loss: 0.135039, Data Loss: 0.377867,  \n",
      "              nn loss: -0.06541336, \n",
      "              D1: 4.35429144, D2: -8.36414242, D3: 8.61023998,\n",
      "              nn loss1: -0.02370611, nn loss2: 0.99853367, nn loss3: -1.04024088\n",
      "              \n",
      "3150/10000 mse loss: 0.653703, minloss: 0.003348\n",
      "              PDE Loss: 0.002201, BC Loss: 0.165447, Data Loss: 0.486055,  \n",
      "              nn loss: 0.02580297, \n",
      "              D1: 3.33943892, D2: -6.65579748, D3: 6.82337618,\n",
      "              nn loss1: -0.02444353, nn loss2: 0.40810800, nn loss3: -0.35786149\n",
      "              \n",
      "3200/10000 mse loss: 0.803092, minloss: 0.003348\n",
      "              PDE Loss: 0.002436, BC Loss: 0.202807, Data Loss: 0.597850,  \n",
      "              nn loss: 0.02318236, \n",
      "              D1: 2.79687452, D2: -5.45751476, D3: 5.57198048,\n",
      "              nn loss1: -0.01025710, nn loss2: 0.44250801, nn loss3: -0.40906855\n",
      "              \n",
      "3250/10000 mse loss: 0.803971, minloss: 0.003348\n",
      "              PDE Loss: 0.002567, BC Loss: 0.275734, Data Loss: 0.525670,  \n",
      "              nn loss: -0.09058924, \n",
      "              D1: 1.62705314, D2: -4.03102350, D3: 4.07023287,\n",
      "              nn loss1: -0.00346369, nn loss2: -0.20039925, nn loss3: 0.11327371\n",
      "              \n",
      "3300/10000 mse loss: 0.741692, minloss: 0.003348\n",
      "              PDE Loss: 0.002226, BC Loss: 0.187644, Data Loss: 0.551821,  \n",
      "              nn loss: -0.21206155, \n",
      "              D1: 1.84291792, D2: -3.50810742, D3: 3.57409811,\n",
      "              nn loss1: -0.00476231, nn loss2: -0.08860506, nn loss3: -0.11869418\n",
      "              \n",
      "3350/10000 mse loss: 0.588916, minloss: 0.003348\n",
      "              PDE Loss: 0.001936, BC Loss: 0.149829, Data Loss: 0.437151,  \n",
      "              nn loss: -0.23550248, \n",
      "              D1: 1.54648364, D2: -3.15394592, D3: 3.24260092,\n",
      "              nn loss1: -0.01021631, nn loss2: -0.15834095, nn loss3: -0.06694523\n",
      "              \n",
      "3400/10000 mse loss: 0.373472, minloss: 0.003348\n",
      "              PDE Loss: 0.001946, BC Loss: 0.100284, Data Loss: 0.271242,  \n",
      "              nn loss: -0.24251625, \n",
      "              D1: 1.43004167, D2: -2.97369790, D3: 3.01299644,\n",
      "              nn loss1: -0.01578846, nn loss2: -0.01714569, nn loss3: -0.20958209\n",
      "              \n",
      "3450/10000 mse loss: 0.195903, minloss: 0.003348\n",
      "              PDE Loss: 0.003249, BC Loss: 0.051459, Data Loss: 0.141195,  \n",
      "              nn loss: -0.17983505, \n",
      "              D1: 1.22121692, D2: -2.91717029, D3: 2.86479139,\n",
      "              nn loss1: 0.02734578, nn loss2: -0.12626716, nn loss3: -0.08091368\n",
      "              \n",
      "3500/10000 mse loss: 0.059912, minloss: 0.003348\n",
      "              PDE Loss: 0.004383, BC Loss: 0.015383, Data Loss: 0.040146,  \n",
      "              nn loss: -0.19645882, \n",
      "              D1: 1.59672511, D2: -3.30392909, D3: 3.39318609,\n",
      "              nn loss1: -0.00745870, nn loss2: -0.15796044, nn loss3: -0.03103967\n",
      "              \n",
      "3550/10000 mse loss: 0.067330, minloss: 0.003348\n",
      "              PDE Loss: 0.006691, BC Loss: 0.019057, Data Loss: 0.041581,  \n",
      "              nn loss: -0.18294775, \n",
      "              D1: 1.21315777, D2: -2.97330523, D3: 3.02185035,\n",
      "              nn loss1: -0.03225763, nn loss2: -0.02243070, nn loss3: -0.12825944\n",
      "              \n",
      "3600/10000 mse loss: 0.045402, minloss: 0.003348\n",
      "              PDE Loss: 0.007409, BC Loss: 0.011265, Data Loss: 0.026728,  \n",
      "              nn loss: -0.20834675, \n",
      "              D1: 1.52125585, D2: -3.31683779, D3: 3.38544178,\n",
      "              nn loss1: -0.05392485, nn loss2: -0.11824802, nn loss3: -0.03617387\n",
      "              \n",
      "3650/10000 mse loss: 0.032450, minloss: 0.003348\n",
      "              PDE Loss: 0.004651, BC Loss: 0.007400, Data Loss: 0.020399,  \n",
      "              nn loss: -0.37552828, \n",
      "              D1: 1.69941521, D2: -3.74183655, D3: 3.70852613,\n",
      "              nn loss1: -0.02859355, nn loss2: -0.28901473, nn loss3: -0.05792001\n",
      "              \n",
      "3700/10000 mse loss: 0.041584, minloss: 0.003348\n",
      "              PDE Loss: 0.005348, BC Loss: 0.012544, Data Loss: 0.023693,  \n",
      "              nn loss: -0.33476412, \n",
      "              D1: 1.98909914, D2: -4.19443083, D3: 4.28644371,\n",
      "              nn loss1: 0.02802021, nn loss2: -0.26816255, nn loss3: -0.09462178\n",
      "              \n",
      "3750/10000 mse loss: 0.040508, minloss: 0.003348\n",
      "              PDE Loss: 0.007827, BC Loss: 0.010298, Data Loss: 0.022383,  \n",
      "              nn loss: -0.57252771, \n",
      "              D1: 2.48104835, D2: -5.08714628, D3: 5.26941633,\n",
      "              nn loss1: 0.05910375, nn loss2: -0.61243725, nn loss3: -0.01919421\n",
      "              \n",
      "3800/10000 mse loss: 0.062781, minloss: 0.003348\n",
      "              PDE Loss: 0.003527, BC Loss: 0.025579, Data Loss: 0.033675,  \n",
      "              nn loss: -0.54285359, \n",
      "              D1: 2.86181283, D2: -5.30162477, D3: 5.30831385,\n",
      "              nn loss1: -0.07065430, nn loss2: 0.15387809, nn loss3: -0.62607741\n",
      "              \n",
      "3850/10000 mse loss: 0.075097, minloss: 0.003348\n",
      "              PDE Loss: 0.005029, BC Loss: 0.022051, Data Loss: 0.048016,  \n",
      "              nn loss: -0.93873543, \n",
      "              D1: 2.76188421, D2: -5.86906958, D3: 5.81875849,\n",
      "              nn loss1: 0.05606092, nn loss2: -0.97004420, nn loss3: -0.02475215\n",
      "              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\Physics_Informed_Neural_Network\\pytorch\\CPINN.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/CPINN.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# zeroes the gradient buffers of all parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/CPINN.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m combined_loss \u001b[39m=\u001b[39m (loss1\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m loss2\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m loss3\u001b[39m.\u001b[39mmean())  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/CPINN.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(combined_loss, \u001b[39m-\u001b[39;49mcombined_loss, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/CPINN.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# print loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/CPINN.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m mse_loss \u001b[39m=\u001b[39m pde_loss \u001b[39m+\u001b[39m bc_loss \u001b[39m+\u001b[39m data_loss\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_acgd.py:165\u001b[0m, in \u001b[0;36mGACGD.step\u001b[1;34m(self, loss_x, loss_y, trigger)\u001b[0m\n\u001b[0;32m    157\u001b[0m prev_x0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m Avp \u001b[39m=\u001b[39m partial(MvProd,\n\u001b[0;32m    159\u001b[0m               grad_fy\u001b[39m=\u001b[39mgrad_fy_vec, grad_gx\u001b[39m=\u001b[39mgrad_gx_vec,\n\u001b[0;32m    160\u001b[0m               x_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_params, y_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m               x_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_reducer, y_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_reducer,\n\u001b[0;32m    164\u001b[0m               rebuild\u001b[39m=\u001b[39mshould_rebuild)\n\u001b[1;32m--> 165\u001b[0m soln, (num_iter, err_history) \u001b[39m=\u001b[39m GMRES(Avp\u001b[39m=\u001b[39;49mAvp, b\u001b[39m=\u001b[39;49mRHS, x0\u001b[39m=\u001b[39;49mprev_x0,\n\u001b[0;32m    166\u001b[0m                                       max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    167\u001b[0m                                       tol\u001b[39m=\u001b[39;49mtol, atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m    168\u001b[0m                                       track\u001b[39m=\u001b[39;49mtrack_flag)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mx0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m soln\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    172\u001b[0m     {\n\u001b[0;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_x\u001b[39m\u001b[39m'\u001b[39m: sq_avg_x,\n\u001b[0;32m    174\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_y\u001b[39m\u001b[39m'\u001b[39m: sq_avg_y\n\u001b[0;32m    175\u001b[0m     }\n\u001b[0;32m    176\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:107\u001b[0m, in \u001b[0;36mGMRES\u001b[1;34m(Avp, b, x0, max_iter, tol, atol, track)\u001b[0m\n\u001b[0;32m    104\u001b[0m ss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_iter, device\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mdevice)  \u001b[39m# sine values at each step\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[1;32m--> 107\u001b[0m     p \u001b[39m=\u001b[39m Avp(V[j])\n\u001b[0;32m    108\u001b[0m     new_v \u001b[39m=\u001b[39m arnoldi(p, V, H, j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# Arnoldi iteration to get the j+1 th ba\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39m# sis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_acgd.py:43\u001b[0m, in \u001b[0;36mMvProd\u001b[1;34m(vec, grad_fy, grad_gx, x_params, y_params, lr_x, lr_y, trigger, x_reducer, y_reducer, rebuild)\u001b[0m\n\u001b[0;32m     41\u001b[0m v1 \u001b[39m=\u001b[39m vec[\u001b[39m0\u001b[39m: len_x]\n\u001b[0;32m     42\u001b[0m v2 \u001b[39m=\u001b[39m vec[len_x: len_x \u001b[39m+\u001b[39m len_y]\n\u001b[1;32m---> 43\u001b[0m h1 \u001b[39m=\u001b[39m Hvp_vec(grad_fy, x_params, v2,\n\u001b[0;32m     44\u001b[0m              retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trigger\u001b[39m=\u001b[39;49mtrigger,\n\u001b[0;32m     45\u001b[0m              reducer\u001b[39m=\u001b[39;49mx_reducer,\n\u001b[0;32m     46\u001b[0m              rebuild\u001b[39m=\u001b[39;49mrebuild)\n\u001b[0;32m     47\u001b[0m p1 \u001b[39m=\u001b[39m v1 \u001b[39m+\u001b[39m lr_x \u001b[39m*\u001b[39m h1\n\u001b[0;32m     48\u001b[0m h2 \u001b[39m=\u001b[39m Hvp_vec(grad_gx, y_params, v1,\n\u001b[0;32m     49\u001b[0m              retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, trigger\u001b[39m=\u001b[39mtrigger,\n\u001b[0;32m     50\u001b[0m              reducer\u001b[39m=\u001b[39my_reducer,\n\u001b[0;32m     51\u001b[0m              rebuild\u001b[39m=\u001b[39mrebuild)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\cgd_utils.py:109\u001b[0m, in \u001b[0;36mHvp_vec\u001b[1;34m(grad_vec, params, vec, retain_graph, trigger, reducer, rebuild)\u001b[0m\n\u001b[0;32m    107\u001b[0m         reducer\u001b[39m.\u001b[39m_rebuild_buckets()\n\u001b[0;32m    108\u001b[0m     reducer\u001b[39m.\u001b[39mprepare_for_backward([])\n\u001b[1;32m--> 109\u001b[0m autograd\u001b[39m.\u001b[39;49mbackward(grad_vec \u001b[39m+\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39m*\u001b[39;49m trigger, grad_tensors\u001b[39m=\u001b[39;49mvec,\n\u001b[0;32m    110\u001b[0m                   inputs\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    111\u001b[0m                   retain_graph\u001b[39m=\u001b[39;49mretain_graph)\n\u001b[0;32m    112\u001b[0m hvp \u001b[39m=\u001b[39m vectorize_grad(params)\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(hvp)\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "# dis_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, r, sigma, T, S_range[-1], S_range, t_range, gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    # normal loss\n",
    "    v1_hat = PINNCGD(n_st_train)\n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(),\n",
    "                          create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    \n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    d1 = D_CGD(n_st_train)[:, [0]]\n",
    "    loss1 = ((d1) * (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat))\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNCGD(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    d2 = D_CGD(bc_st_train)[:, [1]]\n",
    "    loss2 = ((d2) * (bc_hat - bc_v_train))\n",
    "    \n",
    "    \n",
    "    # data Round\n",
    "    y3_hat = PINNCGD(X_train_tensor)\n",
    "    data_loss = lossFunction(y_train_tensor, y3_hat)\n",
    "    \n",
    "    d3 = D_CGD(X_train_tensor)[:, [2]]\n",
    "    loss3 = ((d3) * (y3_hat - y_train_tensor))\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    combined_loss = (loss1.mean() + loss2.mean() + loss3.mean())  \n",
    "    optimizer.step(combined_loss, -combined_loss, 0)\n",
    "    \n",
    "    \n",
    "    # print loss\n",
    "    mse_loss = pde_loss + bc_loss + data_loss\n",
    "    loss_hist.append(mse_loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'''{epoch}/{max_iter} mse loss: {mse_loss.item():.6f}, minloss: {min(loss_hist):.6f}\n",
    "              PDE Loss: {pde_loss.item():.6f}, BC Loss: {bc_loss.item():.6f}, Data Loss: {data_loss.item():.6f},  \n",
    "              nn loss: {combined_loss.item():.8f}, \n",
    "              D1: {d1.mean().item():.8f}, D2: {d2.mean().item():.8f}, D3: {d3.mean().item():.8f},\n",
    "              nn loss1: {loss1.mean().item():.8f}, nn loss2: {loss2.mean().item():.8f}, nn loss3: {loss3.mean().item():.8f}\n",
    "              ''')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "ax.plot(range(30000), loss_hist, marker='o', linestyle='-', color='b', label='Data Points')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_title('Simple Python Plot')\n",
    "\n",
    "# Show legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
