{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 50000, \"bc\":25000, \"fc\":25000}\n",
    "\n",
    "K = 40.0\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1.0\n",
    "S_range = [0.0, 130.0]\n",
    "t_range = [0.0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "# Create the model\n",
    "PINNCGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 5)\n",
    "PINNCGD.to(device)\n",
    "print(PINNCGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_CGD = networks.Discriminator(2, 25, 2)\n",
    "D_CGD.to(device)\n",
    "D_CGD.load_state_dict(D_CGD.state_dict()) # copy weights and stuff\n",
    "print(D_CGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30000\n",
    "\n",
    "tol = 1e-7\n",
    "atol = 1e-20\n",
    "g_iter = 1000\n",
    "lr = 0.002\n",
    "track_cond = lambda x, y: True\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# optimizer = CGDs.BCGD(max_params=D_CGD.parameters(), min_params=PINNCGD.parameters(), device = device,\n",
    "#                  lr_max=lr, lr_min=lr, tol=1e-10, collect_info=True)\n",
    "# optimizer = CGDs.ACGD(max_params=D_CGD.parameters(), min_params=PINNCGD.parameters(),\n",
    "#                  lr_max=lr, lr_min=lr, tol=1e-10, beta=0.99, eps=1e-8, collect_info=True)\n",
    "optimizer = CGDs.GACGD(x_params=D_CGD.parameters(), y_params = PINNCGD.parameters(), max_iter = g_iter,\n",
    "            lr_x=lr, lr_y=lr, tol=tol, atol = atol, eps=1e-8, beta=0.99, track_cond = track_cond)\n",
    "lossFunction = nn.MSELoss()\n",
    "lossfunction2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:124: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2197.)\n",
      "  y, _ = torch.triangular_solve(beta[0:j + 1].unsqueeze(-1), H[0:j + 1, 0:j + 1])  # j x j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 mse loss: 3369.999268, \n",
      "              PDE Loss: 0.00002, BC Loss: 3369.99927, nn loss: -13.373260\n",
      "50/30000 mse loss: 138.388931, \n",
      "              PDE Loss: 0.00013, BC Loss: 138.38881, nn loss: 36.301983\n",
      "100/30000 mse loss: 119.088242, \n",
      "              PDE Loss: 0.00026, BC Loss: 119.08797, nn loss: 42.859566\n",
      "150/30000 mse loss: 69.501778, \n",
      "              PDE Loss: 0.00028, BC Loss: 69.50150, nn loss: 28.484900\n",
      "200/30000 mse loss: 1.102941, \n",
      "              PDE Loss: 0.00168, BC Loss: 1.10126, nn loss: 18.369226\n",
      "250/30000 mse loss: 0.048649, \n",
      "              PDE Loss: 0.00098, BC Loss: 0.04767, nn loss: 0.480703\n",
      "300/30000 mse loss: 0.010515, \n",
      "              PDE Loss: 0.00361, BC Loss: 0.00690, nn loss: 0.397519\n",
      "350/30000 mse loss: 0.010856, \n",
      "              PDE Loss: 0.00148, BC Loss: 0.00938, nn loss: 0.163956\n",
      "400/30000 mse loss: 0.021815, \n",
      "              PDE Loss: 0.00208, BC Loss: 0.01974, nn loss: 0.110427\n",
      "450/30000 mse loss: 0.009231, \n",
      "              PDE Loss: 0.00135, BC Loss: 0.00788, nn loss: 0.228220\n",
      "500/30000 mse loss: 0.003290, \n",
      "              PDE Loss: 0.00129, BC Loss: 0.00200, nn loss: 0.059838\n",
      "550/30000 mse loss: 0.004644, \n",
      "              PDE Loss: 0.00225, BC Loss: 0.00240, nn loss: 0.038585\n",
      "600/30000 mse loss: 0.002040, \n",
      "              PDE Loss: 0.00127, BC Loss: 0.00077, nn loss: 0.032049\n",
      "650/30000 mse loss: 0.004151, \n",
      "              PDE Loss: 0.00106, BC Loss: 0.00310, nn loss: 0.022342\n",
      "700/30000 mse loss: 0.002942, \n",
      "              PDE Loss: 0.00117, BC Loss: 0.00177, nn loss: 0.024196\n",
      "750/30000 mse loss: 0.001586, \n",
      "              PDE Loss: 0.00098, BC Loss: 0.00060, nn loss: 0.016949\n",
      "800/30000 mse loss: 0.001238, \n",
      "              PDE Loss: 0.00079, BC Loss: 0.00045, nn loss: 0.011730\n",
      "850/30000 mse loss: 0.001070, \n",
      "              PDE Loss: 0.00074, BC Loss: 0.00033, nn loss: 0.008450\n",
      "900/30000 mse loss: 0.000935, \n",
      "              PDE Loss: 0.00077, BC Loss: 0.00016, nn loss: 0.007582\n",
      "950/30000 mse loss: 0.001536, \n",
      "              PDE Loss: 0.00094, BC Loss: 0.00059, nn loss: 0.007723\n",
      "1000/30000 mse loss: 0.001800, \n",
      "              PDE Loss: 0.00163, BC Loss: 0.00017, nn loss: 0.012739\n",
      "1050/30000 mse loss: 0.000715, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.00011, nn loss: 0.004125\n",
      "1100/30000 mse loss: 0.001670, \n",
      "              PDE Loss: 0.00101, BC Loss: 0.00066, nn loss: 0.008421\n",
      "1150/30000 mse loss: 0.001634, \n",
      "              PDE Loss: 0.00111, BC Loss: 0.00053, nn loss: 0.015085\n",
      "1200/30000 mse loss: 0.001128, \n",
      "              PDE Loss: 0.00097, BC Loss: 0.00016, nn loss: 0.013127\n",
      "1250/30000 mse loss: 0.002074, \n",
      "              PDE Loss: 0.00160, BC Loss: 0.00047, nn loss: 0.018696\n",
      "1300/30000 mse loss: 0.001935, \n",
      "              PDE Loss: 0.00117, BC Loss: 0.00077, nn loss: 0.021595\n",
      "1350/30000 mse loss: 0.002438, \n",
      "              PDE Loss: 0.00082, BC Loss: 0.00162, nn loss: 0.006655\n",
      "1400/30000 mse loss: 0.005163, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.00456, nn loss: 0.085723\n",
      "1450/30000 mse loss: 0.001958, \n",
      "              PDE Loss: 0.00144, BC Loss: 0.00051, nn loss: 0.046974\n",
      "1500/30000 mse loss: 0.001163, \n",
      "              PDE Loss: 0.00089, BC Loss: 0.00027, nn loss: 0.019072\n",
      "1550/30000 mse loss: 0.001268, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.00065, nn loss: 0.027590\n",
      "1600/30000 mse loss: 0.001924, \n",
      "              PDE Loss: 0.00131, BC Loss: 0.00061, nn loss: 0.038496\n",
      "1650/30000 mse loss: 0.003332, \n",
      "              PDE Loss: 0.00147, BC Loss: 0.00186, nn loss: 0.011692\n",
      "1700/30000 mse loss: 0.001549, \n",
      "              PDE Loss: 0.00113, BC Loss: 0.00042, nn loss: 0.014167\n",
      "1750/30000 mse loss: 0.005123, \n",
      "              PDE Loss: 0.00127, BC Loss: 0.00385, nn loss: 0.005031\n",
      "1800/30000 mse loss: 0.002827, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.00226, nn loss: 0.015936\n",
      "1850/30000 mse loss: 0.001524, \n",
      "              PDE Loss: 0.00095, BC Loss: 0.00057, nn loss: 0.019778\n",
      "1900/30000 mse loss: 0.002284, \n",
      "              PDE Loss: 0.00072, BC Loss: 0.00156, nn loss: 0.011187\n",
      "1950/30000 mse loss: 0.010410, \n",
      "              PDE Loss: 0.00164, BC Loss: 0.00877, nn loss: 0.051829\n",
      "2000/30000 mse loss: 0.004457, \n",
      "              PDE Loss: 0.00200, BC Loss: 0.00246, nn loss: 0.015675\n",
      "2050/30000 mse loss: 0.001785, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.00120, nn loss: 0.028385\n",
      "2100/30000 mse loss: 0.016656, \n",
      "              PDE Loss: 0.00275, BC Loss: 0.01391, nn loss: 0.052138\n",
      "2150/30000 mse loss: 0.006909, \n",
      "              PDE Loss: 0.00166, BC Loss: 0.00525, nn loss: 0.026053\n",
      "2200/30000 mse loss: 0.004994, \n",
      "              PDE Loss: 0.00106, BC Loss: 0.00393, nn loss: 0.018188\n",
      "2250/30000 mse loss: 0.004902, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.00434, nn loss: 0.050641\n",
      "2300/30000 mse loss: 0.008100, \n",
      "              PDE Loss: 0.00366, BC Loss: 0.00444, nn loss: 0.058338\n",
      "2350/30000 mse loss: 0.001541, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.00098, nn loss: 0.038483\n",
      "2400/30000 mse loss: 0.002361, \n",
      "              PDE Loss: 0.00125, BC Loss: 0.00111, nn loss: 0.131729\n",
      "2450/30000 mse loss: 0.001777, \n",
      "              PDE Loss: 0.00097, BC Loss: 0.00081, nn loss: 0.042372\n",
      "2500/30000 mse loss: 0.007057, \n",
      "              PDE Loss: 0.00129, BC Loss: 0.00576, nn loss: 0.095506\n",
      "2550/30000 mse loss: 0.002107, \n",
      "              PDE Loss: 0.00127, BC Loss: 0.00083, nn loss: 0.087883\n",
      "2600/30000 mse loss: 0.003927, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00338, nn loss: 0.023766\n",
      "2650/30000 mse loss: 0.002871, \n",
      "              PDE Loss: 0.00067, BC Loss: 0.00220, nn loss: 0.058733\n",
      "2700/30000 mse loss: 0.017655, \n",
      "              PDE Loss: 0.00073, BC Loss: 0.01693, nn loss: 0.080118\n",
      "2750/30000 mse loss: 0.035240, \n",
      "              PDE Loss: 0.00237, BC Loss: 0.03287, nn loss: 0.103655\n",
      "2800/30000 mse loss: 0.007762, \n",
      "              PDE Loss: 0.00193, BC Loss: 0.00583, nn loss: 0.028322\n",
      "2850/30000 mse loss: 0.017203, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.01658, nn loss: 0.090859\n",
      "2900/30000 mse loss: 0.002631, \n",
      "              PDE Loss: 0.00094, BC Loss: 0.00169, nn loss: 0.046923\n",
      "2950/30000 mse loss: 0.004921, \n",
      "              PDE Loss: 0.00192, BC Loss: 0.00300, nn loss: 0.050844\n",
      "3000/30000 mse loss: 0.008127, \n",
      "              PDE Loss: 0.00192, BC Loss: 0.00620, nn loss: 0.210438\n",
      "3050/30000 mse loss: 0.008264, \n",
      "              PDE Loss: 0.00200, BC Loss: 0.00627, nn loss: 0.048962\n",
      "3100/30000 mse loss: 0.001931, \n",
      "              PDE Loss: 0.00141, BC Loss: 0.00052, nn loss: 0.209002\n",
      "3150/30000 mse loss: 0.002741, \n",
      "              PDE Loss: 0.00064, BC Loss: 0.00210, nn loss: 0.116758\n",
      "3200/30000 mse loss: 0.003040, \n",
      "              PDE Loss: 0.00121, BC Loss: 0.00183, nn loss: 0.150262\n",
      "3250/30000 mse loss: 0.001110, \n",
      "              PDE Loss: 0.00066, BC Loss: 0.00045, nn loss: 0.053222\n",
      "3300/30000 mse loss: 0.001671, \n",
      "              PDE Loss: 0.00084, BC Loss: 0.00083, nn loss: 0.113811\n",
      "3350/30000 mse loss: 0.002908, \n",
      "              PDE Loss: 0.00173, BC Loss: 0.00118, nn loss: 0.074023\n",
      "3400/30000 mse loss: 0.004666, \n",
      "              PDE Loss: 0.00180, BC Loss: 0.00287, nn loss: 0.088665\n",
      "3450/30000 mse loss: 0.005058, \n",
      "              PDE Loss: 0.00168, BC Loss: 0.00338, nn loss: 0.290469\n",
      "3500/30000 mse loss: 0.004893, \n",
      "              PDE Loss: 0.00341, BC Loss: 0.00148, nn loss: 0.119489\n",
      "3550/30000 mse loss: 0.006152, \n",
      "              PDE Loss: 0.00093, BC Loss: 0.00522, nn loss: 0.760843\n",
      "3600/30000 mse loss: 0.002052, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00150, nn loss: 0.129098\n",
      "3650/30000 mse loss: 0.008613, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.00796, nn loss: 0.522597\n",
      "3700/30000 mse loss: 0.042665, \n",
      "              PDE Loss: 0.00562, BC Loss: 0.03704, nn loss: 2.712277\n",
      "3750/30000 mse loss: 0.002678, \n",
      "              PDE Loss: 0.00078, BC Loss: 0.00190, nn loss: 0.481901\n",
      "3800/30000 mse loss: 0.002378, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.00176, nn loss: 0.387833\n",
      "3850/30000 mse loss: 0.001828, \n",
      "              PDE Loss: 0.00078, BC Loss: 0.00105, nn loss: 0.443269\n",
      "3900/30000 mse loss: 0.002569, \n",
      "              PDE Loss: 0.00122, BC Loss: 0.00135, nn loss: 0.314745\n",
      "3950/30000 mse loss: 0.022879, \n",
      "              PDE Loss: 0.00214, BC Loss: 0.02074, nn loss: 0.629772\n",
      "4000/30000 mse loss: 0.001328, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.00077, nn loss: 0.199035\n",
      "4050/30000 mse loss: 0.048641, \n",
      "              PDE Loss: 0.00070, BC Loss: 0.04794, nn loss: 0.227130\n",
      "4100/30000 mse loss: 0.047080, \n",
      "              PDE Loss: 0.00176, BC Loss: 0.04532, nn loss: 2.896514\n",
      "4150/30000 mse loss: 0.025146, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.02460, nn loss: 0.745750\n",
      "4200/30000 mse loss: 0.072340, \n",
      "              PDE Loss: 0.00070, BC Loss: 0.07163, nn loss: 0.552846\n",
      "4250/30000 mse loss: 0.002681, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.00211, nn loss: 0.984228\n",
      "4300/30000 mse loss: 0.053779, \n",
      "              PDE Loss: 0.00514, BC Loss: 0.04864, nn loss: 4.920030\n",
      "4350/30000 mse loss: 0.009987, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00943, nn loss: 1.177604\n",
      "4400/30000 mse loss: 0.011027, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.01048, nn loss: 1.951306\n",
      "4450/30000 mse loss: 0.007259, \n",
      "              PDE Loss: 0.00355, BC Loss: 0.00370, nn loss: 1.383212\n",
      "4500/30000 mse loss: 0.009811, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00926, nn loss: 1.074934\n",
      "4550/30000 mse loss: 0.054138, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.05355, nn loss: 2.085150\n",
      "4600/30000 mse loss: 0.010943, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.01038, nn loss: 1.271195\n",
      "4650/30000 mse loss: 0.008309, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.00775, nn loss: 1.918386\n",
      "4700/30000 mse loss: 0.043082, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.04250, nn loss: 4.534401\n",
      "4750/30000 mse loss: 0.132097, \n",
      "              PDE Loss: 0.00109, BC Loss: 0.13101, nn loss: 6.792478\n",
      "4800/30000 mse loss: 0.012071, \n",
      "              PDE Loss: 0.00075, BC Loss: 0.01132, nn loss: 0.825785\n",
      "4850/30000 mse loss: 0.413369, \n",
      "              PDE Loss: 0.00103, BC Loss: 0.41234, nn loss: 2.093275\n",
      "4900/30000 mse loss: 0.030206, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.02967, nn loss: 3.049937\n",
      "4950/30000 mse loss: 0.046557, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.04596, nn loss: 9.982717\n",
      "5000/30000 mse loss: 0.035087, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.03456, nn loss: 3.912956\n",
      "5050/30000 mse loss: 0.012867, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.01230, nn loss: 3.791146\n",
      "5100/30000 mse loss: 0.161184, \n",
      "              PDE Loss: 0.00378, BC Loss: 0.15740, nn loss: 2.957235\n",
      "5150/30000 mse loss: 0.010555, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.00993, nn loss: 7.885168\n",
      "5200/30000 mse loss: 0.007466, \n",
      "              PDE Loss: 0.00064, BC Loss: 0.00683, nn loss: 4.553901\n",
      "5250/30000 mse loss: 0.012953, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.01240, nn loss: 2.355288\n",
      "5300/30000 mse loss: 0.080685, \n",
      "              PDE Loss: 0.00052, BC Loss: 0.08016, nn loss: 2.058532\n",
      "5350/30000 mse loss: 0.028525, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.02790, nn loss: 2.885684\n",
      "5400/30000 mse loss: 0.115698, \n",
      "              PDE Loss: 0.00078, BC Loss: 0.11492, nn loss: 16.506607\n",
      "5450/30000 mse loss: 0.231544, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.23096, nn loss: 6.562360\n",
      "5500/30000 mse loss: 0.063125, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.06251, nn loss: 12.033468\n",
      "5550/30000 mse loss: 0.005302, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.00469, nn loss: 10.242539\n",
      "5600/30000 mse loss: 0.040599, \n",
      "              PDE Loss: 0.00064, BC Loss: 0.03996, nn loss: 9.486596\n",
      "5650/30000 mse loss: 0.020172, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.01960, nn loss: 5.130352\n",
      "5700/30000 mse loss: 0.047543, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.04700, nn loss: 5.045571\n",
      "5750/30000 mse loss: 0.007910, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.00731, nn loss: 5.983847\n",
      "5800/30000 mse loss: 0.007493, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.00686, nn loss: 3.153513\n",
      "5850/30000 mse loss: 0.044675, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.04414, nn loss: 8.946961\n",
      "5900/30000 mse loss: 0.030494, \n",
      "              PDE Loss: 0.00078, BC Loss: 0.02971, nn loss: 4.324618\n",
      "5950/30000 mse loss: 0.242547, \n",
      "              PDE Loss: 0.00230, BC Loss: 0.24025, nn loss: 2.930176\n",
      "6000/30000 mse loss: 0.035631, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.03509, nn loss: 4.488040\n",
      "6050/30000 mse loss: 0.117971, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.11736, nn loss: 5.610604\n",
      "6100/30000 mse loss: 0.055881, \n",
      "              PDE Loss: 0.00247, BC Loss: 0.05341, nn loss: 2.890666\n",
      "6150/30000 mse loss: 0.103568, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.10292, nn loss: 7.890087\n",
      "6200/30000 mse loss: 0.129835, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.12921, nn loss: 5.614503\n",
      "6250/30000 mse loss: 0.058435, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.05790, nn loss: 26.795284\n",
      "6300/30000 mse loss: 0.060666, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.06009, nn loss: 6.582599\n",
      "6350/30000 mse loss: 0.012213, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.01164, nn loss: 12.788678\n",
      "6400/30000 mse loss: 0.076565, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.07596, nn loss: 20.106110\n",
      "6450/30000 mse loss: 0.012730, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.01211, nn loss: 11.154958\n",
      "6500/30000 mse loss: 0.162001, \n",
      "              PDE Loss: 0.00078, BC Loss: 0.16122, nn loss: 10.280727\n",
      "6550/30000 mse loss: 0.058840, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.05819, nn loss: 15.005165\n",
      "6600/30000 mse loss: 0.452779, \n",
      "              PDE Loss: 0.00677, BC Loss: 0.44601, nn loss: 52.681976\n",
      "6650/30000 mse loss: 0.137786, \n",
      "              PDE Loss: 0.00082, BC Loss: 0.13697, nn loss: 11.028625\n",
      "6700/30000 mse loss: 0.014552, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.01395, nn loss: 15.075605\n",
      "6750/30000 mse loss: 0.021992, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.02145, nn loss: 18.494724\n",
      "6800/30000 mse loss: 0.142789, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.14221, nn loss: 7.143474\n",
      "6850/30000 mse loss: 0.138449, \n",
      "              PDE Loss: 0.00087, BC Loss: 0.13758, nn loss: 11.241962\n",
      "6900/30000 mse loss: 0.289643, \n",
      "              PDE Loss: 0.00119, BC Loss: 0.28846, nn loss: 16.919683\n",
      "6950/30000 mse loss: 0.195936, \n",
      "              PDE Loss: 0.00094, BC Loss: 0.19500, nn loss: 18.114025\n",
      "7000/30000 mse loss: 0.056150, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.05556, nn loss: 10.190824\n",
      "7050/30000 mse loss: 0.078016, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.07737, nn loss: 18.421801\n",
      "7100/30000 mse loss: 0.056293, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.05570, nn loss: 14.316063\n",
      "7150/30000 mse loss: 0.162173, \n",
      "              PDE Loss: 0.00073, BC Loss: 0.16144, nn loss: 16.187782\n",
      "7200/30000 mse loss: 0.043381, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.04281, nn loss: 12.790909\n",
      "7250/30000 mse loss: 0.522266, \n",
      "              PDE Loss: 0.02018, BC Loss: 0.50209, nn loss: 380.111359\n",
      "7300/30000 mse loss: 0.112144, \n",
      "              PDE Loss: 0.00096, BC Loss: 0.11118, nn loss: 55.167049\n",
      "7350/30000 mse loss: 0.008439, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00789, nn loss: 36.857204\n",
      "7400/30000 mse loss: 0.099620, \n",
      "              PDE Loss: 0.00091, BC Loss: 0.09871, nn loss: 27.054705\n",
      "7450/30000 mse loss: 0.152212, \n",
      "              PDE Loss: 0.00114, BC Loss: 0.15107, nn loss: 33.561333\n",
      "7500/30000 mse loss: 0.071076, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.07051, nn loss: 11.920691\n",
      "7550/30000 mse loss: 0.000907, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00035, nn loss: 22.857176\n",
      "7600/30000 mse loss: 0.206820, \n",
      "              PDE Loss: 0.00353, BC Loss: 0.20329, nn loss: 15.950192\n",
      "7650/30000 mse loss: 0.023544, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.02298, nn loss: 15.573185\n",
      "7700/30000 mse loss: 0.240055, \n",
      "              PDE Loss: 0.00106, BC Loss: 0.23899, nn loss: 14.859288\n",
      "7750/30000 mse loss: 0.115574, \n",
      "              PDE Loss: 0.00067, BC Loss: 0.11490, nn loss: 17.066433\n",
      "7800/30000 mse loss: 0.047852, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.04726, nn loss: 21.688019\n",
      "7850/30000 mse loss: 0.082984, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.08245, nn loss: 35.860455\n",
      "7900/30000 mse loss: 0.177632, \n",
      "              PDE Loss: 0.00144, BC Loss: 0.17619, nn loss: 12.797918\n",
      "7950/30000 mse loss: 0.141104, \n",
      "              PDE Loss: 0.00333, BC Loss: 0.13777, nn loss: 30.580282\n",
      "8000/30000 mse loss: 0.003737, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00319, nn loss: 27.147671\n",
      "8050/30000 mse loss: 1.024166, \n",
      "              PDE Loss: 0.00123, BC Loss: 1.02293, nn loss: 5.864281\n",
      "8100/30000 mse loss: 0.081778, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.08122, nn loss: 56.846375\n",
      "8150/30000 mse loss: 0.134832, \n",
      "              PDE Loss: 0.00434, BC Loss: 0.13050, nn loss: 74.567039\n",
      "8200/30000 mse loss: 0.014885, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.01433, nn loss: 73.529488\n",
      "8250/30000 mse loss: 0.149687, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.14903, nn loss: 62.454884\n",
      "8300/30000 mse loss: 0.076528, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.07596, nn loss: 58.001713\n",
      "8350/30000 mse loss: 0.203785, \n",
      "              PDE Loss: 0.00412, BC Loss: 0.19967, nn loss: 39.203789\n",
      "8400/30000 mse loss: 0.029018, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.02847, nn loss: 102.645096\n",
      "8450/30000 mse loss: 0.022311, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.02173, nn loss: 45.182526\n",
      "8500/30000 mse loss: 0.018954, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.01840, nn loss: 46.733810\n",
      "8550/30000 mse loss: 0.540760, \n",
      "              PDE Loss: 0.00070, BC Loss: 0.54006, nn loss: 58.921455\n",
      "8600/30000 mse loss: 0.015675, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.01512, nn loss: 121.841774\n",
      "8650/30000 mse loss: 0.435317, \n",
      "              PDE Loss: 0.00188, BC Loss: 0.43344, nn loss: 25.170864\n",
      "8700/30000 mse loss: 0.735268, \n",
      "              PDE Loss: 0.00212, BC Loss: 0.73314, nn loss: 18.991636\n",
      "8750/30000 mse loss: 0.061441, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.06086, nn loss: 120.414116\n",
      "8800/30000 mse loss: 0.035314, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.03478, nn loss: 58.112976\n",
      "8850/30000 mse loss: 0.058366, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.05779, nn loss: 114.767624\n",
      "8900/30000 mse loss: 0.080868, \n",
      "              PDE Loss: 0.00500, BC Loss: 0.07586, nn loss: 112.148994\n",
      "8950/30000 mse loss: 0.532767, \n",
      "              PDE Loss: 0.00330, BC Loss: 0.52947, nn loss: 68.953842\n",
      "9000/30000 mse loss: 0.174787, \n",
      "              PDE Loss: 0.00071, BC Loss: 0.17408, nn loss: 40.345142\n",
      "9050/30000 mse loss: 0.072832, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.07224, nn loss: 166.843445\n",
      "9100/30000 mse loss: 0.010740, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.01018, nn loss: 62.537094\n",
      "9150/30000 mse loss: 0.036168, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.03564, nn loss: 161.629349\n",
      "9200/30000 mse loss: 0.418431, \n",
      "              PDE Loss: 0.00071, BC Loss: 0.41772, nn loss: 88.805702\n",
      "9250/30000 mse loss: 0.437923, \n",
      "              PDE Loss: 0.00201, BC Loss: 0.43592, nn loss: 55.904186\n",
      "9300/30000 mse loss: 0.066069, \n",
      "              PDE Loss: 0.00110, BC Loss: 0.06497, nn loss: 103.818748\n",
      "9350/30000 mse loss: 0.001120, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.00056, nn loss: 226.017227\n",
      "9400/30000 mse loss: 0.081141, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.08055, nn loss: 98.747696\n",
      "9450/30000 mse loss: 0.119392, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.11882, nn loss: 123.712967\n",
      "9500/30000 mse loss: 0.219134, \n",
      "              PDE Loss: 0.00221, BC Loss: 0.21693, nn loss: 254.715073\n",
      "9550/30000 mse loss: 0.463287, \n",
      "              PDE Loss: 0.00068, BC Loss: 0.46260, nn loss: 148.676361\n",
      "9600/30000 mse loss: 0.157465, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.15691, nn loss: 310.474243\n",
      "9650/30000 mse loss: 0.600043, \n",
      "              PDE Loss: 0.00384, BC Loss: 0.59621, nn loss: 148.413834\n",
      "9700/30000 mse loss: 0.202071, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.20149, nn loss: 222.153549\n",
      "9750/30000 mse loss: 0.821990, \n",
      "              PDE Loss: 0.00238, BC Loss: 0.81961, nn loss: 146.556122\n",
      "9800/30000 mse loss: 0.166758, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.16620, nn loss: 215.735748\n",
      "9850/30000 mse loss: 0.023930, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.02338, nn loss: 406.625610\n",
      "9900/30000 mse loss: 0.534933, \n",
      "              PDE Loss: 0.00235, BC Loss: 0.53259, nn loss: 127.529785\n",
      "9950/30000 mse loss: 0.075119, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.07456, nn loss: 261.461823\n",
      "10000/30000 mse loss: 0.076848, \n",
      "              PDE Loss: 0.00052, BC Loss: 0.07633, nn loss: 487.069489\n",
      "10050/30000 mse loss: 0.227300, \n",
      "              PDE Loss: 0.00076, BC Loss: 0.22654, nn loss: 274.319275\n",
      "10100/30000 mse loss: 0.073107, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.07258, nn loss: 326.592285\n",
      "10150/30000 mse loss: 0.682145, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.68158, nn loss: 339.658508\n",
      "10200/30000 mse loss: 0.004244, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.00368, nn loss: 643.395935\n",
      "10250/30000 mse loss: 0.125958, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.12535, nn loss: 325.485840\n",
      "10300/30000 mse loss: 1.077045, \n",
      "              PDE Loss: 0.05480, BC Loss: 1.02225, nn loss: 6175.077148\n",
      "10350/30000 mse loss: 0.042958, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.04239, nn loss: 508.477875\n",
      "10400/30000 mse loss: 1.526623, \n",
      "              PDE Loss: 0.00515, BC Loss: 1.52147, nn loss: 1860.654541\n",
      "10450/30000 mse loss: 0.131371, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.13081, nn loss: 454.974518\n",
      "10500/30000 mse loss: 0.242968, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.24241, nn loss: 884.050903\n",
      "10550/30000 mse loss: 0.227342, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.22679, nn loss: 359.789368\n",
      "10600/30000 mse loss: 0.180556, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.17997, nn loss: 777.376465\n",
      "10650/30000 mse loss: 1.921877, \n",
      "              PDE Loss: 0.00603, BC Loss: 1.91585, nn loss: 1841.146484\n",
      "10700/30000 mse loss: 0.345128, \n",
      "              PDE Loss: 0.00067, BC Loss: 0.34446, nn loss: 660.965210\n",
      "10750/30000 mse loss: 0.924159, \n",
      "              PDE Loss: 0.00045, BC Loss: 0.92371, nn loss: 992.088257\n",
      "10800/30000 mse loss: 0.041869, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.04133, nn loss: 548.204163\n",
      "10850/30000 mse loss: 0.187971, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.18743, nn loss: 868.031128\n",
      "10900/30000 mse loss: 1.573658, \n",
      "              PDE Loss: 0.00279, BC Loss: 1.57086, nn loss: 630.081848\n",
      "10950/30000 mse loss: 0.198950, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.19835, nn loss: 809.022217\n",
      "11000/30000 mse loss: 0.092684, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.09213, nn loss: 767.682373\n",
      "11050/30000 mse loss: 0.099413, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.09886, nn loss: 1149.888916\n",
      "11100/30000 mse loss: 1.525521, \n",
      "              PDE Loss: 0.00347, BC Loss: 1.52205, nn loss: 1217.560791\n",
      "11150/30000 mse loss: 0.155242, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.15467, nn loss: 902.904175\n",
      "11200/30000 mse loss: 0.061971, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.06139, nn loss: 1542.675781\n",
      "11250/30000 mse loss: 0.281283, \n",
      "              PDE Loss: 0.00064, BC Loss: 0.28064, nn loss: 737.690979\n",
      "11300/30000 mse loss: 0.395871, \n",
      "              PDE Loss: 0.00064, BC Loss: 0.39523, nn loss: 1274.020752\n",
      "11350/30000 mse loss: 0.471582, \n",
      "              PDE Loss: 0.00327, BC Loss: 0.46831, nn loss: 1480.800903\n",
      "11400/30000 mse loss: 0.911770, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.91124, nn loss: 1190.248779\n",
      "11450/30000 mse loss: 0.111923, \n",
      "              PDE Loss: 0.00051, BC Loss: 0.11141, nn loss: 2051.133301\n",
      "11500/30000 mse loss: 0.630352, \n",
      "              PDE Loss: 0.00053, BC Loss: 0.62982, nn loss: 1210.548950\n",
      "11550/30000 mse loss: 0.043800, \n",
      "              PDE Loss: 0.00054, BC Loss: 0.04326, nn loss: 2030.419556\n",
      "11600/30000 mse loss: 0.117142, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.11656, nn loss: 914.460022\n",
      "11650/30000 mse loss: 0.053266, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.05271, nn loss: 1768.829346\n",
      "11700/30000 mse loss: 0.987610, \n",
      "              PDE Loss: 0.01692, BC Loss: 0.97069, nn loss: 12955.592773\n",
      "11750/30000 mse loss: 0.153608, \n",
      "              PDE Loss: 0.00130, BC Loss: 0.15231, nn loss: 1285.866333\n",
      "11800/30000 mse loss: 0.434207, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.43366, nn loss: 2153.502930\n",
      "11850/30000 mse loss: 1.087521, \n",
      "              PDE Loss: 0.00288, BC Loss: 1.08464, nn loss: 4072.922607\n",
      "11900/30000 mse loss: 1.056209, \n",
      "              PDE Loss: 0.00054, BC Loss: 1.05567, nn loss: 2095.735840\n",
      "11950/30000 mse loss: 0.167314, \n",
      "              PDE Loss: 0.00055, BC Loss: 0.16676, nn loss: 3605.340820\n",
      "12000/30000 mse loss: 0.626918, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.62626, nn loss: 1818.748413\n",
      "12050/30000 mse loss: 0.594623, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.59399, nn loss: 2980.384766\n",
      "12100/30000 mse loss: 1.313543, \n",
      "              PDE Loss: 0.00288, BC Loss: 1.31066, nn loss: 3311.712646\n",
      "12150/30000 mse loss: 0.590718, \n",
      "              PDE Loss: 0.00058, BC Loss: 0.59014, nn loss: 3010.255615\n",
      "12200/30000 mse loss: 0.211023, \n",
      "              PDE Loss: 0.00089, BC Loss: 0.21013, nn loss: 5842.577637\n",
      "12250/30000 mse loss: 0.043119, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.04250, nn loss: 2580.861572\n",
      "12300/30000 mse loss: 0.062399, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.06181, nn loss: 4163.910645\n",
      "12350/30000 mse loss: 0.061329, \n",
      "              PDE Loss: 0.00176, BC Loss: 0.05957, nn loss: 3883.787109\n",
      "12400/30000 mse loss: 0.103952, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.10333, nn loss: 3676.255371\n",
      "12450/30000 mse loss: 0.048346, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.04774, nn loss: 7132.616211\n",
      "12500/30000 mse loss: 0.080977, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.08041, nn loss: 4159.892090\n",
      "12550/30000 mse loss: 2.336693, \n",
      "              PDE Loss: 0.00197, BC Loss: 2.33473, nn loss: 9162.123047\n",
      "12600/30000 mse loss: 0.171504, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.17094, nn loss: 4887.863770\n",
      "12650/30000 mse loss: 5.115885, \n",
      "              PDE Loss: 0.00239, BC Loss: 5.11350, nn loss: 7702.728027\n",
      "12700/30000 mse loss: 2.132376, \n",
      "              PDE Loss: 0.00059, BC Loss: 2.13178, nn loss: 5950.324219\n",
      "12750/30000 mse loss: 1.116882, \n",
      "              PDE Loss: 0.00211, BC Loss: 1.11478, nn loss: 9026.958008\n",
      "12800/30000 mse loss: 0.179878, \n",
      "              PDE Loss: 0.00062, BC Loss: 0.17926, nn loss: 4816.039551\n",
      "12850/30000 mse loss: 0.209546, \n",
      "              PDE Loss: 0.00063, BC Loss: 0.20891, nn loss: 7411.529297\n",
      "12900/30000 mse loss: 2.024006, \n",
      "              PDE Loss: 0.00062, BC Loss: 2.02338, nn loss: 5110.191895\n",
      "12950/30000 mse loss: 1.338972, \n",
      "              PDE Loss: 0.00071, BC Loss: 1.33827, nn loss: 7969.867676\n",
      "13000/30000 mse loss: 2.995976, \n",
      "              PDE Loss: 0.00060, BC Loss: 2.99537, nn loss: 5097.529297\n",
      "13050/30000 mse loss: 1.071016, \n",
      "              PDE Loss: 0.00058, BC Loss: 1.07043, nn loss: 7750.580566\n",
      "13100/30000 mse loss: 10.757108, \n",
      "              PDE Loss: 0.00133, BC Loss: 10.75578, nn loss: 6210.934570\n",
      "13150/30000 mse loss: 0.019846, \n",
      "              PDE Loss: 0.00065, BC Loss: 0.01920, nn loss: 8723.368164\n",
      "13200/30000 mse loss: 6.145166, \n",
      "              PDE Loss: 0.00153, BC Loss: 6.14364, nn loss: 6066.449219\n",
      "13250/30000 mse loss: 0.231874, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.23127, nn loss: 9829.174805\n",
      "13300/30000 mse loss: 1.755129, \n",
      "              PDE Loss: 0.00111, BC Loss: 1.75402, nn loss: 9368.543945\n",
      "13350/30000 mse loss: 0.848222, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.84761, nn loss: 11160.757812\n",
      "13400/30000 mse loss: 1.454460, \n",
      "              PDE Loss: 0.00060, BC Loss: 1.45386, nn loss: 9573.224609\n",
      "13450/30000 mse loss: 3.967710, \n",
      "              PDE Loss: 0.00097, BC Loss: 3.96674, nn loss: 9070.656250\n",
      "13500/30000 mse loss: 1.747721, \n",
      "              PDE Loss: 0.00054, BC Loss: 1.74718, nn loss: 14983.883789\n",
      "13550/30000 mse loss: 1.267631, \n",
      "              PDE Loss: 0.00062, BC Loss: 1.26701, nn loss: 9822.186523\n",
      "13600/30000 mse loss: 0.328196, \n",
      "              PDE Loss: 0.00057, BC Loss: 0.32762, nn loss: 13786.646484\n",
      "13650/30000 mse loss: 4.466625, \n",
      "              PDE Loss: 0.00063, BC Loss: 4.46599, nn loss: 12025.058594\n",
      "13700/30000 mse loss: 16.909157, \n",
      "              PDE Loss: 0.00265, BC Loss: 16.90650, nn loss: 20045.630859\n",
      "13750/30000 mse loss: 0.513485, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.51288, nn loss: 14767.213867\n",
      "13800/30000 mse loss: 36.952656, \n",
      "              PDE Loss: 0.00248, BC Loss: 36.95018, nn loss: 26731.578125\n",
      "13850/30000 mse loss: 0.153663, \n",
      "              PDE Loss: 0.00056, BC Loss: 0.15310, nn loss: 16989.867188\n",
      "13900/30000 mse loss: 0.957453, \n",
      "              PDE Loss: 0.00066, BC Loss: 0.95679, nn loss: 13843.889648\n",
      "13950/30000 mse loss: 0.103592, \n",
      "              PDE Loss: 0.00059, BC Loss: 0.10300, nn loss: 17584.023438\n",
      "14000/30000 mse loss: 28.318247, \n",
      "              PDE Loss: 0.00112, BC Loss: 28.31712, nn loss: 12061.327148\n",
      "14050/30000 mse loss: 0.390553, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.38995, nn loss: 18998.199219\n",
      "14100/30000 mse loss: 1.387693, \n",
      "              PDE Loss: 0.00057, BC Loss: 1.38712, nn loss: 14100.277344\n",
      "14150/30000 mse loss: 0.809798, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.80919, nn loss: 18832.673828\n",
      "14200/30000 mse loss: 0.048815, \n",
      "              PDE Loss: 0.00061, BC Loss: 0.04820, nn loss: 16804.013672\n",
      "14250/30000 mse loss: 0.106009, \n",
      "              PDE Loss: 0.00074, BC Loss: 0.10526, nn loss: 23129.316406\n",
      "14300/30000 mse loss: 5.300562, \n",
      "              PDE Loss: 0.00061, BC Loss: 5.29995, nn loss: 17925.785156\n",
      "14350/30000 mse loss: 2.072737, \n",
      "              PDE Loss: 0.00062, BC Loss: 2.07212, nn loss: 22839.388672\n",
      "14400/30000 mse loss: 0.631760, \n",
      "              PDE Loss: 0.00060, BC Loss: 0.63116, nn loss: 21619.175781\n",
      "14450/30000 mse loss: 3.647214, \n",
      "              PDE Loss: 0.00080, BC Loss: 3.64641, nn loss: 30021.810547\n",
      "14500/30000 mse loss: 6.128952, \n",
      "              PDE Loss: 0.00064, BC Loss: 6.12831, nn loss: 21004.427734\n",
      "14550/30000 mse loss: 2.291115, \n",
      "              PDE Loss: 0.00059, BC Loss: 2.29053, nn loss: 27142.890625\n",
      "14600/30000 mse loss: 8.790631, \n",
      "              PDE Loss: 0.00061, BC Loss: 8.79002, nn loss: 22952.378906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m pinn_loss \u001b[39m=\u001b[39m pde_loss\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m bc_loss\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     61\u001b[0m \u001b[39m# combined_loss.backward()\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(\u001b[39m-\u001b[39;49mcombined_loss, combined_loss, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     63\u001b[0m \u001b[39m# optimizer.step(combined_loss)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(pinn_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_acgd.py:165\u001b[0m, in \u001b[0;36mGACGD.step\u001b[1;34m(self, loss_x, loss_y, trigger)\u001b[0m\n\u001b[0;32m    157\u001b[0m prev_x0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m Avp \u001b[39m=\u001b[39m partial(MvProd,\n\u001b[0;32m    159\u001b[0m               grad_fy\u001b[39m=\u001b[39mgrad_fy_vec, grad_gx\u001b[39m=\u001b[39mgrad_gx_vec,\n\u001b[0;32m    160\u001b[0m               x_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_params, y_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m               x_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_reducer, y_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_reducer,\n\u001b[0;32m    164\u001b[0m               rebuild\u001b[39m=\u001b[39mshould_rebuild)\n\u001b[1;32m--> 165\u001b[0m soln, (num_iter, err_history) \u001b[39m=\u001b[39m GMRES(Avp\u001b[39m=\u001b[39;49mAvp, b\u001b[39m=\u001b[39;49mRHS, x0\u001b[39m=\u001b[39;49mprev_x0,\n\u001b[0;32m    166\u001b[0m                                       max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    167\u001b[0m                                       tol\u001b[39m=\u001b[39;49mtol, atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m    168\u001b[0m                                       track\u001b[39m=\u001b[39;49mtrack_flag)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mx0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m soln\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    172\u001b[0m     {\n\u001b[0;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_x\u001b[39m\u001b[39m'\u001b[39m: sq_avg_x,\n\u001b[0;32m    174\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_y\u001b[39m\u001b[39m'\u001b[39m: sq_avg_y\n\u001b[0;32m    175\u001b[0m     }\n\u001b[0;32m    176\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:78\u001b[0m, in \u001b[0;36mGMRES\u001b[1;34m(Avp, b, x0, max_iter, tol, atol, track)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGMRES\u001b[39m(Avp,              \u001b[39m# Linear operator\u001b[39;00m\n\u001b[0;32m     70\u001b[0m           b,                \u001b[39m# RHS of the linear system\u001b[39;00m\n\u001b[0;32m     71\u001b[0m           x0\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,          \u001b[39m# initial guess, tuple has the same shape as b\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m           track\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m       \u001b[39m# If True, keep a history of the relative residual error\u001b[39;00m\n\u001b[0;32m     76\u001b[0m           ):\n\u001b[0;32m     77\u001b[0m     bnorm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(b)\n\u001b[1;32m---> 78\u001b[0m     _check_nan(b, \u001b[39m'\u001b[39;49m\u001b[39mRHS of the system is Nan\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m max_iter \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m bnorm \u001b[39m<\u001b[39m \u001b[39m1e-8\u001b[39m:\n\u001b[0;32m     80\u001b[0m         \u001b[39mreturn\u001b[39;00m b, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:5\u001b[0m, in \u001b[0;36m_check_nan\u001b[1;34m(vec, msg)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_nan\u001b[39m(vec, msg):\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(vec)\u001b[39m.\u001b[39many():\n\u001b[0;32m      6\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # normal loss\n",
    "    # print(n_st_train)\n",
    "    # print(PINNBCGD.output.weight.dtype)\n",
    "    v1_hat = PINNCGD(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(),\n",
    "                          create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    loss1 = D_CGD(n_st_train)[:, [0]] * torch.abs(dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNCGD(bc_st_train)\n",
    "    # print(bc_v_train)\n",
    "    # print('111111111111111111111')\n",
    "    # print(bc_hat)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    loss2 = D_CGD(bc_st_train)[:, [1]] * torch.abs(bc_hat - bc_v_train)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    combined_loss = (loss1.mean() + loss2.mean())\n",
    "    pinn_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    # combined_loss.backward()\n",
    "    optimizer.step(-combined_loss, combined_loss, 0)\n",
    "    # optimizer.step(combined_loss)\n",
    "    \n",
    "    loss_hist.append(pinn_loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'''{epoch}/{max_iter} mse loss: {pinn_loss.item():5f}, \n",
    "              PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, nn loss: {combined_loss.item():5f}''')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "ax.plot(range(30000), loss_hist, marker='o', linestyle='-', color='b', label='Data Points')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_title('Simple Python Plot')\n",
    "\n",
    "# Show legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
