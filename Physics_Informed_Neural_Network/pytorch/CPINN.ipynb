{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 25756696576, reserved: 0, free: 0\n"
     ]
    }
   ],
   "source": [
    "utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 50000, \"bc\":5000, \"fc\":5000}\n",
    "\n",
    "K = 40.0\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1.0\n",
    "S_range = [0.0, 130.0]\n",
    "t_range = [0.0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "layers = np.array([2, 50, 50, 50, 1])\n",
    "\n",
    "# Create the model\n",
    "PINNBCGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "PINNBCGD.to(device)\n",
    "print(PINNBCGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_BCGD = networks.Discriminator(2, 25, 1)\n",
    "D_BCGD.to(device)\n",
    "D_BCGD.load_state_dict(D_BCGD.state_dict()) # copy weights and stuff\n",
    "print(D_BCGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 60000\n",
    "graphPer = 0\n",
    "iter_recordBCGD = 200\n",
    "\n",
    "savePer = 60000\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = CGDs.BCGD(max_params=D_BCGD.parameters(), min_params= PINNBCGD.parameters(), device = device,\n",
    "                 lr_max=3e-5, lr_min=3e-5, tol=1e-10, collect_info=True)\n",
    "lossFunction = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00067, BC Loss: 3559.49292, total loss: 3559.493652, neural network loss: 1333.793213\n",
      "500/60000 PDE Loss: 0.13118, BC Loss: 101.86180, total loss: 101.992981, neural network loss: 145.076080\n",
      "1000/60000 PDE Loss: 0.21456, BC Loss: 98.79087, total loss: 99.005432, neural network loss: 146.534714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m grads \u001b[39m=\u001b[39m tgrad\u001b[39m.\u001b[39mgrad(v1_hat, n_st_train, grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones(v1_hat\u001b[39m.\u001b[39mshape)\u001b[39m.\u001b[39mcuda(), \n\u001b[0;32m     37\u001b[0m                    retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, only_inputs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     38\u001b[0m dVdt, dVdS \u001b[39m=\u001b[39m grads[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), grads[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m grads2nd \u001b[39m=\u001b[39m tgrad\u001b[39m.\u001b[39;49mgrad(dVdS, n_st_train, grad_outputs\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones(dVdS\u001b[39m.\u001b[39;49mshape)\u001b[39m.\u001b[39;49mcuda(), create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, only_inputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m d2VdS2 \u001b[39m=\u001b[39m grads2nd[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m S1 \u001b[39m=\u001b[39m n_st_train[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # normal loss\n",
    "    # print(n_st_train)\n",
    "    # print(PINNBCGD.output.weight.dtype)\n",
    "    v1_hat = PINNBCGD(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    D_output1 = D_BCGD(n_st_train)\n",
    "    loss1 = D_output1 * pde_loss\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNBCGD(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    loss2 = D_BCGD(bc_st_train) * bc_loss\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    combined_loss = loss1.mean() + loss2.mean()\n",
    "    pinn_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    # combined_loss.backward()\n",
    "    optimizer.step(combined_loss)\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{max_iter} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, total loss: {pinn_loss.item():5f}, neural network loss: {combined_loss.item():5f}')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
