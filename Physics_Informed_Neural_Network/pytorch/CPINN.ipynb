{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 25756696576, reserved: 0, free: 0\n"
     ]
    }
   ],
   "source": [
    "utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 50000, \"bc\":5000, \"fc\":5000}\n",
    "\n",
    "K = 40.0\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1.0\n",
    "S_range = [0.0, 130.0]\n",
    "t_range = [0.0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "# Create the model\n",
    "PINNBCGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "PINNBCGD.to(device)\n",
    "print(PINNBCGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_BCGD = networks.Discriminator(2, 25, 1)\n",
    "D_BCGD.to(device)\n",
    "D_BCGD.load_state_dict(D_BCGD.state_dict()) # copy weights and stuff\n",
    "print(D_BCGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 60000\n",
    "graphPer = 0\n",
    "iter_recordBCGD = 200\n",
    "\n",
    "savePer = 60000\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = CGDs.BCGD(max_params=D_BCGD.parameters(), min_params=PINNBCGD.parameters(), device = device,\n",
    "                 lr_max=0.0002, lr_min=0.0002, tol=1e-10, collect_info=True)\n",
    "lossFunction = nn.MSELoss()\n",
    "lossfunction2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00034, BC Loss: 3800.47852,                   mse loss: 3800.478760, nn loss: -16.440866\n",
      "500/60000 PDE Loss: 0.01276, BC Loss: 78.86437,                   mse loss: 78.877129, nn loss: 0.565432\n",
      "1000/60000 PDE Loss: 0.02036, BC Loss: 78.35330,                   mse loss: 78.373657, nn loss: 1.658333\n",
      "1500/60000 PDE Loss: 0.05384, BC Loss: 77.16500,                   mse loss: 77.218849, nn loss: 3.823408\n",
      "2000/60000 PDE Loss: 0.30771, BC Loss: 73.19434,                   mse loss: 73.502052, nn loss: 29.312267\n",
      "2500/60000 PDE Loss: 213.61203, BC Loss: 268.33319,                   mse loss: 481.945221, nn loss: 37.108009\n",
      "3000/60000 PDE Loss: 0.27071, BC Loss: 47.18174,                   mse loss: 47.452450, nn loss: 4.568260\n",
      "3500/60000 PDE Loss: 339.85443, BC Loss: 534.69067,                   mse loss: 874.545105, nn loss: 13.844949\n",
      "4000/60000 PDE Loss: 2.21034, BC Loss: 47.07934,                   mse loss: 49.289677, nn loss: 27.252914\n",
      "4500/60000 PDE Loss: 20.69425, BC Loss: 28.92205,                   mse loss: 49.616299, nn loss: -2.902919\n",
      "5000/60000 PDE Loss: 33.93310, BC Loss: 45.46328,                   mse loss: 79.396378, nn loss: 2.766714\n",
      "5500/60000 PDE Loss: 7.79450, BC Loss: 29.40602,                   mse loss: 37.200516, nn loss: -3.193401\n",
      "6000/60000 PDE Loss: 14.25570, BC Loss: 22.66540,                   mse loss: 36.921097, nn loss: 0.427070\n",
      "6500/60000 PDE Loss: 2.47744, BC Loss: 10.16480,                   mse loss: 12.642241, nn loss: 3.004180\n",
      "7000/60000 PDE Loss: 9.02811, BC Loss: 12.47718,                   mse loss: 21.505287, nn loss: -1.475311\n",
      "7500/60000 PDE Loss: 17.43563, BC Loss: 22.37913,                   mse loss: 39.814766, nn loss: 0.824256\n",
      "8000/60000 PDE Loss: 0.49798, BC Loss: 8.02428,                   mse loss: 8.522260, nn loss: 1.673694\n",
      "8500/60000 PDE Loss: 0.47763, BC Loss: 1.39766,                   mse loss: 1.875290, nn loss: 0.759828\n",
      "9000/60000 PDE Loss: 1.52986, BC Loss: 6.60829,                   mse loss: 8.138152, nn loss: -1.214511\n",
      "9500/60000 PDE Loss: 1.47538, BC Loss: 6.46997,                   mse loss: 7.945351, nn loss: 1.296457\n",
      "10000/60000 PDE Loss: 0.14585, BC Loss: 0.59839,                   mse loss: 0.744245, nn loss: 0.058144\n",
      "10500/60000 PDE Loss: 0.12122, BC Loss: 4.10883,                   mse loss: 4.230042, nn loss: -0.956085\n",
      "11000/60000 PDE Loss: 1.75781, BC Loss: 7.70175,                   mse loss: 9.459565, nn loss: 0.881823\n",
      "11500/60000 PDE Loss: 0.33092, BC Loss: 1.30630,                   mse loss: 1.637225, nn loss: 0.659415\n",
      "12000/60000 PDE Loss: 0.15979, BC Loss: 1.41772,                   mse loss: 1.577514, nn loss: -0.838375\n",
      "12500/60000 PDE Loss: 3.51054, BC Loss: 5.66905,                   mse loss: 9.179586, nn loss: -0.067882\n",
      "13000/60000 PDE Loss: 0.64571, BC Loss: 2.58649,                   mse loss: 3.232198, nn loss: 0.625448\n",
      "13500/60000 PDE Loss: 0.08379, BC Loss: 1.64652,                   mse loss: 1.730313, nn loss: 0.143974\n",
      "14000/60000 PDE Loss: 0.11885, BC Loss: 2.74651,                   mse loss: 2.865366, nn loss: -0.234902\n",
      "14500/60000 PDE Loss: 0.14468, BC Loss: 3.99150,                   mse loss: 4.136174, nn loss: -0.425546\n",
      "15000/60000 PDE Loss: 0.06910, BC Loss: 5.22089,                   mse loss: 5.289981, nn loss: 0.137779\n",
      "15500/60000 PDE Loss: 0.35546, BC Loss: 1.45491,                   mse loss: 1.810371, nn loss: 0.612005\n",
      "16000/60000 PDE Loss: 0.14646, BC Loss: 0.46788,                   mse loss: 0.614343, nn loss: -0.479099\n",
      "16500/60000 PDE Loss: 0.77037, BC Loss: 2.93450,                   mse loss: 3.704868, nn loss: -0.328516\n",
      "17000/60000 PDE Loss: 6.86601, BC Loss: 7.71851,                   mse loss: 14.584519, nn loss: -0.038901\n",
      "17500/60000 PDE Loss: 2.64611, BC Loss: 3.89779,                   mse loss: 6.543896, nn loss: 0.570516\n",
      "18000/60000 PDE Loss: 0.12540, BC Loss: 0.78061,                   mse loss: 0.906014, nn loss: 0.283113\n",
      "18500/60000 PDE Loss: 0.28892, BC Loss: 0.59691,                   mse loss: 0.885826, nn loss: -0.108255\n",
      "19000/60000 PDE Loss: 0.10681, BC Loss: 3.29718,                   mse loss: 3.403991, nn loss: -0.404921\n",
      "19500/60000 PDE Loss: 0.04589, BC Loss: 2.15228,                   mse loss: 2.198168, nn loss: 0.309898\n",
      "20000/60000 PDE Loss: 2.63910, BC Loss: 2.62051,                   mse loss: 5.259610, nn loss: -0.071983\n",
      "20500/60000 PDE Loss: 2.63753, BC Loss: 2.59617,                   mse loss: 5.233704, nn loss: 0.307495\n",
      "21000/60000 PDE Loss: 1.26579, BC Loss: 1.66671,                   mse loss: 2.932495, nn loss: -0.202841\n",
      "21500/60000 PDE Loss: 6.33881, BC Loss: 7.12850,                   mse loss: 13.467311, nn loss: -0.092831\n",
      "22000/60000 PDE Loss: 3.20966, BC Loss: 4.63905,                   mse loss: 7.848715, nn loss: -0.131014\n",
      "22500/60000 PDE Loss: 0.21568, BC Loss: 2.02705,                   mse loss: 2.242733, nn loss: 0.159202\n",
      "23000/60000 PDE Loss: 0.54246, BC Loss: 0.70960,                   mse loss: 1.252064, nn loss: 0.177447\n",
      "23500/60000 PDE Loss: 1.69035, BC Loss: 9.48857,                   mse loss: 11.178923, nn loss: -0.594751\n",
      "24000/60000 PDE Loss: 0.13597, BC Loss: 36.46320,                   mse loss: 36.599174, nn loss: 46.262405\n",
      "24500/60000 PDE Loss: 0.01722, BC Loss: 51.85109,                   mse loss: 51.868309, nn loss: 37.811539\n",
      "25000/60000 PDE Loss: 6.14280, BC Loss: 61.42031,                   mse loss: 67.563110, nn loss: -1.678797\n",
      "25500/60000 PDE Loss: 14.25446, BC Loss: 38.96611,                   mse loss: 53.220573, nn loss: -1.536253\n",
      "26000/60000 PDE Loss: 7.29780, BC Loss: 37.42434,                   mse loss: 44.722149, nn loss: 1.899429\n",
      "26500/60000 PDE Loss: 1.22960, BC Loss: 20.42961,                   mse loss: 21.659218, nn loss: 1.053176\n",
      "27000/60000 PDE Loss: 7.60238, BC Loss: 27.51110,                   mse loss: 35.113480, nn loss: -3.314261\n",
      "27500/60000 PDE Loss: 32.15071, BC Loss: 82.65031,                   mse loss: 114.801025, nn loss: 0.388903\n",
      "28000/60000 PDE Loss: 0.45525, BC Loss: 13.77376,                   mse loss: 14.229010, nn loss: 1.345477\n",
      "28500/60000 PDE Loss: 1.12073, BC Loss: 11.66479,                   mse loss: 12.785522, nn loss: 2.104677\n",
      "29000/60000 PDE Loss: 2.50130, BC Loss: 9.92504,                   mse loss: 12.426336, nn loss: 1.733600\n",
      "29500/60000 PDE Loss: 1.72131, BC Loss: 2.43503,                   mse loss: 4.156343, nn loss: 0.698619\n",
      "30000/60000 PDE Loss: 0.61645, BC Loss: 3.06884,                   mse loss: 3.685287, nn loss: 0.722106\n",
      "30500/60000 PDE Loss: 4.53072, BC Loss: 4.78784,                   mse loss: 9.318553, nn loss: -0.298869\n",
      "31000/60000 PDE Loss: 7.05210, BC Loss: 8.26257,                   mse loss: 15.314665, nn loss: 0.251423\n",
      "31500/60000 PDE Loss: 3.75469, BC Loss: 6.80619,                   mse loss: 10.560876, nn loss: 0.126813\n",
      "32000/60000 PDE Loss: 0.28223, BC Loss: 3.20362,                   mse loss: 3.485851, nn loss: 0.072870\n",
      "32500/60000 PDE Loss: 0.07487, BC Loss: 1.06990,                   mse loss: 1.144772, nn loss: -0.010082\n",
      "33000/60000 PDE Loss: 0.21340, BC Loss: 0.30805,                   mse loss: 0.521454, nn loss: 0.150380\n",
      "33500/60000 PDE Loss: 0.04027, BC Loss: 0.34588,                   mse loss: 0.386149, nn loss: 0.000792\n",
      "34000/60000 PDE Loss: 0.16525, BC Loss: 0.46768,                   mse loss: 0.632923, nn loss: 0.020664\n",
      "34500/60000 PDE Loss: 0.71995, BC Loss: 0.68593,                   mse loss: 1.405879, nn loss: -0.023180\n",
      "35000/60000 PDE Loss: 1.20708, BC Loss: 0.88494,                   mse loss: 2.092018, nn loss: 0.038138\n",
      "35500/60000 PDE Loss: 0.12862, BC Loss: 0.36155,                   mse loss: 0.490169, nn loss: 0.050433\n",
      "36000/60000 PDE Loss: 0.06782, BC Loss: 0.08803,                   mse loss: 0.155849, nn loss: 0.030651\n",
      "36500/60000 PDE Loss: 0.07944, BC Loss: 0.15042,                   mse loss: 0.229854, nn loss: -0.019826\n",
      "37000/60000 PDE Loss: 0.05670, BC Loss: 0.05808,                   mse loss: 0.114781, nn loss: 0.026395\n",
      "37500/60000 PDE Loss: 0.01719, BC Loss: 0.08814,                   mse loss: 0.105332, nn loss: -0.008299\n",
      "38000/60000 PDE Loss: 0.05037, BC Loss: 0.14803,                   mse loss: 0.198401, nn loss: 0.002000\n",
      "38500/60000 PDE Loss: 0.11406, BC Loss: 0.25331,                   mse loss: 0.367363, nn loss: -0.013637\n",
      "39000/60000 PDE Loss: 0.14306, BC Loss: 0.25809,                   mse loss: 0.401144, nn loss: -0.006166\n",
      "39500/60000 PDE Loss: 0.07001, BC Loss: 0.12024,                   mse loss: 0.190247, nn loss: -0.022787\n",
      "40000/60000 PDE Loss: 0.03499, BC Loss: 0.13568,                   mse loss: 0.170671, nn loss: -0.002820\n",
      "40500/60000 PDE Loss: 0.02332, BC Loss: 0.17111,                   mse loss: 0.194424, nn loss: -0.007215\n",
      "41000/60000 PDE Loss: 0.02409, BC Loss: 0.06323,                   mse loss: 0.087321, nn loss: -0.009676\n",
      "41500/60000 PDE Loss: 0.03442, BC Loss: 0.08593,                   mse loss: 0.120351, nn loss: -0.006408\n",
      "42000/60000 PDE Loss: 0.05058, BC Loss: 0.11214,                   mse loss: 0.162727, nn loss: -0.011676\n",
      "42500/60000 PDE Loss: 0.05356, BC Loss: 0.08478,                   mse loss: 0.138340, nn loss: -0.013125\n",
      "43000/60000 PDE Loss: 0.05744, BC Loss: 0.08886,                   mse loss: 0.146307, nn loss: -0.013507\n",
      "43500/60000 PDE Loss: 0.06487, BC Loss: 0.11121,                   mse loss: 0.176082, nn loss: -0.016451\n",
      "44000/60000 PDE Loss: 0.05960, BC Loss: 0.09552,                   mse loss: 0.155124, nn loss: -0.016955\n",
      "44500/60000 PDE Loss: 0.05622, BC Loss: 0.09926,                   mse loss: 0.155478, nn loss: -0.018040\n",
      "45000/60000 PDE Loss: 0.06028, BC Loss: 0.10274,                   mse loss: 0.163011, nn loss: -0.019530\n",
      "45500/60000 PDE Loss: 0.06143, BC Loss: 0.10918,                   mse loss: 0.170611, nn loss: -0.019809\n",
      "46000/60000 PDE Loss: 0.06225, BC Loss: 0.11959,                   mse loss: 0.181836, nn loss: -0.020409\n",
      "46500/60000 PDE Loss: 0.06666, BC Loss: 0.11881,                   mse loss: 0.185470, nn loss: -0.020806\n",
      "47000/60000 PDE Loss: 0.06809, BC Loss: 0.12333,                   mse loss: 0.191419, nn loss: -0.020715\n",
      "47500/60000 PDE Loss: 0.06582, BC Loss: 0.13117,                   mse loss: 0.196986, nn loss: -0.020614\n",
      "48000/60000 PDE Loss: 0.06525, BC Loss: 0.12904,                   mse loss: 0.194285, nn loss: -0.020152\n",
      "48500/60000 PDE Loss: 0.06378, BC Loss: 0.12991,                   mse loss: 0.193690, nn loss: -0.019402\n",
      "49000/60000 PDE Loss: 0.06112, BC Loss: 0.13560,                   mse loss: 0.196715, nn loss: -0.018185\n",
      "49500/60000 PDE Loss: 0.05999, BC Loss: 0.13411,                   mse loss: 0.194108, nn loss: -0.016380\n",
      "50000/60000 PDE Loss: 0.05829, BC Loss: 0.13479,                   mse loss: 0.193072, nn loss: -0.013985\n",
      "50500/60000 PDE Loss: 0.05572, BC Loss: 0.14015,                   mse loss: 0.195864, nn loss: -0.010946\n",
      "51000/60000 PDE Loss: 0.05364, BC Loss: 0.14140,                   mse loss: 0.195036, nn loss: -0.007394\n",
      "51500/60000 PDE Loss: 0.05130, BC Loss: 0.14449,                   mse loss: 0.195782, nn loss: -0.003368\n",
      "52000/60000 PDE Loss: 0.05130, BC Loss: 0.13799,                   mse loss: 0.189293, nn loss: 0.001658\n",
      "52500/60000 PDE Loss: 0.04776, BC Loss: 0.12758,                   mse loss: 0.175344, nn loss: 0.006409\n",
      "53000/60000 PDE Loss: 0.04165, BC Loss: 0.11660,                   mse loss: 0.158249, nn loss: 0.011364\n",
      "53500/60000 PDE Loss: 0.03696, BC Loss: 0.10749,                   mse loss: 0.144457, nn loss: 0.017407\n",
      "54000/60000 PDE Loss: 0.03237, BC Loss: 0.10194,                   mse loss: 0.134307, nn loss: 0.022507\n",
      "54500/60000 PDE Loss: 0.02939, BC Loss: 0.09359,                   mse loss: 0.122985, nn loss: 0.027496\n",
      "55000/60000 PDE Loss: 0.02647, BC Loss: 0.08220,                   mse loss: 0.108665, nn loss: 0.033702\n",
      "55500/60000 PDE Loss: 0.02271, BC Loss: 0.06754,                   mse loss: 0.090246, nn loss: 0.039408\n",
      "56000/60000 PDE Loss: 0.01730, BC Loss: 0.06341,                   mse loss: 0.080714, nn loss: 0.040356\n",
      "56500/60000 PDE Loss: 0.01364, BC Loss: 0.05790,                   mse loss: 0.071539, nn loss: 0.041860\n",
      "57000/60000 PDE Loss: 0.01150, BC Loss: 0.03069,                   mse loss: 0.042191, nn loss: 0.042722\n",
      "57500/60000 PDE Loss: 0.00981, BC Loss: 0.01819,                   mse loss: 0.027998, nn loss: 0.041461\n",
      "58000/60000 PDE Loss: 0.00766, BC Loss: 0.01382,                   mse loss: 0.021485, nn loss: 0.039692\n",
      "58500/60000 PDE Loss: 0.00650, BC Loss: 0.01120,                   mse loss: 0.017699, nn loss: 0.037698\n",
      "59000/60000 PDE Loss: 0.00553, BC Loss: 0.00981,                   mse loss: 0.015339, nn loss: 0.035433\n",
      "59500/60000 PDE Loss: 0.00464, BC Loss: 0.00866,                   mse loss: 0.013297, nn loss: 0.033397\n",
      "run time: 1812.955014705658\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # normal loss\n",
    "    # print(n_st_train)\n",
    "    # print(PINNBCGD.output.weight.dtype)\n",
    "    v1_hat = PINNBCGD(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    loss1 = D_BCGD(n_st_train) * (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNBCGD(bc_st_train)\n",
    "    # print(bc_v_train)\n",
    "    # print('111111111111111111111')\n",
    "    # print(bc_hat)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    loss2 = D_BCGD(bc_st_train) * (bc_hat - bc_v_train)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    combined_loss = loss1.mean() + loss2.mean()\n",
    "    pinn_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    # combined_loss.backward()\n",
    "    optimizer.step(combined_loss)\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{max_iter} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, \\\n",
    "                  mse loss: {pinn_loss.item():5f}, nn loss: {combined_loss.item():5f}')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
