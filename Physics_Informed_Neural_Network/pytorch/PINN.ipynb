{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "import networks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "fnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m lossFunction \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchimize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m lsq_lma\n\u001b[1;32m---> 23\u001b[0m coeffs_list \u001b[39m=\u001b[39m lsq_lma(fnn\u001b[39m.\u001b[39;49mparameters(), function\u001b[39m=\u001b[39;49mlossFunction)\n\u001b[0;32m     24\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(fnn\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00002\u001b[39m)\n\u001b[0;32m     26\u001b[0m samples \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpde\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m50000\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbc\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m5000\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfc\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m5000\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torchimize\\functions\\single\\lma_fun_single.py:71\u001b[0m, in \u001b[0;36mlsq_lma\u001b[1;34m(p, function, jac_function, args, ftol, ptol, gtol, tau, meth, rho1, rho2, bet, gam, max_iter)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     jac_fun \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: jac_function(p, \u001b[39m*\u001b[39margs)\n\u001b[1;32m---> 71\u001b[0m f \u001b[39m=\u001b[39m fun(p)\n\u001b[0;32m     72\u001b[0m j \u001b[39m=\u001b[39m jac_fun(p)\n\u001b[0;32m     73\u001b[0m g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(j\u001b[39m.\u001b[39mT, f)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "import torchimize\n",
    "\n",
    "\n",
    "# parallel levenberg-marquardt for several optimization problems at multiple costs\n",
    "# from torchimize.functions import lsq_lma_parallel\n",
    "# coeffs_list = lsq_lma_parallel(\n",
    "#                     p = initials_batch,\n",
    "#                     function = multi_cost_fun_batch,\n",
    "#                     jac_function = multi_jac_fun_batch,\n",
    "#                     args = (other_args,),\n",
    "#                     wvec = torch.ones(5, device='cuda', dtype=initials_batch.dtype),\n",
    "#                     ftol = 1e-8,\n",
    "#                     ptol = 1e-8,\n",
    "#                     gtol = 1e-8,\n",
    "#                     meth = 'marq',\n",
    "#                     max_iter = 40,\n",
    "#                 )\n",
    "\n",
    "n_epochs = 60000\n",
    "lossFunction = nn.MSELoss()\n",
    "\n",
    "from torchimize.functions import lsq_lma\n",
    "coeffs_list = lsq_lma(fnn.parameters(), function=lossFunction)\n",
    "optimizer = optim.Adam(fnn.parameters(), lr=0.00002)\n",
    "\n",
    "samples = {\"pde\": 50000, \"bc\":5000, \"fc\":5000}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "- For each iteration in the training loop, we are sampling data for the three physical conditions of the PDE.\n",
    "- Then we are calculating the loss three times on the same model, accumulating them into a combined objective function to be minimised for the Neural Network.\n",
    "- The first loss is the differential equation loss. Here we are trying to minimise the PDE by calculating gradients and forming the PDE itself.\n",
    "- The remaining losses are calculated for boundary value and initial value conditions for the PDE.\n",
    "- Mean Squared Error loss function `nn.MSELoss()` is chosen as the criterion to be minimised and \n",
    "- Adam optimizer `nn.optim.Adam(lr=3e-5)` with a learning rate of 0.00003 is chosen for performing the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # PDE Round\n",
    "    y1_hat = fnn(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads)\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads2nd)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    # print(d2VdS2)\n",
    "    # for item in d2VdS2:\n",
    "    #     if item[0] != 0.0:\n",
    "    #         print(item)\n",
    "    # if d2VdS2[0][0] != 0.0:\n",
    "    #     print(d2VdS2) \n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    \n",
    "    # BC Round\n",
    "    y21_hat = fnn(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y21_hat)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    coeffs_list.zero_grad()\n",
    "    combined_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    combined_loss.backward()\n",
    "    coeffs_list.step()\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, total loss: {combined_loss.item():5f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
