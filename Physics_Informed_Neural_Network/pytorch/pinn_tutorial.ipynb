{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider Net as our solution u_theta(x,t)\n",
    "\n",
    "\"\"\"\n",
    "When forming the network, we have to keep in mind the number of inputs and outputs\n",
    "In ur case: #inputs = 2 (x,t)\n",
    "and #outputs = 1\n",
    "\n",
    "You can add ass many hidden layers as you want with as many neurons.\n",
    "More complex the network, the more prepared it is to find complex solutions, but it also requires more data.\n",
    "\n",
    "Let us create this network:\n",
    "min 5 hidden layer with 5 neurons each.\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(2,5)\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.hidden_layer5 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (2) Model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDE as loss function. Thus would use the network which we call as u_theta\n",
    "def f(x,t, net):\n",
    "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
    "    ## Based on our f = du/dx - 2du/dt - u, we need du/dx and du/dt\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_x - 2*u_t - u\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data from Boundary Conditions\n",
    "# u(x,0)=6e^(-3x)\n",
    "## BC just gives us datapoints for training\n",
    "\n",
    "# BC tells us that for any x in range[0,2] and time=0, the value of u is given by 6e^(-3x)\n",
    "# Take say 500 random numbers of x\n",
    "x_bc = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
    "t_bc = np.zeros((500,1))\n",
    "# compute u based on BC\n",
    "u_bc = 6*np.exp(-3*x_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (3) Training / Fitting\n",
    "iterations = 10000\n",
    "previous_validation_loss = 99999999.0\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    \n",
    "    # Loss based on boundary conditions\n",
    "    pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "    pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "    pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    net_bc_out = net(pt_x_bc, pt_t_bc) # output of u(x,t)\n",
    "    mse_u = mse_cost_function(net_bc_out, pt_u_bc)\n",
    "    \n",
    "    # Loss based on PDE\n",
    "    x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
    "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "    all_zeros = np.zeros((500,1))\n",
    "    \n",
    "    \n",
    "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    f_out = f(pt_x_collocation, pt_t_collocation, net) # output of f(x,t)\n",
    "    mse_f = mse_cost_function(f_out, pt_all_zeros)\n",
    "    \n",
    "    # Combining the loss functions\n",
    "    loss = mse_u + mse_f\n",
    "    \n",
    "    \n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch,\"Traning Loss:\",loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "x=np.arange(0,2,0.02)\n",
    "t=np.arange(0,1,0.02)\n",
    "ms_x, ms_t = np.meshgrid(x, t)\n",
    "## Just because meshgrid is used, we need to do the following adjustment\n",
    "x = np.ravel(ms_x).reshape(-1,1)\n",
    "t = np.ravel(ms_t).reshape(-1,1)\n",
    "\n",
    "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)\n",
    "pt_u = net(pt_x,pt_t)\n",
    "u=pt_u.data.cpu().numpy()\n",
    "ms_u = u.reshape(ms_x.shape)\n",
    "\n",
    "surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "             \n",
    "             \n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(net.state_dict(), \"model_uxt.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
