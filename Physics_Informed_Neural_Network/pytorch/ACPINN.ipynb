{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 50000, \"bc\":5000, \"fc\":5000}\n",
    "\n",
    "K = 40.0\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1.0\n",
    "S_range = [0.0, 130.0]\n",
    "t_range = [0.0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "# Create the model\n",
    "PINNGACGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "PINNGACGD.to(device)\n",
    "print(PINNGACGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_GACGD = networks.Discriminator(2, 25, 1)\n",
    "D_GACGD.to(device)\n",
    "D_GACGD.load_state_dict(D_GACGD.state_dict()) # copy weights and stuff\n",
    "print(D_GACGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30000\n",
    "\n",
    "# Define loss function and optimizer\n",
    "tol = 1e-7\n",
    "atol = 1e-20\n",
    "g_iter = 1000\n",
    "lr = 0.004\n",
    "track_cond = lambda x, y:  True\n",
    "\n",
    "optimizer = CGDs.GACGD(x_params=D_GACGD.parameters(), y_params = PINNGACGD.parameters(), max_iter = g_iter,\n",
    "            lr_x=lr, lr_y=lr, tol=tol, atol = atol, eps=1e-8, beta=0.99, track_cond = track_cond)\n",
    "lossFunction = nn.MSELoss()\n",
    "lossfunction2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengr\\anaconda3\\lib\\site-packages\\CGDs\\gmres_torch.py:124: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2197.)\n",
      "  y, _ = torch.triangular_solve(beta[0:j + 1].unsqueeze(-1), H[0:j + 1, 0:j + 1])  # j x j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 PDE Loss: 0.00043, BC Loss: 3355.69971,                   mse loss: 3355.700195, nn loss: 31.829561\n",
      "50/30000 PDE Loss: 29.58006, BC Loss: 54.00415,                   mse loss: 83.584213, nn loss: 0.989069\n",
      "100/30000 PDE Loss: 7.43215, BC Loss: 6.53805,                   mse loss: 13.970201, nn loss: -31.367092\n",
      "150/30000 PDE Loss: 11.27877, BC Loss: 15.98076,                   mse loss: 27.259525, nn loss: 22.534605\n",
      "200/30000 PDE Loss: 6.26722, BC Loss: 5.74800,                   mse loss: 12.015221, nn loss: 10.084473\n",
      "250/30000 PDE Loss: 2.82239, BC Loss: 1.46543,                   mse loss: 4.287813, nn loss: 28.946959\n",
      "300/30000 PDE Loss: 0.38315, BC Loss: 0.19079,                   mse loss: 0.573934, nn loss: 20.032360\n",
      "350/30000 PDE Loss: 0.07494, BC Loss: 0.04639,                   mse loss: 0.121323, nn loss: 7.415416\n",
      "400/30000 PDE Loss: 0.06898, BC Loss: 0.02136,                   mse loss: 0.090336, nn loss: 1.762081\n",
      "450/30000 PDE Loss: 0.08340, BC Loss: 0.01771,                   mse loss: 0.101103, nn loss: 2.121652\n",
      "500/30000 PDE Loss: 0.10178, BC Loss: 0.02474,                   mse loss: 0.126520, nn loss: 5.167575\n",
      "550/30000 PDE Loss: 0.08139, BC Loss: 0.01191,                   mse loss: 0.093301, nn loss: 7.606192\n",
      "600/30000 PDE Loss: 0.06997, BC Loss: 0.01651,                   mse loss: 0.086477, nn loss: 18.079113\n",
      "650/30000 PDE Loss: 0.05386, BC Loss: 0.00360,                   mse loss: 0.057454, nn loss: 17.853661\n",
      "700/30000 PDE Loss: 0.04912, BC Loss: 0.00773,                   mse loss: 0.056851, nn loss: 20.042095\n",
      "750/30000 PDE Loss: 0.04803, BC Loss: 0.01003,                   mse loss: 0.058057, nn loss: 11.690784\n",
      "800/30000 PDE Loss: 0.04901, BC Loss: 0.01606,                   mse loss: 0.065078, nn loss: 62.014435\n",
      "850/30000 PDE Loss: 0.04323, BC Loss: 0.01550,                   mse loss: 0.058732, nn loss: 224.049103\n",
      "900/30000 PDE Loss: 0.03411, BC Loss: 0.01297,                   mse loss: 0.047082, nn loss: 841.654419\n",
      "950/30000 PDE Loss: 0.04301, BC Loss: 0.00665,                   mse loss: 0.049665, nn loss: 1721.209839\n",
      "1000/30000 PDE Loss: 0.03055, BC Loss: 0.00484,                   mse loss: 0.035394, nn loss: 3188.603271\n",
      "1050/30000 PDE Loss: 0.02715, BC Loss: 0.00209,                   mse loss: 0.029243, nn loss: 4578.632812\n",
      "1100/30000 PDE Loss: 0.02037, BC Loss: 0.00704,                   mse loss: 0.027419, nn loss: 8262.522461\n",
      "1150/30000 PDE Loss: 0.01051, BC Loss: 0.00268,                   mse loss: 0.013195, nn loss: 9018.767578\n",
      "1200/30000 PDE Loss: 0.00801, BC Loss: 0.00164,                   mse loss: 0.009657, nn loss: 8149.711914\n",
      "1250/30000 PDE Loss: 0.00988, BC Loss: 0.00452,                   mse loss: 0.014400, nn loss: 8535.489258\n",
      "1300/30000 PDE Loss: 0.00758, BC Loss: 0.00088,                   mse loss: 0.008463, nn loss: 7923.225098\n",
      "1350/30000 PDE Loss: 0.01225, BC Loss: 0.00093,                   mse loss: 0.013180, nn loss: 10699.244141\n",
      "1400/30000 PDE Loss: 0.05371, BC Loss: 0.03339,                   mse loss: 0.087108, nn loss: 15480.137695\n",
      "1450/30000 PDE Loss: 0.00567, BC Loss: 0.00083,                   mse loss: 0.006499, nn loss: 2828.112549\n",
      "1500/30000 PDE Loss: 0.00369, BC Loss: 0.00067,                   mse loss: 0.004359, nn loss: 374.049194\n",
      "1550/30000 PDE Loss: 0.00430, BC Loss: 0.00112,                   mse loss: 0.005411, nn loss: 125.073090\n",
      "1600/30000 PDE Loss: 0.00541, BC Loss: 0.00237,                   mse loss: 0.007781, nn loss: 3412.364502\n",
      "1650/30000 PDE Loss: 0.00493, BC Loss: 0.00668,                   mse loss: 0.011612, nn loss: 1448.254517\n",
      "1700/30000 PDE Loss: 0.00456, BC Loss: 0.00315,                   mse loss: 0.007708, nn loss: 3004.184326\n",
      "1750/30000 PDE Loss: 0.00428, BC Loss: 0.00224,                   mse loss: 0.006523, nn loss: 7013.031250\n",
      "1800/30000 PDE Loss: 0.00342, BC Loss: 0.00133,                   mse loss: 0.004751, nn loss: 10547.861328\n",
      "1850/30000 PDE Loss: 0.00325, BC Loss: 0.00176,                   mse loss: 0.005004, nn loss: 4712.409180\n",
      "1900/30000 PDE Loss: 0.00190, BC Loss: 0.00104,                   mse loss: 0.002936, nn loss: 5335.561035\n",
      "1950/30000 PDE Loss: 0.00197, BC Loss: 0.00085,                   mse loss: 0.002813, nn loss: 6976.527832\n",
      "2000/30000 PDE Loss: 0.00158, BC Loss: 0.00125,                   mse loss: 0.002830, nn loss: 13867.817383\n",
      "2050/30000 PDE Loss: 0.00235, BC Loss: 0.00096,                   mse loss: 0.003313, nn loss: 17189.304688\n",
      "2100/30000 PDE Loss: 0.00218, BC Loss: 0.00113,                   mse loss: 0.003309, nn loss: 25663.259766\n",
      "2150/30000 PDE Loss: 0.00194, BC Loss: 0.00125,                   mse loss: 0.003184, nn loss: 32337.630859\n",
      "2200/30000 PDE Loss: 0.00227, BC Loss: 0.00167,                   mse loss: 0.003940, nn loss: 44501.664062\n",
      "2250/30000 PDE Loss: 0.00217, BC Loss: 0.00193,                   mse loss: 0.004103, nn loss: 62095.539062\n",
      "2300/30000 PDE Loss: 0.00200, BC Loss: 0.00213,                   mse loss: 0.004131, nn loss: 88013.406250\n",
      "2350/30000 PDE Loss: 0.00192, BC Loss: 0.00166,                   mse loss: 0.003573, nn loss: 126442.578125\n",
      "2400/30000 PDE Loss: 0.00168, BC Loss: 0.00238,                   mse loss: 0.004061, nn loss: 175740.125000\n",
      "2450/30000 PDE Loss: 0.00168, BC Loss: 0.00135,                   mse loss: 0.003030, nn loss: 59829.796875\n",
      "2500/30000 PDE Loss: 0.00158, BC Loss: 0.00234,                   mse loss: 0.003914, nn loss: 50786.843750\n",
      "2550/30000 PDE Loss: 0.00159, BC Loss: 0.00127,                   mse loss: 0.002865, nn loss: 55911.644531\n",
      "2600/30000 PDE Loss: 0.00192, BC Loss: 0.00139,                   mse loss: 0.003309, nn loss: 59850.539062\n",
      "2650/30000 PDE Loss: 0.00218, BC Loss: 0.00109,                   mse loss: 0.003265, nn loss: 84613.710938\n",
      "2700/30000 PDE Loss: 0.00342, BC Loss: 0.00063,                   mse loss: 0.004042, nn loss: 77625.445312\n",
      "2750/30000 PDE Loss: 0.04458, BC Loss: 0.00851,                   mse loss: 0.053090, nn loss: -130975.234375\n",
      "2800/30000 PDE Loss: 0.00605, BC Loss: 0.00519,                   mse loss: 0.011240, nn loss: 51533.976562\n",
      "2850/30000 PDE Loss: 0.01484, BC Loss: 0.01120,                   mse loss: 0.026045, nn loss: 39467.437500\n",
      "2900/30000 PDE Loss: 0.00594, BC Loss: 0.00708,                   mse loss: 0.013020, nn loss: 37912.281250\n",
      "2950/30000 PDE Loss: 0.00336, BC Loss: 0.00354,                   mse loss: 0.006901, nn loss: 45299.976562\n",
      "3000/30000 PDE Loss: 0.00439, BC Loss: 0.00330,                   mse loss: 0.007682, nn loss: 62850.015625\n",
      "3050/30000 PDE Loss: 0.00522, BC Loss: 0.00358,                   mse loss: 0.008802, nn loss: 89573.046875\n",
      "3100/30000 PDE Loss: 0.00535, BC Loss: 0.00603,                   mse loss: 0.011383, nn loss: 141626.984375\n",
      "3150/30000 PDE Loss: 0.01413, BC Loss: 0.02263,                   mse loss: 0.036761, nn loss: 177424.781250\n",
      "3200/30000 PDE Loss: 0.00209, BC Loss: 0.00542,                   mse loss: 0.007503, nn loss: 166614.906250\n",
      "3250/30000 PDE Loss: 0.25674, BC Loss: 0.28666,                   mse loss: 0.543402, nn loss: 202796.312500\n",
      "3300/30000 PDE Loss: 0.00517, BC Loss: 0.00641,                   mse loss: 0.011574, nn loss: 131700.859375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m pinn_loss \u001b[39m=\u001b[39m pde_loss\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m bc_loss\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     60\u001b[0m \u001b[39m# combined_loss.backward()\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(\u001b[39m-\u001b[39;49mcombined_loss, combined_loss, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     63\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(combined_loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hengr\\anaconda3\\lib\\site-packages\\CGDs\\gmres_acgd.py:165\u001b[0m, in \u001b[0;36mGACGD.step\u001b[1;34m(self, loss_x, loss_y, trigger)\u001b[0m\n\u001b[0;32m    157\u001b[0m prev_x0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m Avp \u001b[39m=\u001b[39m partial(MvProd,\n\u001b[0;32m    159\u001b[0m               grad_fy\u001b[39m=\u001b[39mgrad_fy_vec, grad_gx\u001b[39m=\u001b[39mgrad_gx_vec,\n\u001b[0;32m    160\u001b[0m               x_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_params, y_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m               x_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_reducer, y_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_reducer,\n\u001b[0;32m    164\u001b[0m               rebuild\u001b[39m=\u001b[39mshould_rebuild)\n\u001b[1;32m--> 165\u001b[0m soln, (num_iter, err_history) \u001b[39m=\u001b[39m GMRES(Avp\u001b[39m=\u001b[39;49mAvp, b\u001b[39m=\u001b[39;49mRHS, x0\u001b[39m=\u001b[39;49mprev_x0,\n\u001b[0;32m    166\u001b[0m                                       max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    167\u001b[0m                                       tol\u001b[39m=\u001b[39;49mtol, atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m    168\u001b[0m                                       track\u001b[39m=\u001b[39;49mtrack_flag)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mx0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m soln\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    172\u001b[0m     {\n\u001b[0;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_x\u001b[39m\u001b[39m'\u001b[39m: sq_avg_x,\n\u001b[0;32m    174\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_y\u001b[39m\u001b[39m'\u001b[39m: sq_avg_y\n\u001b[0;32m    175\u001b[0m     }\n\u001b[0;32m    176\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\anaconda3\\lib\\site-packages\\CGDs\\gmres_torch.py:107\u001b[0m, in \u001b[0;36mGMRES\u001b[1;34m(Avp, b, x0, max_iter, tol, atol, track)\u001b[0m\n\u001b[0;32m    104\u001b[0m ss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_iter, device\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mdevice)  \u001b[39m# sine values at each step\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[1;32m--> 107\u001b[0m     p \u001b[39m=\u001b[39m Avp(V[j])\n\u001b[0;32m    108\u001b[0m     new_v \u001b[39m=\u001b[39m arnoldi(p, V, H, j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# Arnoldi iteration to get the j+1 th ba\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39m# sis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\anaconda3\\lib\\site-packages\\CGDs\\gmres_acgd.py:43\u001b[0m, in \u001b[0;36mMvProd\u001b[1;34m(vec, grad_fy, grad_gx, x_params, y_params, lr_x, lr_y, trigger, x_reducer, y_reducer, rebuild)\u001b[0m\n\u001b[0;32m     41\u001b[0m v1 \u001b[39m=\u001b[39m vec[\u001b[39m0\u001b[39m: len_x]\n\u001b[0;32m     42\u001b[0m v2 \u001b[39m=\u001b[39m vec[len_x: len_x \u001b[39m+\u001b[39m len_y]\n\u001b[1;32m---> 43\u001b[0m h1 \u001b[39m=\u001b[39m Hvp_vec(grad_fy, x_params, v2,\n\u001b[0;32m     44\u001b[0m              retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trigger\u001b[39m=\u001b[39;49mtrigger,\n\u001b[0;32m     45\u001b[0m              reducer\u001b[39m=\u001b[39;49mx_reducer,\n\u001b[0;32m     46\u001b[0m              rebuild\u001b[39m=\u001b[39;49mrebuild)\n\u001b[0;32m     47\u001b[0m p1 \u001b[39m=\u001b[39m v1 \u001b[39m+\u001b[39m lr_x \u001b[39m*\u001b[39m h1\n\u001b[0;32m     48\u001b[0m h2 \u001b[39m=\u001b[39m Hvp_vec(grad_gx, y_params, v1,\n\u001b[0;32m     49\u001b[0m              retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, trigger\u001b[39m=\u001b[39mtrigger,\n\u001b[0;32m     50\u001b[0m              reducer\u001b[39m=\u001b[39my_reducer,\n\u001b[0;32m     51\u001b[0m              rebuild\u001b[39m=\u001b[39mrebuild)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\anaconda3\\lib\\site-packages\\CGDs\\cgd_utils.py:113\u001b[0m, in \u001b[0;36mHvp_vec\u001b[1;34m(grad_vec, params, vec, retain_graph, trigger, reducer, rebuild)\u001b[0m\n\u001b[0;32m    109\u001b[0m autograd\u001b[39m.\u001b[39mbackward(grad_vec \u001b[39m+\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39m*\u001b[39m trigger, grad_tensors\u001b[39m=\u001b[39mvec,\n\u001b[0;32m    110\u001b[0m                   inputs\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    111\u001b[0m                   retain_graph\u001b[39m=\u001b[39mretain_graph)\n\u001b[0;32m    112\u001b[0m hvp \u001b[39m=\u001b[39m vectorize_grad(params)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(hvp)\u001b[39m.\u001b[39many():\n\u001b[0;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mhvp Nan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m hvp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # normal loss\n",
    "    # print(n_st_train)\n",
    "    # print(PINNBCGD.output.weight.dtype)\n",
    "    v1_hat = PINNGACGD(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    loss1 = D_GACGD(n_st_train) * (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNGACGD(bc_st_train)\n",
    "    # print(bc_v_train)\n",
    "    # print('111111111111111111111')\n",
    "    # print(bc_hat)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    loss2 = D_GACGD(bc_st_train) * (bc_hat - bc_v_train)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    combined_loss = (loss1.mean() + loss2.mean())\n",
    "    pinn_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    # combined_loss.backward()\n",
    "    optimizer.step(-combined_loss, combined_loss, 0)\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'{epoch}/{max_iter} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, \\\n",
    "                  mse loss: {pinn_loss.item():5f}')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
