{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import utils\n",
    "\n",
    "import CGDs\n",
    "import importlib\n",
    "importlib.reload(CGDs)\n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "from torch import from_numpy\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 50000, \"bc\":5000, \"fc\":5000}\n",
    "\n",
    "K = 40.0\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1.0\n",
    "S_range = [0.0, 130.0]\n",
    "t_range = [0.0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "# Create the model\n",
    "PINNGACGD = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "PINNGACGD.to(device)\n",
    "print(PINNGACGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (map): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D_GACGD = networks.Discriminator(2, 25, 1)\n",
    "D_GACGD.to(device)\n",
    "D_GACGD.load_state_dict(D_GACGD.state_dict()) # copy weights and stuff\n",
    "print(D_GACGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30000\n",
    "\n",
    "# Define loss function and optimizer\n",
    "tol = 1e-7\n",
    "atol = 1e-20\n",
    "g_iter = 1000\n",
    "lr = 0.004\n",
    "track_cond = lambda x, y:  True\n",
    "\n",
    "optimizer = CGDs.GACGD(x_params=D_GACGD.parameters(), y_params = PINNGACGD.parameters(), max_iter = g_iter,\n",
    "            lr_x=lr, lr_y=lr, tol=tol, atol = atol, eps=1e-8, beta=0.99, track_cond = track_cond)\n",
    "lossFunction = nn.MSELoss()\n",
    "lossfunction2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:124: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2197.)\n",
      "  y, _ = torch.triangular_solve(beta[0:j + 1].unsqueeze(-1), H[0:j + 1, 0:j + 1])  # j x j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 PDE Loss: 0.00004, BC Loss: 3767.23755,                   mse loss: 3767.237549, nn loss: 69.193764\n",
      "50/30000 PDE Loss: 0.00056, BC Loss: 3571.03589,                   mse loss: 3571.036377, nn loss: -68.868706\n",
      "100/30000 PDE Loss: 0.00037, BC Loss: 2033.70459,                   mse loss: 2033.704956, nn loss: -259.540680\n",
      "150/30000 PDE Loss: 0.00033, BC Loss: 827.20709,                   mse loss: 827.207397, nn loss: -333.180695\n",
      "200/30000 PDE Loss: 0.00025, BC Loss: 94.08070,                   mse loss: 94.080948, nn loss: -73.118706\n",
      "250/30000 PDE Loss: 0.00195, BC Loss: 426.69113,                   mse loss: 426.693085, nn loss: 145.734299\n",
      "300/30000 PDE Loss: 0.00039, BC Loss: 81.82114,                   mse loss: 81.821526, nn loss: -32.491920\n",
      "350/30000 PDE Loss: 0.00492, BC Loss: 455.58328,                   mse loss: 455.588196, nn loss: -85.995163\n",
      "400/30000 PDE Loss: 0.00042, BC Loss: 145.28755,                   mse loss: 145.287964, nn loss: 58.004745\n",
      "450/30000 PDE Loss: 0.01190, BC Loss: 115.83463,                   mse loss: 115.846527, nn loss: 10.529264\n",
      "500/30000 PDE Loss: 0.00617, BC Loss: 102.81575,                   mse loss: 102.821922, nn loss: -7.066687\n",
      "550/30000 PDE Loss: 0.00827, BC Loss: 77.03088,                   mse loss: 77.039154, nn loss: 1.035578\n",
      "600/30000 PDE Loss: 0.01136, BC Loss: 75.15965,                   mse loss: 75.171013, nn loss: -2.962326\n",
      "650/30000 PDE Loss: 0.01090, BC Loss: 76.74033,                   mse loss: 76.751228, nn loss: -2.187952\n",
      "700/30000 PDE Loss: 0.01261, BC Loss: 75.26057,                   mse loss: 75.273170, nn loss: -3.208957\n",
      "750/30000 PDE Loss: 0.01346, BC Loss: 75.01445,                   mse loss: 75.027908, nn loss: -3.714223\n",
      "800/30000 PDE Loss: 0.01494, BC Loss: 74.95227,                   mse loss: 74.967216, nn loss: -4.222863\n",
      "850/30000 PDE Loss: 0.01722, BC Loss: 74.86993,                   mse loss: 74.887154, nn loss: -4.907433\n",
      "900/30000 PDE Loss: 0.02069, BC Loss: 74.76270,                   mse loss: 74.783394, nn loss: -5.714323\n",
      "950/30000 PDE Loss: 0.02605, BC Loss: 74.61675,                   mse loss: 74.642807, nn loss: -6.784518\n",
      "1000/30000 PDE Loss: 0.03456, BC Loss: 74.39584,                   mse loss: 74.430389, nn loss: -8.258780\n",
      "1050/30000 PDE Loss: 0.04827, BC Loss: 74.02783,                   mse loss: 74.076103, nn loss: -10.348355\n",
      "1100/30000 PDE Loss: 0.07302, BC Loss: 73.43542,                   mse loss: 73.508446, nn loss: -13.345615\n",
      "1150/30000 PDE Loss: 0.12306, BC Loss: 72.58018,                   mse loss: 72.703247, nn loss: -17.660992\n",
      "1200/30000 PDE Loss: 0.23917, BC Loss: 71.30884,                   mse loss: 71.548004, nn loss: -23.911596\n",
      "1250/30000 PDE Loss: 0.53195, BC Loss: 69.37753,                   mse loss: 69.909485, nn loss: -32.491440\n",
      "1300/30000 PDE Loss: 1.32443, BC Loss: 66.33427,                   mse loss: 67.658707, nn loss: -43.106960\n",
      "1350/30000 PDE Loss: 3.21373, BC Loss: 62.42232,                   mse loss: 65.636047, nn loss: -54.513206\n",
      "1400/30000 PDE Loss: 7.48755, BC Loss: 58.15520,                   mse loss: 65.642754, nn loss: -58.795315\n",
      "1450/30000 PDE Loss: 15.75758, BC Loss: 54.90400,                   mse loss: 70.661568, nn loss: -50.954758\n",
      "1500/30000 PDE Loss: 28.71417, BC Loss: 54.27423,                   mse loss: 82.988411, nn loss: -35.584736\n",
      "1550/30000 PDE Loss: 44.58051, BC Loss: 57.18786,                   mse loss: 101.768372, nn loss: -24.401756\n",
      "1600/30000 PDE Loss: 54.18744, BC Loss: 60.16207,                   mse loss: 114.349503, nn loss: -30.625122\n",
      "1650/30000 PDE Loss: 58.41993, BC Loss: 62.15794,                   mse loss: 120.577873, nn loss: -36.822224\n",
      "1700/30000 PDE Loss: 60.89033, BC Loss: 63.73887,                   mse loss: 124.629196, nn loss: -45.211670\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m pinn_loss \u001b[39m=\u001b[39m pde_loss\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m bc_loss\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     60\u001b[0m \u001b[39m# combined_loss.backward()\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(combined_loss, \u001b[39m-\u001b[39;49mcombined_loss)\n\u001b[0;32m     63\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(combined_loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_acgd.py:165\u001b[0m, in \u001b[0;36mGACGD.step\u001b[1;34m(self, loss_x, loss_y, trigger)\u001b[0m\n\u001b[0;32m    157\u001b[0m prev_x0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m Avp \u001b[39m=\u001b[39m partial(MvProd,\n\u001b[0;32m    159\u001b[0m               grad_fy\u001b[39m=\u001b[39mgrad_fy_vec, grad_gx\u001b[39m=\u001b[39mgrad_gx_vec,\n\u001b[0;32m    160\u001b[0m               x_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_params, y_params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m               x_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_reducer, y_reducer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_reducer,\n\u001b[0;32m    164\u001b[0m               rebuild\u001b[39m=\u001b[39mshould_rebuild)\n\u001b[1;32m--> 165\u001b[0m soln, (num_iter, err_history) \u001b[39m=\u001b[39m GMRES(Avp\u001b[39m=\u001b[39;49mAvp, b\u001b[39m=\u001b[39;49mRHS, x0\u001b[39m=\u001b[39;49mprev_x0,\n\u001b[0;32m    166\u001b[0m                                       max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    167\u001b[0m                                       tol\u001b[39m=\u001b[39;49mtol, atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m    168\u001b[0m                                       track\u001b[39m=\u001b[39;49mtrack_flag)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mx0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m soln\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    172\u001b[0m     {\n\u001b[0;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_x\u001b[39m\u001b[39m'\u001b[39m: sq_avg_x,\n\u001b[0;32m    174\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msq_exp_avg_y\u001b[39m\u001b[39m'\u001b[39m: sq_avg_y\n\u001b[0;32m    175\u001b[0m     }\n\u001b[0;32m    176\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_torch.py:107\u001b[0m, in \u001b[0;36mGMRES\u001b[1;34m(Avp, b, x0, max_iter, tol, atol, track)\u001b[0m\n\u001b[0;32m    104\u001b[0m ss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_iter, device\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mdevice)  \u001b[39m# sine values at each step\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[1;32m--> 107\u001b[0m     p \u001b[39m=\u001b[39m Avp(V[j])\n\u001b[0;32m    108\u001b[0m     new_v \u001b[39m=\u001b[39m arnoldi(p, V, H, j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# Arnoldi iteration to get the j+1 th ba\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39m# sis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\gmres_acgd.py:48\u001b[0m, in \u001b[0;36mMvProd\u001b[1;34m(vec, grad_fy, grad_gx, x_params, y_params, lr_x, lr_y, trigger, x_reducer, y_reducer, rebuild)\u001b[0m\n\u001b[0;32m     43\u001b[0m h1 \u001b[39m=\u001b[39m Hvp_vec(grad_fy, x_params, v2,\n\u001b[0;32m     44\u001b[0m              retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, trigger\u001b[39m=\u001b[39mtrigger,\n\u001b[0;32m     45\u001b[0m              reducer\u001b[39m=\u001b[39mx_reducer,\n\u001b[0;32m     46\u001b[0m              rebuild\u001b[39m=\u001b[39mrebuild)\n\u001b[0;32m     47\u001b[0m p1 \u001b[39m=\u001b[39m v1 \u001b[39m+\u001b[39m lr_x \u001b[39m*\u001b[39m h1\n\u001b[1;32m---> 48\u001b[0m h2 \u001b[39m=\u001b[39m Hvp_vec(grad_gx, y_params, v1,\n\u001b[0;32m     49\u001b[0m              retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trigger\u001b[39m=\u001b[39;49mtrigger,\n\u001b[0;32m     50\u001b[0m              reducer\u001b[39m=\u001b[39;49my_reducer,\n\u001b[0;32m     51\u001b[0m              rebuild\u001b[39m=\u001b[39;49mrebuild)\n\u001b[0;32m     52\u001b[0m p2 \u001b[39m=\u001b[39m v2 \u001b[39m+\u001b[39m lr_y \u001b[39m*\u001b[39m h2\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([p1, p2])\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\CGDs\\cgd_utils.py:109\u001b[0m, in \u001b[0;36mHvp_vec\u001b[1;34m(grad_vec, params, vec, retain_graph, trigger, reducer, rebuild)\u001b[0m\n\u001b[0;32m    107\u001b[0m         reducer\u001b[39m.\u001b[39m_rebuild_buckets()\n\u001b[0;32m    108\u001b[0m     reducer\u001b[39m.\u001b[39mprepare_for_backward([])\n\u001b[1;32m--> 109\u001b[0m autograd\u001b[39m.\u001b[39;49mbackward(grad_vec \u001b[39m+\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39m*\u001b[39;49m trigger, grad_tensors\u001b[39m=\u001b[39;49mvec,\n\u001b[0;32m    110\u001b[0m                   inputs\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    111\u001b[0m                   retain_graph\u001b[39m=\u001b[39;49mretain_graph)\n\u001b[0;32m    112\u001b[0m hvp \u001b[39m=\u001b[39m vectorize_grad(params)\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(hvp)\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    \n",
    "    optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # normal loss\n",
    "    # print(n_st_train)\n",
    "    # print(PINNBCGD.output.weight.dtype)\n",
    "    v1_hat = PINNGACGD(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(v1_hat, n_st_train, grad_outputs=torch.ones(v1_hat.shape).cuda(), \n",
    "                       retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    loss1 = D_GACGD(n_st_train) * (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*v1_hat)\n",
    "    \n",
    "    \n",
    "    # boundary condition loss\n",
    "    bc_hat = PINNGACGD(bc_st_train)\n",
    "    # print(bc_v_train)\n",
    "    # print('111111111111111111111')\n",
    "    # print(bc_hat)\n",
    "    bc_loss = lossFunction(bc_v_train, bc_hat)\n",
    "    \n",
    "    loss2 = D_GACGD(bc_st_train) * (bc_hat - bc_v_train)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    combined_loss = (loss1.mean() + loss2.mean())\n",
    "    pinn_loss = pde_loss.mean() + bc_loss.mean()\n",
    "    # combined_loss.backward()\n",
    "    optimizer.step(combined_loss, -combined_loss)\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'{epoch}/{max_iter} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, \\\n",
    "                  mse loss: {pinn_loss.item():5f}, nn loss: {combined_loss.item():5f}')\n",
    "        pass\n",
    "        \n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
