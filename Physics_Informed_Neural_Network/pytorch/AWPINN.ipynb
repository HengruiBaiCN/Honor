{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networks\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "seed = 1234\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_size)])\n",
    "        self.layers.extend([nn.Linear(hidden_size, hidden_size) for _ in range(n_layers - 1)])\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# layers = [2, 50, 50, 50, 50, 50, 50, 50, 50, 1] # Network structure\n",
    "net = Net(2, 50,1,8).to(device) #  Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60000\n",
    "lossFunction = nn.MSELoss()\n",
    "\n",
    "from torchimize.functions import lsq_lma\n",
    "# coeffs_list = lsq_lma(fnn.parameters(), function=lossFunction)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "x_f_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_label_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "optimizer_adam_weight = torch.optim.Adam([x_f_s] + [x_label_s], lr=0.0003)\n",
    "\n",
    "\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00001, BC Loss: 3357.17090, nn_loss: 3357.170898, total loss: 3357.17090, minimum loss: 3357.17090\n",
      "the weight is 1.00000, 1.00000\n",
      "500/60000 PDE Loss: 0.00259, BC Loss: 12.00583, nn_loss: 11.898733, total loss: 12.00841, minimum loss: 0.13378\n",
      "the weight is 1.15721, 0.99083\n",
      "1000/60000 PDE Loss: 0.00193, BC Loss: 0.55265, nn_loss: 0.549831, total loss: 0.55458, minimum loss: 0.11225\n",
      "the weight is 1.34884, 0.99018\n",
      "1500/60000 PDE Loss: 0.00479, BC Loss: 0.17529, nn_loss: 0.181032, total loss: 0.18008, minimum loss: 0.10863\n",
      "the weight is 1.56959, 0.98984\n",
      "2000/60000 PDE Loss: 0.00425, BC Loss: 0.30080, nn_loss: 0.305410, total loss: 0.30505, minimum loss: 0.10863\n",
      "the weight is 1.82502, 0.98952\n",
      "2500/60000 PDE Loss: 0.00310, BC Loss: 0.11650, nn_loss: 0.121846, total loss: 0.11960, minimum loss: 0.10863\n",
      "the weight is 2.12163, 0.98939\n",
      "3000/60000 PDE Loss: 0.00157, BC Loss: 0.12841, nn_loss: 0.130893, total loss: 0.12998, minimum loss: 0.10863\n",
      "the weight is 2.46634, 0.98917\n",
      "3500/60000 PDE Loss: 0.00164, BC Loss: 0.13919, nn_loss: 0.142361, total loss: 0.14084, minimum loss: 0.10863\n",
      "the weight is 2.86701, 0.98888\n",
      "4000/60000 PDE Loss: 0.00151, BC Loss: 0.32427, nn_loss: 0.325716, total loss: 0.32578, minimum loss: 0.10863\n",
      "the weight is 3.33198, 0.98893\n",
      "4500/60000 PDE Loss: 0.00128, BC Loss: 0.16856, nn_loss: 0.171454, total loss: 0.16985, minimum loss: 0.10863\n",
      "the weight is 3.87175, 0.98770\n",
      "5000/60000 PDE Loss: 0.00116, BC Loss: 0.12089, nn_loss: 0.124728, total loss: 0.12205, minimum loss: 0.10812\n",
      "the weight is 4.49796, 0.98853\n",
      "5500/60000 PDE Loss: 0.00089, BC Loss: 0.50099, nn_loss: 0.499479, total loss: 0.50188, minimum loss: 0.10812\n",
      "the weight is 5.22633, 0.98774\n",
      "6000/60000 PDE Loss: 0.00104, BC Loss: 0.11309, nn_loss: 0.118165, total loss: 0.11413, minimum loss: 0.10812\n",
      "the weight is 6.07240, 0.98904\n",
      "6500/60000 PDE Loss: 0.00099, BC Loss: 0.12229, nn_loss: 0.128052, total loss: 0.12328, minimum loss: 0.10812\n",
      "the weight is 7.05448, 0.99000\n",
      "7000/60000 PDE Loss: 0.00075, BC Loss: 0.87375, nn_loss: 0.874917, total loss: 0.87450, minimum loss: 0.10812\n",
      "the weight is 8.19545, 0.99432\n",
      "7500/60000 PDE Loss: 0.00082, BC Loss: 0.12536, nn_loss: 0.132516, total loss: 0.12618, minimum loss: 0.10450\n",
      "the weight is 9.52126, 0.99460\n",
      "8000/60000 PDE Loss: 0.00076, BC Loss: 0.12961, nn_loss: 0.137640, total loss: 0.13037, minimum loss: 0.10450\n",
      "the weight is 11.06075, 0.99672\n",
      "8500/60000 PDE Loss: 0.00075, BC Loss: 0.11369, nn_loss: 0.123299, total loss: 0.11444, minimum loss: 0.10450\n",
      "the weight is 12.84851, 0.99976\n",
      "9000/60000 PDE Loss: 0.00074, BC Loss: 0.09811, nn_loss: 0.110119, total loss: 0.09885, minimum loss: 0.09855\n",
      "the weight is 14.92443, 1.01001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\Physics_Informed_Neural_Network\\pytorch\\AWPINN.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/AWPINN.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/AWPINN.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m total_loss \u001b[39m=\u001b[39m pde_loss \u001b[39m+\u001b[39m bc_loss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/AWPINN.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m combined_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx_f_s\u001b[39m.\u001b[39mdetach()) \u001b[39m*\u001b[39m pde_loss \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39;49mx_label_s\u001b[39m.\u001b[39;49mdetach()) \u001b[39m*\u001b[39m bc_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/AWPINN.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m combined_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/AWPINN.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "\n",
    "\n",
    "    # PDE Round\n",
    "    y1_hat = net(n_st_train)\n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads)\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads2nd)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    \n",
    "    # conditions Round\n",
    "    y21_hat = net(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y21_hat)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = pde_loss + bc_loss\n",
    "    combined_loss = torch.exp(-x_f_s.detach()) * pde_loss + torch.exp(-x_label_s.detach()) * bc_loss\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_hist.append(total_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, nn_loss: {combined_loss.item():5f}, total loss: {total_loss.item():.5f}, minimum loss: {min(loss_hist):.5f}')\n",
    "        print(f'the weight is {torch.exp(-x_f_s.detach()).item():.5f}, {torch.exp(-x_label_s.detach()).item():.5f}')\n",
    "    \n",
    "    # update the weight\n",
    "    optimizer_adam_weight.zero_grad()\n",
    "    loss = torch.exp(-x_f_s) * pde_loss.detach() + x_f_s + torch.exp(-x_label_s) * bc_loss.detach() + x_label_s\n",
    "    loss.backward()\n",
    "    optimizer_adam_weight.step()\n",
    "    pass\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('run time:', end_time - start_time)\n",
    "print('Adam done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
