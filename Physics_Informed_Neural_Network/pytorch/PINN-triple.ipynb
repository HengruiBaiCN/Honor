{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "import networks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn = networks.FeedforwardNeuralNetwork(2, 50, 1, 3)\n",
    "fnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100000\n",
    "lossFunction = nn.MSELoss()\n",
    "optimizer = optim.Adam(fnn.parameters(), lr=0.00002)\n",
    "\n",
    "samples = {\"pde\": 50000, \"bc\":25000, \"fc\":25000}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "- For each iteration in the training loop, we are sampling data for the three physical conditions of the PDE.\n",
    "- Then we are calculating the loss three times on the same model, accumulating them into a combined objective function to be minimised for the Neural Network.\n",
    "- The first loss is the differential equation loss. Here we are trying to minimise the PDE by calculating gradients and forming the PDE itself.\n",
    "- The remaining losses are calculated for boundary value and initial value conditions for the PDE.\n",
    "- Mean Squared Error loss function `nn.MSELoss()` is chosen as the criterion to be minimised and \n",
    "- Adam optimizer `nn.optim.Adam(lr=3e-5)` with a learning rate of 0.00003 is chosen for performing the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100000 PDE Loss: 0.00027, BC Loss: 3969.172363, fc loss:  1783.238403, L2 loss: 5752.411133, minimum loss: 5752.41113\n",
      "500/100000 PDE Loss: 0.00316, BC Loss: 2035.379028, fc loss:  813.045715, L2 loss: 2848.427979, minimum loss: 2848.42798\n",
      "1000/100000 PDE Loss: 0.00428, BC Loss: 438.634644, fc loss:  182.124573, L2 loss: 620.763489, minimum loss: 620.76349\n",
      "1500/100000 PDE Loss: 0.00091, BC Loss: 52.295654, fc loss:  170.160873, L2 loss: 222.457443, minimum loss: 222.45744\n",
      "2000/100000 PDE Loss: 0.00022, BC Loss: 29.550224, fc loss:  185.996078, L2 loss: 215.546524, minimum loss: 215.54652\n",
      "2500/100000 PDE Loss: 0.00024, BC Loss: 28.692659, fc loss:  185.191650, L2 loss: 213.884552, minimum loss: 213.88455\n",
      "3000/100000 PDE Loss: 0.00022, BC Loss: 28.412632, fc loss:  183.427200, L2 loss: 211.840057, minimum loss: 211.84006\n",
      "3500/100000 PDE Loss: 0.00020, BC Loss: 28.074532, fc loss:  181.280380, L2 loss: 209.355103, minimum loss: 209.35510\n",
      "4000/100000 PDE Loss: 0.00029, BC Loss: 27.734022, fc loss:  178.581375, L2 loss: 206.315689, minimum loss: 206.31569\n",
      "4500/100000 PDE Loss: 0.00032, BC Loss: 27.267874, fc loss:  175.588501, L2 loss: 202.856689, minimum loss: 202.85669\n",
      "5000/100000 PDE Loss: 0.00033, BC Loss: 26.744329, fc loss:  172.145157, L2 loss: 198.889816, minimum loss: 198.88982\n",
      "5500/100000 PDE Loss: 0.00065, BC Loss: 26.120035, fc loss:  168.216797, L2 loss: 194.337479, minimum loss: 194.33748\n",
      "6000/100000 PDE Loss: 0.00048, BC Loss: 25.543983, fc loss:  163.580521, L2 loss: 189.124985, minimum loss: 189.12498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\Physics_Informed_Neural_Network\\pytorch\\PINN-triple.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/PINN-triple.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# print(grads)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/PINN-triple.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m dVdt, dVdS \u001b[39m=\u001b[39m grads[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), grads[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/PINN-triple.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m grads2nd \u001b[39m=\u001b[39m tgrad\u001b[39m.\u001b[39mgrad(dVdS, n_st_train, grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mones(dVdS\u001b[39m.\u001b[39;49mshape)\u001b[39m.\u001b[39mcuda(), create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, only_inputs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/PINN-triple.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# print(grads2nd)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/pytorch/PINN-triple.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m d2VdS2 \u001b[39m=\u001b[39m grads2nd[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    i_st_train, i_v_train, bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData3(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    i_st_train = torch.from_numpy(i_st_train).float().requires_grad_().to(device)\n",
    "    i_v_train = torch.from_numpy(i_v_train).float().to(device)\n",
    "    \n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    # PDE Round\n",
    "    y1_hat = fnn(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads)\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads2nd)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    \n",
    "    # BC Round\n",
    "    y21_hat = fnn(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y21_hat)\n",
    "    \n",
    "    # final condition loss\n",
    "    fc_hat = fnn(i_st_train)\n",
    "    fc_loss = lossFunction(i_v_train, fc_hat)\n",
    "    \n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad()\n",
    "    combined_loss = 0.8 * pde_loss + 0.001 * bc_loss + 0.001 * fc_loss\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    mse_loss = pde_loss + bc_loss + fc_loss\n",
    "    \n",
    "    loss_hist.append(mse_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.6f}, fc loss: {fc_loss.item(): 5f}, L2 loss: {mse_loss.item():6f}, minimum loss: {min(loss_hist):.5f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
