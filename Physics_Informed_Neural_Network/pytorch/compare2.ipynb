{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Formula\n",
    "The Black–Scholes formula calculates the price of European put and call options. This price is consistent with the Black–Scholes equation. This follows since the formula can be obtained by solving the equation for the corresponding terminal and boundary conditions:\n",
    "$$    \n",
    "\n",
    "    {\\begin{aligned}&C(0,t)=0{\\text{ for all }}t\\\\&C(S,t)\\rightarrow S-K{\\text{ as }}S\\rightarrow \\infty \\\\&C(S,T)=\\max\\{S-K,0\\}\\end{aligned}}\n",
    "\n",
    "    $$\n",
    "\n",
    "The value of a call option for a non-dividend-paying underlying stock in terms of the Black–Scholes parameters is:\n",
    "\n",
    "$$\n",
    "    {\\begin{aligned}C(S_{t},t)&=N(d_{+})S_{t}-N(d_{-})Ke^{-r(T-t)}\\\\d_{+}&={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S_{t}}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{-}&=d_{+}-\\sigma {\\sqrt {T-t}}\\\\\\end{aligned}}\n",
    "    $$\n",
    "\n",
    "The price of a corresponding put option based on put–call parity with discount factor $e^{{-r(T-t)}}$ is:\n",
    "$$\n",
    "    {\\begin{aligned}P(S_{t},t)&=Ke^{-r(T-t)}-S_{t}+C(S_{t},t)\\\\&=N(-d_{-})Ke^{-r(T-t)}-N(-d_{+})S_{t}\\end{aligned}}\\,\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.Tensor([80]).requires_grad_()\n",
    "t = torch.Tensor([0]).requires_grad_()\n",
    "sigma = torch.Tensor([0.3]).requires_grad_()\n",
    "r = torch.Tensor([0.05]).requires_grad_()\n",
    "K = torch.Tensor([70])\n",
    "T = torch.Tensor([1])\n",
    "t2m = T-t\n",
    "d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * t2m)/(sigma * torch.sqrt(t2m))\n",
    "d2 = d1 - sigma * torch.sqrt(t2m)\n",
    "N0 = lambda value: 0.5 * (1 + torch.erf((value/2**0.5)))\n",
    "Nd1 = N0(d1)\n",
    "Nd2 = N0(d2)\n",
    "C = S* Nd1 - K* Nd2 *torch.exp(-r*t2m)\n",
    "print(\"Option Price:\", C.item()) #17.01496"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Equation\n",
    "The gradient is calculated using the auto gradient method in pytorch.\n",
    "\n",
    "$$\n",
    "{\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n",
    "$$\n",
    "\n",
    "To check the correctness of the calculation, it uses the Greeks equation from the Black-Scholes Formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCdt, = tgrad.grad(C, t, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "dCdS, = tgrad.grad(C, S, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "d2CdS2, = tgrad.grad(dCdS, S, grad_outputs=torch.ones(dCdS.shape), create_graph=True, only_inputs=True)\n",
    "dCdvol, = tgrad.grad(C, sigma, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "\n",
    "dCdr, = tgrad.grad(C, r, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "theta, delta, gamma, vega, rho = -dCdt[0], dCdS[0], d2CdS2[0], dCdvol[0], dCdr[0]\n",
    "\n",
    "for og in [theta, delta, gamma, vega, rho]:\n",
    "    print(f'{og.item():.4f}')\n",
    "\n",
    "    # Theta 5.8385\n",
    "    # Delta 0.7769\n",
    "    # Gamma 0.0124\n",
    "    # Vega 23.8776\n",
    "    # Rho 45.1372\n",
    "\n",
    "    # print((-theta + 0.5*sigma**2 * S**2*gamma + r*S*delta - r*C).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, \n",
    "                       r, \n",
    "                       sigma, \n",
    "                       T, \n",
    "                       S_range[-1], \n",
    "                       S_range, \n",
    "                       t_range, \n",
    "                       gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.scatter([sublist[0] for sublist in n_st_train], [sublist[1] for sublist in n_st_train], marker='.',alpha=0.3)\n",
    "plt.scatter([sublist[0] for sublist in bc_st_train], [sublist[1] for sublist in bc_st_train], marker='X')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('Option Price s')\n",
    "\n",
    "plt.title('Positions of collocation points and boundary data');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30000\n",
    "sizes=[2, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "w1 = 3\n",
    "w2 = 0.2\n",
    "\n",
    "lossFunction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PINN models\n",
    "pinn = utils.network_dispatcher('pinn', sizes, 'relu', 0, None, 10.0).to(device=device)\n",
    "wpinn = utils.network_dispatcher('pinn', sizes, 'relu', 0, None, 10.0).to(device=device)\n",
    "awpinn = utils.network_dispatcher('pinn', sizes, 'relu', 0, None, 10.0).to(device=device)\n",
    "\n",
    "ipinn = utils.network_dispatcher('ipinn', sizes, 'relu', 0.0, 0.1, 10.0).to(device=device)\n",
    "wipinn = utils.network_dispatcher('ipinn', sizes, 'relu', 0.0, 0.1, 10.0).to(device=device)\n",
    "awipinn = utils.network_dispatcher('ipinn', sizes, 'relu', 0.0, 0.1, 10.0).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampling(K, r, sigma, T, Smax, S_range, t_range, gs, num_bc, num_fc, num_nc, RNG_key=123):\n",
    "    # sampling\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "        utils.trainingData(K, r, sigma, T, Smax, S_range, t_range, gs, num_bc, num_fc, num_nc, RNG_key)\n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "            \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)\n",
    "    \n",
    "    return n_st_train, n_v_train, bc_st_train, bc_v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction):\n",
    "    \n",
    "    # pde loss\n",
    "    y_hat = model(n_st_train)\n",
    "    \n",
    "    grads = tgrad.grad(y_hat, n_st_train, grad_outputs=torch.ones(y_hat.shape).cuda(), \n",
    "                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), \n",
    "                    create_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y_hat)\n",
    "    \n",
    "    # boudary condition loss\n",
    "    y2_hat = model(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y2_hat)\n",
    "    \n",
    "    return pde_loss, bc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_dispatcher(pde_loss, bc_loss, adaptive_rate, model, w1, w2, adaptive_weight, x_f_s, x_label_s):\n",
    "    '''\n",
    "    @param adaptive_rate: bool, whether to use adaptive rate or not\n",
    "    @param model: model, used to get the local recovery term\n",
    "    @param w1: weight for pde loss\n",
    "    @param w2: weight for bc loss\n",
    "    @param adaptive_weight: bool, whether to use adaptive weight or not\n",
    "    @return: loss\n",
    "    '''\n",
    "    loss = None\n",
    "    if adaptive_rate:\n",
    "        local_recovery_terms = torch.tensor([torch.mean(model.regressor[layer][0].A.data) for layer in range(len(model.regressor) - 1)])\n",
    "        slope_recovery_term = 1 / torch.mean(torch.exp(local_recovery_terms))\n",
    "        loss = w1 * pde_loss + w2 * bc_loss + slope_recovery_term\n",
    "    elif adaptive_weight:\n",
    "        loss = torch.exp(-x_f_s.detach()) * pde_loss + torch.exp(-x_label_s.detach()) * bc_loss\n",
    "    else:\n",
    "        loss = w1 * pde_loss + w2 * bc_loss\n",
    "    \n",
    "    mse_loss = pde_loss + bc_loss\n",
    "    return loss, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and optimizer\n",
    "optimizer1 = utils.optimizer_dispatcher('adam', pinn.parameters(), lr)\n",
    "optimizer2 = utils.optimizer_dispatcher('adam', wpinn.parameters(), lr)\n",
    "optimizer3 = utils.optimizer_dispatcher('adam', awpinn.parameters(), lr)\n",
    "\n",
    "optimizer4 = utils.optimizer_dispatcher('adam', ipinn.parameters(), lr)\n",
    "optimizer5 = utils.optimizer_dispatcher('adam', wipinn.parameters(), lr)\n",
    "optimizer6 = utils.optimizer_dispatcher('adam', awipinn.parameters(), lr)\n",
    "\n",
    "\n",
    "# adaptive weight\n",
    "x_f_s1 = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_label_s1 = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "optimizer_adam_weight1 = torch.optim.Adam([x_f_s1] + [x_label_s1], lr=0.0003)\n",
    "\n",
    "# adaptive weight\n",
    "x_f_s2 = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_label_s2 = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "optimizer_adam_weight2 = torch.optim.Adam([x_f_s2] + [x_label_s2], lr=0.0003)\n",
    "\n",
    "# training\n",
    "loss_hist1 = []\n",
    "loss_hist2 = []\n",
    "loss_hist3 = []\n",
    "loss_hist4 = []\n",
    "loss_hist5 = []\n",
    "loss_hist6 = []\n",
    "\n",
    "log_loss_hist = []\n",
    "# logging.info(f'{model}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "start_time = timer()\n",
    "\n",
    "# training loop\n",
    "for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "\n",
    "    n_st_train, n_v_train, bc_st_train, bc_v_train = data_sampling(K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123)\n",
    "    \n",
    "    pde_loss1, bc_loss1 = loss_fn(pinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    pde_loss2, bc_loss2 = loss_fn(wpinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    pde_loss3, bc_loss3 = loss_fn(awpinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    \n",
    "    pde_loss4, bc_loss4 = loss_fn(ipinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    pde_loss5, bc_loss5 = loss_fn(wipinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    pde_loss6, bc_loss6 = loss_fn(awipinn, bc_st_train, bc_v_train, n_st_train, n_v_train, lossFunction)\n",
    "    \n",
    "    \n",
    "    loss1, mse_loss1 = loss_dispatcher(pde_loss1, bc_loss1, None, pinn, w1, w2, False, 0, 0)\n",
    "    loss2, mse_loss2 = loss_dispatcher(pde_loss2, bc_loss2, None, wpinn, w1, w2, False, 0, 0)\n",
    "    loss3, mse_loss3 = loss_dispatcher(pde_loss3, bc_loss3, None, awpinn, w1, w2, True, x_f_s1, x_label_s1)\n",
    "    \n",
    "    loss4, mse_loss4 = loss_dispatcher(pde_loss4, bc_loss4, 0.1, ipinn, w1, w2, False, 0, 0)\n",
    "    loss5, mse_loss5 = loss_dispatcher(pde_loss5, bc_loss5, 0.1, wipinn, w1, w2, False, 0, 0)\n",
    "    loss6, mse_loss6 = loss_dispatcher(pde_loss6, bc_loss6, 0.1, awipinn, w1, w2, True, x_f_s2, x_label_s2)\n",
    "    \n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    loss1.backward()\n",
    "    optimizer1.step()\n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    optimizer3.zero_grad()\n",
    "    loss3.backward()\n",
    "    optimizer3.step()\n",
    "    \n",
    "    optimizer4.zero_grad()\n",
    "    loss4.backward()\n",
    "    optimizer4.step()\n",
    "    \n",
    "    optimizer5.zero_grad()\n",
    "    loss5.backward()\n",
    "    optimizer5.step()\n",
    "    \n",
    "    optimizer6.zero_grad()\n",
    "    loss6.backward()\n",
    "    optimizer6.step()\n",
    "    \n",
    "    \n",
    "    loss_hist1.append(mse_loss1.item())\n",
    "    loss_hist2.append(mse_loss2.item())\n",
    "    loss_hist3.append(mse_loss3.item())\n",
    "    loss_hist4.append(mse_loss4.item())\n",
    "    loss_hist5.append(mse_loss5.item())\n",
    "    loss_hist6.append(mse_loss6.item())\n",
    "    \n",
    "    \n",
    "    optimizer_adam_weight1.zero_grad()\n",
    "    awloss1 = torch.exp(-x_f_s1) * pde_loss3.detach() + x_f_s1 + torch.exp(-x_label_s1) * bc_loss3.detach() + x_label_s1\n",
    "    awloss1.backward()\n",
    "    optimizer_adam_weight1.step()\n",
    "    \n",
    "    optimizer_adam_weight2.zero_grad()\n",
    "    awloss2 = torch.exp(-x_f_s2) * pde_loss6.detach() + x_f_s2 + torch.exp(-x_label_s2) * bc_loss6.detach() + x_label_s2\n",
    "    awloss2.backward()\n",
    "    optimizer_adam_weight2.step()\n",
    "    \n",
    "    \n",
    "elapsed = timer() - start_time\n",
    "logging.info(f'Training finished. Elapsed time: {elapsed} s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: pinn\n",
    "2: wpinn\n",
    "3: awpinn\n",
    "4: ipinn\n",
    "5: wipinn\n",
    "6: awipinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,16))\n",
    "plt.plot(range(3000), [np.log(item) for item in loss_hist1[:3000]], color='blue', label='pinn')\n",
    "# plt.plot(range(3000), [np.log(item) for item in loss_hist2[:3000]], label='wpinn')\n",
    "plt.plot(range(3000), [np.log(item) for item in loss_hist3[:3000]], color='green', label='awpinn')\n",
    "plt.plot(range(3000), [np.log(item) for item in loss_hist4[:3000]], color='red', label='ipinn')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(f'Speed of convergence: w1: {w1}, w2: {w2}')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(range(6000), [np.log(item) for item in loss_hist1[:6000]], color='blue', label='pinn')\n",
    "# plt.plot(range(6000), [np.log(item) for item in loss_hist2[:6000]], color='red', label='wpinn')\n",
    "plt.plot(range(6000), [np.log(item) for item in loss_hist3[:6000]], color='green', label='awpinn')\n",
    "plt.plot(range(6000), [np.log(item) for item in loss_hist4[:6000]], label='ipinn')\n",
    "# plt.plot(range(6000), [np.log(item) for item in loss_hist5[:6000]], label='wipinn')\n",
    "plt.plot(range(6000), [np.log(item) for item in loss_hist6[:6000]], label='awipinn')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(f'Speed of convergence: w1: {w1}, w2: {w2}')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,16))\n",
    "plt.plot(range(1000), loss_hist1[:1000], color='blue', label='pinn')\n",
    "# plt.plot(range(1000), loss_hist2[:1000], color='red', label='wpinn')\n",
    "plt.plot(range(1000), loss_hist3[:1000], color='black', label='awpinn')\n",
    "plt.plot(range(1000), loss_hist4[:1000], label='ipinn')\n",
    "# plt.plot(range(1000), loss_hist5[:1000], label='wipinn')\n",
    "plt.plot(range(1000), loss_hist6[:1000], label='awipinn')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(f'Speed of convergence: w1: {w1}, w2: {w2}')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,12))\n",
    "plt.plot(range(29000), loss_hist1[1000:30000], color='blue', label='pinn')\n",
    "# plt.plot(range(29000), loss_hist2[1000:30000], color='red', label='wpinn')\n",
    "plt.plot(range(29000), loss_hist3[1000:30000], color='black', label='awpinn')\n",
    "plt.plot(range(29000), loss_hist4[1000:30000], label='ipinn')\n",
    "# plt.plot(range(29000), loss_hist5[1000:30000], label='wipinn')\n",
    "plt.plot(range(29000), loss_hist6[1000:30000], label='awipinn')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Minimum loss')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(24,12))\n",
    "# plt.plot(range(30000), [np.log(item) for item in loss_list[:30000]], color='blue', label='pinn')\n",
    "# plt.plot(range(30000), [np.log(item) for item in loss_list2[:30000]], color='red', label='ipinn')\n",
    "# plt.plot(range(30000), [np.log(item) for item in loss_list3[:30000]], color='green', label='awpinn')\n",
    "# plt.plot(range(30000), [np.log(item) for item in loss_list4[:30000]], color='yellow', label='wpinn')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend()\n",
    "# plt.title(f'Speed of convergence: w1: {w1}, w2: {w2}')\n",
    "# plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
