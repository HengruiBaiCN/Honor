{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLinear(nn.Linear):\n",
    "    r\"\"\"Applies a linear transformation to the input data as follows\n",
    "    :math:`y = naxA^T + b`.\n",
    "    More details available in Jagtap, A. D. et al. Locally adaptive\n",
    "    activation functions with slope recovery for deep and\n",
    "    physics-informed neural networks, Proc. R. Soc. 2020.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        The size of each input sample\n",
    "    out_features : int \n",
    "        The size of each output sample\n",
    "    bias : bool, optional\n",
    "        If set to ``False``, the layer will not learn an additive bias\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice \n",
    "        (self.A is the learnable parameter which is initialized by the \n",
    "        self.adaptive rate. To create a learnable parameter for each neuron, \n",
    "        it multiplies the self.adaptive rate to the number of input features.)\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(AdaptiveLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.adaptive_rate = adaptive_rate\n",
    "        self.adaptive_rate_scaler = adaptive_rate_scaler\n",
    "        if self.adaptive_rate:\n",
    "            self.A = nn.Parameter(self.adaptive_rate * torch.ones(self.in_features))\n",
    "            if not self.adaptive_rate_scaler:\n",
    "                self.adaptive_rate_scaler = 10.0\n",
    "            \n",
    "    def forward(self, input):\n",
    "        if self.adaptive_rate:\n",
    "            return nn.functional.linear(self.adaptive_rate_scaler * self.A * input, self.weight, self.bias)\n",
    "        return nn.functional.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, '\n",
    "            f'adaptive_rate={self.adaptive_rate is not None}, adaptive_rate_scaler={self.adaptive_rate_scaler is not None}'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    r\"\"\"Neural approximator for the unknown function that is supposed\n",
    "    to be solved.\n",
    "\n",
    "    More details available in Raissi, M. et al. Physics-informed neural\n",
    "    networks: A deep learning framework for solving forward and inverse\n",
    "    problems involving nonlinear partial differential equations, J.\n",
    "    Comput. Phys. 2019.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sizes : list\n",
    "        Each element represents the number of neuron per layer\n",
    "    activation : callable \n",
    "        Activation function\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate for regulrization during training process and\n",
    "        uncertainty quantification by means of Monte Carlo dropout\n",
    "        procedure while performing evaluation\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, activation, dropout_rate=0.0, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            *[Net.linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler)\n",
    "            for in_features, out_features in zip(sizes[:-1], sizes[1:-1])],     \n",
    "            AdaptiveLinear(sizes[-2], sizes[-1]) # output layer is regular linear transformation\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler):\n",
    "        activation_dispatcher = nn.ModuleDict([\n",
    "            ['lrelu', nn.LeakyReLU()],\n",
    "            ['relu', nn.ReLU()],\n",
    "            ['tanh', nn.Tanh()],\n",
    "            ['sigmoid', nn.Sigmoid()],\n",
    "            # ['swish', Swish()]\n",
    "        ])\n",
    "        return nn.Sequential(\n",
    "            AdaptiveLinear(in_features, out_features, adaptive_rate=adaptive_rate, adaptive_rate_scaler=adaptive_rate_scaler),\n",
    "            activation_dispatcher[activation],\n",
    "            nn.Dropout(dropout_rate),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPINN = Net(\n",
    "    sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1], activation='relu', dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0\n",
    "    )\n",
    "IPINN.cuda()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "# physical constraints and sampling\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500000\n",
    "adaptive_rate = 0.1\n",
    "lossFunction = nn.MSELoss()\n",
    "lr = 3e-5\n",
    "optimizer = optim.Adam(IPINN.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linspace: space the batch of data linearly, otherwise random, type=float\n",
    "- domain: Boundaries of the solution domain, type=float\n",
    "- batch_size: The number of adata points for optimization per epoch, type=float\n",
    "- rhs: right-hand-side forcing function, type=float\n",
    "- boundary_conditions: boundaru conditions on boundaries of the domain, type=float\n",
    "- adaptive rate: add additional adaptive rate parameter to activation function, type=float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.autograd as tgrad\n",
    "\n",
    "loss_hist = []\n",
    "relative_l2_hist = []\n",
    "min_train_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = time.time()\n",
    "\n",
    "# tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "\n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, r, sigma, T, S_range[-1], S_range, t_range, gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)   \n",
    "    \n",
    "    # pde residual loss\n",
    "    y1_hat = IPINN(n_st_train)\n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), \n",
    "                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), \n",
    "                    create_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    # boudary condition loss\n",
    "    y2_hat = IPINN(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y2_hat)\n",
    "    \n",
    "    # sample training data loss\n",
    "    y3_hat = IPINN(X_train_tensor)\n",
    "    data_loss = lossFunction(y_train_tensor, y3_hat)\n",
    "    \n",
    "    # relative l2 error\n",
    "    y4_hat = IPINN(X_test_tensor)\n",
    "    relative_l2 = torch.sqrt(torch.sum((y_test_tensor - y4_hat)**2))/torch.sqrt(torch.sum(y_test_tensor**2))\n",
    "    \n",
    "    # total loss\n",
    "    local_recovery_terms = torch.tensor([torch.mean(IPINN.regressor[layer][0].A.data) for layer in range(len(IPINN.regressor) - 1)])\n",
    "    slope_recovery_term = 1 / torch.mean(torch.exp(local_recovery_terms))\n",
    "    loss = 2 *pde_loss + 1*bc_loss + slope_recovery_term + 1 * data_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    mse_loss = pde_loss + bc_loss + data_loss\n",
    "    loss_hist.append(mse_loss.item())\n",
    "    # if _ % 5000 == 0:\n",
    "    #     print(f'{_}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, data loss:{data_loss.item():.5f}, reg_loss: {slope_recovery_term.item(): 5f}, total loss: {mse_loss.item():5f}, minimum loss: {min(loss_hist):.5f}')\n",
    "\n",
    "    if mse_loss.item() < min_train_loss and mse_loss.item() < 0.003 or _ % 10000 == 0:\n",
    "        print(f'{_}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, data loss:{data_loss.item():.5f}, reg_loss: {slope_recovery_term.item(): 5f}, total loss: {mse_loss.item():5f}, minimum loss: {min(loss_hist):.5f}')\n",
    "        min_train_loss = mse_loss.item()\n",
    "        final_model = IPINN.state_dict()\n",
    "    pass\n",
    "    \n",
    "elapsed = timer() - start_time\n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)\n",
    "logging.info(f'Training finished. Elapsed time: {elapsed} s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(range(n_epochs), loss_hist)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('IPINN')\n",
    "\n",
    "import pandas as pd\n",
    "loss_hist = pd.DataFrame(loss_hist)\n",
    "loss_hist.to_csv('ipinn_loss_hist3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
