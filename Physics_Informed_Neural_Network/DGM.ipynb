{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct copy from: https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Formula\n",
    "The Black–Scholes formula calculates the price of European put and call options. This price is consistent with the Black–Scholes equation. This follows since the formula can be obtained by solving the equation for the corresponding terminal and boundary conditions:\n",
    "$$    \n",
    "\n",
    "    {\\begin{aligned}&C(0,t)=0{\\text{ for all }}t\\\\&C(S,t)\\rightarrow S-K{\\text{ as }}S\\rightarrow \\infty \\\\&C(S,T)=\\max\\{S-K,0\\}\\end{aligned}}\n",
    "\n",
    "    $$\n",
    "\n",
    "The value of a call option for a non-dividend-paying underlying stock in terms of the Black–Scholes parameters is:\n",
    "\n",
    "$$\n",
    "    {\\begin{aligned}C(S_{t},t)&=N(d_{+})S_{t}-N(d_{-})Ke^{-r(T-t)}\\\\d_{+}&={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S_{t}}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{-}&=d_{+}-\\sigma {\\sqrt {T-t}}\\\\\\end{aligned}}\n",
    "    $$\n",
    "\n",
    "The price of a corresponding put option based on put–call parity with discount factor $e^{{-r(T-t)}}$ is:\n",
    "$$\n",
    "    {\\begin{aligned}P(S_{t},t)&=Ke^{-r(T-t)}-S_{t}+C(S_{t},t)\\\\&=N(-d_{-})Ke^{-r(T-t)}-N(-d_{+})S_{t}\\end{aligned}}\\,\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option Price: 17.01496124267578\n"
     ]
    }
   ],
   "source": [
    "S = torch.Tensor([80]).requires_grad_()\n",
    "t = torch.Tensor([0]).requires_grad_()\n",
    "sigma = torch.Tensor([0.3]).requires_grad_()\n",
    "r = torch.Tensor([0.05]).requires_grad_()\n",
    "K = torch.Tensor([70])\n",
    "T = torch.Tensor([1])\n",
    "t2m = T-t\n",
    "d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * t2m)/(sigma * torch.sqrt(t2m))\n",
    "d2 = d1 - sigma * torch.sqrt(t2m)\n",
    "N0 = lambda value: 0.5 * (1 + torch.erf((value/2**0.5)))\n",
    "Nd1 = N0(d1)\n",
    "Nd2 = N0(d2)\n",
    "C = S* Nd1 - K* Nd2 *torch.exp(-r*t2m)\n",
    "print(\"Option Price:\", C.item()) #17.01496"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Equation\n",
    "The gradient is calculated using the auto gradient method in pytorch.\n",
    "\n",
    "$\n",
    "{\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n",
    "$\n",
    "\n",
    "To check the correctness of the calculation, it uses the Greeks equation from the Black-Scholes Formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8385\n",
      "0.0\n",
      "0.7769\n",
      "0.0\n",
      "0.0124\n",
      "0.0\n",
      "23.8776\n",
      "0.0\n",
      "45.1372\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dCdt, = tgrad.grad(C, t, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "dCdS, = tgrad.grad(C, S, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "d2CdS2, = tgrad.grad(dCdS, S, grad_outputs=torch.ones(dCdS.shape), create_graph=True, only_inputs=True)\n",
    "dCdvol, = tgrad.grad(C, sigma, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "\n",
    "dCdr, = tgrad.grad(C, r, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "theta, delta, gamma, vega, rho = -dCdt[0], dCdS[0], d2CdS2[0], dCdvol[0], dCdr[0]\n",
    "\n",
    "for og in [theta, delta, gamma, vega, rho]:\n",
    "    print(f'{og.item():.4f}')\n",
    "\n",
    "    # Theta 5.8385\n",
    "    # Delta 0.7769\n",
    "    # Gamma 0.0124\n",
    "    # Vega 23.8776\n",
    "    # Rho 45.1372\n",
    "\n",
    "    print((-theta + 0.5*sigma**2 * S**2*gamma + r*S*delta - r*C).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, Initial Value conditions and the PDE itself.\n",
    "\n",
    "The data samples are generated by the three functions:\n",
    "\n",
    "1.    Sampler of data inputs for t and S for Differential Loss get_diff_data()\n",
    "2.    Sampler of data inputs satisfying the boundary conditions for the PDE get_bvp_data()\n",
    "3.    Sampler of data inputs satisfying the initial value conditions for the PDE get_ivp_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y2 = (S_range[-1] - K*np.exp(-r*(T-X2[:, 0].reshape(-1)))).reshape(-1, 1)\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method Model Construction\n",
    "\n",
    "The Neural Network Model based on https://arxiv.org/abs/1708.07469?context=q-fin.MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMCell(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, n_layers=3, output_dim=1):\n",
    "    super(DGMCell, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.n = n_layers\n",
    "\n",
    "    self.sig_act = nn.Tanh()\n",
    "\n",
    "    self.Sw = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "\n",
    "    self.Uz = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsz = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Ug = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsg = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Ur = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsr = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    \n",
    "    self.Uh = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsh = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Wf = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    S1 = self.Sw(x)\n",
    "    for i in range(self.n):\n",
    "      if i==0:\n",
    "        S = S1\n",
    "      else:\n",
    "        S = self.sig_act(out)\n",
    "      Z = self.sig_act(self.Uz(x) + self.Wsz(S))\n",
    "      G = self.sig_act(self.Ug(x) + self.Wsg(S1))\n",
    "      R = self.sig_act(self.Ur(x) + self.Wsr(S))\n",
    "      H = self.sig_act(self.Uh(x) + self.Wsh(S*R))\n",
    "      out = (1-G)*H + Z*S\n",
    "    out = self.Wf(out)\n",
    "    return out\n",
    "\n",
    "model = DGMCell(2, 100, 3, 1)\n",
    "model.cuda()\n",
    "\n",
    "n_epochs = 60000\n",
    "samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "- For each iteration in the training loop, we are sampling data for the three physical conditions of the PDE.\n",
    "- Then we are calculating the loss three times on the same model, accumulating them into a combined objective function to be minimised for the Neural Network.\n",
    "- The first loss is the differential equation loss. Here we are trying to minimise the PDE by calculating gradients and forming the PDE itself.\n",
    "- The remaining losses are calculated for boundary value and initial value conditions for the PDE.\n",
    "- Mean Squared Error loss function nn.MSELoss() is chosen as the criterion to be minimised and Adam optimizer nn.optim.Adam(lr=3e-5)with a learning rate of 0.00003 is chosen for performing the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.01338, BVP1 Loss: 0.02709, BVP2 Loss: 8147.93896, IVP Loss: 1815.49670,\n",
      "500/60000 PDE Loss: 0.13744, BVP1 Loss: 0.00000, BVP2 Loss: 7500.84277, IVP Loss: 1621.46240,\n",
      "1000/60000 PDE Loss: 0.31907, BVP1 Loss: 0.00000, BVP2 Loss: 7072.85498, IVP Loss: 1449.63599,\n",
      "1500/60000 PDE Loss: 0.50636, BVP1 Loss: 0.00000, BVP2 Loss: 6678.98535, IVP Loss: 1395.76489,\n",
      "2000/60000 PDE Loss: 0.66489, BVP1 Loss: 0.00000, BVP2 Loss: 6306.20947, IVP Loss: 1239.31921,\n",
      "2500/60000 PDE Loss: 0.77491, BVP1 Loss: 0.00000, BVP2 Loss: 5943.13525, IVP Loss: 1119.76660,\n",
      "3000/60000 PDE Loss: 0.87458, BVP1 Loss: 0.00000, BVP2 Loss: 5579.56445, IVP Loss: 1041.39478,\n",
      "3500/60000 PDE Loss: 0.88761, BVP1 Loss: 0.00000, BVP2 Loss: 5230.24805, IVP Loss: 926.33887,\n",
      "4000/60000 PDE Loss: 0.96484, BVP1 Loss: 0.00000, BVP2 Loss: 4899.56348, IVP Loss: 872.02026,\n",
      "4500/60000 PDE Loss: 0.97828, BVP1 Loss: 0.00000, BVP2 Loss: 4584.65967, IVP Loss: 740.83447,\n",
      "5000/60000 PDE Loss: 1.19748, BVP1 Loss: 0.00000, BVP2 Loss: 4289.59277, IVP Loss: 683.85980,\n",
      "5500/60000 PDE Loss: 1.24527, BVP1 Loss: 0.00000, BVP2 Loss: 3999.31152, IVP Loss: 634.69873,\n",
      "6000/60000 PDE Loss: 1.29793, BVP1 Loss: 0.00000, BVP2 Loss: 3716.64917, IVP Loss: 555.69104,\n",
      "6500/60000 PDE Loss: 1.41286, BVP1 Loss: 0.00000, BVP2 Loss: 3447.60718, IVP Loss: 513.97479,\n",
      "7000/60000 PDE Loss: 1.50837, BVP1 Loss: 0.00000, BVP2 Loss: 3191.25562, IVP Loss: 438.73120,\n",
      "7500/60000 PDE Loss: 1.67135, BVP1 Loss: 0.00000, BVP2 Loss: 2940.92432, IVP Loss: 385.97815,\n",
      "8000/60000 PDE Loss: 1.86955, BVP1 Loss: 0.00000, BVP2 Loss: 2705.66382, IVP Loss: 336.93735,\n",
      "8500/60000 PDE Loss: 2.23108, BVP1 Loss: 0.00000, BVP2 Loss: 2478.48438, IVP Loss: 291.72540,\n",
      "9000/60000 PDE Loss: 2.27589, BVP1 Loss: 0.00000, BVP2 Loss: 2265.13550, IVP Loss: 256.03116,\n",
      "9500/60000 PDE Loss: 2.78755, BVP1 Loss: 0.00000, BVP2 Loss: 2059.82446, IVP Loss: 233.63829,\n",
      "10000/60000 PDE Loss: 3.17811, BVP1 Loss: 0.00000, BVP2 Loss: 1866.57336, IVP Loss: 189.56975,\n",
      "10500/60000 PDE Loss: 3.56036, BVP1 Loss: 0.00000, BVP2 Loss: 1683.51636, IVP Loss: 155.68857,\n",
      "11000/60000 PDE Loss: 4.39642, BVP1 Loss: 0.00000, BVP2 Loss: 1509.88281, IVP Loss: 143.15198,\n",
      "11500/60000 PDE Loss: 5.29520, BVP1 Loss: 0.00000, BVP2 Loss: 1347.92078, IVP Loss: 121.74672,\n",
      "12000/60000 PDE Loss: 5.37092, BVP1 Loss: 0.00000, BVP2 Loss: 1196.48853, IVP Loss: 103.65224,\n",
      "12500/60000 PDE Loss: 6.34590, BVP1 Loss: 0.00000, BVP2 Loss: 1054.37036, IVP Loss: 82.76508,\n",
      "13000/60000 PDE Loss: 7.30840, BVP1 Loss: 0.00000, BVP2 Loss: 922.07520, IVP Loss: 65.45251,\n",
      "13500/60000 PDE Loss: 7.88864, BVP1 Loss: 0.00000, BVP2 Loss: 800.66003, IVP Loss: 51.71409,\n",
      "14000/60000 PDE Loss: 9.62121, BVP1 Loss: 0.00000, BVP2 Loss: 688.36621, IVP Loss: 42.14385,\n",
      "14500/60000 PDE Loss: 10.59964, BVP1 Loss: 0.00000, BVP2 Loss: 587.01978, IVP Loss: 36.54981,\n",
      "15000/60000 PDE Loss: 13.65646, BVP1 Loss: 0.00000, BVP2 Loss: 492.88828, IVP Loss: 28.40548,\n",
      "15500/60000 PDE Loss: 13.84224, BVP1 Loss: 0.00000, BVP2 Loss: 412.16428, IVP Loss: 21.46089,\n",
      "16000/60000 PDE Loss: 13.77555, BVP1 Loss: 0.00000, BVP2 Loss: 338.27109, IVP Loss: 16.84740,\n",
      "16500/60000 PDE Loss: 13.00085, BVP1 Loss: 0.00000, BVP2 Loss: 275.82825, IVP Loss: 13.35246,\n",
      "17000/60000 PDE Loss: 16.55333, BVP1 Loss: 0.00000, BVP2 Loss: 218.17870, IVP Loss: 11.52640,\n",
      "17500/60000 PDE Loss: 17.39083, BVP1 Loss: 0.00000, BVP2 Loss: 171.93170, IVP Loss: 9.91492,\n",
      "18000/60000 PDE Loss: 16.98884, BVP1 Loss: 0.00000, BVP2 Loss: 131.44899, IVP Loss: 9.94217,\n",
      "18500/60000 PDE Loss: 20.13873, BVP1 Loss: 0.00000, BVP2 Loss: 99.19023, IVP Loss: 6.87477,\n",
      "19000/60000 PDE Loss: 34.77961, BVP1 Loss: 0.00000, BVP2 Loss: 65.90366, IVP Loss: 6.95848,\n",
      "19500/60000 PDE Loss: 9.33796, BVP1 Loss: 0.00000, BVP2 Loss: 54.46514, IVP Loss: 3.99224,\n",
      "20000/60000 PDE Loss: 4.91862, BVP1 Loss: 0.00000, BVP2 Loss: 40.22760, IVP Loss: 3.22692,\n",
      "20500/60000 PDE Loss: 12.97873, BVP1 Loss: 0.00000, BVP2 Loss: 43.52434, IVP Loss: 6.46805,\n",
      "21000/60000 PDE Loss: 7.08899, BVP1 Loss: 0.00000, BVP2 Loss: 26.08881, IVP Loss: 4.79715,\n",
      "21500/60000 PDE Loss: 6.03012, BVP1 Loss: 0.00001, BVP2 Loss: 17.51471, IVP Loss: 3.23419,\n",
      "22000/60000 PDE Loss: 3.79278, BVP1 Loss: 0.00000, BVP2 Loss: 11.30462, IVP Loss: 1.43479,\n",
      "22500/60000 PDE Loss: 2.57981, BVP1 Loss: 0.00000, BVP2 Loss: 6.08366, IVP Loss: 0.64912,\n",
      "23000/60000 PDE Loss: 1.28145, BVP1 Loss: 0.00000, BVP2 Loss: 3.01524, IVP Loss: 0.35697,\n",
      "23500/60000 PDE Loss: 0.78441, BVP1 Loss: 0.00000, BVP2 Loss: 1.35904, IVP Loss: 0.13291,\n",
      "24000/60000 PDE Loss: 0.28110, BVP1 Loss: 0.00003, BVP2 Loss: 0.33808, IVP Loss: 0.02935,\n",
      "24500/60000 PDE Loss: 0.19928, BVP1 Loss: 0.00001, BVP2 Loss: 0.14891, IVP Loss: 0.01231,\n",
      "25000/60000 PDE Loss: 0.15886, BVP1 Loss: 0.00000, BVP2 Loss: 0.06972, IVP Loss: 0.01192,\n",
      "25500/60000 PDE Loss: 0.10739, BVP1 Loss: 0.00001, BVP2 Loss: 0.04018, IVP Loss: 0.00773,\n",
      "26000/60000 PDE Loss: 0.06369, BVP1 Loss: 0.00000, BVP2 Loss: 0.02670, IVP Loss: 0.00835,\n",
      "26500/60000 PDE Loss: 0.03957, BVP1 Loss: 0.00000, BVP2 Loss: 0.01779, IVP Loss: 0.00582,\n",
      "27000/60000 PDE Loss: 0.16564, BVP1 Loss: 0.00000, BVP2 Loss: 0.40743, IVP Loss: 0.03111,\n",
      "27500/60000 PDE Loss: 0.08827, BVP1 Loss: 0.00000, BVP2 Loss: 0.08946, IVP Loss: 0.01379,\n",
      "28000/60000 PDE Loss: 0.04693, BVP1 Loss: 0.00000, BVP2 Loss: 0.03667, IVP Loss: 0.00928,\n",
      "28500/60000 PDE Loss: 0.03892, BVP1 Loss: 0.00000, BVP2 Loss: 0.01665, IVP Loss: 0.00808,\n",
      "29000/60000 PDE Loss: 0.02988, BVP1 Loss: 0.00001, BVP2 Loss: 0.00811, IVP Loss: 0.00759,\n",
      "29500/60000 PDE Loss: 0.02549, BVP1 Loss: 0.00000, BVP2 Loss: 0.00454, IVP Loss: 0.00587,\n",
      "30000/60000 PDE Loss: 0.01555, BVP1 Loss: 0.00000, BVP2 Loss: 0.00168, IVP Loss: 0.00683,\n",
      "30500/60000 PDE Loss: 0.01446, BVP1 Loss: 0.00000, BVP2 Loss: 0.00024, IVP Loss: 0.00555,\n",
      "31000/60000 PDE Loss: 0.01252, BVP1 Loss: 0.00000, BVP2 Loss: 0.00016, IVP Loss: 0.00591,\n",
      "31500/60000 PDE Loss: 0.01149, BVP1 Loss: 0.00000, BVP2 Loss: 0.00010, IVP Loss: 0.00593,\n",
      "32000/60000 PDE Loss: 0.00841, BVP1 Loss: 0.00000, BVP2 Loss: 0.00009, IVP Loss: 0.00462,\n",
      "32500/60000 PDE Loss: 0.00943, BVP1 Loss: 0.00000, BVP2 Loss: 0.00011, IVP Loss: 0.00483,\n",
      "33000/60000 PDE Loss: 0.00812, BVP1 Loss: 0.00000, BVP2 Loss: 0.00006, IVP Loss: 0.00517,\n",
      "33500/60000 PDE Loss: 0.00905, BVP1 Loss: 0.00000, BVP2 Loss: 0.00004, IVP Loss: 0.00656,\n",
      "34000/60000 PDE Loss: 0.00779, BVP1 Loss: 0.00000, BVP2 Loss: 0.00004, IVP Loss: 0.00391,\n",
      "34500/60000 PDE Loss: 0.00753, BVP1 Loss: 0.00000, BVP2 Loss: 0.00020, IVP Loss: 0.00492,\n",
      "35000/60000 PDE Loss: 0.00542, BVP1 Loss: 0.00000, BVP2 Loss: 0.00002, IVP Loss: 0.00530,\n",
      "35500/60000 PDE Loss: 0.00707, BVP1 Loss: 0.00000, BVP2 Loss: 0.00013, IVP Loss: 0.00537,\n",
      "36000/60000 PDE Loss: 0.00539, BVP1 Loss: 0.00001, BVP2 Loss: 0.00007, IVP Loss: 0.00346,\n",
      "36500/60000 PDE Loss: 0.00517, BVP1 Loss: 0.00000, BVP2 Loss: 0.00003, IVP Loss: 0.00580,\n",
      "37000/60000 PDE Loss: 0.00426, BVP1 Loss: 0.00000, BVP2 Loss: 0.00015, IVP Loss: 0.00510,\n",
      "37500/60000 PDE Loss: 0.01128, BVP1 Loss: 0.00000, BVP2 Loss: 0.00020, IVP Loss: 0.00567,\n",
      "38000/60000 PDE Loss: 0.00398, BVP1 Loss: 0.00000, BVP2 Loss: 0.00003, IVP Loss: 0.00521,\n",
      "38500/60000 PDE Loss: 0.00326, BVP1 Loss: 0.00000, BVP2 Loss: 0.00020, IVP Loss: 0.00559,\n",
      "39000/60000 PDE Loss: 0.00332, BVP1 Loss: 0.00000, BVP2 Loss: 0.00027, IVP Loss: 0.00418,\n",
      "39500/60000 PDE Loss: 0.00414, BVP1 Loss: 0.00002, BVP2 Loss: 0.00001, IVP Loss: 0.00437,\n",
      "40000/60000 PDE Loss: 0.00352, BVP1 Loss: 0.00000, BVP2 Loss: 0.00003, IVP Loss: 0.00484,\n",
      "40500/60000 PDE Loss: 0.00345, BVP1 Loss: 0.00000, BVP2 Loss: 0.00043, IVP Loss: 0.00409,\n",
      "41000/60000 PDE Loss: 0.00383, BVP1 Loss: 0.00000, BVP2 Loss: 0.00001, IVP Loss: 0.00482,\n",
      "41500/60000 PDE Loss: 0.00262, BVP1 Loss: 0.00000, BVP2 Loss: 0.00010, IVP Loss: 0.00508,\n",
      "42000/60000 PDE Loss: 0.00311, BVP1 Loss: 0.00000, BVP2 Loss: 0.00001, IVP Loss: 0.00426,\n",
      "42500/60000 PDE Loss: 0.00251, BVP1 Loss: 0.00001, BVP2 Loss: 0.00001, IVP Loss: 0.00460,\n",
      "43000/60000 PDE Loss: 0.00241, BVP1 Loss: 0.00000, BVP2 Loss: 0.00002, IVP Loss: 0.00485,\n",
      "43500/60000 PDE Loss: 0.00284, BVP1 Loss: 0.00002, BVP2 Loss: 0.00003, IVP Loss: 0.00376,\n",
      "44000/60000 PDE Loss: 0.00253, BVP1 Loss: 0.00000, BVP2 Loss: 0.00068, IVP Loss: 0.00627,\n",
      "44500/60000 PDE Loss: 0.00231, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00426,\n",
      "45000/60000 PDE Loss: 0.00282, BVP1 Loss: 0.00000, BVP2 Loss: 0.00045, IVP Loss: 0.00464,\n",
      "45500/60000 PDE Loss: 0.00233, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00372,\n",
      "46000/60000 PDE Loss: 0.00225, BVP1 Loss: 0.00000, BVP2 Loss: 0.00007, IVP Loss: 0.00352,\n",
      "46500/60000 PDE Loss: 0.00250, BVP1 Loss: 0.00000, BVP2 Loss: 0.00001, IVP Loss: 0.00600,\n",
      "47000/60000 PDE Loss: 0.00447, BVP1 Loss: 0.00001, BVP2 Loss: 0.00010, IVP Loss: 0.00367,\n",
      "47500/60000 PDE Loss: 0.00235, BVP1 Loss: 0.00000, BVP2 Loss: 0.00004, IVP Loss: 0.00461,\n",
      "48000/60000 PDE Loss: 0.00296, BVP1 Loss: 0.00000, BVP2 Loss: 0.00015, IVP Loss: 0.00370,\n",
      "48500/60000 PDE Loss: 0.00397, BVP1 Loss: 0.00001, BVP2 Loss: 0.00018, IVP Loss: 0.00394,\n",
      "49000/60000 PDE Loss: 0.00275, BVP1 Loss: 0.00000, BVP2 Loss: 0.00031, IVP Loss: 0.00432,\n",
      "49500/60000 PDE Loss: 0.00269, BVP1 Loss: 0.00000, BVP2 Loss: 0.00063, IVP Loss: 0.00518,\n",
      "50000/60000 PDE Loss: 0.00258, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00360,\n",
      "50500/60000 PDE Loss: 0.00350, BVP1 Loss: 0.00000, BVP2 Loss: 0.00049, IVP Loss: 0.00353,\n",
      "51000/60000 PDE Loss: 0.00242, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00433,\n",
      "51500/60000 PDE Loss: 0.00227, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00368,\n",
      "52000/60000 PDE Loss: 0.00388, BVP1 Loss: 0.00001, BVP2 Loss: 0.00088, IVP Loss: 0.00578,\n",
      "52500/60000 PDE Loss: 0.00216, BVP1 Loss: 0.00000, BVP2 Loss: 0.00014, IVP Loss: 0.00414,\n",
      "53000/60000 PDE Loss: 0.00273, BVP1 Loss: 0.00000, BVP2 Loss: 0.00011, IVP Loss: 0.00312,\n",
      "53500/60000 PDE Loss: 0.00218, BVP1 Loss: 0.00000, BVP2 Loss: 0.00004, IVP Loss: 0.00328,\n",
      "54000/60000 PDE Loss: 0.00211, BVP1 Loss: 0.00001, BVP2 Loss: 0.00005, IVP Loss: 0.00343,\n",
      "54500/60000 PDE Loss: 0.00264, BVP1 Loss: 0.00000, BVP2 Loss: 0.00002, IVP Loss: 0.00401,\n",
      "55000/60000 PDE Loss: 0.00298, BVP1 Loss: 0.00000, BVP2 Loss: 0.00019, IVP Loss: 0.00286,\n",
      "55500/60000 PDE Loss: 0.00242, BVP1 Loss: 0.00001, BVP2 Loss: 0.00008, IVP Loss: 0.00364,\n",
      "56000/60000 PDE Loss: 0.00209, BVP1 Loss: 0.00000, BVP2 Loss: 0.00002, IVP Loss: 0.00319,\n",
      "56500/60000 PDE Loss: 0.00169, BVP1 Loss: 0.00001, BVP2 Loss: 0.00025, IVP Loss: 0.00357,\n",
      "57000/60000 PDE Loss: 0.00189, BVP1 Loss: 0.00000, BVP2 Loss: 0.00002, IVP Loss: 0.00265,\n",
      "57500/60000 PDE Loss: 0.00275, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00300,\n",
      "58000/60000 PDE Loss: 0.00240, BVP1 Loss: 0.00000, BVP2 Loss: 0.00001, IVP Loss: 0.00280,\n",
      "58500/60000 PDE Loss: 0.00214, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00307,\n",
      "59000/60000 PDE Loss: 0.00161, BVP1 Loss: 0.00000, BVP2 Loss: 0.00000, IVP Loss: 0.00329,\n",
      "59500/60000 PDE Loss: 0.00201, BVP1 Loss: 0.00000, BVP2 Loss: 0.00006, IVP Loss: 0.00270,\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # PDE Round\n",
    "    X1, y1 = get_diff_data(samples['pde'])\n",
    "    X1 = torch.from_numpy(X1).float().requires_grad_().cuda()\n",
    "    y1 = torch.from_numpy(y1).float().cuda()\n",
    "    \n",
    "    y1_hat = model(X1)\n",
    "    \n",
    "    grads = tgrad.grad(y1_hat, X1, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, X1, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = X1[:, 1].view(-1, 1)\n",
    "    pde_loss = criterion(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    \n",
    "    # BVP Round\n",
    "    X21, y21, X22, y22 = get_bvp_data(samples['bvp'])\n",
    "    \n",
    "    X21 = torch.from_numpy(X21).float().cuda()\n",
    "    y21 = torch.from_numpy(y21).float().cuda()\n",
    "    \n",
    "    X22 = torch.from_numpy(X22).float().cuda()\n",
    "    y22 = torch.from_numpy(y22).float().cuda()\n",
    "    \n",
    "    y21_hat = model(X21)\n",
    "    bvp1_loss = criterion(y21, y21_hat)\n",
    "    \n",
    "    y22_hat = model(X22)\n",
    "    bvp2_loss = criterion(y22, y22_hat)\n",
    "    \n",
    "    \n",
    "    # IVP Round\n",
    "    X3, y3 = get_ivp_data(samples['ivp'])\n",
    "    \n",
    "    X3 = torch.from_numpy(X3).float().cuda()\n",
    "    y3 = torch.from_numpy(y3).float().cuda()\n",
    "    \n",
    "    y3_hat = model(X3)\n",
    "    ivp_loss = criterion(y3, y3_hat)\n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad()\n",
    "    combined_loss = pde_loss + bvp1_loss + bvp2_loss + ivp_loss\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BVP1 Loss: {bvp1_loss.item():.5f}, BVP2 Loss: {bvp2_loss.item():.5f}, IVP Loss: {ivp_loss.item():.5f},')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Put Option Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x, val: np.fmax(K-x, val)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1], 0).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y2 = K*np.ones((n, 1))\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Round\n",
    "X1, y1 = get_diff_data(samples['pde'])\n",
    "X1 = torch.from_numpy(X1).float().requires_grad_().cuda()\n",
    "y1 = torch.from_numpy(y1).float().cuda()\n",
    "y1_hat = model(X1)\n",
    "\n",
    "grads = tgrad.grad(y1_hat, X1, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "grads2nd = tgrad.grad(dVdS, X1, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "\n",
    "S1 = X1[:, 1].view(-1, 1)\n",
    "yint = torch.max(K - S1, torch.zeros_like(S1))\n",
    "\n",
    "pde = (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)*(y1_hat - yint)\n",
    "pde_loss = criterion(pde, torch.zeros_like(pde)) + criterion(torch.max(-y1_hat + yint, torch.zeros_like(yint)),  torch.zeros_like(yint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
