{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A direct copy from: https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Formula\n",
    "The Black–Scholes formula calculates the price of European put and call options. This price is consistent with the Black–Scholes equation. This follows since the formula can be obtained by solving the equation for the corresponding terminal and boundary conditions:\n",
    "$$    \n",
    "\n",
    "    {\\begin{aligned}&C(0,t)=0{\\text{ for all }}t\\\\&C(S,t)\\rightarrow S-K{\\text{ as }}S\\rightarrow \\infty \\\\&C(S,T)=\\max\\{S-K,0\\}\\end{aligned}}\n",
    "\n",
    "    $$\n",
    "\n",
    "The value of a call option for a non-dividend-paying underlying stock in terms of the Black–Scholes parameters is:\n",
    "\n",
    "$$\n",
    "    {\\begin{aligned}C(S_{t},t)&=N(d_{+})S_{t}-N(d_{-})Ke^{-r(T-t)}\\\\d_{+}&={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S_{t}}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{-}&=d_{+}-\\sigma {\\sqrt {T-t}}\\\\\\end{aligned}}\n",
    "    $$\n",
    "\n",
    "The price of a corresponding put option based on put–call parity with discount factor $e^{{-r(T-t)}}$ is:\n",
    "$$\n",
    "    {\\begin{aligned}P(S_{t},t)&=Ke^{-r(T-t)}-S_{t}+C(S_{t},t)\\\\&=N(-d_{-})Ke^{-r(T-t)}-N(-d_{+})S_{t}\\end{aligned}}\\,\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option Price: 17.01496124267578\n"
     ]
    }
   ],
   "source": [
    "S = torch.Tensor([80]).requires_grad_()\n",
    "t = torch.Tensor([0]).requires_grad_()\n",
    "sigma = torch.Tensor([0.3]).requires_grad_()\n",
    "r = torch.Tensor([0.05]).requires_grad_()\n",
    "K = torch.Tensor([70])\n",
    "T = torch.Tensor([1])\n",
    "t2m = T-t\n",
    "d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * t2m)/(sigma * torch.sqrt(t2m))\n",
    "d2 = d1 - sigma * torch.sqrt(t2m)\n",
    "N0 = lambda value: 0.5 * (1 + torch.erf((value/2**0.5)))\n",
    "Nd1 = N0(d1)\n",
    "Nd2 = N0(d2)\n",
    "C = S* Nd1 - K* Nd2 *torch.exp(-r*t2m)\n",
    "print(\"Option Price:\", C.item()) #17.01496"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Scholes Equation\n",
    "The gradient is calculated using the auto gradient method in pytorch.\n",
    "\n",
    "$\n",
    "{\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n",
    "$\n",
    "\n",
    "To check the correctness of the calculation, it uses the Greeks equation from the Black-Scholes Formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8385\n",
      "0.0\n",
      "0.7769\n",
      "0.0\n",
      "0.0124\n",
      "0.0\n",
      "23.8776\n",
      "0.0\n",
      "45.1372\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dCdt, = tgrad.grad(C, t, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "dCdS, = tgrad.grad(C, S, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "d2CdS2, = tgrad.grad(dCdS, S, grad_outputs=torch.ones(dCdS.shape), create_graph=True, only_inputs=True)\n",
    "dCdvol, = tgrad.grad(C, sigma, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "\n",
    "dCdr, = tgrad.grad(C, r, grad_outputs=torch.ones(C.shape), create_graph=True, only_inputs=True)\n",
    "theta, delta, gamma, vega, rho = -dCdt[0], dCdS[0], d2CdS2[0], dCdvol[0], dCdr[0]\n",
    "\n",
    "for og in [theta, delta, gamma, vega, rho]:\n",
    "    print(f'{og.item():.4f}')\n",
    "\n",
    "    # Theta 5.8385\n",
    "    # Delta 0.7769\n",
    "    # Gamma 0.0124\n",
    "    # Vega 23.8776\n",
    "    # Rho 45.1372\n",
    "\n",
    "    print((-theta + 0.5*sigma**2 * S**2*gamma + r*S*delta - r*C).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, Initial Value conditions and the PDE itself.\n",
    "\n",
    "The data samples are generated by the three functions:\n",
    "\n",
    "1.    Sampler of data inputs for t and S for Differential Loss get_diff_data()\n",
    "2.    Sampler of data inputs satisfying the boundary conditions for the PDE get_bvp_data()\n",
    "3.    Sampler of data inputs satisfying the initial value conditions for the PDE get_ivp_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y2 = (S_range[-1] - K*np.exp(-r*(T-X2[:, 0].reshape(-1)))).reshape(-1, 1)\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method Model Construction\n",
    "\n",
    "The Neural Network Model based on https://arxiv.org/abs/1708.07469?context=q-fin.MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMCell(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, n_layers=3, output_dim=1):\n",
    "    super(DGMCell, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.n = n_layers\n",
    "\n",
    "    self.sig_act = nn.Tanh()\n",
    "\n",
    "    self.Sw = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "\n",
    "    self.Uz = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsz = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Ug = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsg = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Ur = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsr = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    \n",
    "    self.Uh = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.Wsh = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    self.Wf = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    S1 = self.Sw(x)\n",
    "    for i in range(self.n):\n",
    "      if i==0:\n",
    "        S = S1\n",
    "      else:\n",
    "        S = self.sig_act(out)\n",
    "      Z = self.sig_act(self.Uz(x) + self.Wsz(S))\n",
    "      G = self.sig_act(self.Ug(x) + self.Wsg(S1))\n",
    "      R = self.sig_act(self.Ur(x) + self.Wsr(S))\n",
    "      H = self.sig_act(self.Uh(x) + self.Wsh(S*R))\n",
    "      out = (1-G)*H + Z*S\n",
    "    out = self.Wf(out)\n",
    "    return out\n",
    "\n",
    "model = DGMCell(2, 100, 3, 1)\n",
    "model.cuda()\n",
    "\n",
    "n_epochs = 60000\n",
    "samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "- For each iteration in the training loop, we are sampling data for the three physical conditions of the PDE.\n",
    "- Then we are calculating the loss three times on the same model, accumulating them into a combined objective function to be minimised for the Neural Network.\n",
    "- The first loss is the differential equation loss. Here we are trying to minimise the PDE by calculating gradients and forming the PDE itself.\n",
    "- The remaining losses are calculated for boundary value and initial value conditions for the PDE.\n",
    "- Mean Squared Error loss function nn.MSELoss() is chosen as the criterion to be minimised and Adam optimizer nn.optim.Adam(lr=3e-5)with a learning rate of 0.00003 is chosen for performing the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00088, BVP1 Loss: 0.26766, BVP2 Loss: 8182.09082, IVP Loss: 1837.09436,\n",
      "500/60000 PDE Loss: 0.09695, BVP1 Loss: 0.00002, BVP2 Loss: 7612.56152, IVP Loss: 1659.96338,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     47\u001b[0m combined_loss \u001b[39m=\u001b[39m pde_loss \u001b[39m+\u001b[39m bvp1_loss \u001b[39m+\u001b[39m bvp2_loss \u001b[39m+\u001b[39m ivp_loss\n\u001b[1;32m---> 48\u001b[0m combined_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     51\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(combined_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # PDE Round\n",
    "    X1, y1 = get_diff_data(samples['pde'])\n",
    "    X1 = torch.from_numpy(X1).float().requires_grad_().cuda()\n",
    "    y1 = torch.from_numpy(y1).float().cuda()\n",
    "    \n",
    "    y1_hat = model(X1)\n",
    "    \n",
    "    grads = tgrad.grad(y1_hat, X1, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, X1, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = X1[:, 1].view(-1, 1)\n",
    "    pde_loss = criterion(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    \n",
    "    # BVP Round\n",
    "    X21, y21, X22, y22 = get_bvp_data(samples['bvp'])\n",
    "    \n",
    "    X21 = torch.from_numpy(X21).float().cuda()\n",
    "    y21 = torch.from_numpy(y21).float().cuda()\n",
    "    \n",
    "    X22 = torch.from_numpy(X22).float().cuda()\n",
    "    y22 = torch.from_numpy(y22).float().cuda()\n",
    "    \n",
    "    y21_hat = model(X21)\n",
    "    bvp1_loss = criterion(y21, y21_hat)\n",
    "    \n",
    "    y22_hat = model(X22)\n",
    "    bvp2_loss = criterion(y22, y22_hat)\n",
    "    \n",
    "    \n",
    "    # IVP Round\n",
    "    X3, y3 = get_ivp_data(samples['ivp'])\n",
    "    \n",
    "    X3 = torch.from_numpy(X3).float().cuda()\n",
    "    y3 = torch.from_numpy(y3).float().cuda()\n",
    "    \n",
    "    y3_hat = model(X3)\n",
    "    ivp_loss = criterion(y3, y3_hat)\n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad()\n",
    "    combined_loss = pde_loss + bvp1_loss + bvp2_loss + ivp_loss\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_hist.append(combined_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BVP1 Loss: {bvp1_loss.item():.5f}, BVP2 Loss: {bvp2_loss.item():.5f}, IVP Loss: {ivp_loss.item():.5f},')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Put Option Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x, val: np.fmax(K-x, val)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1], 0).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y2 = K*np.ones((n, 1))\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Round\n",
    "X1, y1 = get_diff_data(samples['pde'])\n",
    "X1 = torch.from_numpy(X1).float().requires_grad_().cuda()\n",
    "y1 = torch.from_numpy(y1).float().cuda()\n",
    "y1_hat = model(X1)\n",
    "\n",
    "grads = tgrad.grad(y1_hat, X1, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "grads2nd = tgrad.grad(dVdS, X1, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "\n",
    "S1 = X1[:, 1].view(-1, 1)\n",
    "yint = torch.max(K - S1, torch.zeros_like(S1))\n",
    "\n",
    "pde = (dVdt + 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)*(y1_hat - yint)\n",
    "pde_loss = criterion(pde, torch.zeros_like(pde)) + criterion(torch.max(-y1_hat + yint, torch.zeros_like(yint)),  torch.zeros_like(yint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
