{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.14.0-dev20230525\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, i illustrate physics informed nerual netowrks (PINNs) to solve the Balck Scholes Equation as proposed in\n",
    "\n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations*. [arXiv 1711.10561](https://arxiv.org/abs/1711.10561) \n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations*. [arXiv 1711.10566](https://arxiv.org/abs/1711.10566) \n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations*. J. Comp. Phys. 378 pp. 686-707 [DOI: 10.1016/j.jcp.2018.10.045](https://www.sciencedirect.com/science/article/pii/S0021999118307125) \n",
    "\n",
    "This notebook is partially based on another implementation of the PINN approach published on [Google Colab by janblechschmidt](https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681), [Medium by Harsha Andey](https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681), [GitHub by pierremtb](https://github.com/pierremtb/PINNs-TF2.0) as well as the original code, see  [Maziar Raissi on GitHub](https://github.com/maziarraissi/PINNs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We describe the PINN approach for approximating the solution $u:[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$ of an evolution equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\partial_t u (t,x) + \\mathcal{N}[u](t,x) &= 0, && (t,x) \\in (0,T] \\times \\mathcal{D},\\\\\n",
    "    u(0,x) &= u_0(x) \\quad && x \\in \\mathcal{D},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}$ is a nonlinear differential operator acting on $u$, \n",
    "$\\mathcal{D} \\subset \\mathbb{R}^d$ a bounded domain,\n",
    "$T$ denotes the final time and\n",
    "$u_0: \\mathcal{D} \\to \\mathbb{R}$ the prescribed initial data.\n",
    "Although the methodology allows for different types of boundary conditions, we restrict our discussion to the inhomogeneous Dirichlet case and prescribe\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "        \\hspace{7em} u(t,x) &= u_b(t,x)  && \\quad (t,x) \\in (0,T] \\times \\partial \\mathcal{D},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\partial \\mathcal{D}$ denotes the boundary of the domain $\\mathcal{D}$ and $u_b: (0,T] \\times \\partial \\mathcal{D} \\to \\mathbb{R}$ the given boundary data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "The method constructs a neural network approximation\n",
    "\n",
    "$$\n",
    "u_\\theta(t,x) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "of the solution of nonlinear PDE, where $u_\\theta :[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$ denotes a function realized by a neural network with parameters $\\theta$.\n",
    "\n",
    "The continuous time approach for the parabolic PDE as described in ([Raissi et al., 2017 (Part I)](https://arxiv.org/abs/1711.10561)) is based on the (strong) residual of a given neural network approximation $u_\\theta \\colon [0,T] \\times \\mathcal{D} \\to \\mathbb{R} $ of the solution $u$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r_\\theta (t,x) := \\partial_t u_\\theta (t,x) + \\mathcal{N}[u_\\theta] (t,x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To incorporate this PDE residual $r_\\theta$ into a loss function to be minimized, PINNs require a further differentiation to evaluate the differential operators $\\partial_t u_\\theta$ and $\\mathcal{N}[u_\\theta]$.\n",
    "Thus the PINN term $r_\\theta$ shares the same parameters as the original network $u_\\theta(t,x)$, but respects the underlying \"physics\" of the nonlinear PDE.\n",
    "Both types of derivatives can be easily determined through automatic differentiation with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch.\n",
    "\n",
    "The PINN approach for the solution of the initial and boundary value problem now proceeds by minimization of the loss functional\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_\\theta(X) := \\phi_\\theta^r(X^r) + \\phi_\\theta^0(X^0) + \\phi_\\theta^b(X^b),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $X$ denotes the collection of training data and the loss function $\\phi_\\theta$ contains the following terms:\n",
    "\n",
    "  - the mean squared residual\n",
    "$$\n",
    "  \\begin{align*}\n",
    "   \\phi_\\theta^r(X^r) := \\frac{1}{N_r}\\sum_{i=1}^{N_r} \\left|r_\\theta\\left(t_i^r, x_i^r\\right)\\right|^2\n",
    "\\end{align*}\n",
    "$$\n",
    "in a number of collocation points $X^r:=\\{(t_i^r, x_i^r)\\}_{i=1}^{N_r} \\subset (0,T] \\times \\mathcal{D}$, where $r_\\theta$ is the physics-informed neural network,\n",
    "  - the mean squared misfit with respect to the initial and boundary conditions\n",
    "$$\n",
    "    \\begin{align*}\n",
    "   \\phi_\\theta^0(X^0) \n",
    "   := \n",
    "   \\frac{1}{N_0}\n",
    "   \\sum_{i=1}^{N_0} \\left|u_\\theta\\left(t_i^0, x_i^0\\right) - u_0\\left(x_i^0\\right)\\right|^2\n",
    "   \\quad \\text{ and } \\quad\n",
    "   \\phi_\\theta^b(X^b) \n",
    "   := \n",
    "   \\frac{1}{N_b}\n",
    "   \\sum_{i=1}^{N_b} \\left|u_\\theta\\left(t_i^b, x_i^b\\right) - u_b\\left(t_i^b, x_i^b\\right)\\right|^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "in a number of points $X^0:=\\{(t^0_i,x^0_i)\\}_{i=1}^{N_0} \\subset \\{0\\} \\times \\mathcal{D}$ and $X^b:=\\{(t^b_i,x^b_i)\\}_{i=1}^{N_b} \\subset (0,T] \\times \\partial \\mathcal{D}$, where $u_\\theta$ is the neural network approximation of the solution $u\\colon[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$.\n",
    "\n",
    "Note that the training data $X$ consists entirely of time-space coordinates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Scholes Model\n",
    "For a European call option:\n",
    "\n",
    "$$\n",
    "C(S, t) = SN(d1) - Ke^(-rt)N(d2)\n",
    "$$\n",
    "\n",
    "For a European put option:\n",
    "\n",
    "$$\n",
    "P(S, t) = Ke^(-rt)N(-d2) - SN(-d1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- C(S,t) is the value of the call option at time t\n",
    "- P(S,t) is the value of the put option at time t\n",
    "- S is the spot price of the underlying asset\n",
    "- K is the strike price\n",
    "- r is the risk-free interest rate\n",
    "- t is the time to maturity\n",
    "- N() is the cumulative distribution function of the standard normal distribution\n",
    "- $d1 = [ln(S/K) + (r + 0.5σ²)(T-t)] / σ√(T-t)$\n",
    "- $d2 = d1 - σ√(T-t)$\n",
    "The variables within the square root sign σ and T represent the volatility of returns of the underlying asset and the time to maturity respectively.\n",
    "\n",
    "The Black-Scholes model assumes that markets are efficient which means that there are no arbitrage opportunities, i.e., it is impossible to make a riskless profit. It also assumes that the volatility of the underlying asset is constant over time, and that the returns on the underlying asset are normally distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Scholes Partial Differential Equation\n",
    "$$\n",
    "{\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n",
    "$$\n",
    "\n",
    "where we have greeks in the formula to measuer the sensitivity of the value of a portfolio to a small change in a given undrelying parameter. More explanation can be found in [Option Greeks from Investopedia by JohnSumma](https://www.investopedia.com/trading/getting-to-know-the-greeks/).\n",
    "\n",
    "Five main greeks are:\n",
    "- Delta: ${\\frac {\\partial V}{\\partial S}}$ Measures impact of a change in the price of underlying\n",
    "- Gamma: ${\\frac {\\partial ^{2}V}{\\partial S^{2}}}$ Measures the rate of change of delta\n",
    "- Theta: ${\\frac {\\partial V}{\\partial t}}$ Measures impact of a change in time remaining\n",
    "- Vega: ${\\frac {\\partial V}{\\partial \\sigma}}$ Measures impact of a change in volatility\n",
    "- Rho: ${\\frac {\\partial V}{\\partial r}}$ Measures the sensitivity of Derivative price w.r.t interest rate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "S = tf.Variable([80.0], dtype=tf.float32)\n",
    "t = tf.Variable([0.0], dtype=tf.float32)\n",
    "sigma = tf.Variable([0.3], dtype=tf.float32)\n",
    "r = tf.Variable([0.05], dtype=tf.float32)\n",
    "K = tf.constant([70.0], dtype=tf.float32)\n",
    "T = tf.constant([1.0], dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the Greeks, we use the `tf.GradientTape` for computing gradients as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.8385\n",
      "0.7769\n",
      "0.0124\n",
      "23.8776\n",
      "45.1372\n",
      "Result: 0.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([t, S, sigma, r])\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([t, S, sigma, r])\n",
    "        t2m = T - t\n",
    "        d1 = (tf.math.log(S / K) + (r + 0.5 * sigma**2) * t2m) / (sigma * tf.sqrt(t2m))\n",
    "        d2 = d1 - sigma * tf.sqrt(t2m)\n",
    "        N0 = lambda value: 0.5 * (1 + tf.math.erf(value / tf.sqrt(2.0))) # type: ignore\n",
    "        Nd1 = N0(d1)\n",
    "        Nd2 = N0(d2)\n",
    "        C = S * Nd1 - K * Nd2 * tf.exp(-r * t2m)\n",
    "    theta = tape2.gradient(C, t, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    delta = tape2.gradient(C, S, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    vega = tape2.gradient(C, sigma, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    rho = tape2.gradient(C, r, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    \n",
    "\n",
    "gamma = tape.gradient(delta, S, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "\n",
    "\n",
    "for og in [theta, delta, gamma, vega, rho]:\n",
    "    print(f'{og:.4f}')\n",
    "\n",
    "result = (theta + 0.5 * sigma**2 * S**2 * gamma + r * S * delta - r * C).numpy()\n",
    "print(f\"Result: {result[0]:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the option price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option Price: tf.Tensor(17.014957, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "C = S * Nd1 - K * Nd2 * tf.exp(-r * t2m)\n",
    "\n",
    "print(\"Option Price:\", C[0]) #17.01496"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, Initial Value conditions and the PDE itself.\n",
    "\n",
    "The data samples are generated by the three functions:\n",
    "- Sampler of data inputs for t and S for Differential Loss `get_diff_data()`\n",
    "- Sampler of data inputs satisfying the boundary conditions for the PDE `get_bvp_data()`\n",
    "- Sampler of data inputs satisfying the initial value conditions for the PDE `get_ivp_data()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y2 = (S_range[-1] - K*np.exp(-r*(T-X2[:, 0].reshape(-1)))).reshape(-1, 1)\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up network architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Galerkin Method Model Construction\n",
    "The Neural Network based on the below model architecture given in Reference[1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DGMCell(tf.keras.Model):\n",
    "#     def __init__(self, input_dim, hidden_dim, n_layers=3, output_dim=1):\n",
    "#         super(DGMCell, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.n = n_layers\n",
    "\n",
    "#         self.sig_act = tf.keras.activations.tanh\n",
    "\n",
    "#         self.Sw = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Uz = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsz = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Ug = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsg = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Ur = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsr = tf.keras.layers.Dense(self.hidden_dim)\n",
    "        \n",
    "#         self.Uh = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsh = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Wf = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "#     def call(self, x):\n",
    "#         S1 = self.Sw(x)\n",
    "#         for i in range(self.n):\n",
    "#             if i==0:\n",
    "#                 S = S1\n",
    "#             else:\n",
    "#                 S = self.sig_act(out)\n",
    "#             Z = self.sig_act(self.Uz(x) + self.Wsz(S))\n",
    "#             G = self.sig_act(self.Ug(x) + self.Wsg(S1))\n",
    "#             R = self.sig_act(self.Ur(x) + self.Wsr(S))\n",
    "#             H = self.sig_act(self.Uh(x) + self.Wsh(S*R))\n",
    "#             out = (1-G)*H + Z*S\n",
    "#         out = self.Wf(out)\n",
    "#         return out\n",
    "\n",
    "# model = DGMCell(2, 100, 3, 1)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# n_epochs = 60000\n",
    "# samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "# criterion = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A feedforward neural network of the following architecture\n",
    "- one input layer with 2 neurons\n",
    "- 8 fully connected layers each containing 20 neurons and each followed by a hyperboloc tangent actiavtion function,\n",
    "- one fully connected ouput layer\n",
    "\n",
    "This setting results in a network with 3021 trainable parameters (first hidden layer: 2⋅20+20=60; seven intermediate layers: each 20⋅20+20=420; output layer: 20⋅1+1=21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3021 (11.80 KB)\n",
      "Trainable params: 3021 (11.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 2  # Number of input features\n",
    "hidden_size = 20  # Number of neurons in the hidden layer\n",
    "output_size = 1   # Number of output features\n",
    "n_layers = 8\n",
    "learning_rate = 3e-5\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(input_size,))\n",
    "\n",
    "# Add hidden layers using a for loop\n",
    "x = inputs\n",
    "for i in range(n_layers):\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    \n",
    "# Output layer\n",
    "outputs = tf.keras.layers.Dense(output_size)(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "n_epochs = 60000\n",
    "samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "criterion = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00011, BVP1 Loss: 0.00007, BVP2 Loss: 7284.34863, IVP Loss: 1627.46265,\n",
      "500/60000 PDE Loss: 0.02954, BVP1 Loss: 11.81256, BVP2 Loss: 7664.87695, IVP Loss: 1660.55750,\n",
      "1000/60000 PDE Loss: 0.11038, BVP1 Loss: 44.15009, BVP2 Loss: 7113.64795, IVP Loss: 1433.27441,\n",
      "1500/60000 PDE Loss: 0.22924, BVP1 Loss: 91.69463, BVP2 Loss: 6627.12891, IVP Loss: 1364.97559,\n",
      "2000/60000 PDE Loss: 0.37543, BVP1 Loss: 150.17056, BVP2 Loss: 6198.16699, IVP Loss: 1266.52478,\n",
      "2500/60000 PDE Loss: 0.54049, BVP1 Loss: 216.19650, BVP2 Loss: 5818.31396, IVP Loss: 1178.24902,\n",
      "3000/60000 PDE Loss: 0.71749, BVP1 Loss: 286.99170, BVP2 Loss: 5480.41504, IVP Loss: 1129.14905,\n",
      "3500/60000 PDE Loss: 0.90111, BVP1 Loss: 360.44141, BVP2 Loss: 5184.84619, IVP Loss: 1063.84119,\n",
      "4000/60000 PDE Loss: 1.08725, BVP1 Loss: 434.89270, BVP2 Loss: 4918.24219, IVP Loss: 1001.18958,\n",
      "4500/60000 PDE Loss: 1.27269, BVP1 Loss: 509.06946, BVP2 Loss: 4682.86865, IVP Loss: 973.80878,\n",
      "5000/60000 PDE Loss: 1.45482, BVP1 Loss: 581.92078, BVP2 Loss: 4471.21533, IVP Loss: 949.70361,\n",
      "5500/60000 PDE Loss: 1.63204, BVP1 Loss: 652.80469, BVP2 Loss: 4281.08008, IVP Loss: 885.25201,\n",
      "6000/60000 PDE Loss: 1.80287, BVP1 Loss: 721.13495, BVP2 Loss: 4112.54102, IVP Loss: 923.27179,\n",
      "6500/60000 PDE Loss: 1.96628, BVP1 Loss: 786.50208, BVP2 Loss: 3959.91870, IVP Loss: 910.48419,\n",
      "7000/60000 PDE Loss: 2.12180, BVP1 Loss: 848.70398, BVP2 Loss: 3826.99487, IVP Loss: 904.04968,\n",
      "7500/60000 PDE Loss: 2.26907, BVP1 Loss: 907.61298, BVP2 Loss: 3704.45874, IVP Loss: 896.77917,\n",
      "8000/60000 PDE Loss: 2.40799, BVP1 Loss: 963.17639, BVP2 Loss: 3594.51050, IVP Loss: 871.80981,\n",
      "8500/60000 PDE Loss: 2.53871, BVP1 Loss: 1015.46259, BVP2 Loss: 3495.74048, IVP Loss: 895.33282,\n",
      "9000/60000 PDE Loss: 2.66111, BVP1 Loss: 1064.41675, BVP2 Loss: 3404.91284, IVP Loss: 877.83478,\n",
      "9500/60000 PDE Loss: 2.77559, BVP1 Loss: 1110.20264, BVP2 Loss: 3326.09644, IVP Loss: 901.86951,\n",
      "10000/60000 PDE Loss: 2.88210, BVP1 Loss: 1152.80457, BVP2 Loss: 3253.20117, IVP Loss: 901.60541,\n",
      "10500/60000 PDE Loss: 2.98135, BVP1 Loss: 1192.49756, BVP2 Loss: 3185.23438, IVP Loss: 905.44531,\n",
      "11000/60000 PDE Loss: 3.07336, BVP1 Loss: 1229.30103, BVP2 Loss: 3128.27490, IVP Loss: 904.00677,\n",
      "11500/60000 PDE Loss: 3.15894, BVP1 Loss: 1263.52283, BVP2 Loss: 3073.26099, IVP Loss: 930.16370,\n",
      "12000/60000 PDE Loss: 3.23809, BVP1 Loss: 1295.18701, BVP2 Loss: 3024.40649, IVP Loss: 923.73560,\n",
      "12500/60000 PDE Loss: 3.31137, BVP1 Loss: 1324.16040, BVP2 Loss: 2979.61719, IVP Loss: 932.44458,\n",
      "13000/60000 PDE Loss: 3.37961, BVP1 Loss: 1350.12012, BVP2 Loss: 2940.84082, IVP Loss: 931.18701,\n",
      "13500/60000 PDE Loss: 3.44657, BVP1 Loss: 1369.09412, BVP2 Loss: 2905.08252, IVP Loss: 948.89398,\n",
      "14000/60000 PDE Loss: 3.56336, BVP1 Loss: 1323.95361, BVP2 Loss: 2870.35547, IVP Loss: 935.65759,\n",
      "14500/60000 PDE Loss: 6.30073, BVP1 Loss: 1.34378, BVP2 Loss: 2759.67725, IVP Loss: 949.38092,\n",
      "15000/60000 PDE Loss: 14.17409, BVP1 Loss: 0.26207, BVP2 Loss: 895.58362, IVP Loss: 1773.54944,\n",
      "15500/60000 PDE Loss: 9.74303, BVP1 Loss: 3.24051, BVP2 Loss: 897.75842, IVP Loss: 1801.53760,\n",
      "16000/60000 PDE Loss: 10.39582, BVP1 Loss: 0.20989, BVP2 Loss: 903.29462, IVP Loss: 1733.83936,\n",
      "16500/60000 PDE Loss: 17.01249, BVP1 Loss: 0.01466, BVP2 Loss: 870.72083, IVP Loss: 1748.83704,\n",
      "17000/60000 PDE Loss: 12.66416, BVP1 Loss: 2.10190, BVP2 Loss: 919.86102, IVP Loss: 1786.72400,\n",
      "17500/60000 PDE Loss: 13.77018, BVP1 Loss: 0.00056, BVP2 Loss: 953.02380, IVP Loss: 1691.33875,\n",
      "18000/60000 PDE Loss: 4.12161, BVP1 Loss: 1651.12866, BVP2 Loss: 2535.90503, IVP Loss: 971.93109,\n",
      "18500/60000 PDE Loss: 3.91568, BVP1 Loss: 1660.80884, BVP2 Loss: 2524.01123, IVP Loss: 972.20282,\n",
      "19000/60000 PDE Loss: 4.15478, BVP1 Loss: 1661.90710, BVP2 Loss: 2521.77295, IVP Loss: 985.62640,\n",
      "19500/60000 PDE Loss: 4.15775, BVP1 Loss: 1663.09753, BVP2 Loss: 2520.63989, IVP Loss: 990.90308,\n",
      "20000/60000 PDE Loss: 4.12408, BVP1 Loss: 1649.62891, BVP2 Loss: 2536.45410, IVP Loss: 983.21863,\n",
      "20500/60000 PDE Loss: 4.12437, BVP1 Loss: 1649.74915, BVP2 Loss: 2538.18213, IVP Loss: 977.66882,\n",
      "21000/60000 PDE Loss: 4.14691, BVP1 Loss: 1658.76477, BVP2 Loss: 2526.06372, IVP Loss: 1016.25897,\n",
      "21500/60000 PDE Loss: 4.11600, BVP1 Loss: 1646.40234, BVP2 Loss: 2541.36670, IVP Loss: 988.59412,\n",
      "22000/60000 PDE Loss: 4.15353, BVP1 Loss: 1661.41016, BVP2 Loss: 2523.09814, IVP Loss: 1006.64600,\n",
      "22500/60000 PDE Loss: 4.10935, BVP1 Loss: 1643.73816, BVP2 Loss: 2544.34473, IVP Loss: 995.13739,\n",
      "23000/60000 PDE Loss: 4.11863, BVP1 Loss: 1647.45288, BVP2 Loss: 2539.92310, IVP Loss: 998.87701,\n",
      "23500/60000 PDE Loss: 4.12897, BVP1 Loss: 1651.59070, BVP2 Loss: 2535.08203, IVP Loss: 971.61780,\n",
      "24000/60000 PDE Loss: 4.15796, BVP1 Loss: 1663.18384, BVP2 Loss: 2520.54224, IVP Loss: 1005.33221,\n",
      "24500/60000 PDE Loss: 4.11797, BVP1 Loss: 1647.18738, BVP2 Loss: 2540.34326, IVP Loss: 994.91040,\n",
      "25000/60000 PDE Loss: 4.12353, BVP1 Loss: 1649.40710, BVP2 Loss: 2537.94092, IVP Loss: 982.19067,\n",
      "25500/60000 PDE Loss: 4.16101, BVP1 Loss: 1664.40247, BVP2 Loss: 2518.55127, IVP Loss: 982.94031,\n",
      "26000/60000 PDE Loss: 4.12844, BVP1 Loss: 1651.37488, BVP2 Loss: 2536.11841, IVP Loss: 981.30280,\n",
      "26500/60000 PDE Loss: 4.14551, BVP1 Loss: 1658.20398, BVP2 Loss: 2526.19092, IVP Loss: 999.78082,\n",
      "27000/60000 PDE Loss: 4.12439, BVP1 Loss: 1649.75378, BVP2 Loss: 2536.87842, IVP Loss: 976.69098,\n",
      "27500/60000 PDE Loss: 4.14895, BVP1 Loss: 1659.57886, BVP2 Loss: 2526.16406, IVP Loss: 985.38940,\n",
      "28000/60000 PDE Loss: 4.11644, BVP1 Loss: 1646.57715, BVP2 Loss: 2540.85815, IVP Loss: 979.77917,\n",
      "28500/60000 PDE Loss: 4.12250, BVP1 Loss: 1648.99902, BVP2 Loss: 2537.82520, IVP Loss: 969.47638,\n",
      "29000/60000 PDE Loss: 4.10266, BVP1 Loss: 1641.06262, BVP2 Loss: 2546.02881, IVP Loss: 979.19659,\n",
      "29500/60000 PDE Loss: 4.16195, BVP1 Loss: 1664.78137, BVP2 Loss: 2518.31812, IVP Loss: 998.42859,\n",
      "30000/60000 PDE Loss: 4.12780, BVP1 Loss: 1651.12134, BVP2 Loss: 2535.53491, IVP Loss: 990.98102,\n",
      "30500/60000 PDE Loss: 4.12905, BVP1 Loss: 1651.62183, BVP2 Loss: 2534.54370, IVP Loss: 1005.85199,\n",
      "31000/60000 PDE Loss: 4.11943, BVP1 Loss: 1647.76880, BVP2 Loss: 2538.38550, IVP Loss: 988.61603,\n",
      "31500/60000 PDE Loss: 4.14435, BVP1 Loss: 1657.73743, BVP2 Loss: 2526.77393, IVP Loss: 972.74982,\n",
      "32000/60000 PDE Loss: 4.14893, BVP1 Loss: 1659.57434, BVP2 Loss: 2525.34521, IVP Loss: 975.68799,\n",
      "32500/60000 PDE Loss: 4.14323, BVP1 Loss: 1657.29395, BVP2 Loss: 2528.93726, IVP Loss: 968.76428,\n",
      "33000/60000 PDE Loss: 4.17065, BVP1 Loss: 1668.25391, BVP2 Loss: 2513.72192, IVP Loss: 985.67401,\n",
      "33500/60000 PDE Loss: 4.09310, BVP1 Loss: 1637.23767, BVP2 Loss: 2552.49951, IVP Loss: 998.38843,\n",
      "34000/60000 PDE Loss: 4.14416, BVP1 Loss: 1657.65955, BVP2 Loss: 2527.48291, IVP Loss: 984.97382,\n",
      "34500/60000 PDE Loss: 4.13241, BVP1 Loss: 1652.96765, BVP2 Loss: 2532.79590, IVP Loss: 976.85291,\n",
      "35000/60000 PDE Loss: 4.11303, BVP1 Loss: 1645.20703, BVP2 Loss: 2542.76294, IVP Loss: 981.84082,\n",
      "35500/60000 PDE Loss: 4.14966, BVP1 Loss: 1659.86328, BVP2 Loss: 2525.03271, IVP Loss: 992.04968,\n",
      "36000/60000 PDE Loss: 4.14964, BVP1 Loss: 1659.85620, BVP2 Loss: 2524.41382, IVP Loss: 990.44678,\n",
      "36500/60000 PDE Loss: 4.17040, BVP1 Loss: 1668.15710, BVP2 Loss: 2514.14331, IVP Loss: 986.81360,\n",
      "37000/60000 PDE Loss: 4.10329, BVP1 Loss: 1641.31567, BVP2 Loss: 2548.05005, IVP Loss: 1001.48718,\n",
      "37500/60000 PDE Loss: 4.12524, BVP1 Loss: 1650.09338, BVP2 Loss: 2536.54175, IVP Loss: 981.43219,\n",
      "38000/60000 PDE Loss: 4.11296, BVP1 Loss: 1645.18384, BVP2 Loss: 2542.37671, IVP Loss: 980.18982,\n",
      "38500/60000 PDE Loss: 4.13296, BVP1 Loss: 1653.18652, BVP2 Loss: 2533.45874, IVP Loss: 1011.77417,\n",
      "39000/60000 PDE Loss: 4.14127, BVP1 Loss: 1656.51184, BVP2 Loss: 2528.38037, IVP Loss: 982.13501,\n",
      "39500/60000 PDE Loss: 4.15674, BVP1 Loss: 1662.70227, BVP2 Loss: 2521.30908, IVP Loss: 991.32843,\n",
      "40000/60000 PDE Loss: 4.15430, BVP1 Loss: 1661.71863, BVP2 Loss: 2522.18628, IVP Loss: 992.08032,\n",
      "40500/60000 PDE Loss: 4.13782, BVP1 Loss: 1655.12866, BVP2 Loss: 2529.28711, IVP Loss: 984.26971,\n",
      "41000/60000 PDE Loss: 4.16766, BVP1 Loss: 1667.06262, BVP2 Loss: 2515.94995, IVP Loss: 1005.33478,\n",
      "41500/60000 PDE Loss: 4.13206, BVP1 Loss: 1652.82422, BVP2 Loss: 2533.40918, IVP Loss: 1001.36700,\n",
      "42000/60000 PDE Loss: 4.12579, BVP1 Loss: 1650.31335, BVP2 Loss: 2535.24634, IVP Loss: 993.56097,\n",
      "42500/60000 PDE Loss: 4.12559, BVP1 Loss: 1650.23364, BVP2 Loss: 2538.09351, IVP Loss: 968.49921,\n",
      "43000/60000 PDE Loss: 4.13110, BVP1 Loss: 1652.43726, BVP2 Loss: 2533.61548, IVP Loss: 991.77899,\n",
      "43500/60000 PDE Loss: 4.10626, BVP1 Loss: 1642.50012, BVP2 Loss: 2546.60034, IVP Loss: 979.00391,\n",
      "44000/60000 PDE Loss: 4.14189, BVP1 Loss: 1656.75305, BVP2 Loss: 2528.49609, IVP Loss: 990.58551,\n",
      "44500/60000 PDE Loss: 4.12177, BVP1 Loss: 1648.70679, BVP2 Loss: 2537.64233, IVP Loss: 997.73541,\n",
      "45000/60000 PDE Loss: 4.14581, BVP1 Loss: 1658.32776, BVP2 Loss: 2525.89160, IVP Loss: 987.23132,\n",
      "45500/60000 PDE Loss: 4.14144, BVP1 Loss: 1656.57825, BVP2 Loss: 2527.54370, IVP Loss: 1008.20319,\n",
      "46000/60000 PDE Loss: 4.13633, BVP1 Loss: 1654.53137, BVP2 Loss: 2531.02173, IVP Loss: 975.94360,\n",
      "46500/60000 PDE Loss: 4.14026, BVP1 Loss: 1656.10864, BVP2 Loss: 2528.68555, IVP Loss: 984.60260,\n",
      "47000/60000 PDE Loss: 4.12620, BVP1 Loss: 1650.48120, BVP2 Loss: 2535.47534, IVP Loss: 989.88208,\n",
      "47500/60000 PDE Loss: 4.12313, BVP1 Loss: 1649.25098, BVP2 Loss: 2538.32959, IVP Loss: 995.13892,\n",
      "48000/60000 PDE Loss: 4.12864, BVP1 Loss: 1651.45325, BVP2 Loss: 2536.48877, IVP Loss: 984.40570,\n",
      "48500/60000 PDE Loss: 4.11775, BVP1 Loss: 1647.10547, BVP2 Loss: 2539.82202, IVP Loss: 1003.93298,\n",
      "49000/60000 PDE Loss: 4.12932, BVP1 Loss: 1651.72266, BVP2 Loss: 2533.91650, IVP Loss: 995.58099,\n",
      "49500/60000 PDE Loss: 4.14538, BVP1 Loss: 1658.15234, BVP2 Loss: 2526.38403, IVP Loss: 977.57458,\n",
      "50000/60000 PDE Loss: 4.11521, BVP1 Loss: 1646.08105, BVP2 Loss: 2542.03809, IVP Loss: 992.14148,\n",
      "50500/60000 PDE Loss: 4.13225, BVP1 Loss: 1652.89453, BVP2 Loss: 2532.15430, IVP Loss: 995.98383,\n",
      "51000/60000 PDE Loss: 4.13032, BVP1 Loss: 1652.12805, BVP2 Loss: 2534.38672, IVP Loss: 992.38751,\n",
      "51500/60000 PDE Loss: 4.13971, BVP1 Loss: 1655.87891, BVP2 Loss: 2530.61230, IVP Loss: 992.31769,\n",
      "52000/60000 PDE Loss: 4.13580, BVP1 Loss: 1654.32422, BVP2 Loss: 2531.15430, IVP Loss: 994.81763,\n",
      "52500/60000 PDE Loss: 4.14239, BVP1 Loss: 1656.95325, BVP2 Loss: 2528.33105, IVP Loss: 984.64978,\n",
      "53000/60000 PDE Loss: 4.12335, BVP1 Loss: 1649.34375, BVP2 Loss: 2537.41968, IVP Loss: 978.51678,\n",
      "53500/60000 PDE Loss: 4.14976, BVP1 Loss: 1659.90540, BVP2 Loss: 2525.62769, IVP Loss: 981.81079,\n",
      "54000/60000 PDE Loss: 4.13117, BVP1 Loss: 1652.46887, BVP2 Loss: 2531.85400, IVP Loss: 991.56982,\n",
      "54500/60000 PDE Loss: 4.14942, BVP1 Loss: 1659.76855, BVP2 Loss: 2525.66162, IVP Loss: 994.71417,\n",
      "55000/60000 PDE Loss: 4.11407, BVP1 Loss: 1645.62512, BVP2 Loss: 2542.00806, IVP Loss: 976.76459,\n",
      "55500/60000 PDE Loss: 4.15722, BVP1 Loss: 1662.88965, BVP2 Loss: 2520.13599, IVP Loss: 988.95923,\n",
      "56000/60000 PDE Loss: 4.16229, BVP1 Loss: 1664.91003, BVP2 Loss: 2518.63794, IVP Loss: 1003.78882,\n",
      "56500/60000 PDE Loss: 4.13528, BVP1 Loss: 1654.11023, BVP2 Loss: 2531.84131, IVP Loss: 983.76050,\n",
      "57000/60000 PDE Loss: 4.16537, BVP1 Loss: 1666.15234, BVP2 Loss: 2516.92651, IVP Loss: 986.50232,\n",
      "57500/60000 PDE Loss: 4.12891, BVP1 Loss: 1651.56262, BVP2 Loss: 2534.61011, IVP Loss: 983.48639,\n",
      "58000/60000 PDE Loss: 4.11895, BVP1 Loss: 1647.57800, BVP2 Loss: 2540.17163, IVP Loss: 980.49860,\n",
      "58500/60000 PDE Loss: 4.13112, BVP1 Loss: 1652.45215, BVP2 Loss: 2533.48071, IVP Loss: 1002.59808,\n",
      "59000/60000 PDE Loss: 4.13550, BVP1 Loss: 1654.20288, BVP2 Loss: 2530.40967, IVP Loss: 988.21338,\n",
      "59500/60000 PDE Loss: 4.12438, BVP1 Loss: 1649.75012, BVP2 Loss: 2536.80200, IVP Loss: 987.62579,\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \n",
    "        # PDE Round\n",
    "        X1, y1 = get_diff_data(samples['pde'])\n",
    "        X1 = tf.convert_to_tensor(X1, dtype=tf.float32)\n",
    "        y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as t2:\n",
    "            t2.watch(X1)\n",
    "            with tf.GradientTape(persistent=True) as t1:\n",
    "                t1.watch(X1)\n",
    "                y1_hat = model(X1, training=True)\n",
    "            grads = t1.gradient(y1_hat, X1, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        dVdt, dVdS = tf.reshape(grads[:, 0], [-1, 1]), tf.reshape(grads[:, 1], [-1, 1])\n",
    "        grads2nd = t2.gradient(dVdS, X1, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        \n",
    "        d2VdS2 = tf.reshape(grads2nd[:, 1], [-1, 1])\n",
    "        S1 = tf.reshape(X1[:, 1], [-1, 1])\n",
    "        pde_loss = criterion(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "\n",
    "        # BVP Round\n",
    "        X21, y21, X22, y22 = get_bvp_data(samples['bvp'])\n",
    "\n",
    "        X21 = tf.convert_to_tensor(X21, dtype=tf.float32)\n",
    "        y21 = tf.convert_to_tensor(y21, dtype=tf.float32)\n",
    "\n",
    "        X22 = tf.convert_to_tensor(X22, dtype=tf.float32)\n",
    "        y22 = tf.convert_to_tensor(y22, dtype=tf.float32)\n",
    "\n",
    "        y21_hat = model(X21, training=True)\n",
    "        bvp1_loss = criterion(y21, y21_hat)\n",
    "\n",
    "        y22_hat = model(X22, training=True)\n",
    "        bvp2_loss = criterion(y22, y22_hat)\n",
    "\n",
    "        # IVP Round\n",
    "        X3, y3 = get_ivp_data(samples['ivp'])\n",
    "\n",
    "        X3 = tf.convert_to_tensor(X3, dtype=tf.float32)\n",
    "        y3 = tf.convert_to_tensor(y3, dtype=tf.float32)\n",
    "\n",
    "        y3_hat = model(X3, training=True)\n",
    "        ivp_loss = criterion(y3, y3_hat)\n",
    "\n",
    "        # Combined loss\n",
    "        combined_loss = pde_loss + bvp1_loss + bvp2_loss + ivp_loss\n",
    "\n",
    "    # Backpropagation and update\n",
    "    gradients = tape.gradient(combined_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_hist.append(combined_loss.numpy())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.numpy():.5f}, BVP1 Loss: {bvp1_loss.numpy():.5f}, BVP2 Loss: {bvp2_loss.numpy():.5f}, IVP Loss: {ivp_loss.numpy():.5f},')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
