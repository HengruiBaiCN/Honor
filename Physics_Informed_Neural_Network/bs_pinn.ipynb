{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.14.0-dev20230525\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, i illustrate physics informed nerual netowrks (PINNs) to solve the Balck Scholes Equation as proposed in\n",
    "\n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations*. [arXiv 1711.10561](https://arxiv.org/abs/1711.10561) \n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations*. [arXiv 1711.10566](https://arxiv.org/abs/1711.10566) \n",
    "- Maziar Raissi, Paris Perdikaris, George Em Karniadakis. *Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations*. J. Comp. Phys. 378 pp. 686-707 [DOI: 10.1016/j.jcp.2018.10.045](https://www.sciencedirect.com/science/article/pii/S0021999118307125) \n",
    "\n",
    "This notebook is partially based on another implementation of the PINN approach published on [Google Colab by janblechschmidt](https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681), [Medium by Harsha Andey](https://medium.com/@andeyharsha15/deep-neural-networks-for-solving-differential-equations-in-finance-da662ef0681), [GitHub by pierremtb](https://github.com/pierremtb/PINNs-TF2.0) as well as the original code, see  [Maziar Raissi on GitHub](https://github.com/maziarraissi/PINNs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We describe the PINN approach for approximating the solution $u:[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$ of an evolution equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\partial_t u (t,x) + \\mathcal{N}[u](t,x) &= 0, && (t,x) \\in (0,T] \\times \\mathcal{D},\\\\\n",
    "    u(0,x) &= u_0(x) \\quad && x \\in \\mathcal{D},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}$ is a nonlinear differential operator acting on $u$, \n",
    "$\\mathcal{D} \\subset \\mathbb{R}^d$ a bounded domain,\n",
    "$T$ denotes the final time and\n",
    "$u_0: \\mathcal{D} \\to \\mathbb{R}$ the prescribed initial data.\n",
    "Although the methodology allows for different types of boundary conditions, we restrict our discussion to the inhomogeneous Dirichlet case and prescribe\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "        \\hspace{7em} u(t,x) &= u_b(t,x)  && \\quad (t,x) \\in (0,T] \\times \\partial \\mathcal{D},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\partial \\mathcal{D}$ denotes the boundary of the domain $\\mathcal{D}$ and $u_b: (0,T] \\times \\partial \\mathcal{D} \\to \\mathbb{R}$ the given boundary data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "The method constructs a neural network approximation\n",
    "\n",
    "$$\n",
    "u_\\theta(t,x) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "of the solution of nonlinear PDE, where $u_\\theta :[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$ denotes a function realized by a neural network with parameters $\\theta$.\n",
    "\n",
    "The continuous time approach for the parabolic PDE as described in ([Raissi et al., 2017 (Part I)](https://arxiv.org/abs/1711.10561)) is based on the (strong) residual of a given neural network approximation $u_\\theta \\colon [0,T] \\times \\mathcal{D} \\to \\mathbb{R} $ of the solution $u$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r_\\theta (t,x) := \\partial_t u_\\theta (t,x) + \\mathcal{N}[u_\\theta] (t,x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To incorporate this PDE residual $r_\\theta$ into a loss function to be minimized, PINNs require a further differentiation to evaluate the differential operators $\\partial_t u_\\theta$ and $\\mathcal{N}[u_\\theta]$.\n",
    "Thus the PINN term $r_\\theta$ shares the same parameters as the original network $u_\\theta(t,x)$, but respects the underlying \"physics\" of the nonlinear PDE.\n",
    "Both types of derivatives can be easily determined through automatic differentiation with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch.\n",
    "\n",
    "The PINN approach for the solution of the initial and boundary value problem now proceeds by minimization of the loss functional\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_\\theta(X) := \\phi_\\theta^r(X^r) + \\phi_\\theta^0(X^0) + \\phi_\\theta^b(X^b),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $X$ denotes the collection of training data and the loss function $\\phi_\\theta$ contains the following terms:\n",
    "\n",
    "  - the mean squared residual\n",
    "$$\n",
    "  \\begin{align*}\n",
    "   \\phi_\\theta^r(X^r) := \\frac{1}{N_r}\\sum_{i=1}^{N_r} \\left|r_\\theta\\left(t_i^r, x_i^r\\right)\\right|^2\n",
    "\\end{align*}\n",
    "$$\n",
    "in a number of collocation points $X^r:=\\{(t_i^r, x_i^r)\\}_{i=1}^{N_r} \\subset (0,T] \\times \\mathcal{D}$, where $r_\\theta$ is the physics-informed neural network,\n",
    "  - the mean squared misfit with respect to the initial and boundary conditions\n",
    "$$\n",
    "    \\begin{align*}\n",
    "   \\phi_\\theta^0(X^0) \n",
    "   := \n",
    "   \\frac{1}{N_0}\n",
    "   \\sum_{i=1}^{N_0} \\left|u_\\theta\\left(t_i^0, x_i^0\\right) - u_0\\left(x_i^0\\right)\\right|^2\n",
    "   \\quad \\text{ and } \\quad\n",
    "   \\phi_\\theta^b(X^b) \n",
    "   := \n",
    "   \\frac{1}{N_b}\n",
    "   \\sum_{i=1}^{N_b} \\left|u_\\theta\\left(t_i^b, x_i^b\\right) - u_b\\left(t_i^b, x_i^b\\right)\\right|^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "in a number of points $X^0:=\\{(t^0_i,x^0_i)\\}_{i=1}^{N_0} \\subset \\{0\\} \\times \\mathcal{D}$ and $X^b:=\\{(t^b_i,x^b_i)\\}_{i=1}^{N_b} \\subset (0,T] \\times \\partial \\mathcal{D}$, where $u_\\theta$ is the neural network approximation of the solution $u\\colon[0,T] \\times \\mathcal{D} \\to \\mathbb{R}$.\n",
    "\n",
    "Note that the training data $X$ consists entirely of time-space coordinates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Scholes Model\n",
    "For a European call option:\n",
    "\n",
    "$$\n",
    "C(S, t) = SN(d1) - Ke^(-rt)N(d2)\n",
    "$$\n",
    "\n",
    "For a European put option:\n",
    "\n",
    "$$\n",
    "P(S, t) = Ke^(-rt)N(-d2) - SN(-d1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- C(S,t) is the value of the call option at time t\n",
    "- P(S,t) is the value of the put option at time t\n",
    "- S is the spot price of the underlying asset\n",
    "- K is the strike price\n",
    "- r is the risk-free interest rate\n",
    "- t is the time to maturity\n",
    "- N() is the cumulative distribution function of the standard normal distribution\n",
    "- $d1 = [ln(S/K) + (r + 0.5σ²)(T-t)] / σ√(T-t)$\n",
    "- $d2 = d1 - σ√(T-t)$\n",
    "The variables within the square root sign σ and T represent the volatility of returns of the underlying asset and the time to maturity respectively.\n",
    "\n",
    "The Black-Scholes model assumes that markets are efficient which means that there are no arbitrage opportunities, i.e., it is impossible to make a riskless profit. It also assumes that the volatility of the underlying asset is constant over time, and that the returns on the underlying asset are normally distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Scholes Partial Differential Equation\n",
    "$$\n",
    "{\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n",
    "$$\n",
    "\n",
    "where we have greeks in the formula to measuer the sensitivity of the value of a portfolio to a small change in a given undrelying parameter. More explanation can be found in [Option Greeks from Investopedia by JohnSumma](https://www.investopedia.com/trading/getting-to-know-the-greeks/).\n",
    "\n",
    "Five main greeks are:\n",
    "- Delta: ${\\frac {\\partial V}{\\partial S}}$ Measures impact of a change in the price of underlying\n",
    "- Gamma: ${\\frac {\\partial ^{2}V}{\\partial S^{2}}}$ Measures the rate of change of delta\n",
    "- Theta: ${\\frac {\\partial V}{\\partial t}}$ Measures impact of a change in time remaining\n",
    "- Vega: ${\\frac {\\partial V}{\\partial \\sigma}}$ Measures impact of a change in volatility\n",
    "- Rho: ${\\frac {\\partial V}{\\partial r}}$ Measures the sensitivity of Derivative price w.r.t interest rate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "S = tf.Variable([80.0], dtype=tf.float32)\n",
    "t = tf.Variable([0.0], dtype=tf.float32)\n",
    "sigma = tf.Variable([0.3], dtype=tf.float32)\n",
    "r = tf.Variable([0.05], dtype=tf.float32)\n",
    "K = tf.constant([70.0], dtype=tf.float32)\n",
    "T = tf.constant([1.0], dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the Greeks, we use the `tf.GradientTape` for computing gradients as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.8385\n",
      "0.7769\n",
      "0.0124\n",
      "23.8776\n",
      "45.1372\n",
      "Result: 0.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([t, S, sigma, r])\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([t, S, sigma, r])\n",
    "        t2m = T - t\n",
    "        d1 = (tf.math.log(S / K) + (r + 0.5 * sigma**2) * t2m) / (sigma * tf.sqrt(t2m))\n",
    "        d2 = d1 - sigma * tf.sqrt(t2m)\n",
    "        N0 = lambda value: 0.5 * (1 + tf.math.erf(value / tf.sqrt(2.0))) # type: ignore\n",
    "        Nd1 = N0(d1)\n",
    "        Nd2 = N0(d2)\n",
    "        C = S * Nd1 - K * Nd2 * tf.exp(-r * t2m)\n",
    "    theta = tape2.gradient(C, t, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    delta = tape2.gradient(C, S, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    vega = tape2.gradient(C, sigma, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    rho = tape2.gradient(C, r, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "    \n",
    "\n",
    "gamma = tape.gradient(delta, S, unconnected_gradients=tf.UnconnectedGradients.ZERO)[0]\n",
    "\n",
    "\n",
    "for og in [theta, delta, gamma, vega, rho]:\n",
    "    print(f'{og:.4f}')\n",
    "\n",
    "result = (theta + 0.5 * sigma**2 * S**2 * gamma + r * S * delta - r * C).numpy()\n",
    "print(f\"Result: {result[0]:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the option price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option Price: tf.Tensor(17.014957, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "C = S * Nd1 - K * Nd2 * tf.exp(-r * t2m)\n",
    "\n",
    "print(\"Option Price:\", C[0]) #17.01496"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, Initial Value conditions and the PDE itself.\n",
    "\n",
    "The data samples are generated by the three functions:\n",
    "- Sampler of data inputs for t and S for Differential Loss `get_diff_data()`\n",
    "- Sampler of data inputs satisfying the boundary conditions for the PDE `get_bvp_data()`\n",
    "- Sampler of data inputs satisfying the initial value conditions for the PDE `get_ivp_data()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "r = 0.05\n",
    "sigma = 0.25\n",
    "T = 1\n",
    "S_range = [0, 130]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "\n",
    "\n",
    "def get_diff_data(n):\n",
    "    X = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "def get_ivp_data(n):\n",
    "    X = np.concatenate([np.ones((n, 1)),\n",
    "                    np.random.uniform(*S_range, (n, 1))], axis=1)\n",
    "    y = gs(X[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "  \n",
    "def get_bvp_data(n):\n",
    "    X1 = np.concatenate([np.random.uniform(*t_range, (n, 1)),\n",
    "                        S_range[0] * np.ones((n, 1))], axis=1)\n",
    "    y1 = np.zeros((n, 1))\n",
    "    \n",
    "    X2 = np.concatenate([np.random.uniform(*t_range, (n, 1)), \n",
    "                        S_range[-1] * np.ones((n, 1))], axis=1)\n",
    "    y2 = (S_range[-1] - K*np.exp(-r*(T-X2[:, 0].reshape(-1)))).reshape(-1, 1)\n",
    "    \n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up network architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Galerkin Method Model Construction\n",
    "The Neural Network based on the below model architecture given in Reference[1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DGMCell(tf.keras.Model):\n",
    "#     def __init__(self, input_dim, hidden_dim, n_layers=3, output_dim=1):\n",
    "#         super(DGMCell, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.n = n_layers\n",
    "\n",
    "#         self.sig_act = tf.keras.activations.tanh\n",
    "\n",
    "#         self.Sw = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Uz = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsz = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Ug = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsg = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Ur = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsr = tf.keras.layers.Dense(self.hidden_dim)\n",
    "        \n",
    "#         self.Uh = tf.keras.layers.Dense(self.hidden_dim)\n",
    "#         self.Wsh = tf.keras.layers.Dense(self.hidden_dim)\n",
    "\n",
    "#         self.Wf = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "#     def call(self, x):\n",
    "#         S1 = self.Sw(x)\n",
    "#         for i in range(self.n):\n",
    "#             if i==0:\n",
    "#                 S = S1\n",
    "#             else:\n",
    "#                 S = self.sig_act(out)\n",
    "#             Z = self.sig_act(self.Uz(x) + self.Wsz(S))\n",
    "#             G = self.sig_act(self.Ug(x) + self.Wsg(S1))\n",
    "#             R = self.sig_act(self.Ur(x) + self.Wsr(S))\n",
    "#             H = self.sig_act(self.Uh(x) + self.Wsh(S*R))\n",
    "#             out = (1-G)*H + Z*S\n",
    "#         out = self.Wf(out)\n",
    "#         return out\n",
    "\n",
    "# model = DGMCell(2, 100, 3, 1)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# n_epochs = 60000\n",
    "# samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "# criterion = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A feedforward neural network of the following architecture\n",
    "- one input layer with 2 neurons\n",
    "- 8 fully connected layers each containing 20 neurons and each followed by a hyperboloc tangent actiavtion function,\n",
    "- one fully connected ouput layer\n",
    "\n",
    "This setting results in a network with 3021 trainable parameters (first hidden layer: 2⋅20+20=60; seven intermediate layers: each 20⋅20+20=420; output layer: 20⋅1+1=21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 20)                60        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3021 (11.80 KB)\n",
      "Trainable params: 3021 (11.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 2  # Number of input features\n",
    "hidden_size = 20  # Number of neurons in the hidden layer\n",
    "output_size = 1   # Number of output features\n",
    "n_layers = 8\n",
    "learning_rate = 3e-5\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(input_size,))\n",
    "\n",
    "# Add hidden layers using a for loop\n",
    "x = inputs\n",
    "for i in range(n_layers):\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    \n",
    "# Output layer\n",
    "outputs = tf.keras.layers.Dense(output_size)(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "n_epochs = 60000\n",
    "samples = {\"pde\": 5000, \"bvp\":5000, \"ivp\":5000}\n",
    "criterion = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/60000 PDE Loss: 0.00015, BVP1 Loss: 0.00011, BVP2 Loss: 7961.09424, IVP Loss: 1826.66638,\n",
      "500/60000 PDE Loss: 0.12104, BVP1 Loss: 47.89546, BVP2 Loss: 7064.88721, IVP Loss: 1536.90479,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m X22 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(X22, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     31\u001b[0m y22 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(y22, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 33\u001b[0m y21_hat \u001b[39m=\u001b[39m model(X21, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     34\u001b[0m bvp1_loss \u001b[39m=\u001b[39m criterion(y21, y21_hat)\n\u001b[0;32m     36\u001b[0m y22_hat \u001b[39m=\u001b[39m model(X22, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\engine\\training.py:588\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    586\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 588\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:1150\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1149\u001b[0m ):\n\u001b[1;32m-> 1150\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\engine\\functional.py:512\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\engine\\functional.py:669\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 669\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    671\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    673\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    674\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:1150\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1149\u001b[0m ):\n\u001b[1;32m-> 1150\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:252\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m         outputs\u001b[39m.\u001b[39mset_shape(output_shape)\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mbias_add(outputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n\u001b[0;32m    254\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(outputs)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3555\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   3552\u001b[0m   value \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(value, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3553\u001b[0m   bias \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(bias, dtype\u001b[39m=\u001b[39mvalue\u001b[39m.\u001b[39mdtype, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 3555\u001b[0m \u001b[39mreturn\u001b[39;00m gen_nn_ops\u001b[39m.\u001b[39;49mbias_add(value, bias, data_format\u001b[39m=\u001b[39;49mdata_format, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:902\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    901\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    903\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mBiasAdd\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, value, bias, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_format)\n\u001b[0;32m    904\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    905\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \n",
    "        # PDE Round\n",
    "        X1, y1 = get_diff_data(samples['pde'])\n",
    "        X1 = tf.convert_to_tensor(X1, dtype=tf.float32)\n",
    "        y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as t2:\n",
    "            t2.watch(X1)\n",
    "            with tf.GradientTape(persistent=True) as t1:\n",
    "                t1.watch(X1)\n",
    "                y1_hat = model(X1, training=True)\n",
    "            grads = t1.gradient(y1_hat, X1, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        dVdt, dVdS = tf.reshape(grads[:, 0], [-1, 1]), tf.reshape(grads[:, 1], [-1, 1])\n",
    "        grads2nd = t2.gradient(dVdS, X1, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        \n",
    "        d2VdS2 = tf.reshape(grads2nd[:, 1], [-1, 1])\n",
    "        S1 = tf.reshape(X1[:, 1], [-1, 1])\n",
    "        pde_loss = criterion(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "\n",
    "        # BVP Round\n",
    "        X21, y21, X22, y22 = get_bvp_data(samples['bvp'])\n",
    "\n",
    "        X21 = tf.convert_to_tensor(X21, dtype=tf.float32)\n",
    "        y21 = tf.convert_to_tensor(y21, dtype=tf.float32)\n",
    "\n",
    "        X22 = tf.convert_to_tensor(X22, dtype=tf.float32)\n",
    "        y22 = tf.convert_to_tensor(y22, dtype=tf.float32)\n",
    "\n",
    "        y21_hat = model(X21, training=True)\n",
    "        bvp1_loss = criterion(y21, y21_hat)\n",
    "\n",
    "        y22_hat = model(X22, training=True)\n",
    "        bvp2_loss = criterion(y22, y22_hat)\n",
    "\n",
    "        # IVP Round\n",
    "        X3, y3 = get_ivp_data(samples['ivp'])\n",
    "\n",
    "        X3 = tf.convert_to_tensor(X3, dtype=tf.float32)\n",
    "        y3 = tf.convert_to_tensor(y3, dtype=tf.float32)\n",
    "\n",
    "        y3_hat = model(X3, training=True)\n",
    "        ivp_loss = criterion(y3, y3_hat)\n",
    "\n",
    "        # Combined loss\n",
    "        combined_loss = pde_loss + bvp1_loss + bvp2_loss + ivp_loss\n",
    "\n",
    "    # Backpropagation and update\n",
    "    gradients = tape.gradient(combined_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_hist.append(combined_loss.numpy())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.numpy():.5f}, BVP1 Loss: {bvp1_loss.numpy():.5f}, BVP2 Loss: {bvp2_loss.numpy():.5f}, IVP Loss: {ivp_loss.numpy():.5f},')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
