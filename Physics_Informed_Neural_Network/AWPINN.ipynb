{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networks\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# seed = 1234\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
       "    (1-7): 7 x Linear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8) #  Network initialization\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "lossFunction = nn.MSELoss()\n",
    "lr = 3e-5\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "x_f_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_label_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "x_data_s = torch.tensor(0.).float().to(device).requires_grad_(True)\n",
    "optimizer_adam_weight = optim.Adam([x_f_s] + [x_label_s] + [x_data_s], lr=lr*0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical loss samples\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 PDE Loss: 0.00000, BC Loss: 673.62469, data loss: 423.42343, total loss: 1097.04810, minimum loss: 1097.04810\n",
      "the weight is 1.00000, 1.00000. 1.00000, the parameter is -0.00000, 0.00000, 0.00000\n",
      "500/5000 PDE Loss: 0.07696, BC Loss: 19.71959, data loss: 14.16921, total loss: 33.96577, minimum loss: 33.96577\n",
      "the weight is 1.00000, 1.00000. 1.00000, the parameter is -0.00000, 0.00000, 0.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\Physics_Informed_Neural_Network\\AWPINN.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/AWPINN.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/AWPINN.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m combined_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx_f_s\u001b[39m.\u001b[39mdetach()) \u001b[39m*\u001b[39m pde_loss \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx_label_s\u001b[39m.\u001b[39mdetach()) \u001b[39m*\u001b[39m bc_loss \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx_data_s\u001b[39m.\u001b[39mdetach()) \u001b[39m*\u001b[39m data_loss \u001b[39m+\u001b[39m x_data_s \u001b[39m+\u001b[39m x_label_s \u001b[39m+\u001b[39m x_f_s\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/AWPINN.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m combined_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/AWPINN.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/Physics_Informed_Neural_Network/AWPINN.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# update the weight\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    bc_st_train, bc_v_train, n_st_train, n_v_train = \\\n",
    "    utils.trainingData(K, r, sigma, T, S_range[-1], S_range, t_range, gs, \n",
    "                       samples['bc'], \n",
    "                       samples['fc'], \n",
    "                       samples['pde'], \n",
    "                       RNG_key=123)\n",
    "    \n",
    "    # save training data points to tensor and send to device\n",
    "    n_st_train = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    n_v_train = torch.from_numpy(n_v_train).float().to(device)\n",
    "    \n",
    "    bc_st_train = torch.from_numpy(bc_st_train).float().to(device)\n",
    "    bc_v_train = torch.from_numpy(bc_v_train).float().to(device)   \n",
    "    \n",
    "    # pde residual loss\n",
    "    y1_hat = net(n_st_train)\n",
    "    grads = tgrad.grad(y1_hat, n_st_train, grad_outputs=torch.ones(y1_hat.shape).cuda(), \n",
    "                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, n_st_train, grad_outputs=torch.ones(dVdS.shape).cuda(), \n",
    "                    create_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "    S1 = n_st_train[:, 1].view(-1, 1)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    pde_loss = lossFunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "    \n",
    "    # boudary condition loss\n",
    "    y2_hat = net(bc_st_train)\n",
    "    bc_loss = lossFunction(bc_v_train, y2_hat)\n",
    "    \n",
    "    # sample training data loss\n",
    "    y3_hat = net(X_train_tensor)\n",
    "    data_loss = lossFunction(y_train_tensor, y3_hat)\n",
    "    \n",
    "    # Backpropagation and Update\n",
    "    optimizer.zero_grad()\n",
    "    combined_loss = torch.exp(-x_f_s.detach()) * pde_loss + torch.exp(-x_label_s.detach()) * bc_loss + torch.exp(-x_data_s.detach()) * data_loss + x_data_s + x_label_s + x_f_s\n",
    "    combined_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update the weight\n",
    "    optimizer_adam_weight.zero_grad()\n",
    "    loss = torch.exp(-x_f_s) * pde_loss.detach() + torch.exp(-x_label_s) * bc_loss.detach() + torch.exp(-x_data_s) * data_loss.detach() + x_data_s + x_label_s + x_f_s\n",
    "    loss.backward()\n",
    "    optimizer_adam_weight.step()\n",
    "    \n",
    "    # record the loss\n",
    "    mse_loss = pde_loss + bc_loss + data_loss\n",
    "    loss_hist.append(mse_loss.item())\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'{epoch}/{n_epochs} PDE Loss: {pde_loss.item():.5f}, BC Loss: {bc_loss.item():.5f}, data loss: {data_loss.item():.5f}, total loss: {mse_loss.item():.5f}, minimum loss: {min(loss_hist):.5f}')\n",
    "        print(f'the weight is {torch.exp(-x_f_s.detach()).item():.5f}, {torch.exp(-x_label_s.detach()).item():.5f}. {torch.exp(-x_data_s.detach()).item():.5f}, the parameter is {x_f_s.item():.5f}, {x_label_s.item():.5f}, {x_data_s.item():.5f}')\n",
    "    pass\n",
    "\n",
    "end_time = time.time()\n",
    "print('run time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = net(X_test_tensor)\n",
    "    test_loss = lossFunction(test_outputs, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093.721435546875\n",
      "79.7577896118164\n",
      "3.6771140098571777\n",
      "0.07435338199138641\n",
      "0.027979634702205658\n",
      "0.011776741594076157\n",
      "0.005533682648092508\n",
      "0.003371631260961294\n",
      "0.0028509057592600584\n",
      "0.0028797779232263565\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    if i % 500 == 0:\n",
    "        print(loss_hist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlr = 3e-5\\nlr = lr\\n1107.655029296875\\n91.27940368652344\\n11.102180480957031\\n1.2101805210113525\\n0.004867682233452797\\n0.003594964276999235\\n0.0033446408342570066\\n0.00316116982139647\\n0.003008157480508089\\n0.002895315643399954\\n\\nlr = lr*0.001\\n1097.4002685546875\\n47.1202392578125\\n8.797338485717773\\n0.13662375509738922\\n0.003648734651505947\\n0.003518293611705303\\n0.0034056329168379307\\n0.0035889376886188984\\n0.0031821425072848797\\n0.0030838181264698505\\n\\nlr = lr*0.00001\\n1104.6170654296875\\n24.64035415649414\\n12.116193771362305\\n0.4078262448310852\\n0.007686748169362545\\n0.004782408010214567\\n0.0036287037655711174\\n0.003150198608636856\\n0.003064548596739769\\n0.0028546450193971395\\n\\n\\nlr = lr*0.0000001\\n1088.1455078125\\n24.247705459594727\\n5.023616790771484\\n0.005683783441781998\\n0.0031367226038128138\\n0.0033775828778743744\\n0.003024038393050432\\n0.0030635555740445852\\n0.003273585345596075\\n0.0029113166965544224\\nversion 2\\n1088.095947265625\\n151.35958862304688\\n9.170495986938477\\n0.06665617227554321\\n0.01708376407623291\\n0.0105251120403409\\n0.008027860894799232\\n0.006927900947630405\\n0.005990810226649046\\n0.005765970330685377\\nversion 3\\n1093.8992919921875\\n29.58930206298828\\n6.046297073364258\\n0.027456315234303474\\n0.010994084179401398\\n0.006388682406395674\\n0.004713485017418861\\n0.003993072081357241\\n0.004130946937948465\\n0.003682968905195594\\nversion 4\\n1096.48853\\n562.53668\\n14.12750\\n\\nlr = lr*0.000000001\\nthe performance is not good and close to normal pinns\\n1088.857421875\\n38.67045593261719\\n6.390939712524414\\n0.22132942080497742\\n0.12249445170164108\\n0.07766617089509964\\n0.051127832382917404\\n0.03556811437010765\\n0.023616353049874306\\n0.014950141310691833\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lr = 3e-5\n",
    "lr = lr\n",
    "1107.655029296875\n",
    "91.27940368652344\n",
    "11.102180480957031\n",
    "1.2101805210113525\n",
    "0.004867682233452797\n",
    "0.003594964276999235\n",
    "0.0033446408342570066\n",
    "0.00316116982139647\n",
    "0.003008157480508089\n",
    "0.002895315643399954\n",
    "\n",
    "lr = lr*0.001\n",
    "1097.4002685546875\n",
    "47.1202392578125\n",
    "8.797338485717773\n",
    "0.13662375509738922\n",
    "0.003648734651505947\n",
    "0.003518293611705303\n",
    "0.0034056329168379307\n",
    "0.0035889376886188984\n",
    "0.0031821425072848797\n",
    "0.0030838181264698505\n",
    "\n",
    "lr = lr*0.00001\n",
    "1104.6170654296875\n",
    "24.64035415649414\n",
    "12.116193771362305\n",
    "0.4078262448310852\n",
    "0.007686748169362545\n",
    "0.004782408010214567\n",
    "0.0036287037655711174\n",
    "0.003150198608636856\n",
    "0.003064548596739769\n",
    "0.0028546450193971395\n",
    "version 2:\n",
    "1093.721435546875\n",
    "79.7577896118164\n",
    "3.6771140098571777\n",
    "0.07435338199138641\n",
    "0.027979634702205658\n",
    "0.011776741594076157\n",
    "0.005533682648092508\n",
    "0.003371631260961294\n",
    "0.0028509057592600584\n",
    "0.0028797779232263565\n",
    "\n",
    "\n",
    "lr = lr*0.0000001\n",
    "1088.1455078125\n",
    "24.247705459594727\n",
    "5.023616790771484\n",
    "0.005683783441781998\n",
    "0.0031367226038128138\n",
    "0.0033775828778743744\n",
    "0.003024038393050432\n",
    "0.0030635555740445852\n",
    "0.003273585345596075\n",
    "0.0029113166965544224\n",
    "version 2\n",
    "1088.095947265625\n",
    "151.35958862304688\n",
    "9.170495986938477\n",
    "0.06665617227554321\n",
    "0.01708376407623291\n",
    "0.0105251120403409\n",
    "0.008027860894799232\n",
    "0.006927900947630405\n",
    "0.005990810226649046\n",
    "0.005765970330685377\n",
    "version 3\n",
    "1093.8992919921875\n",
    "29.58930206298828\n",
    "6.046297073364258\n",
    "0.027456315234303474\n",
    "0.010994084179401398\n",
    "0.006388682406395674\n",
    "0.004713485017418861\n",
    "0.003993072081357241\n",
    "0.004130946937948465\n",
    "0.003682968905195594\n",
    "version 4\n",
    "1096.48853\n",
    "562.53668\n",
    "14.12750\n",
    "\n",
    "lr = lr*0.000000001\n",
    "the performance is not good and close to normal pinns\n",
    "1088.857421875\n",
    "38.67045593261719\n",
    "6.390939712524414\n",
    "0.22132942080497742\n",
    "0.12249445170164108\n",
    "0.07766617089509964\n",
    "0.051127832382917404\n",
    "0.03556811437010765\n",
    "0.023616353049874306\n",
    "0.014950141310691833\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
