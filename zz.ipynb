{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_OPTIMIZERS = ['bfgs', 'sgd', 'adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    r\"\"\"Applies the element-wise function:\n",
    "\n",
    "    .. math::\n",
    "        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return torch.sigmoid(input) * input\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    r\"\"\"Return parsed arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    argparse.Namespace\n",
    "        Parsed input arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--cuda',\n",
    "        action='store_true', help='Use CUDA GPU for training if available')\n",
    "    parser.add_argument('--domain',\n",
    "        type=float, nargs=2, default=[0.0, 1.0], help='Boundaries of the solution domain')\n",
    "    parser.add_argument('--boundary_conditions',\n",
    "        type=float, nargs=2, default=[1.0, 1.0], help='Boundary conditions on boundaries of the domain')\n",
    "    parser.add_argument('--rhs',\n",
    "        type=float, default=-10.0, help='Right-hand-side forcing function')\n",
    "    parser.add_argument('--n_layers',\n",
    "        type=int, default=3, help='The number of hidden layers of the neural network')\n",
    "    parser.add_argument('--n_units',\n",
    "        type=int, default=50, help='The number of neurons per hidden layer')\n",
    "    parser.add_argument('--activation',\n",
    "        type=str, default='tanh', help='activation function')\n",
    "    parser.add_argument('--optimizer',\n",
    "        type=str, default='adam', choices=SUPPORTED_OPTIMIZERS, help='Optimization procedure')\n",
    "    parser.add_argument('--n_epochs',\n",
    "        type=int, default=1000, help='The number of training epochs')\n",
    "    parser.add_argument('--batch_size',\n",
    "        type=int, default=101, help='The number of data points for optimization per epoch')\n",
    "    parser.add_argument('--linspace',\n",
    "        action='store_true', help='Space the batch of data linearly, otherwise random')\n",
    "    parser.add_argument('--learning_rate',\n",
    "        type=float, default=1e-3, help='Learning rate applied for gradient based optimizers')\n",
    "    parser.add_argument('--dropout_rate',\n",
    "        type=float, default=0.0, help='Dropout regularization rate')\n",
    "    parser.add_argument('--apply_mcdropout',\n",
    "        action='store_true', help='Apply MCdropout for uncertainty quantification')\n",
    "    parser.add_argument('--adaptive_rate',\n",
    "        type=float, help='Add additional adaptive rate parameter to activation function')\n",
    "    parser.add_argument('--adaptive_rate_scaler',\n",
    "        type=float, help='Apply constant scaler to the adaptive rate')\n",
    "    parser.add_argument('--save_fig',\n",
    "        type=str, help='Save figure with specified name')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def optimizer_dispatcher(optimizer, parameters, learning_rate):\n",
    "    r\"\"\"Return optimization function from `SUPPORTED_OPTIMIZERS`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : str\n",
    "        Optimization function name\n",
    "    parameters : callable\n",
    "        Network parameters\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    callable\n",
    "        Optimization function\n",
    "    \"\"\"\n",
    "    assert isinstance(optimizer, (str, )), '`optimizer` type must be str.'\n",
    "    optimizer = optimizer.lower()\n",
    "    assert optimizer in SUPPORTED_OPTIMIZERS, 'Invalid optimizer. Falling to default.'\n",
    "    if optimizer == 'bfgs':\n",
    "        return torch.optim.LBFGS(parameters, line_search_fn=\"strong_wolfe\")\n",
    "    elif optimizer == 'sgd':\n",
    "        return torch.optim.SGD(parameters, lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-2*learning_rate)\n",
    "    else:\n",
    "        return torch.optim.Adam(parameters, lr=learning_rate, betas=(0.9, 0.999), eps=1e-5)\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    r\"\"\"Applies the element-wise function:\n",
    "\n",
    "    .. math::\n",
    "        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return torch.sigmoid(input) * input\n",
    "        \n",
    "\n",
    "class AdaptiveLinear(nn.Linear):\n",
    "    r\"\"\"Applies a linear transformation to the input data as follows\n",
    "    :math:`y = naxA^T + b`.\n",
    "    More details available in Jagtap, A. D. et al. Locally adaptive\n",
    "    activation functions with slope recovery for deep and\n",
    "    physics-informed neural networks, Proc. R. Soc. 2020.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        The size of each input sample\n",
    "    out_features : int \n",
    "        The size of each output sample\n",
    "    bias : bool, optional\n",
    "        If set to ``False``, the layer will not learn an additive bias\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(AdaptiveLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.adaptive_rate = adaptive_rate\n",
    "        self.adaptive_rate_scaler = adaptive_rate_scaler\n",
    "        if self.adaptive_rate:\n",
    "            self.A = nn.Parameter(self.adaptive_rate * torch.ones(self.in_features))\n",
    "            if not self.adaptive_rate_scaler:\n",
    "                self.adaptive_rate_scaler = 10.0\n",
    "            \n",
    "    def forward(self, input):\n",
    "        if self.adaptive_rate:\n",
    "            return nn.functional.linear(self.adaptive_rate_scaler * self.A * input, self.weight, self.bias)\n",
    "        return nn.functional.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, '\n",
    "            f'adaptive_rate={self.adaptive_rate is not None}, adaptive_rate_scaler={self.adaptive_rate_scaler is not None}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_poisson(x, rhs, boundary_conditions):\n",
    "    r\"\"\"Solve 1-D Poisson equation of simple form as follows:\n",
    "    .. math::\n",
    "        \\frac{\\mathrm{d} \\phi^2}{\\mathrm{d} x^2} = f(x)\n",
    "\n",
    "    with known Dirichlet boundary conditions on arbitrary solution\n",
    "    domain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Independent variable to solve Poisson equation with respect to\n",
    "    rhs : float\n",
    "        Value of the scalar right hand side function\n",
    "    boundary_conditions : tuple or list\n",
    "        Boundary conditions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Poisson equation analytic solution, :math:`phi(x)`\n",
    "    \"\"\"\n",
    "    x0 = x.min()\n",
    "    x1 = x.max()\n",
    "    C1 = (\n",
    "        1 / (x1 - x0) \n",
    "        * (boundary_conditions[1] - rhs / 2 * x1**2 + rhs / 2 * x0**2 - boundary_conditions[0])\n",
    "    )\n",
    "    C2 = boundary_conditions[0] - rhs / 2 * x0**2 - C1 * x0 \n",
    "    return rhs / 2 * x**2 + C1 * x + C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    r\"\"\"Neural approximator for the unknown function that is supposed\n",
    "    to be solved.\n",
    "\n",
    "    More details available in Raissi, M. et al. Physics-informed neural\n",
    "    networks: A deep learning framework for solving forward and inverse\n",
    "    problems involving nonlinear partial differential equations, J.\n",
    "    Comput. Phys. 2019.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sizes : list\n",
    "        Each element represents the number of neuron per layer\n",
    "    activation : callable \n",
    "        Activation function\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate for regulrization during training process and\n",
    "        uncertainty quantification by means of Monte Carlo dropout\n",
    "        procedure while performing evaluation\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, activation, dropout_rate=0.0, adaptive_rate=None, adaptive_rate_scaler=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            *[Net.linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler)\n",
    "            for in_features, out_features in zip(sizes[:-1], sizes[1:-1])],     \n",
    "            AdaptiveLinear(sizes[-2], sizes[-1]) # output layer is regular linear transformation\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_block(in_features, out_features, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler):\n",
    "        activation_dispatcher = nn.ModuleDict([\n",
    "            ['lrelu', nn.LeakyReLU()],\n",
    "            ['relu', nn.ReLU()],\n",
    "            ['tanh', nn.Tanh()],\n",
    "            ['sigmoid', nn.Sigmoid()],\n",
    "            ['swish', Swish()]\n",
    "        ])\n",
    "        return nn.Sequential(\n",
    "            AdaptiveLinear(in_features, out_features, adaptive_rate=adaptive_rate, adaptive_rate_scaler=adaptive_rate_scaler),\n",
    "            activation_dispatcher[activation],\n",
    "            nn.Dropout(dropout_rate),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        device, domain, boundary_conditions, rhs,\n",
    "        sizes, activation, optimizer, n_epochs, batch_size, linspace, learning_rate,\n",
    "        dropout_rate,\n",
    "        adaptive_rate, adaptive_rate_scaler\n",
    "        ):\n",
    "    r\"\"\"Train PINN and return trained network alongside loss over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : str\n",
    "        Specifiy `cuda` if CUDA-enabled GPU is available, otherwise\n",
    "        specify `cpu`\n",
    "    domain : tuple or list\n",
    "        Boundaries of the solution domain\n",
    "    boundary_conditions : tuple or list\n",
    "        Boundary conditions\n",
    "    rhs : float\n",
    "        Value of the scalar right hand side function\n",
    "    sizes : list\n",
    "        Each element represents the number of neuron per layer\n",
    "    activation : callable \n",
    "        Activation function\n",
    "    optimizer : callable\n",
    "        Optimization procedure\n",
    "    n_epochs : int\n",
    "        The number of training epochs\n",
    "    batch_size : int\n",
    "        The number of data points for optimization per epoch\n",
    "    linspace : bool\n",
    "        Space the batch of data linearly, otherwise random\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate for regulrization during training process and\n",
    "        uncertainty quantification by means of Monte Carlo dropout\n",
    "        procedure while performing evaluation\n",
    "    adaptive_rate : float, optional\n",
    "        Scalable adaptive rate parameter for activation function that\n",
    "        is added layer-wise for each neuron separately. It is treated\n",
    "        as learnable parameter and will be optimized using a optimizer\n",
    "        of choice\n",
    "    adaptive_rate_scaler : float, optional\n",
    "        Fixed, pre-defined, scaling factor for adaptive activation\n",
    "        functions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    net : Net\n",
    "        Trained function approximator\n",
    "    loss_list : list\n",
    "        Loss values during training process\n",
    "    \"\"\"\n",
    "    net = Net(sizes, activation, dropout_rate, adaptive_rate, adaptive_rate_scaler).to(device=device)\n",
    "    optimizer = optimizer_dispatcher(optimizer, net.parameters(), learning_rate)\n",
    "    loss_list = []\n",
    "    logging.info(f'{net}\\n')\n",
    "    logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "    start_time = timer()\n",
    "    for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "        def closure():\n",
    "            if linspace:\n",
    "                x = torch.linspace(*domain, steps=batch_size, device=device).unsqueeze(-1)\n",
    "            else:\n",
    "                x = (domain[0] - domain[1]) * torch.rand(size=(batch_size, ), device=device).unsqueeze(-1) + domain[1]\n",
    "            x.requires_grad = True\n",
    "\n",
    "            phi = net(x)\n",
    "            x.grad = None\n",
    "            phi.backward(torch.ones_like(x, device=device), create_graph=True)\n",
    "            phi_x = x.grad\n",
    "            x.grad = None\n",
    "            phi_x.backward(torch.ones_like(x, device=device), create_graph=True)\n",
    "            phi_xx = x.grad\n",
    "            domain_residual = phi_xx - rhs(x)\n",
    "\n",
    "            boundaries = torch.tensor(domain, device=device).unsqueeze(-1)\n",
    "            boundaries.requires_grad = True\n",
    "            boundary_residual = net(boundaries) - torch.tensor(boundary_conditions, device=device).unsqueeze(-1)\n",
    "\n",
    "            if adaptive_rate:\n",
    "                local_recovery_terms = torch.tensor([torch.mean(net.regressor[layer][0].A.data) for layer in range(len(net.regressor) - 1)])\n",
    "                slope_recovery_term = 1 / torch.mean(torch.exp(local_recovery_terms))\n",
    "                loss = (torch.mean(domain_residual ** 2) + torch.mean(boundary_residual ** 2)) + slope_recovery_term\n",
    "            else:\n",
    "                loss = (torch.mean(domain_residual ** 2) + torch.mean(boundary_residual ** 2))\n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "    elapsed = timer() - start_time\n",
    "    logging.info(f'Training finished. Elapsed time: {elapsed} s\\n')\n",
    "    return net, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_viz(\n",
    "        device, domain, boundary_conditions, rhs,\n",
    "        net, loss_list,\n",
    "        apply_mcdropout,\n",
    "        save_fig\n",
    "        ):\n",
    "    r\"\"\"Evaluate and visualize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : str\n",
    "        Specifiy `cuda` if CUDA-enabled GPU is available, otherwise\n",
    "        specify `cpu`\n",
    "    domain : tuple or list\n",
    "        Boundaries of the solution domain\n",
    "    boundary_conditions : tuple or list\n",
    "        Boundary conditions\n",
    "    rhs : float\n",
    "        Value of the scalar right hand side function\n",
    "    net : Net\n",
    "        Trained function approximator\n",
    "    loss_list : list\n",
    "        Loss values during training process\n",
    "    apply_mcdropout : bool\n",
    "        Apply Monte Carlo dropout for uncertainty quantification if set\n",
    "        to `True`\n",
    "    save_fig : bool\n",
    "        Save figure\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    x = torch.linspace(*domain, 101, device=device).unsqueeze(-1)\n",
    "    rhs = rhs(x).cpu().detach().numpy().ravel()\n",
    "    if apply_mcdropout:\n",
    "        y_pred_mc = np.empty((1000, x.shape[0]))\n",
    "        for i in range(1000):\n",
    "            y_pred = net(x)\n",
    "            y_pred_mc[i, :] = y_pred.cpu().detach().numpy().ravel()\n",
    "        net.eval()\n",
    "        y_pred = np.mean(y_pred_mc, axis=0)\n",
    "        y_ci = np.std(y_pred_mc, axis=0)\n",
    "    else:\n",
    "        net.eval() \n",
    "        y_pred = net(x)\n",
    "        y_pred = y_pred.cpu().detach().numpy().ravel()\n",
    "    x = x.cpu().detach().numpy().ravel()\n",
    "    y = solve_poisson(x, rhs, boundary_conditions)\n",
    "    rmse_val = np.sqrt(np.mean((y - y_pred)**2))\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "    ax[0].plot(x, y, 'k-', linewidth=2, label='Analytic solution')\n",
    "    ax[0].plot(x, y_pred, 'r--', dashes=(3, 4), linewidth=3, label='PINN solution')\n",
    "    if apply_mcdropout:\n",
    "        ax[0].fill_between(x, y_pred + 2*y_ci, y_pred - 2*y_ci, color='r', alpha=0.1, label='95% CI')\n",
    "    ax[0].set_xlabel('x')\n",
    "    ax[0].set_ylabel('y')\n",
    "    ax[0].set_title(f'RMSE = {rmse_val:.6f}')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(loss_list, 'r-')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_xlabel('training epoch')\n",
    "    ax[1].set_ylabel('loss value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join('figs', f'{save_fig}.png'), format='png', bbox_inches='tight', dpi=200)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    args = parse_arguments()\n",
    "    if args.cuda:\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    logging.info(f'Device: {device}\\n')\n",
    "    rhs = lambda x: torch.tensor([args.rhs], device=device)\n",
    "    domain = args.domain\n",
    "    boundary_conditions = args.boundary_conditions\n",
    "\n",
    "    # configure neural network\n",
    "    sizes = [1] + args.n_layers * [args.n_units] + [1]\n",
    "    activation = args.activation\n",
    "    optimizer = args.optimizer\n",
    "    n_epochs = args.n_epochs\n",
    "    batch_size = args.batch_size\n",
    "    linspace = args.linspace\n",
    "    learning_rate = args.learning_rate\n",
    "    dropout_rate = args.dropout_rate\n",
    "    apply_mcdropout = args.apply_mcdropout\n",
    "    adaptive_rate = args.adaptive_rate\n",
    "    adaptive_rate_scaler = args.adaptive_rate_scaler\n",
    "    save_fig = args.save_fig\n",
    "\n",
    "    # trainining process\n",
    "    net, loss_list = train(\n",
    "        device, domain, boundary_conditions, rhs,\n",
    "        sizes, activation, optimizer, n_epochs, batch_size, linspace, learning_rate,\n",
    "        dropout_rate,\n",
    "        adaptive_rate, adaptive_rate_scaler\n",
    "        )\n",
    "    \n",
    "    # evaluation\n",
    "    eval_and_viz(\n",
    "        device, domain, boundary_conditions, rhs,\n",
    "        net, loss_list,\n",
    "        apply_mcdropout,\n",
    "        save_fig\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
