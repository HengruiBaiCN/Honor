{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/100], x: -0.758408, f(x): 2.091198, x.grad: 0.603980\n",
      "Step [20/100], x: -0.974059, f(x): 2.001051, x.grad: 0.064852\n",
      "Step [30/100], x: -0.997215, f(x): 2.000012, x.grad: 0.006963\n",
      "Step [40/100], x: -0.999701, f(x): 2.000000, x.grad: 0.000748\n",
      "Step [50/100], x: -0.999968, f(x): 2.000000, x.grad: 0.000080\n",
      "Step [60/100], x: -0.999997, f(x): 2.000000, x.grad: 0.000009\n",
      "Step [70/100], x: -1.000000, f(x): 2.000000, x.grad: 0.000001\n",
      "Step [80/100], x: -1.000000, f(x): 2.000000, x.grad: 0.000000\n",
      "Step [90/100], x: -1.000000, f(x): 2.000000, x.grad: 0.000000\n",
      "Step [100/100], x: -1.000000, f(x): 2.000000, x.grad: 0.000000\n",
      "Optimized x: -0.9999998807907104\n",
      "Optimized f(x): 2.0\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function: f(x) = x^2 + 2x + 3\n",
    "def f(x):\n",
    "    return x**2 + 2*x + 3\n",
    "\n",
    "# Create a tensor with a value of 2.0\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Forward pass: Compute the function value and create a computation graph\n",
    "y = f(x)\n",
    "\n",
    "# Perform backward pass: Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Get the gradient of the function w.r.t. the input x\n",
    "gradient = x.grad\n",
    "\n",
    "# Optimize the function using gradient descent\n",
    "learning_rate = 0.1\n",
    "num_steps = 100\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass: Compute the function value and create a computation graph\n",
    "    y = f(x)\n",
    "\n",
    "    # Perform backward pass: Compute gradients\n",
    "    y.backward()\n",
    "\n",
    "    # Update x using gradient descent\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "        \n",
    "    # check the gradient before clear it\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step [{step+1}/{num_steps}], x: {x.item():.6f}, f(x): {y.item():.6f}, x.grad: {gradient.item():.6f}\")\n",
    "        \n",
    "    # Clear the gradients after each step\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(\"Optimized x:\", x.item())\n",
    "print(\"Optimized f(x):\", f(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second order derivaives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value (z): 18.0\n",
      "First-order gradient (dz/dx): 9.0\n",
      "First-order gradient (dz/dy): 11.0\n",
      "Second-order gradient (d^2z/dx^2): 2.0\n",
      "Second-order gradient (d^2z/dxdy): 3.0\n",
      "Second-order gradient (d^2z/dydx): 3.0\n",
      "Second-order gradient (d^2z/dy^2): 2.0\n"
     ]
    }
   ],
   "source": [
    "# Define the function: f(x, y) = x^2 + 3xy + y^2 + 2x + 3y\n",
    "def f(x, y):\n",
    "    return x**2 + 3 * x * y + y**2 + 2 * x + 3 * y\n",
    "\n",
    "# Create two tensors with values of 2.0 and 1.0\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Forward pass: Compute the function value and create a computation graph\n",
    "z = f(x, y)\n",
    "\n",
    "# Perform first-order backward pass: Compute gradients\n",
    "grads = torch.autograd.grad(z, [x, y], create_graph=True)\n",
    "\n",
    "# Get the first-order gradients (gradient w.r.t. x and y)\n",
    "grad_x, grad_y = grads\n",
    "\n",
    "# Perform second-order backward pass: Compute the Hessian matrix\n",
    "# Compute the second-order derivatives by taking the gradient of gradients\n",
    "hessian = []\n",
    "for grad in grads:\n",
    "    hess = torch.autograd.grad(grad, [x, y], retain_graph=True)\n",
    "    hessian.append(hess)\n",
    "\n",
    "# Get the second-order derivatives (Hessian matrix)\n",
    "hessian_xx, hessian_xy = hessian[0]\n",
    "hessian_yx, hessian_yy = hessian[1]\n",
    "\n",
    "# Print the results\n",
    "print(\"Function value (z):\", z.item())\n",
    "print(\"First-order gradient (dz/dx):\", grad_x.item())\n",
    "print(\"First-order gradient (dz/dy):\", grad_y.item())\n",
    "print(\"Second-order gradient (d^2z/dx^2):\", hessian_xx.item())\n",
    "print(\"Second-order gradient (d^2z/dxdy):\", hessian_xy.item())\n",
    "print(\"Second-order gradient (d^2z/dydx):\", hessian_yx.item())\n",
    "print(\"Second-order gradient (d^2z/dy^2):\", hessian_yy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
