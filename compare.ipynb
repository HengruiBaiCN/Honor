{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    \n",
    "def save_model(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(data, os.path.join(path, filename))\n",
    "    pass\n",
    "\n",
    "def save_loss(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    data.to_csv(os.path.join(path, filename), index=False)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "# model parameters\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "\n",
    "# training parameters\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AWPINN with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "n_epochs = 5000\n",
    "lr_rate_list = [lr/10000, lr/100, lr/10, lr, lr*10, lr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awpinn/v2/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awipinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN and IPINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/pinn\"\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/ipinn\"\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibpinn and awipinn with different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "awpinn_weights = [[1.5, 1, 1], [5, 1, 1], [1, 1, 2], [1, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awpinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awipinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "# ipinn_loss_weight = [[1, 1, 1], [1, 1.5, 1.5], [1, 2, 2] [1.5, 1, 1], [2, 1, 1], [3, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1.5-1-1', [1.5,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1.5-1-1', [1.5,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/3-1-1', [3,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/3-1-1', [3,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/3-1-1', [3,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/3-1-1', [3,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1.5-1.5', [1,1.5,1.5])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1.5-1.5', [1,1.5,1.5], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1.5-1.5', [1,1.5,1.5], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1.5-1.5', [1,1.5,1.5], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:41<00:00, 61.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:41<00:00, 62.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:39<00:00, 62.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:39<00:00, 62.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:33<00:00, 65.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:35<00:00, 64.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:40<00:00, 62.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:56<00:00, 42.23it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:51<00:00, 43.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:54<00:00, 42.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:57<00:00, 42.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:57<00:00, 42.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:47<00:00, 34.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:21<00:00, 31.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:22<00:00, 31.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:21<00:00, 31.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:11<00:00, 32.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:36<00:00, 46.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:41<00:00, 45.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:39<00:00, 45.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:15<00:00, 51.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:42<00:00, 61.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:00<00:00, 41.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:01<00:00, 41.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:08<00:00, 40.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:03<00:00, 41.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:03<00:00, 41.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:01<00:00, 41.45it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:00<00:00, 41.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-2-2', [1,2,2])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-2-2', [1,2,2], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-2-2', [1,2,2], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-2-2', [1,2,2], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.90it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 116.28it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 114.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.52it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:12<00:00, 75.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.74it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:13<00:00, 74.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:19<00:00, 71.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.52it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:14<00:00, 74.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:16<00:00, 73.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:34<00:00, 64.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:07<00:00, 78.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.21it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.18it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:31<00:00, 109.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.18it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.34it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.64it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:12<00:00, 75.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.51it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.22it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.67it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.59it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.95it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 116.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.64it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.21it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.25it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 117.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 81.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:07<00:00, 78.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.81it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.29it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:33<00:00, 107.50it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.11it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.35it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:11<00:00, 75.90it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.19it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.51it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:13<00:00, 74.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.34it/s]\n"
     ]
    }
   ],
   "source": [
    "sizes=[2, 25, 25, 25, 25, 25, 25, 25, 25, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "    \n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[2, 50, 50, 50, 50, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "# net.to(device)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "# loss_hist6 = []\n",
    "# logging.info(f'{net}\\n')\n",
    "# logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "# min_train_loss = float(\"inf\")  # Initialize with a large value\n",
    "# final_model = None\n",
    "# start_time = timer()\n",
    "# for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "#     prediction = net(X_train_tensor)\n",
    "#     loss = lossFunction(prediction, y_train_tensor)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     loss_hist6.append(loss.item())\n",
    "    \n",
    "#     if loss.item() < min_train_loss:\n",
    "#         min_train_loss = loss.item()\n",
    "#         final_model = net.state_dict()\n",
    "#     pass\n",
    "\n",
    "# torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# # Save the training loss history as a CSV file\n",
    "# loss_df = pd.DataFrame(loss_hist6)\n",
    "# loss_df.to_csv('default/nn_loss.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
