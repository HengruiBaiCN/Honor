{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "loss_weights = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "\n",
    "n_epochs = 5000\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    torch.save(wpinn.state_dict(), f\"weight-test/pinn/{w1}-{w2}-{w3}.pth\")  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'weight-loss/pinn/{w1}-{w2}-{w3}_loss.csv', index=False)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPINN with Different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    torch.save(wipinn.state_dict(), f\"weight-test/ipinn/{w1}-{w2}-{w3}.pth\")  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'weight-loss/ipinn/{w1}-{w2}-{w3}_loss.csv', index=False)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "# model1.to(device)\n",
    "# model1.load_state_dict(torch.load('default/pinn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.load_state_dict(torch.load('default/awpinn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.load_state_dict(torch.load('default/nn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "# model2 = networks.ImprovedNeuralNetwork(sizes, 'relu', 0, 0.1, 10.0)\n",
    "# model2.to(device)\n",
    "# model2.load_state_dict(torch.load('default/ipinn.pth'))\n",
    "# model2.eval()\n",
    "# prediction = model2(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.load_state_dict(torch.load('default/awipinn.pth'))\n",
    "# model2.eval()\n",
    "# prediction = model2(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:49<00:00, 58.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:50<00:00, 58.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:53<00:00, 57.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.86it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:04<00:00, 40.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:09<00:00, 40.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:05<00:00, 40.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:03<00:00, 41.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:08<00:00, 40.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:15<00:00, 39.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    pinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=0, loss_weights=loss_weights, adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(min_model, f\"Trained_model/pinn/model_{i}.pth\")\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'Training_loss/pinn/loss_{i}.csv', index=False)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "average_loss_df.to_csv('Thesis/pinn_average_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [06:19<00:00, 26.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:33<00:00, 36.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:17<00:00, 38.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:15<00:00, 39.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:15<00:00, 39.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:17<00:00, 38.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:17<00:00, 38.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:16<00:00, 39.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:17<00:00, 38.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:18<00:00, 38.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    ipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate =0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    \n",
    "    # Save the model's state dictionary\n",
    "    torch.save(min_model, f\"Trained_model/ipinn/model_{i}.pth\")\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'Training_loss/ipinn/loss_{i}.csv', index=False)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "average_loss_df.to_csv('Thesis/ipinn_average_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:55<00:00, 56.84it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:51<00:00, 58.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:50<00:00, 58.79it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:27<00:00, 67.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:29<00:00, 66.81it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:26<00:00, 68.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:28<00:00, 67.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    awpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(min_model, f\"Trained_model/awpinn/model_{i}_-3.pth\")\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'Training_loss/awpinn/loss_{i}_-3.csv', index=False)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "average_loss_df.to_csv('Thesis/awpinn_average_losses_-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [03:35<00:00, 46.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:35<00:00, 46.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.84it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:34<00:00, 46.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:40<00:00, 45.26it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.67it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:39<00:00, 45.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:40<00:00, 45.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(min_model, f\"Trained_model/awipinn/model_{i}_-3.pth\")\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    loss_df.to_csv(f'Training_loss/awipinn/loss_{i}_-3.csv', index=False)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "average_loss_df.to_csv('Thesis/awipinn_average_losses_-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:39<00:00, 100.74it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory default does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\compare.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/compare.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         final_model \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/compare.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/compare.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(final_model, \u001b[39m\"\u001b[39;49m\u001b[39mdefault/nn.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Save the model's state dictionary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/compare.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Save the training loss history as a CSV file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/compare.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(loss_hist6)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory default does not exist."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "loss_hist6 = []\n",
    "logging.info(f'{net}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "min_train_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = timer()\n",
    "for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "    prediction = net(X_train_tensor)\n",
    "    loss = lossFunction(prediction, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist6.append(loss.item())\n",
    "    \n",
    "    if loss.item() < min_train_loss:\n",
    "        min_train_loss = loss.item()\n",
    "        final_model = net.state_dict()\n",
    "    pass\n",
    "\n",
    "torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# Save the training loss history as a CSV file\n",
    "loss_df = pd.DataFrame(loss_hist6)\n",
    "loss_df.to_csv('default/nn_loss.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
