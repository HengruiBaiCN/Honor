{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    \n",
    "def save_model(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(data, os.path.join(path, filename))\n",
    "    pass\n",
    "\n",
    "def save_loss(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    data.to_csv(os.path.join(path, filename), index=False)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "# model parameters\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "\n",
    "# training parameters\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AWPINN with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "n_epochs = 5000\n",
    "lr_rate_list = [lr/10000, lr/100, lr/10, lr, lr*10, lr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awpinn/v2/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awipinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN and IPINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/pinn\"\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/ipinn\"\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibpinn and awipinn with different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "awpinn_weights = [[1.5, 1, 1], [5, 1, 1], [1, 1, 2], [1, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awpinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awipinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "# ipinn_loss_weight = [[1, 1, 1], [1, 1.5, 1.5], [1, 2, 2] [1.5, 1, 1], [2, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1.5-1-1', [1.5,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1.5-1-1', [1.5,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1.5-1.5', [1,1.5,1.5])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1.5-1.5', [1,1.5,1.5], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1.5-1.5', [1,1.5,1.5], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1.5-1.5', [1,1.5,1.5], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:41<00:00, 61.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:41<00:00, 62.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:39<00:00, 62.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:39<00:00, 62.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:33<00:00, 65.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:35<00:00, 64.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:40<00:00, 62.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:56<00:00, 42.23it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:51<00:00, 43.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:54<00:00, 42.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:57<00:00, 42.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:57<00:00, 42.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:47<00:00, 34.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:21<00:00, 31.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:22<00:00, 31.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:21<00:00, 31.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:11<00:00, 32.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:36<00:00, 46.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:41<00:00, 45.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:39<00:00, 45.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:15<00:00, 51.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:42<00:00, 61.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:00<00:00, 41.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:01<00:00, 41.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:08<00:00, 40.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:03<00:00, 41.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:03<00:00, 41.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:01<00:00, 41.45it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:02<00:00, 41.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:00<00:00, 41.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-2-2', [1,2,2])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-2-2', [1,2,2], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-2-2', [1,2,2], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-2-2', [1,2,2], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.90it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 116.28it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 114.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.52it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:12<00:00, 75.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.74it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:13<00:00, 74.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:19<00:00, 71.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:17<00:00, 72.52it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:14<00:00, 74.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:16<00:00, 73.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:34<00:00, 64.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:07<00:00, 78.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.21it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.18it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:31<00:00, 109.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.18it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.34it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.64it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:12<00:00, 75.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.51it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.22it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.67it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.59it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.95it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 116.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.64it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.21it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.25it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 117.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 81.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:07<00:00, 78.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.81it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.29it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:26<00:00, 115.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:33<00:00, 107.50it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.11it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.35it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:11<00:00, 75.90it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.19it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.51it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:13<00:00, 74.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.34it/s]\n"
     ]
    }
   ],
   "source": [
    "sizes=[2, 25, 25, 25, 25, 25, 25, 25, 25, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "    \n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[2, 50, 50, 50, 50, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:53<00:00, 88.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 422.00335693359375 relative L2 0.9973743557929993\n",
      "training 421.9865417480469 relative L2 0.9973543882369995\n",
      "training 421.9696350097656 relative L2 0.997334361076355\n",
      "training 421.9526672363281 relative L2 0.9973140954971313\n",
      "training 421.9355773925781 relative L2 0.9972938895225525\n",
      "training 421.9184265136719 relative L2 0.9972734451293945\n",
      "training 421.9011535644531 relative L2 0.997252881526947\n",
      "training 421.8837585449219 relative L2 0.9972321391105652\n",
      "training 421.8661804199219 relative L2 0.997211217880249\n",
      "training 421.8485107421875 relative L2 0.9971901774406433\n",
      "training 421.8307189941406 relative L2 0.9971688985824585\n",
      "training 421.812744140625 relative L2 0.9971476197242737\n",
      "training 421.794677734375 relative L2 0.9971259236335754\n",
      "training 421.7763977050781 relative L2 0.9971041679382324\n",
      "training 421.7579040527344 relative L2 0.9970819354057312\n",
      "training 421.73919677734375 relative L2 0.9970595240592957\n",
      "training 421.72021484375 relative L2 0.997036874294281\n",
      "training 421.7010192871094 relative L2 0.9970139861106873\n",
      "training 421.6816711425781 relative L2 0.9969906210899353\n",
      "training 421.6620788574219 relative L2 0.9969673156738281\n",
      "training 421.64227294921875 relative L2 0.9969435930252075\n",
      "training 421.6222229003906 relative L2 0.9969193339347839\n",
      "training 421.60174560546875 relative L2 0.9968938827514648\n",
      "training 421.5802307128906 relative L2 0.9968681335449219\n",
      "training 421.5583801269531 relative L2 0.9968420267105103\n",
      "training 421.536376953125 relative L2 0.9968157410621643\n",
      "training 421.51416015625 relative L2 0.9967891573905945\n",
      "training 421.49169921875 relative L2 0.9967623949050903\n",
      "training 421.4691162109375 relative L2 0.9967358708381653\n",
      "training 421.4466552734375 relative L2 0.9967100024223328\n",
      "training 421.4247741699219 relative L2 0.9966844320297241\n",
      "training 421.4031982421875 relative L2 0.9966590404510498\n",
      "training 421.3816223144531 relative L2 0.9966334700584412\n",
      "training 421.3600769042969 relative L2 0.9966078400611877\n",
      "training 421.33843994140625 relative L2 0.9965822696685791\n",
      "training 421.3168029785156 relative L2 0.9965565204620361\n",
      "training 421.2950439453125 relative L2 0.9965305924415588\n",
      "training 421.2731628417969 relative L2 0.9965046048164368\n",
      "training 421.2510986328125 relative L2 0.9964781999588013\n",
      "training 421.22882080078125 relative L2 0.9964515566825867\n",
      "training 421.206298828125 relative L2 0.9964245557785034\n",
      "training 421.1835021972656 relative L2 0.9963973760604858\n",
      "training 421.16046142578125 relative L2 0.9963696599006653\n",
      "training 421.1370544433594 relative L2 0.9963415861129761\n",
      "training 421.11334228515625 relative L2 0.9963132739067078\n",
      "training 421.0893859863281 relative L2 0.9962847828865051\n",
      "training 421.0652770996094 relative L2 0.9962560534477234\n",
      "training 421.041015625 relative L2 0.9962272644042969\n",
      "training 421.0167541503906 relative L2 0.9961984753608704\n",
      "training 420.9923095703125 relative L2 0.9961692690849304\n",
      "training 420.9676818847656 relative L2 0.9961400032043457\n",
      "training 420.9429931640625 relative L2 0.9961105585098267\n",
      "training 420.91815185546875 relative L2 0.9960809350013733\n",
      "training 420.8930969238281 relative L2 0.9960512518882751\n",
      "training 420.867919921875 relative L2 0.9960211515426636\n",
      "training 420.8425598144531 relative L2 0.9959908723831177\n",
      "training 420.81695556640625 relative L2 0.9959604144096375\n",
      "training 420.79119873046875 relative L2 0.9959295392036438\n",
      "training 420.76519775390625 relative L2 0.9958985447883606\n",
      "training 420.7390441894531 relative L2 0.9958672523498535\n",
      "training 420.7125549316406 relative L2 0.9958356618881226\n",
      "training 420.6858825683594 relative L2 0.9958038330078125\n",
      "training 420.658935546875 relative L2 0.995771586894989\n",
      "training 420.6317443847656 relative L2 0.9957391619682312\n",
      "training 420.60430908203125 relative L2 0.99570631980896\n",
      "training 420.57659912109375 relative L2 0.9956730604171753\n",
      "training 420.548583984375 relative L2 0.9956398010253906\n",
      "training 420.5204162597656 relative L2 0.9956063032150269\n",
      "training 420.4921569824219 relative L2 0.9955725073814392\n",
      "training 420.463623046875 relative L2 0.9955382943153381\n",
      "training 420.4347839355469 relative L2 0.9955039024353027\n",
      "training 420.40557861328125 relative L2 0.9954689741134644\n",
      "training 420.37615966796875 relative L2 0.9954336881637573\n",
      "training 420.3463439941406 relative L2 0.9953980445861816\n",
      "training 420.3163146972656 relative L2 0.9953622817993164\n",
      "training 420.28607177734375 relative L2 0.9953262805938721\n",
      "training 420.255615234375 relative L2 0.9952899813652039\n",
      "training 420.22503662109375 relative L2 0.9952540397644043\n",
      "training 420.1946716308594 relative L2 0.9952182769775391\n",
      "training 420.1645202636719 relative L2 0.9951826333999634\n",
      "training 420.1344299316406 relative L2 0.9951468110084534\n",
      "training 420.1042175292969 relative L2 0.9951109886169434\n",
      "training 420.07391357421875 relative L2 0.9950746893882751\n",
      "training 420.0433349609375 relative L2 0.9950382709503174\n",
      "training 420.0125427246094 relative L2 0.9950013160705566\n",
      "training 419.9814758300781 relative L2 0.9949641227722168\n",
      "training 419.9500732421875 relative L2 0.9949266314506531\n",
      "training 419.9183044433594 relative L2 0.9948886632919312\n",
      "training 419.8863220214844 relative L2 0.994850218296051\n",
      "training 419.8539123535156 relative L2 0.9948115348815918\n",
      "training 419.8211975097656 relative L2 0.9947723746299744\n",
      "training 419.7882385253906 relative L2 0.9947327971458435\n",
      "training 419.7548828125 relative L2 0.9946929216384888\n",
      "training 419.72119140625 relative L2 0.9946526288986206\n",
      "training 419.68719482421875 relative L2 0.9946118593215942\n",
      "training 419.65283203125 relative L2 0.9945706725120544\n",
      "training 419.61810302734375 relative L2 0.9945290684700012\n",
      "training 419.5829772949219 relative L2 0.9944870471954346\n",
      "training 419.5475158691406 relative L2 0.9944444298744202\n",
      "training 419.5115966796875 relative L2 0.9944015145301819\n",
      "training 419.475341796875 relative L2 0.9943580627441406\n",
      "training 419.4386291503906 relative L2 0.9943138957023621\n",
      "training 419.4014587402344 relative L2 0.9942693710327148\n",
      "training 419.3639221191406 relative L2 0.9942243695259094\n",
      "training 419.3259582519531 relative L2 0.9941788911819458\n",
      "training 419.28759765625 relative L2 0.994132936000824\n",
      "training 419.248779296875 relative L2 0.9940864443778992\n",
      "training 419.2095947265625 relative L2 0.9940394163131714\n",
      "training 419.1700439453125 relative L2 0.9939919114112854\n",
      "training 419.1299743652344 relative L2 0.9939440488815308\n",
      "training 419.089599609375 relative L2 0.9938956499099731\n",
      "training 419.0487060546875 relative L2 0.9938466548919678\n",
      "training 419.0074768066406 relative L2 0.9937971830368042\n",
      "training 418.9658203125 relative L2 0.9937472939491272\n",
      "training 418.9237060546875 relative L2 0.993696928024292\n",
      "training 418.88116455078125 relative L2 0.9936461448669434\n",
      "training 418.8385009765625 relative L2 0.9935957789421082\n",
      "training 418.7959899902344 relative L2 0.9935450553894043\n",
      "training 418.7532653808594 relative L2 0.9934938549995422\n",
      "training 418.7100830078125 relative L2 0.9934419989585876\n",
      "training 418.6663818359375 relative L2 0.9933894872665405\n",
      "training 418.6221923828125 relative L2 0.9933364391326904\n",
      "training 418.5775146484375 relative L2 0.9932826161384583\n",
      "training 418.5321960449219 relative L2 0.9932281970977783\n",
      "training 418.4862976074219 relative L2 0.9931730628013611\n",
      "training 418.43988037109375 relative L2 0.9931171536445618\n",
      "training 418.392822265625 relative L2 0.9930606484413147\n",
      "training 418.34515380859375 relative L2 0.9930033683776855\n",
      "training 418.2968444824219 relative L2 0.9929451942443848\n",
      "training 418.2478942871094 relative L2 0.992886483669281\n",
      "training 418.1985168457031 relative L2 0.9928270578384399\n",
      "training 418.1484375 relative L2 0.9927668571472168\n",
      "training 418.09771728515625 relative L2 0.9927056431770325\n",
      "training 418.0462341308594 relative L2 0.9926435947418213\n",
      "training 417.9939880371094 relative L2 0.9925807118415833\n",
      "training 417.9409484863281 relative L2 0.992516815662384\n",
      "training 417.8871765136719 relative L2 0.9924522638320923\n",
      "training 417.8328552246094 relative L2 0.9923869967460632\n",
      "training 417.7779235839844 relative L2 0.9923210144042969\n",
      "training 417.72235107421875 relative L2 0.9922541975975037\n",
      "training 417.6661376953125 relative L2 0.9921865463256836\n",
      "training 417.60919189453125 relative L2 0.9921180009841919\n",
      "training 417.551513671875 relative L2 0.992048442363739\n",
      "training 417.4929504394531 relative L2 0.9919779300689697\n",
      "training 417.4336242675781 relative L2 0.9919063448905945\n",
      "training 417.3734436035156 relative L2 0.9918338060379028\n",
      "training 417.3123779296875 relative L2 0.9917601943016052\n",
      "training 417.25042724609375 relative L2 0.9916855692863464\n",
      "training 417.1876220703125 relative L2 0.9916098117828369\n",
      "training 417.1238708496094 relative L2 0.9915328025817871\n",
      "training 417.0591125488281 relative L2 0.9914543628692627\n",
      "training 416.9931945800781 relative L2 0.9913743734359741\n",
      "training 416.9259033203125 relative L2 0.9912926554679871\n",
      "training 416.85723876953125 relative L2 0.9912093281745911\n",
      "training 416.787109375 relative L2 0.9911244511604309\n",
      "training 416.71575927734375 relative L2 0.9910380244255066\n",
      "training 416.6430969238281 relative L2 0.9909502267837524\n",
      "training 416.56927490234375 relative L2 0.9908609986305237\n",
      "training 416.4942321777344 relative L2 0.9907703399658203\n",
      "training 416.4179992675781 relative L2 0.9906781315803528\n",
      "training 416.340576171875 relative L2 0.9905844926834106\n",
      "training 416.2619934082031 relative L2 0.9904895424842834\n",
      "training 416.1821594238281 relative L2 0.9903931021690369\n",
      "training 416.1010437011719 relative L2 0.9902949929237366\n",
      "training 416.0187072753906 relative L2 0.9901955723762512\n",
      "training 415.93511962890625 relative L2 0.9900945425033569\n",
      "training 415.8502197265625 relative L2 0.9899918437004089\n",
      "training 415.7640380859375 relative L2 0.9898877739906311\n",
      "training 415.67657470703125 relative L2 0.9897820353507996\n",
      "training 415.58782958984375 relative L2 0.9896748065948486\n",
      "training 415.4978332519531 relative L2 0.9895662069320679\n",
      "training 415.40655517578125 relative L2 0.9894559979438782\n",
      "training 415.3140563964844 relative L2 0.9893442392349243\n",
      "training 415.2203063964844 relative L2 0.9892309904098511\n",
      "training 415.125244140625 relative L2 0.9891161322593689\n",
      "training 415.02886962890625 relative L2 0.9889998435974121\n",
      "training 414.9312744140625 relative L2 0.9888817667961121\n",
      "training 414.832275390625 relative L2 0.9887621402740479\n",
      "training 414.7319030761719 relative L2 0.9886410236358643\n",
      "training 414.63031005859375 relative L2 0.9885184168815613\n",
      "training 414.5274963378906 relative L2 0.9883944392204285\n",
      "training 414.4234619140625 relative L2 0.9882687926292419\n",
      "training 414.3181457519531 relative L2 0.988141655921936\n",
      "training 414.2115173339844 relative L2 0.9880127310752869\n",
      "training 414.10357666015625 relative L2 0.9878823757171631\n",
      "training 413.9942321777344 relative L2 0.9877501130104065\n",
      "training 413.8835144042969 relative L2 0.9876163601875305\n",
      "training 413.7713623046875 relative L2 0.987480878829956\n",
      "training 413.65777587890625 relative L2 0.9873433709144592\n",
      "training 413.5426330566406 relative L2 0.9872037172317505\n",
      "training 413.4256591796875 relative L2 0.9870617389678955\n",
      "training 413.3067932128906 relative L2 0.9869173169136047\n",
      "training 413.1859130859375 relative L2 0.986770510673523\n",
      "training 413.0629577636719 relative L2 0.986621081829071\n",
      "training 412.9378967285156 relative L2 0.986469030380249\n",
      "training 412.8105773925781 relative L2 0.9863139986991882\n",
      "training 412.680908203125 relative L2 0.986156165599823\n",
      "training 412.548828125 relative L2 0.9859954714775085\n",
      "training 412.4143981933594 relative L2 0.9858314990997314\n",
      "training 412.2772216796875 relative L2 0.9856641888618469\n",
      "training 412.13726806640625 relative L2 0.9854934215545654\n",
      "training 411.9944763183594 relative L2 0.9853192567825317\n",
      "training 411.848876953125 relative L2 0.9851418137550354\n",
      "training 411.7005615234375 relative L2 0.9849609732627869\n",
      "training 411.54937744140625 relative L2 0.9847766160964966\n",
      "training 411.395263671875 relative L2 0.984588623046875\n",
      "training 411.2381896972656 relative L2 0.9843969345092773\n",
      "training 411.0781555175781 relative L2 0.9842017292976379\n",
      "training 410.9151916503906 relative L2 0.984002947807312\n",
      "training 410.7491760253906 relative L2 0.9838005304336548\n",
      "training 410.5802001953125 relative L2 0.9835942387580872\n",
      "training 410.4079895019531 relative L2 0.9833839535713196\n",
      "training 410.2325439453125 relative L2 0.983169674873352\n",
      "training 410.0538330078125 relative L2 0.9829514026641846\n",
      "training 409.8717346191406 relative L2 0.9827289581298828\n",
      "training 409.6863098144531 relative L2 0.9825024604797363\n",
      "training 409.4974365234375 relative L2 0.9822716116905212\n",
      "training 409.30511474609375 relative L2 0.9820365905761719\n",
      "training 409.1092834472656 relative L2 0.9817975163459778\n",
      "training 408.9100646972656 relative L2 0.9815545678138733\n",
      "training 408.7076721191406 relative L2 0.9813071489334106\n",
      "training 408.5017395019531 relative L2 0.9810549020767212\n",
      "training 408.2917785644531 relative L2 0.9807979464530945\n",
      "training 408.0779113769531 relative L2 0.9805360436439514\n",
      "training 407.8600769042969 relative L2 0.980269730091095\n",
      "training 407.6385498046875 relative L2 0.9799985885620117\n",
      "training 407.41302490234375 relative L2 0.9797223210334778\n",
      "training 407.183349609375 relative L2 0.9794408679008484\n",
      "training 406.9494323730469 relative L2 0.979154109954834\n",
      "training 406.71124267578125 relative L2 0.9788621664047241\n",
      "training 406.4687194824219 relative L2 0.9785647988319397\n",
      "training 406.2218322753906 relative L2 0.978262186050415\n",
      "training 405.97064208984375 relative L2 0.9779541492462158\n",
      "training 405.71502685546875 relative L2 0.9776408076286316\n",
      "training 405.4551086425781 relative L2 0.9773220419883728\n",
      "training 405.19091796875 relative L2 0.9769977927207947\n",
      "training 404.9220275878906 relative L2 0.9766676425933838\n",
      "training 404.6485595703125 relative L2 0.9763321876525879\n",
      "training 404.3706359863281 relative L2 0.9759917855262756\n",
      "training 404.0887145996094 relative L2 0.9756463766098022\n",
      "training 403.8027648925781 relative L2 0.9752955436706543\n",
      "training 403.51239013671875 relative L2 0.9749391078948975\n",
      "training 403.21759033203125 relative L2 0.9745769500732422\n",
      "training 402.91815185546875 relative L2 0.974209189414978\n",
      "training 402.6140441894531 relative L2 0.9738353490829468\n",
      "training 402.30523681640625 relative L2 0.9734557867050171\n",
      "training 401.99169921875 relative L2 0.9730703234672546\n",
      "training 401.6734619140625 relative L2 0.9726793766021729\n",
      "training 401.3507995605469 relative L2 0.9722822904586792\n",
      "training 401.023193359375 relative L2 0.9718788266181946\n",
      "training 400.6904602050781 relative L2 0.9714696407318115\n",
      "training 400.3531799316406 relative L2 0.9710543155670166\n",
      "training 400.0110168457031 relative L2 0.9706327319145203\n",
      "training 399.66375732421875 relative L2 0.970204770565033\n",
      "training 399.3114013671875 relative L2 0.9697702527046204\n",
      "training 398.95391845703125 relative L2 0.9693295955657959\n",
      "training 398.5914306640625 relative L2 0.9688825011253357\n",
      "training 398.22393798828125 relative L2 0.9684287905693054\n",
      "training 397.8510437011719 relative L2 0.9679687023162842\n",
      "training 397.47308349609375 relative L2 0.9675018191337585\n",
      "training 397.0897521972656 relative L2 0.9670277833938599\n",
      "training 396.70086669921875 relative L2 0.9665467143058777\n",
      "training 396.3063049316406 relative L2 0.9660587310791016\n",
      "training 395.9062194824219 relative L2 0.9655634164810181\n",
      "training 395.5003967285156 relative L2 0.9650608897209167\n",
      "training 395.0888366699219 relative L2 0.9645512104034424\n",
      "training 394.6716613769531 relative L2 0.9640343189239502\n",
      "training 394.248779296875 relative L2 0.9635097980499268\n",
      "training 393.8199157714844 relative L2 0.9629775881767273\n",
      "training 393.3850402832031 relative L2 0.9624378681182861\n",
      "training 392.94415283203125 relative L2 0.9618901610374451\n",
      "training 392.4971008300781 relative L2 0.9613342881202698\n",
      "training 392.0436706542969 relative L2 0.9607709050178528\n",
      "training 391.584228515625 relative L2 0.9601998329162598\n",
      "training 391.1188659667969 relative L2 0.9596208930015564\n",
      "training 390.6475524902344 relative L2 0.9590343236923218\n",
      "training 390.17010498046875 relative L2 0.9584397077560425\n",
      "training 389.68646240234375 relative L2 0.9578369855880737\n",
      "training 389.1966247558594 relative L2 0.9572262167930603\n",
      "training 388.7004699707031 relative L2 0.9566107988357544\n",
      "training 388.20086669921875 relative L2 0.9559881091117859\n",
      "training 387.6956787109375 relative L2 0.9553569555282593\n",
      "training 387.18408203125 relative L2 0.9547173380851746\n",
      "training 386.66583251953125 relative L2 0.9540688991546631\n",
      "training 386.1407775878906 relative L2 0.9534114599227905\n",
      "training 385.6088562011719 relative L2 0.9527450203895569\n",
      "training 385.0699462890625 relative L2 0.952069103717804\n",
      "training 384.52386474609375 relative L2 0.9513837099075317\n",
      "training 383.970458984375 relative L2 0.9506886601448059\n",
      "training 383.4097595214844 relative L2 0.9499840140342712\n",
      "training 382.8415832519531 relative L2 0.9492692947387695\n",
      "training 382.2658386230469 relative L2 0.9485445618629456\n",
      "training 381.68243408203125 relative L2 0.9478096961975098\n",
      "training 381.0912170410156 relative L2 0.9470644593238831\n",
      "training 380.4922180175781 relative L2 0.9463086724281311\n",
      "training 379.8853454589844 relative L2 0.9455418586730957\n",
      "training 379.26995849609375 relative L2 0.9447633028030396\n",
      "training 378.6456604003906 relative L2 0.9439724683761597\n",
      "training 378.0120849609375 relative L2 0.9431689381599426\n",
      "training 377.3688659667969 relative L2 0.9423527717590332\n",
      "training 376.7160339355469 relative L2 0.9415237307548523\n",
      "training 376.0535888671875 relative L2 0.9406821131706238\n",
      "training 375.3816833496094 relative L2 0.9398278594017029\n",
      "training 374.7003173828125 relative L2 0.9389611482620239\n",
      "training 374.0095520019531 relative L2 0.9380817413330078\n",
      "training 373.30926513671875 relative L2 0.9371897578239441\n",
      "training 372.5998229980469 relative L2 0.9362866282463074\n",
      "training 371.8821105957031 relative L2 0.9353719353675842\n",
      "training 371.156005859375 relative L2 0.9344451427459717\n",
      "training 370.4208679199219 relative L2 0.9335057735443115\n",
      "training 369.6766052246094 relative L2 0.9325535893440247\n",
      "training 368.9228820800781 relative L2 0.9315884709358215\n",
      "training 368.1597595214844 relative L2 0.9306102395057678\n",
      "training 367.38702392578125 relative L2 0.9296185970306396\n",
      "training 366.6045227050781 relative L2 0.9286133646965027\n",
      "training 365.81219482421875 relative L2 0.9275946021080017\n",
      "training 365.01007080078125 relative L2 0.9265621304512024\n",
      "training 364.197998046875 relative L2 0.9255155920982361\n",
      "training 363.37579345703125 relative L2 0.924454927444458\n",
      "training 362.5435791015625 relative L2 0.9233802556991577\n",
      "training 361.7012023925781 relative L2 0.9222912788391113\n",
      "training 360.8486328125 relative L2 0.9211878776550293\n",
      "training 359.9858703613281 relative L2 0.9200699329376221\n",
      "training 359.1126708984375 relative L2 0.9189369082450867\n",
      "training 358.2288818359375 relative L2 0.917788565158844\n",
      "training 357.3341979980469 relative L2 0.9166247844696045\n",
      "training 356.4286804199219 relative L2 0.9154454469680786\n",
      "training 355.5121765136719 relative L2 0.9142504930496216\n",
      "training 354.584716796875 relative L2 0.9130396842956543\n",
      "training 353.6461486816406 relative L2 0.9118129014968872\n",
      "training 352.6965637207031 relative L2 0.910569965839386\n",
      "training 351.73577880859375 relative L2 0.9093104600906372\n",
      "training 350.763427734375 relative L2 0.908034086227417\n",
      "training 349.77947998046875 relative L2 0.9067404866218567\n",
      "training 348.78375244140625 relative L2 0.9054296612739563\n",
      "training 347.7761535644531 relative L2 0.9041013121604919\n",
      "training 346.7565612792969 relative L2 0.9027553200721741\n",
      "training 345.7249450683594 relative L2 0.901391327381134\n",
      "training 344.68115234375 relative L2 0.9000093936920166\n",
      "training 343.62506103515625 relative L2 0.8986088037490845\n",
      "training 342.556640625 relative L2 0.8971898555755615\n",
      "training 341.4757080078125 relative L2 0.8957520723342896\n",
      "training 340.3822326660156 relative L2 0.8942952156066895\n",
      "training 339.27606201171875 relative L2 0.8928192853927612\n",
      "training 338.15716552734375 relative L2 0.8913237452507019\n",
      "training 337.0253601074219 relative L2 0.8898086547851562\n",
      "training 335.8806457519531 relative L2 0.8882735371589661\n",
      "training 334.72283935546875 relative L2 0.8867183327674866\n",
      "training 333.55194091796875 relative L2 0.8851427435874939\n",
      "training 332.36773681640625 relative L2 0.8835466504096985\n",
      "training 331.1702575683594 relative L2 0.8819296956062317\n",
      "training 329.9593505859375 relative L2 0.8802916407585144\n",
      "training 328.73492431640625 relative L2 0.8786323666572571\n",
      "training 327.4969177246094 relative L2 0.8769515156745911\n",
      "training 326.2452087402344 relative L2 0.875248908996582\n",
      "training 324.9797058105469 relative L2 0.8735242486000061\n",
      "training 323.7004089355469 relative L2 0.871777355670929\n",
      "training 322.40716552734375 relative L2 0.8700079917907715\n",
      "training 321.09991455078125 relative L2 0.8682159185409546\n",
      "training 319.7785339355469 relative L2 0.8664008378982544\n",
      "training 318.4430236816406 relative L2 0.8645625710487366\n",
      "training 317.09326171875 relative L2 0.8627007603645325\n",
      "training 315.7292175292969 relative L2 0.8608153462409973\n",
      "training 314.3507995605469 relative L2 0.8589059114456177\n",
      "training 312.95794677734375 relative L2 0.856972336769104\n",
      "training 311.55059814453125 relative L2 0.8550142645835876\n",
      "training 310.1286315917969 relative L2 0.8530315160751343\n",
      "training 308.69207763671875 relative L2 0.85102379322052\n",
      "training 307.2408752441406 relative L2 0.8489909172058105\n",
      "training 305.77490234375 relative L2 0.846931517124176\n",
      "training 304.29339599609375 relative L2 0.8448443412780762\n",
      "training 302.7956237792969 relative L2 0.8427292108535767\n",
      "training 301.2815246582031 relative L2 0.8405863642692566\n",
      "training 299.75146484375 relative L2 0.8384158611297607\n",
      "training 298.20562744140625 relative L2 0.8362176418304443\n",
      "training 296.6441345214844 relative L2 0.8339917063713074\n",
      "training 295.0671081542969 relative L2 0.8317378163337708\n",
      "training 293.4745788574219 relative L2 0.8294559121131897\n",
      "training 291.8666076660156 relative L2 0.8271458148956299\n",
      "training 290.24322509765625 relative L2 0.8248070478439331\n",
      "training 288.60443115234375 relative L2 0.8224397301673889\n",
      "training 286.9503479003906 relative L2 0.8200437426567078\n",
      "training 285.281005859375 relative L2 0.8176189064979553\n",
      "training 283.59649658203125 relative L2 0.815165102481842\n",
      "training 281.89703369140625 relative L2 0.812682569026947\n",
      "training 280.1827697753906 relative L2 0.8101715445518494\n",
      "training 278.4541931152344 relative L2 0.807632327079773\n",
      "training 276.7116394042969 relative L2 0.8050632476806641\n",
      "training 274.9541931152344 relative L2 0.8024636507034302\n",
      "training 273.1815490722656 relative L2 0.7998331785202026\n",
      "training 271.3937072753906 relative L2 0.7971716523170471\n",
      "training 269.5906982421875 relative L2 0.7944788932800293\n",
      "training 267.77264404296875 relative L2 0.7917545437812805\n",
      "training 265.9395446777344 relative L2 0.7889984250068665\n",
      "training 264.0914611816406 relative L2 0.786210298538208\n",
      "training 262.2284851074219 relative L2 0.7833901047706604\n",
      "training 260.3507995605469 relative L2 0.7805373072624207\n",
      "training 258.458251953125 relative L2 0.777651846408844\n",
      "training 256.5511169433594 relative L2 0.7747336030006409\n",
      "training 254.6294403076172 relative L2 0.7717821002006531\n",
      "training 252.6932373046875 relative L2 0.7687972784042358\n",
      "training 250.74266052246094 relative L2 0.7657788395881653\n",
      "training 248.77781677246094 relative L2 0.7627266049385071\n",
      "training 246.79881286621094 relative L2 0.7596403360366821\n",
      "training 244.80580139160156 relative L2 0.7565198540687561\n",
      "training 242.79891967773438 relative L2 0.7533649802207947\n",
      "training 240.7783203125 relative L2 0.7501754760742188\n",
      "training 238.74411010742188 relative L2 0.7469509840011597\n",
      "training 236.69631958007812 relative L2 0.7436909079551697\n",
      "training 234.6349334716797 relative L2 0.7403950095176697\n",
      "training 232.56007385253906 relative L2 0.7370630502700806\n",
      "training 230.4718780517578 relative L2 0.7336949110031128\n",
      "training 228.3705596923828 relative L2 0.7302904725074768\n",
      "training 226.25634765625 relative L2 0.7268496155738831\n",
      "training 224.12954711914062 relative L2 0.7233722805976868\n",
      "training 221.99038696289062 relative L2 0.7198583483695984\n",
      "training 219.8391571044922 relative L2 0.7163078188896179\n",
      "training 217.6761474609375 relative L2 0.7127205729484558\n",
      "training 215.5016326904297 relative L2 0.7090965509414673\n",
      "training 213.31594848632812 relative L2 0.7054358124732971\n",
      "training 211.1193389892578 relative L2 0.7017380595207214\n",
      "training 208.9121551513672 relative L2 0.698003351688385\n",
      "training 206.69464111328125 relative L2 0.6942316293716431\n",
      "training 204.46713256835938 relative L2 0.690422773361206\n",
      "training 202.22998046875 relative L2 0.6865770220756531\n",
      "training 199.9835968017578 relative L2 0.6826938986778259\n",
      "training 197.7281494140625 relative L2 0.6787738800048828\n",
      "training 195.4642791748047 relative L2 0.674816906452179\n",
      "training 193.19219970703125 relative L2 0.6708228588104248\n",
      "training 190.912353515625 relative L2 0.6667918562889099\n",
      "training 188.62509155273438 relative L2 0.6627241373062134\n",
      "training 186.33103942871094 relative L2 0.6586191058158875\n",
      "training 184.0301513671875 relative L2 0.6544778347015381\n",
      "training 181.7234344482422 relative L2 0.6502999663352966\n",
      "training 179.41107177734375 relative L2 0.6460858583450317\n",
      "training 177.09365844726562 relative L2 0.6418355703353882\n",
      "training 174.77157592773438 relative L2 0.6375492215156555\n",
      "training 172.44534301757812 relative L2 0.6332268714904785\n",
      "training 170.11529541015625 relative L2 0.628868579864502\n",
      "training 167.78199768066406 relative L2 0.6244747638702393\n",
      "training 165.4459686279297 relative L2 0.6200458407402039\n",
      "training 163.10780334472656 relative L2 0.6155819892883301\n",
      "training 160.76808166503906 relative L2 0.6110835671424866\n",
      "training 158.42730712890625 relative L2 0.606550931930542\n",
      "training 156.08612060546875 relative L2 0.6019843816757202\n",
      "training 153.74505615234375 relative L2 0.5973843932151794\n",
      "training 151.40467834472656 relative L2 0.5927512049674988\n",
      "training 149.06558227539062 relative L2 0.5880853533744812\n",
      "training 146.7283935546875 relative L2 0.5833872556686401\n",
      "training 144.3937225341797 relative L2 0.5786574482917786\n",
      "training 142.06219482421875 relative L2 0.5738964676856995\n",
      "training 139.7344512939453 relative L2 0.569104790687561\n",
      "training 137.4111328125 relative L2 0.5642831325531006\n",
      "training 135.0929412841797 relative L2 0.5594325065612793\n",
      "training 132.7806854248047 relative L2 0.5545524954795837\n",
      "training 130.47462463378906 relative L2 0.5496448874473572\n",
      "training 128.1758575439453 relative L2 0.5447099804878235\n",
      "training 125.88491821289062 relative L2 0.5397483706474304\n",
      "training 123.6024169921875 relative L2 0.5347610712051392\n",
      "training 121.32911682128906 relative L2 0.529748797416687\n",
      "training 119.06568908691406 relative L2 0.5247123837471008\n",
      "training 116.8128433227539 relative L2 0.5196529030799866\n",
      "training 114.57131958007812 relative L2 0.5145711898803711\n",
      "training 112.3418197631836 relative L2 0.509468138217926\n",
      "training 110.12501525878906 relative L2 0.5043449401855469\n",
      "training 107.92167663574219 relative L2 0.499203085899353\n",
      "training 105.73271942138672 relative L2 0.4940423369407654\n",
      "training 103.55821990966797 relative L2 0.4888719618320465\n",
      "training 101.4023666381836 relative L2 0.48367199301719666\n",
      "training 99.25701904296875 relative L2 0.4784645438194275\n",
      "training 97.13156127929688 relative L2 0.47324398159980774\n",
      "training 95.02385711669922 relative L2 0.46801215410232544\n",
      "training 92.93479919433594 relative L2 0.46277186274528503\n",
      "training 90.86564636230469 relative L2 0.45752227306365967\n",
      "training 88.81615447998047 relative L2 0.45226410031318665\n",
      "training 86.78678131103516 relative L2 0.4470004737377167\n",
      "training 84.77877044677734 relative L2 0.4417339563369751\n",
      "training 82.79317474365234 relative L2 0.4364643394947052\n",
      "training 80.82996368408203 relative L2 0.43119457364082336\n",
      "training 78.89026641845703 relative L2 0.42592576146125793\n",
      "training 76.97445678710938 relative L2 0.42065951228141785\n",
      "training 75.08309173583984 relative L2 0.41539764404296875\n",
      "training 73.21681213378906 relative L2 0.4101419746875763\n",
      "training 71.3761978149414 relative L2 0.40489456057548523\n",
      "training 69.56182861328125 relative L2 0.39965760707855225\n",
      "training 67.77436065673828 relative L2 0.39443305134773254\n",
      "training 66.01432037353516 relative L2 0.38922303915023804\n",
      "training 64.28223419189453 relative L2 0.3840293288230896\n",
      "training 62.5784797668457 relative L2 0.3788536787033081\n",
      "training 60.90339660644531 relative L2 0.37369871139526367\n",
      "training 59.25761413574219 relative L2 0.368567556142807\n",
      "training 57.64179992675781 relative L2 0.3634645342826843\n",
      "training 56.057003021240234 relative L2 0.35839924216270447\n",
      "training 54.50574493408203 relative L2 0.35335925221443176\n",
      "training 52.98383712768555 relative L2 0.34834668040275574\n",
      "training 51.49161911010742 relative L2 0.3433711528778076\n",
      "training 50.03147888183594 relative L2 0.33843663334846497\n",
      "training 48.60409927368164 relative L2 0.3335443139076233\n",
      "training 47.209327697753906 relative L2 0.32869577407836914\n",
      "training 45.8470573425293 relative L2 0.3238930106163025\n",
      "training 44.51729965209961 relative L2 0.31913837790489197\n",
      "training 43.22015380859375 relative L2 0.31443461775779724\n",
      "training 41.955745697021484 relative L2 0.3097843527793884\n",
      "training 40.72416305541992 relative L2 0.3051900565624237\n",
      "training 39.525421142578125 relative L2 0.30065420269966125\n",
      "training 38.3594856262207 relative L2 0.296179860830307\n",
      "training 37.22645568847656 relative L2 0.29176950454711914\n",
      "training 36.126243591308594 relative L2 0.28742659091949463\n",
      "training 35.05897521972656 relative L2 0.2831535339355469\n",
      "training 34.024478912353516 relative L2 0.27895230054855347\n",
      "training 33.02246856689453 relative L2 0.27482348680496216\n",
      "training 32.05231475830078 relative L2 0.2707726061344147\n",
      "training 31.114553451538086 relative L2 0.2667936384677887\n",
      "training 30.206974029541016 relative L2 0.26289868354797363\n",
      "training 29.331541061401367 relative L2 0.2590857446193695\n",
      "training 28.487009048461914 relative L2 0.25535622239112854\n",
      "training 27.672880172729492 relative L2 0.2517112195491791\n",
      "training 26.888599395751953 relative L2 0.24815170466899872\n",
      "training 26.13359260559082 relative L2 0.24467912316322327\n",
      "training 25.40736961364746 relative L2 0.24129512906074524\n",
      "training 24.70950698852539 relative L2 0.2380056381225586\n",
      "training 24.040433883666992 relative L2 0.23486237227916718\n",
      "training 23.40964126586914 relative L2 0.23175036907196045\n",
      "training 22.793411254882812 relative L2 0.22873815894126892\n",
      "training 22.204771041870117 relative L2 0.2258419394493103\n",
      "training 21.64606475830078 relative L2 0.22304540872573853\n",
      "training 21.11332893371582 relative L2 0.22034554183483124\n",
      "training 20.605287551879883 relative L2 0.21774141490459442\n",
      "training 20.121109008789062 relative L2 0.21523252129554749\n",
      "training 19.660079956054688 relative L2 0.21281874179840088\n",
      "training 19.2215518951416 relative L2 0.21050110459327698\n",
      "training 18.805139541625977 relative L2 0.20828290283679962\n",
      "training 18.410839080810547 relative L2 0.2061595320701599\n",
      "training 18.037311553955078 relative L2 0.2041207253932953\n",
      "training 17.682249069213867 relative L2 0.2021685391664505\n",
      "training 17.34557342529297 relative L2 0.2003086656332016\n",
      "training 17.02782440185547 relative L2 0.19854232668876648\n",
      "training 16.728757858276367 relative L2 0.19687138497829437\n",
      "training 16.448261260986328 relative L2 0.19528567790985107\n",
      "training 16.18427276611328 relative L2 0.19376806914806366\n",
      "training 15.933599472045898 relative L2 0.19233055412769318\n",
      "training 15.69796371459961 relative L2 0.19099020957946777\n",
      "training 15.479826927185059 relative L2 0.1897234469652176\n",
      "training 15.275076866149902 relative L2 0.18852314352989197\n",
      "training 15.082293510437012 relative L2 0.18738816678524017\n",
      "training 14.901119232177734 relative L2 0.18631631135940552\n",
      "training 14.731033325195312 relative L2 0.18531455099582672\n",
      "training 14.572932243347168 relative L2 0.18437886238098145\n",
      "training 14.426024436950684 relative L2 0.1835029125213623\n",
      "training 14.289172172546387 relative L2 0.18268291652202606\n",
      "training 14.16163444519043 relative L2 0.18191565573215485\n",
      "training 14.04281997680664 relative L2 0.18119560182094574\n",
      "training 13.931756019592285 relative L2 0.18052203953266144\n",
      "training 13.828264236450195 relative L2 0.1798967719078064\n",
      "training 13.732528686523438 relative L2 0.17932212352752686\n",
      "training 13.644837379455566 relative L2 0.17879049479961395\n",
      "training 13.56396198272705 relative L2 0.17829380929470062\n",
      "training 13.488616943359375 relative L2 0.1778314858675003\n",
      "training 13.418665885925293 relative L2 0.17740151286125183\n",
      "training 13.353763580322266 relative L2 0.1770048439502716\n",
      "training 13.294027328491211 relative L2 0.17663873732089996\n",
      "training 13.239013671875 relative L2 0.17630048096179962\n",
      "training 13.188292503356934 relative L2 0.17598599195480347\n",
      "training 13.141210556030273 relative L2 0.17569318413734436\n",
      "training 13.097440719604492 relative L2 0.17542211711406708\n",
      "training 13.056990623474121 relative L2 0.17517279088497162\n",
      "training 13.019838333129883 relative L2 0.17494143545627594\n",
      "training 12.985422134399414 relative L2 0.1747262179851532\n",
      "training 12.953438758850098 relative L2 0.17452532052993774\n",
      "training 12.923614501953125 relative L2 0.1743375062942505\n",
      "training 12.895771980285645 relative L2 0.17416246235370636\n",
      "training 12.869848251342773 relative L2 0.17399904131889343\n",
      "training 12.845669746398926 relative L2 0.17384541034698486\n",
      "training 12.822964668273926 relative L2 0.17370013892650604\n",
      "training 12.801509857177734 relative L2 0.17356154322624207\n",
      "training 12.781065940856934 relative L2 0.17342950403690338\n",
      "training 12.761603355407715 relative L2 0.1733027994632721\n",
      "training 12.742941856384277 relative L2 0.17318019270896912\n",
      "training 12.724897384643555 relative L2 0.17306044697761536\n",
      "training 12.707284927368164 relative L2 0.1729421615600586\n",
      "training 12.689909934997559 relative L2 0.17282409965991974\n",
      "training 12.672576904296875 relative L2 0.17270514369010925\n",
      "training 12.655121803283691 relative L2 0.17258384823799133\n",
      "training 12.637335777282715 relative L2 0.17245876789093018\n",
      "training 12.619007110595703 relative L2 0.17232848703861237\n",
      "training 12.599931716918945 relative L2 0.17219266295433044\n",
      "training 12.58005428314209 relative L2 0.17205388844013214\n",
      "training 12.5597562789917 relative L2 0.17192389070987701\n",
      "training 12.540757179260254 relative L2 0.1718088835477829\n",
      "training 12.5239896774292 relative L2 0.17170032858848572\n",
      "training 12.508170127868652 relative L2 0.1716117560863495\n",
      "training 12.495266914367676 relative L2 0.17153997719287872\n",
      "training 12.484825134277344 relative L2 0.1714700162410736\n",
      "training 12.474640846252441 relative L2 0.17139478027820587\n",
      "training 12.46369743347168 relative L2 0.17131532728672028\n",
      "training 12.452140808105469 relative L2 0.17123085260391235\n",
      "training 12.439851760864258 relative L2 0.1711440235376358\n",
      "training 12.427225112915039 relative L2 0.17106366157531738\n",
      "training 12.41555118560791 relative L2 0.17100709676742554\n",
      "training 12.40736198425293 relative L2 0.17095258831977844\n",
      "training 12.399456024169922 relative L2 0.17089687287807465\n",
      "training 12.39138126373291 relative L2 0.17084211111068726\n",
      "training 12.383447647094727 relative L2 0.17078754305839539\n",
      "training 12.375544548034668 relative L2 0.17073296010494232\n",
      "training 12.367643356323242 relative L2 0.17067839205265045\n",
      "training 12.359735488891602 relative L2 0.17062389850616455\n",
      "training 12.35183334350586 relative L2 0.17056888341903687\n",
      "training 12.343859672546387 relative L2 0.17051351070404053\n",
      "training 12.335844993591309 relative L2 0.17045848071575165\n",
      "training 12.327889442443848 relative L2 0.17040406167507172\n",
      "training 12.320023536682129 relative L2 0.17034977674484253\n",
      "training 12.312171936035156 relative L2 0.17029577493667603\n",
      "training 12.304365158081055 relative L2 0.1702415943145752\n",
      "training 12.296534538269043 relative L2 0.17018763720989227\n",
      "training 12.288742065429688 relative L2 0.17013345658779144\n",
      "training 12.280916213989258 relative L2 0.17007964849472046\n",
      "training 12.273151397705078 relative L2 0.1700260043144226\n",
      "training 12.265412330627441 relative L2 0.1699725240468979\n",
      "training 12.257704734802246 relative L2 0.16991907358169556\n",
      "training 12.250001907348633 relative L2 0.1698656529188156\n",
      "training 12.242301940917969 relative L2 0.16981226205825806\n",
      "training 12.234606742858887 relative L2 0.16975879669189453\n",
      "training 12.226908683776855 relative L2 0.16970524191856384\n",
      "training 12.219200134277344 relative L2 0.1696515679359436\n",
      "training 12.211481094360352 relative L2 0.16959768533706665\n",
      "training 12.203736305236816 relative L2 0.1695435792207718\n",
      "training 12.195958137512207 relative L2 0.16948911547660828\n",
      "training 12.188135147094727 relative L2 0.169434055685997\n",
      "training 12.180233001708984 relative L2 0.16937832534313202\n",
      "training 12.172237396240234 relative L2 0.16932174563407898\n",
      "training 12.164122581481934 relative L2 0.16926413774490356\n",
      "training 12.155861854553223 relative L2 0.16920512914657593\n",
      "training 12.147397994995117 relative L2 0.16914428770542145\n",
      "training 12.138681411743164 relative L2 0.16908103227615356\n",
      "training 12.129626274108887 relative L2 0.16901490092277527\n",
      "training 12.120146751403809 relative L2 0.1689458042383194\n",
      "training 12.110249519348145 relative L2 0.16887857019901276\n",
      "training 12.100626945495605 relative L2 0.16882328689098358\n",
      "training 12.092729568481445 relative L2 0.16876789927482605\n",
      "training 12.084822654724121 relative L2 0.1687120795249939\n",
      "training 12.076848030090332 relative L2 0.1686556488275528\n",
      "training 12.068794250488281 relative L2 0.16859877109527588\n",
      "training 12.06067180633545 relative L2 0.16854117810726166\n",
      "training 12.052452087402344 relative L2 0.16848278045654297\n",
      "training 12.044127464294434 relative L2 0.1684235781431198\n",
      "training 12.035689353942871 relative L2 0.16836339235305786\n",
      "training 12.027119636535645 relative L2 0.16830211877822876\n",
      "training 12.018390655517578 relative L2 0.16823960840702057\n",
      "training 12.009490013122559 relative L2 0.1681758016347885\n",
      "training 12.000399589538574 relative L2 0.16811028122901917\n",
      "training 11.991073608398438 relative L2 0.16804279386997223\n",
      "training 11.981475830078125 relative L2 0.16797305643558502\n",
      "training 11.971564292907715 relative L2 0.1679006963968277\n",
      "training 11.961280822753906 relative L2 0.16782507300376892\n",
      "training 11.950539588928223 relative L2 0.16774557530879974\n",
      "training 11.939239501953125 relative L2 0.16766107082366943\n",
      "training 11.927234649658203 relative L2 0.1675702929496765\n",
      "training 11.91433334350586 relative L2 0.16747158765792847\n",
      "training 11.900320053100586 relative L2 0.16736531257629395\n",
      "training 11.885231971740723 relative L2 0.16726304590702057\n",
      "training 11.870743751525879 relative L2 0.16716919839382172\n",
      "training 11.85746955871582 relative L2 0.167070671916008\n",
      "training 11.84350872039795 relative L2 0.16696560382843018\n",
      "training 11.828620910644531 relative L2 0.16685344278812408\n",
      "training 11.812731742858887 relative L2 0.16673417389392853\n",
      "training 11.795849800109863 relative L2 0.16660861670970917\n",
      "training 11.778097152709961 relative L2 0.16648174822330475\n",
      "training 11.76018238067627 relative L2 0.16636896133422852\n",
      "training 11.744255065917969 relative L2 0.16629745066165924\n",
      "training 11.734220504760742 relative L2 0.1662365347146988\n",
      "training 11.725654602050781 relative L2 0.1661718189716339\n",
      "training 11.71654224395752 relative L2 0.16610361635684967\n",
      "training 11.706940650939941 relative L2 0.16603043675422668\n",
      "training 11.696647644042969 relative L2 0.1659528762102127\n",
      "training 11.685747146606445 relative L2 0.16587021946907043\n",
      "training 11.674137115478516 relative L2 0.16577866673469543\n",
      "training 11.661290168762207 relative L2 0.1656794548034668\n",
      "training 11.647335052490234 relative L2 0.1655704379081726\n",
      "training 11.632027626037598 relative L2 0.16544243693351746\n",
      "training 11.614042282104492 relative L2 0.16528946161270142\n",
      "training 11.592545509338379 relative L2 0.1651456356048584\n",
      "training 11.57238483428955 relative L2 0.1650765836238861\n",
      "training 11.562767028808594 relative L2 0.16501563787460327\n",
      "training 11.554242134094238 relative L2 0.16495199501514435\n",
      "training 11.545320510864258 relative L2 0.164886936545372\n",
      "training 11.536191940307617 relative L2 0.16482074558734894\n",
      "training 11.526908874511719 relative L2 0.16475406289100647\n",
      "training 11.517569541931152 relative L2 0.16468729078769684\n",
      "training 11.50822639465332 relative L2 0.16462033987045288\n",
      "training 11.49886703491211 relative L2 0.1645532101392746\n",
      "training 11.489494323730469 relative L2 0.16448402404785156\n",
      "training 11.479854583740234 relative L2 0.16441495716571808\n",
      "training 11.470247268676758 relative L2 0.16434785723686218\n",
      "training 11.460909843444824 relative L2 0.16428180038928986\n",
      "training 11.451702117919922 relative L2 0.16421601176261902\n",
      "training 11.442532539367676 relative L2 0.16414882242679596\n",
      "training 11.433161735534668 relative L2 0.16408096253871918\n",
      "training 11.423702239990234 relative L2 0.16401296854019165\n",
      "training 11.41423225402832 relative L2 0.16394494473934174\n",
      "training 11.404769897460938 relative L2 0.16387800872325897\n",
      "training 11.3954496383667 relative L2 0.1638105809688568\n",
      "training 11.386080741882324 relative L2 0.16374091804027557\n",
      "training 11.376420021057129 relative L2 0.16367170214653015\n",
      "training 11.366825103759766 relative L2 0.16360248625278473\n",
      "training 11.357224464416504 relative L2 0.16353192925453186\n",
      "training 11.347439765930176 relative L2 0.16345924139022827\n",
      "training 11.337359428405762 relative L2 0.16339071094989777\n",
      "training 11.327861785888672 relative L2 0.16332274675369263\n",
      "training 11.318443298339844 relative L2 0.16325412690639496\n",
      "training 11.308944702148438 relative L2 0.1631854921579361\n",
      "training 11.299447059631348 relative L2 0.16311660408973694\n",
      "training 11.289918899536133 relative L2 0.16304761171340942\n",
      "training 11.280377388000488 relative L2 0.16297811269760132\n",
      "training 11.270770072937012 relative L2 0.1629084199666977\n",
      "training 11.261139869689941 relative L2 0.16283851861953735\n",
      "training 11.251482009887695 relative L2 0.16276854276657104\n",
      "training 11.241823196411133 relative L2 0.16269797086715698\n",
      "training 11.23208999633789 relative L2 0.16262716054916382\n",
      "training 11.222328186035156 relative L2 0.16255636513233185\n",
      "training 11.21257209777832 relative L2 0.1624847948551178\n",
      "training 11.202714920043945 relative L2 0.1624125838279724\n",
      "training 11.192774772644043 relative L2 0.16233988106250763\n",
      "training 11.18277359008789 relative L2 0.16226644814014435\n",
      "training 11.172679901123047 relative L2 0.16219213604927063\n",
      "training 11.162473678588867 relative L2 0.16211670637130737\n",
      "training 11.15212345123291 relative L2 0.16203993558883667\n",
      "training 11.141593933105469 relative L2 0.16196149587631226\n",
      "training 11.130839347839355 relative L2 0.1618807017803192\n",
      "training 11.11977481842041 relative L2 0.16179539263248444\n",
      "training 11.108097076416016 relative L2 0.1617012321949005\n",
      "training 11.095210075378418 relative L2 0.1616027057170868\n",
      "training 11.0817232131958 relative L2 0.16151387989521027\n",
      "training 11.069574356079102 relative L2 0.16141974925994873\n",
      "training 11.056715965270996 relative L2 0.16131971776485443\n",
      "training 11.04307746887207 relative L2 0.16121217608451843\n",
      "training 11.02840518951416 relative L2 0.1610950529575348\n",
      "training 11.01241683959961 relative L2 0.16096551716327667\n",
      "training 10.99472427368164 relative L2 0.16082343459129333\n",
      "training 10.975329399108887 relative L2 0.1606755256652832\n",
      "training 10.955145835876465 relative L2 0.16053450107574463\n",
      "training 10.935978889465332 relative L2 0.16040359437465668\n",
      "training 10.91820240020752 relative L2 0.16026568412780762\n",
      "training 10.899465560913086 relative L2 0.1601293385028839\n",
      "training 10.88094425201416 relative L2 0.16003720462322235\n",
      "training 10.868454933166504 relative L2 0.15994234383106232\n",
      "training 10.8555908203125 relative L2 0.15984036028385162\n",
      "training 10.841767311096191 relative L2 0.159732386469841\n",
      "training 10.827144622802734 relative L2 0.15961742401123047\n",
      "training 10.811592102050781 relative L2 0.15949419140815735\n",
      "training 10.794934272766113 relative L2 0.15936021506786346\n",
      "training 10.776802062988281 relative L2 0.1592121571302414\n",
      "training 10.756760597229004 relative L2 0.15905161201953888\n",
      "training 10.73505687713623 relative L2 0.15887519717216492\n",
      "training 10.711267471313477 relative L2 0.15871433913707733\n",
      "training 10.689591407775879 relative L2 0.15862196683883667\n",
      "training 10.677214622497559 relative L2 0.15854236483573914\n",
      "training 10.666526794433594 relative L2 0.15846143662929535\n",
      "training 10.655653953552246 relative L2 0.15837912261486053\n",
      "training 10.644584655761719 relative L2 0.15829457342624664\n",
      "training 10.63322639465332 relative L2 0.15820945799350739\n",
      "training 10.621798515319824 relative L2 0.15812258422374725\n",
      "training 10.610151290893555 relative L2 0.15803642570972443\n",
      "training 10.598609924316406 relative L2 0.15794910490512848\n",
      "training 10.586935043334961 relative L2 0.15786170959472656\n",
      "training 10.575249671936035 relative L2 0.15777356922626495\n",
      "training 10.563465118408203 relative L2 0.15768371522426605\n",
      "training 10.551445960998535 relative L2 0.15759259462356567\n",
      "training 10.539253234863281 relative L2 0.15750157833099365\n",
      "training 10.527073860168457 relative L2 0.1574091762304306\n",
      "training 10.514737129211426 relative L2 0.1573144644498825\n",
      "training 10.502121925354004 relative L2 0.15721921622753143\n",
      "training 10.489441871643066 relative L2 0.15712232887744904\n",
      "training 10.476542472839355 relative L2 0.15702255070209503\n",
      "training 10.4632568359375 relative L2 0.15691940486431122\n",
      "training 10.44951343536377 relative L2 0.15681174397468567\n",
      "training 10.435174942016602 relative L2 0.15670059621334076\n",
      "training 10.420388221740723 relative L2 0.15659870207309723\n",
      "training 10.406848907470703 relative L2 0.15651047229766846\n",
      "training 10.39513111114502 relative L2 0.1564219892024994\n",
      "training 10.383391380310059 relative L2 0.15633165836334229\n",
      "training 10.371423721313477 relative L2 0.15624095499515533\n",
      "training 10.359415054321289 relative L2 0.15615099668502808\n",
      "training 10.347513198852539 relative L2 0.15606144070625305\n",
      "training 10.33565616607666 relative L2 0.15597185492515564\n",
      "training 10.3237943649292 relative L2 0.15588173270225525\n",
      "training 10.31187629699707 relative L2 0.1557912528514862\n",
      "training 10.299927711486816 relative L2 0.15570087730884552\n",
      "training 10.28800106048584 relative L2 0.15561093389987946\n",
      "training 10.276137351989746 relative L2 0.155520960688591\n",
      "training 10.264270782470703 relative L2 0.15543073415756226\n",
      "training 10.252375602722168 relative L2 0.15534034371376038\n",
      "training 10.240470886230469 relative L2 0.155249685049057\n",
      "training 10.228544235229492 relative L2 0.15515916049480438\n",
      "training 10.216647148132324 relative L2 0.15506857633590698\n",
      "training 10.204750061035156 relative L2 0.15497781336307526\n",
      "training 10.19283676147461 relative L2 0.15488694608211517\n",
      "training 10.180910110473633 relative L2 0.1547960788011551\n",
      "training 10.168986320495605 relative L2 0.15470510721206665\n",
      "training 10.157054901123047 relative L2 0.1546136885881424\n",
      "training 10.145073890686035 relative L2 0.15452177822589874\n",
      "training 10.133042335510254 relative L2 0.15442971885204315\n",
      "training 10.12099838256836 relative L2 0.15433740615844727\n",
      "training 10.108926773071289 relative L2 0.15424475073814392\n",
      "training 10.096819877624512 relative L2 0.15415167808532715\n",
      "training 10.08466625213623 relative L2 0.1540582925081253\n",
      "training 10.072480201721191 relative L2 0.1539643406867981\n",
      "training 10.060224533081055 relative L2 0.15386991202831268\n",
      "training 10.047913551330566 relative L2 0.1537749171257019\n",
      "training 10.035534858703613 relative L2 0.15367914736270905\n",
      "training 10.023062705993652 relative L2 0.15358251333236694\n",
      "training 10.010485649108887 relative L2 0.15348486602306366\n",
      "training 9.997782707214355 relative L2 0.15338599681854248\n",
      "training 9.984933853149414 relative L2 0.15328601002693176\n",
      "training 9.97194766998291 relative L2 0.15318438410758972\n",
      "training 9.95875072479248 relative L2 0.1530810296535492\n",
      "training 9.945345878601074 relative L2 0.15297561883926392\n",
      "training 9.931682586669922 relative L2 0.15286782383918762\n",
      "training 9.917712211608887 relative L2 0.15275731682777405\n",
      "training 9.903392791748047 relative L2 0.15264391899108887\n",
      "training 9.888710021972656 relative L2 0.15252813696861267\n",
      "training 9.873733520507812 relative L2 0.1524127721786499\n",
      "training 9.858823776245117 relative L2 0.15230847895145416\n",
      "training 9.845359802246094 relative L2 0.152211993932724\n",
      "training 9.832906723022461 relative L2 0.1521156132221222\n",
      "training 9.82046890258789 relative L2 0.1520194709300995\n",
      "training 9.808074951171875 relative L2 0.15192146599292755\n",
      "training 9.795458793640137 relative L2 0.15182289481163025\n",
      "training 9.782780647277832 relative L2 0.15172387659549713\n",
      "training 9.770051002502441 relative L2 0.15162503719329834\n",
      "training 9.757349967956543 relative L2 0.15152519941329956\n",
      "training 9.744528770446777 relative L2 0.1514245569705963\n",
      "training 9.731614112854004 relative L2 0.15132297575473785\n",
      "training 9.718589782714844 relative L2 0.15122036635875702\n",
      "training 9.70544147491455 relative L2 0.15111683309078217\n",
      "training 9.692180633544922 relative L2 0.15101177990436554\n",
      "training 9.678733825683594 relative L2 0.15090559422969818\n",
      "training 9.665146827697754 relative L2 0.15079763531684875\n",
      "training 9.65134334564209 relative L2 0.1506875604391098\n",
      "training 9.637282371520996 relative L2 0.15057460963726044\n",
      "training 9.622867584228516 relative L2 0.15045861899852753\n",
      "training 9.60806941986084 relative L2 0.15033918619155884\n",
      "training 9.592843055725098 relative L2 0.1502191424369812\n",
      "training 9.577547073364258 relative L2 0.1501050889492035\n",
      "training 9.563033103942871 relative L2 0.14999058842658997\n",
      "training 9.548471450805664 relative L2 0.14987432956695557\n",
      "training 9.533696174621582 relative L2 0.1497562676668167\n",
      "training 9.518696784973145 relative L2 0.14963684976100922\n",
      "training 9.503536224365234 relative L2 0.14951640367507935\n",
      "training 9.488258361816406 relative L2 0.14939621090888977\n",
      "training 9.473027229309082 relative L2 0.14927788078784943\n",
      "training 9.45804214477539 relative L2 0.14916439354419708\n",
      "training 9.443686485290527 relative L2 0.1490582376718521\n",
      "training 9.430283546447754 relative L2 0.14895476400852203\n",
      "training 9.417220115661621 relative L2 0.14885085821151733\n",
      "training 9.40410327911377 relative L2 0.1487465500831604\n",
      "training 9.390942573547363 relative L2 0.148641899228096\n",
      "training 9.3777494430542 relative L2 0.14853687584400177\n",
      "training 9.36452865600586 relative L2 0.1484317034482956\n",
      "training 9.351298332214355 relative L2 0.14832629263401031\n",
      "training 9.338045120239258 relative L2 0.14822065830230713\n",
      "training 9.324775695800781 relative L2 0.14811475574970245\n",
      "training 9.311478614807129 relative L2 0.1480085402727127\n",
      "training 9.2981538772583 relative L2 0.14790226519107819\n",
      "training 9.284828186035156 relative L2 0.14779579639434814\n",
      "training 9.271484375 relative L2 0.1476891189813614\n",
      "training 9.258126258850098 relative L2 0.1475822776556015\n",
      "training 9.244759559631348 relative L2 0.1474752575159073\n",
      "training 9.231377601623535 relative L2 0.14736802875995636\n",
      "training 9.217977523803711 relative L2 0.1472606360912323\n",
      "training 9.204566955566406 relative L2 0.1471528708934784\n",
      "training 9.191122055053711 relative L2 0.14704486727714539\n",
      "training 9.177656173706055 relative L2 0.1469365358352661\n",
      "training 9.164166450500488 relative L2 0.14682786166667938\n",
      "training 9.150642395019531 relative L2 0.14671896398067474\n",
      "training 9.137101173400879 relative L2 0.1466098576784134\n",
      "training 9.123540878295898 relative L2 0.14650104939937592\n",
      "training 9.110026359558105 relative L2 0.14639224112033844\n",
      "training 9.096522331237793 relative L2 0.14628279209136963\n",
      "training 9.082953453063965 relative L2 0.14617255330085754\n",
      "training 9.069297790527344 relative L2 0.14606162905693054\n",
      "training 9.055563926696777 relative L2 0.1459503173828125\n",
      "training 9.041792869567871 relative L2 0.14583882689476013\n",
      "training 9.028005599975586 relative L2 0.14572694897651672\n",
      "training 9.014179229736328 relative L2 0.14561466872692108\n",
      "training 9.00031566619873 relative L2 0.14550168812274933\n",
      "training 8.986377716064453 relative L2 0.14538787305355072\n",
      "training 8.97235107421875 relative L2 0.14527301490306854\n",
      "training 8.958209991455078 relative L2 0.14515697956085205\n",
      "training 8.943937301635742 relative L2 0.14503948390483856\n",
      "training 8.929496765136719 relative L2 0.14492017030715942\n",
      "training 8.914838790893555 relative L2 0.14479883015155792\n",
      "training 8.899940490722656 relative L2 0.14467541873455048\n",
      "training 8.88479995727539 relative L2 0.14455370604991913\n",
      "training 8.869889259338379 relative L2 0.14443762600421906\n",
      "training 8.855688095092773 relative L2 0.1443200260400772\n",
      "training 8.84131145477295 relative L2 0.144200399518013\n",
      "training 8.826693534851074 relative L2 0.14407844841480255\n",
      "training 8.81180191040039 relative L2 0.14395356178283691\n",
      "training 8.7965726852417 relative L2 0.14382509887218475\n",
      "training 8.78090763092041 relative L2 0.14369192719459534\n",
      "training 8.76468563079834 relative L2 0.1435525119304657\n",
      "training 8.747724533081055 relative L2 0.1434047520160675\n",
      "training 8.729756355285645 relative L2 0.14324615895748138\n",
      "training 8.710472106933594 relative L2 0.14307503402233124\n",
      "training 8.689687728881836 relative L2 0.14290589094161987\n",
      "training 8.669175148010254 relative L2 0.14278379082679749\n",
      "training 8.654417037963867 relative L2 0.14266854524612427\n",
      "training 8.640481948852539 relative L2 0.14254935085773468\n",
      "training 8.626070976257324 relative L2 0.14242851734161377\n",
      "training 8.611473083496094 relative L2 0.1423085331916809\n",
      "training 8.5969820022583 relative L2 0.14218762516975403\n",
      "training 8.582391738891602 relative L2 0.1420641988515854\n",
      "training 8.567522048950195 relative L2 0.1419389694929123\n",
      "training 8.552456855773926 relative L2 0.1418142169713974\n",
      "training 8.537464141845703 relative L2 0.14169085025787354\n",
      "training 8.522650718688965 relative L2 0.14156785607337952\n",
      "training 8.507896423339844 relative L2 0.14144396781921387\n",
      "training 8.493049621582031 relative L2 0.14131979644298553\n",
      "training 8.478178977966309 relative L2 0.1411956399679184\n",
      "training 8.463311195373535 relative L2 0.14107106626033783\n",
      "training 8.448406219482422 relative L2 0.14094579219818115\n",
      "training 8.433432579040527 relative L2 0.14081984758377075\n",
      "training 8.418394088745117 relative L2 0.1406937837600708\n",
      "training 8.403361320495605 relative L2 0.14056788384914398\n",
      "training 8.388361930847168 relative L2 0.14044129848480225\n",
      "training 8.373292922973633 relative L2 0.14031414687633514\n",
      "training 8.358171463012695 relative L2 0.14018698036670685\n",
      "training 8.343059539794922 relative L2 0.1400594413280487\n",
      "training 8.327911376953125 relative L2 0.1399315595626831\n",
      "training 8.312736511230469 relative L2 0.13980332016944885\n",
      "training 8.297533988952637 relative L2 0.13967470824718475\n",
      "training 8.282308578491211 relative L2 0.13954570889472961\n",
      "training 8.267054557800293 relative L2 0.13941627740859985\n",
      "training 8.251766204833984 relative L2 0.13928650319576263\n",
      "training 8.236454963684082 relative L2 0.13915637135505676\n",
      "training 8.221110343933105 relative L2 0.13902586698532104\n",
      "training 8.205728530883789 relative L2 0.13889501988887787\n",
      "training 8.190315246582031 relative L2 0.1387636959552765\n",
      "training 8.174861907958984 relative L2 0.1386326551437378\n",
      "training 8.159459114074707 relative L2 0.13850142061710358\n",
      "training 8.144052505493164 relative L2 0.1383691430091858\n",
      "training 8.12854290008545 relative L2 0.1382358819246292\n",
      "training 8.112936973571777 relative L2 0.1381019949913025\n",
      "training 8.097273826599121 relative L2 0.13796798884868622\n",
      "training 8.081609725952148 relative L2 0.13783380389213562\n",
      "training 8.065934181213379 relative L2 0.13769908249378204\n",
      "training 8.050207138061523 relative L2 0.1375638097524643\n",
      "training 8.034430503845215 relative L2 0.13742786645889282\n",
      "training 8.018593788146973 relative L2 0.13729116320610046\n",
      "training 8.002692222595215 relative L2 0.1371537297964096\n",
      "training 7.986727237701416 relative L2 0.13701564073562622\n",
      "training 7.970699787139893 relative L2 0.1368768960237503\n",
      "training 7.954606056213379 relative L2 0.13673736155033112\n",
      "training 7.938436508178711 relative L2 0.13659703731536865\n",
      "training 7.922189235687256 relative L2 0.1364559829235077\n",
      "training 7.905877113342285 relative L2 0.136314257979393\n",
      "training 7.889503479003906 relative L2 0.13617165386676788\n",
      "training 7.873043537139893 relative L2 0.13602787256240845\n",
      "training 7.8564677238464355 relative L2 0.13588298857212067\n",
      "training 7.839782238006592 relative L2 0.13573713600635529\n",
      "training 7.823004722595215 relative L2 0.13559028506278992\n",
      "training 7.80612850189209 relative L2 0.13544218242168427\n",
      "training 7.789134979248047 relative L2 0.13529275357723236\n",
      "training 7.7720112800598145 relative L2 0.13514193892478943\n",
      "training 7.75474214553833 relative L2 0.13498972356319427\n",
      "training 7.737328052520752 relative L2 0.13483595848083496\n",
      "training 7.719754695892334 relative L2 0.13468068838119507\n",
      "training 7.702028751373291 relative L2 0.13452373445034027\n",
      "training 7.684127330780029 relative L2 0.13436482846736908\n",
      "training 7.666024684906006 relative L2 0.13420388102531433\n",
      "training 7.647712230682373 relative L2 0.1340407282114029\n",
      "training 7.629171848297119 relative L2 0.13387532532215118\n",
      "training 7.610395908355713 relative L2 0.13370740413665771\n",
      "training 7.591363430023193 relative L2 0.13353681564331055\n",
      "training 7.572046279907227 relative L2 0.13336317241191864\n",
      "training 7.552412509918213 relative L2 0.1331862509250641\n",
      "training 7.532443523406982 relative L2 0.13300587236881256\n",
      "training 7.512110233306885 relative L2 0.13282179832458496\n",
      "training 7.491375923156738 relative L2 0.13263367116451263\n",
      "training 7.4702043533325195 relative L2 0.13244102895259857\n",
      "training 7.448557376861572 relative L2 0.13224366307258606\n",
      "training 7.426398754119873 relative L2 0.13204114139080048\n",
      "training 7.403692245483398 relative L2 0.1318332403898239\n",
      "training 7.380417346954346 relative L2 0.1316198855638504\n",
      "training 7.356573104858398 relative L2 0.13140128552913666\n",
      "training 7.332192420959473 relative L2 0.13117863237857819\n",
      "training 7.307398796081543 relative L2 0.1309542953968048\n",
      "training 7.282458782196045 relative L2 0.13073328137397766\n",
      "training 7.257922172546387 relative L2 0.13052479922771454\n",
      "training 7.234798431396484 relative L2 0.13034050166606903\n",
      "training 7.214419841766357 relative L2 0.1301766037940979\n",
      "training 7.196316242218018 relative L2 0.13001897931098938\n",
      "training 7.17890739440918 relative L2 0.1298578530550003\n",
      "training 7.161144256591797 relative L2 0.12969504296779633\n",
      "training 7.143238544464111 relative L2 0.12953037023544312\n",
      "training 7.1251630783081055 relative L2 0.1293639838695526\n",
      "training 7.106922149658203 relative L2 0.12919750809669495\n",
      "training 7.088687419891357 relative L2 0.1290304958820343\n",
      "training 7.070419788360596 relative L2 0.12886333465576172\n",
      "training 7.052155017852783 relative L2 0.12869708240032196\n",
      "training 7.034002304077148 relative L2 0.1285310536623001\n",
      "training 7.015904903411865 relative L2 0.12836472690105438\n",
      "training 6.997807502746582 relative L2 0.12819789350032806\n",
      "training 6.9796905517578125 relative L2 0.12803028523921967\n",
      "training 6.961519241333008 relative L2 0.12786194682121277\n",
      "training 6.9432854652404785 relative L2 0.12769269943237305\n",
      "training 6.924964904785156 relative L2 0.1275225728750229\n",
      "training 6.906562328338623 relative L2 0.12735161185264587\n",
      "training 6.8880934715271 relative L2 0.12717965245246887\n",
      "training 6.869543552398682 relative L2 0.1270061731338501\n",
      "training 6.850864410400391 relative L2 0.12683092057704926\n",
      "training 6.832028388977051 relative L2 0.1266537308692932\n",
      "training 6.81301212310791 relative L2 0.12647408246994019\n",
      "training 6.7937517166137695 relative L2 0.12629073858261108\n",
      "training 6.7741193771362305 relative L2 0.1261005848646164\n",
      "training 6.7537841796875 relative L2 0.12590888142585754\n",
      "training 6.733311653137207 relative L2 0.12572531402111053\n",
      "training 6.713737487792969 relative L2 0.12553811073303223\n",
      "training 6.693807125091553 relative L2 0.12534689903259277\n",
      "training 6.6734819412231445 relative L2 0.125151127576828\n",
      "training 6.652705669403076 relative L2 0.12495023012161255\n",
      "training 6.63140869140625 relative L2 0.12474338710308075\n",
      "training 6.609510898590088 relative L2 0.12452976405620575\n",
      "training 6.5869293212890625 relative L2 0.12430895119905472\n",
      "training 6.563628196716309 relative L2 0.12408233433961868\n",
      "training 6.539761066436768 relative L2 0.12385871261358261\n",
      "training 6.516253471374512 relative L2 0.12365615367889404\n",
      "training 6.495016574859619 relative L2 0.12345963716506958\n",
      "training 6.474418640136719 relative L2 0.12325900793075562\n",
      "training 6.453423500061035 relative L2 0.12305483967065811\n",
      "training 6.432096481323242 relative L2 0.12285597622394562\n",
      "training 6.41136360168457 relative L2 0.12264562398195267\n",
      "training 6.389475345611572 relative L2 0.1224251464009285\n",
      "training 6.36657190322876 relative L2 0.12220852077007294\n",
      "training 6.344103813171387 relative L2 0.12199261784553528\n",
      "training 6.3217387199401855 relative L2 0.12177707254886627\n",
      "training 6.299448013305664 relative L2 0.12156746536493301\n",
      "training 6.277807235717773 relative L2 0.12137291580438614\n",
      "training 6.2577805519104 relative L2 0.1211874857544899\n",
      "training 6.238709926605225 relative L2 0.12100056558847427\n",
      "training 6.219512462615967 relative L2 0.12081224471330643\n",
      "training 6.200206756591797 relative L2 0.12062278389930725\n",
      "training 6.1808271408081055 relative L2 0.1204320415854454\n",
      "training 6.161353588104248 relative L2 0.1202419176697731\n",
      "training 6.141969203948975 relative L2 0.12005239725112915\n",
      "training 6.122669219970703 relative L2 0.11985985189676285\n",
      "training 6.103079795837402 relative L2 0.11966895312070847\n",
      "training 6.083684921264648 relative L2 0.11947725713253021\n",
      "training 6.064248561859131 relative L2 0.11928428709506989\n",
      "training 6.044727325439453 relative L2 0.11909035593271255\n",
      "training 6.025149822235107 relative L2 0.11889561265707016\n",
      "training 6.005516052246094 relative L2 0.1187010407447815\n",
      "training 5.985922336578369 relative L2 0.11850468814373016\n",
      "training 5.966175556182861 relative L2 0.1183089166879654\n",
      "training 5.94651985168457 relative L2 0.11811255663633347\n",
      "training 5.9268412590026855 relative L2 0.11791514605283737\n",
      "training 5.907095909118652 relative L2 0.11771685630083084\n",
      "training 5.88730001449585 relative L2 0.11751781404018402\n",
      "training 5.867461204528809 relative L2 0.11731970310211182\n",
      "training 5.847744464874268 relative L2 0.11711884289979935\n",
      "training 5.827785968780518 relative L2 0.11691883206367493\n",
      "training 5.807943820953369 relative L2 0.11671792715787888\n",
      "training 5.788045883178711 relative L2 0.11651595681905746\n",
      "training 5.76807975769043 relative L2 0.11631318926811218\n",
      "training 5.748072147369385 relative L2 0.11611033976078033\n",
      "training 5.728093147277832 relative L2 0.11590610444545746\n",
      "training 5.708011627197266 relative L2 0.11570148169994354\n",
      "training 5.687926292419434 relative L2 0.11549584567546844\n",
      "training 5.667771816253662 relative L2 0.11528969556093216\n",
      "training 5.647603511810303 relative L2 0.11508205533027649\n",
      "training 5.627331733703613 relative L2 0.1148737445473671\n",
      "training 5.607031345367432 relative L2 0.11466404050588608\n",
      "training 5.5866312980651855 relative L2 0.11445342004299164\n",
      "training 5.566175937652588 relative L2 0.11424104124307632\n",
      "training 5.545588493347168 relative L2 0.11402761191129684\n",
      "training 5.524933815002441 relative L2 0.11381236463785172\n",
      "training 5.504143714904785 relative L2 0.11359510570764542\n",
      "training 5.483204364776611 relative L2 0.11337612569332123\n",
      "training 5.462144374847412 relative L2 0.11315494030714035\n",
      "training 5.440908432006836 relative L2 0.11293313652276993\n",
      "training 5.419642448425293 relative L2 0.11270996928215027\n",
      "training 5.398287773132324 relative L2 0.11248450726270676\n",
      "training 5.376765251159668 relative L2 0.11225549876689911\n",
      "training 5.354950904846191 relative L2 0.11202190816402435\n",
      "training 5.332742214202881 relative L2 0.11178289353847504\n",
      "training 5.310054779052734 relative L2 0.11153796315193176\n",
      "training 5.2868571281433105 relative L2 0.1112869530916214\n",
      "training 5.263134956359863 relative L2 0.11102946847677231\n",
      "training 5.238844394683838 relative L2 0.11076443642377853\n",
      "training 5.213897705078125 relative L2 0.11049102991819382\n",
      "training 5.188225746154785 relative L2 0.1102091521024704\n",
      "training 5.161832332611084 relative L2 0.10992167890071869\n",
      "training 5.134978771209717 relative L2 0.1096402108669281\n",
      "training 5.108739852905273 relative L2 0.10939837992191315\n",
      "training 5.086271286010742 relative L2 0.10918587446212769\n",
      "training 5.066552639007568 relative L2 0.10897160321474075\n",
      "training 5.0467071533203125 relative L2 0.108753502368927\n",
      "training 5.026564121246338 relative L2 0.1085330918431282\n",
      "training 5.006266117095947 relative L2 0.10831175744533539\n",
      "training 4.985923767089844 relative L2 0.10808826982975006\n",
      "training 4.965415000915527 relative L2 0.1078631728887558\n",
      "training 4.944788932800293 relative L2 0.10763727128505707\n",
      "training 4.924135208129883 relative L2 0.10741070657968521\n",
      "training 4.903478145599365 relative L2 0.10718169808387756\n",
      "training 4.882654666900635 relative L2 0.10695155709981918\n",
      "training 4.861774921417236 relative L2 0.1067206934094429\n",
      "training 4.840861797332764 relative L2 0.10649038106203079\n",
      "training 4.8200297355651855 relative L2 0.10625886917114258\n",
      "training 4.799130916595459 relative L2 0.10602609068155289\n",
      "training 4.778172969818115 relative L2 0.10579152405261993\n",
      "training 4.757112503051758 relative L2 0.10555552691221237\n",
      "training 4.735975742340088 relative L2 0.10531779378652573\n",
      "training 4.714725017547607 relative L2 0.10507791489362717\n",
      "training 4.693317890167236 relative L2 0.10483618825674057\n",
      "training 4.671782493591309 relative L2 0.10459218174219131\n",
      "training 4.650094032287598 relative L2 0.10434596985578537\n",
      "training 4.628266334533691 relative L2 0.10409709811210632\n",
      "training 4.606259346008301 relative L2 0.10384569317102432\n",
      "training 4.584085941314697 relative L2 0.10359244793653488\n",
      "training 4.561802387237549 relative L2 0.1033383160829544\n",
      "training 4.539488792419434 relative L2 0.10308681428432465\n",
      "training 4.517451763153076 relative L2 0.1028444692492485\n",
      "training 4.496273994445801 relative L2 0.10261056572198868\n",
      "training 4.475880146026611 relative L2 0.10237648338079453\n",
      "training 4.455515384674072 relative L2 0.10214145481586456\n",
      "training 4.435122489929199 relative L2 0.1019054427742958\n",
      "training 4.414697647094727 relative L2 0.10166872292757034\n",
      "training 4.394258499145508 relative L2 0.10143132507801056\n",
      "training 4.373804092407227 relative L2 0.10119309276342392\n",
      "training 4.353320598602295 relative L2 0.10095369070768356\n",
      "training 4.332785606384277 relative L2 0.10071263462305069\n",
      "training 4.312158584594727 relative L2 0.10047018527984619\n",
      "training 4.29146671295166 relative L2 0.10022923350334167\n",
      "training 4.270954132080078 relative L2 0.09998947381973267\n",
      "training 4.250586986541748 relative L2 0.0997486561536789\n",
      "training 4.230172634124756 relative L2 0.09950730204582214\n",
      "training 4.209763526916504 relative L2 0.09926497936248779\n",
      "training 4.189330101013184 relative L2 0.09902086108922958\n",
      "training 4.168798923492432 relative L2 0.09877143055200577\n",
      "training 4.147872447967529 relative L2 0.09851017594337463\n",
      "training 4.126002788543701 relative L2 0.09826565533876419\n",
      "training 4.105586051940918 relative L2 0.09801992774009705\n",
      "training 4.085104942321777 relative L2 0.09777279943227768\n",
      "training 4.064559459686279 relative L2 0.0975245013833046\n",
      "training 4.043981075286865 relative L2 0.0972752571105957\n",
      "training 4.023391246795654 relative L2 0.0970252975821495\n",
      "training 4.002796173095703 relative L2 0.09677457064390182\n",
      "training 3.982180595397949 relative L2 0.0965234711766243\n",
      "training 3.9615721702575684 relative L2 0.09627199918031693\n",
      "training 3.940981864929199 relative L2 0.09601955860853195\n",
      "training 3.9203743934631348 relative L2 0.09576638042926788\n",
      "training 3.899775505065918 relative L2 0.09551297873258591\n",
      "training 3.8792130947113037 relative L2 0.09525887668132782\n",
      "training 3.8586437702178955 relative L2 0.09500417858362198\n",
      "training 3.8380722999572754 relative L2 0.09474899619817734\n",
      "training 3.8175137042999268 relative L2 0.09449422359466553\n",
      "training 3.7970433235168457 relative L2 0.094239741563797\n",
      "training 3.7766613960266113 relative L2 0.09398531913757324\n",
      "training 3.7563416957855225 relative L2 0.09373060613870621\n",
      "training 3.7360501289367676 relative L2 0.09347479045391083\n",
      "training 3.7157211303710938 relative L2 0.09321776777505875\n",
      "training 3.695343255996704 relative L2 0.09295999258756638\n",
      "training 3.6749608516693115 relative L2 0.09270218014717102\n",
      "training 3.6546378135681152 relative L2 0.09244456142187119\n",
      "training 3.6343908309936523 relative L2 0.09218697994947433\n",
      "training 3.614204168319702 relative L2 0.0919291228055954\n",
      "training 3.5940496921539307 relative L2 0.0916706919670105\n",
      "training 3.573901891708374 relative L2 0.09141107648611069\n",
      "training 3.553718090057373 relative L2 0.09115023165941238\n",
      "training 3.5334949493408203 relative L2 0.0908883661031723\n",
      "training 3.5132527351379395 relative L2 0.09062564373016357\n",
      "training 3.49300479888916 relative L2 0.09036210179328918\n",
      "training 3.4727554321289062 relative L2 0.09009747207164764\n",
      "training 3.452483654022217 relative L2 0.08983156830072403\n",
      "training 3.4321718215942383 relative L2 0.08956412225961685\n",
      "training 3.411799669265747 relative L2 0.08929543197154999\n",
      "training 3.3913917541503906 relative L2 0.08902519196271896\n",
      "training 3.3709309101104736 relative L2 0.08875316381454468\n",
      "training 3.350396156311035 relative L2 0.08847896009683609\n",
      "training 3.3297595977783203 relative L2 0.08820224553346634\n",
      "training 3.3089964389801025 relative L2 0.08792251348495483\n",
      "training 3.288073778152466 relative L2 0.08763913065195084\n",
      "training 3.2669496536254883 relative L2 0.08735138177871704\n",
      "training 3.2455692291259766 relative L2 0.08705832809209824\n",
      "training 3.223865509033203 relative L2 0.08675887435674667\n",
      "training 3.2017581462860107 relative L2 0.08645184338092804\n",
      "training 3.1791610717773438 relative L2 0.0861361101269722\n",
      "training 3.1560051441192627 relative L2 0.08581249415874481\n",
      "training 3.1323587894439697 relative L2 0.08548925817012787\n",
      "training 3.108840227127075 relative L2 0.08518396317958832\n",
      "training 3.08669376373291 relative L2 0.08493135869503021\n",
      "training 3.068431854248047 relative L2 0.08468137681484222\n",
      "training 3.0503909587860107 relative L2 0.08442569524049759\n",
      "training 3.0320098400115967 relative L2 0.08416441082954407\n",
      "training 3.0133087635040283 relative L2 0.08389904350042343\n",
      "training 2.9943835735321045 relative L2 0.0836302861571312\n",
      "training 2.9752683639526367 relative L2 0.08335823565721512\n",
      "training 2.9559662342071533 relative L2 0.08308271318674088\n",
      "training 2.936479330062866 relative L2 0.08280348777770996\n",
      "training 2.9168055057525635 relative L2 0.08252149075269699\n",
      "training 2.897014617919922 relative L2 0.08223719894886017\n",
      "training 2.877131700515747 relative L2 0.08195028454065323\n",
      "training 2.8571267127990723 relative L2 0.08166112005710602\n",
      "training 2.837031602859497 relative L2 0.08136987686157227\n",
      "training 2.8168673515319824 relative L2 0.08107621967792511\n",
      "training 2.7966148853302 relative L2 0.08078048378229141\n",
      "training 2.776292324066162 relative L2 0.08048547804355621\n",
      "training 2.7560842037200928 relative L2 0.08020440489053726\n",
      "training 2.7368929386138916 relative L2 0.07992986589670181\n",
      "training 2.718228578567505 relative L2 0.07965032756328583\n",
      "training 2.6992881298065186 relative L2 0.07936263829469681\n",
      "training 2.6798579692840576 relative L2 0.07906676828861237\n",
      "training 2.659942626953125 relative L2 0.07876575738191605\n",
      "training 2.6397509574890137 relative L2 0.07846509665250778\n",
      "training 2.6196625232696533 relative L2 0.0781688541173935\n",
      "training 2.599947452545166 relative L2 0.07787294685840607\n",
      "training 2.580319404602051 relative L2 0.07757212221622467\n",
      "training 2.560439348220825 relative L2 0.07726546376943588\n",
      "training 2.5402584075927734 relative L2 0.07695265859365463\n",
      "training 2.519761085510254 relative L2 0.07663318514823914\n",
      "training 2.4989097118377686 relative L2 0.07630555331707001\n",
      "training 2.4776203632354736 relative L2 0.07596871256828308\n",
      "training 2.4558162689208984 relative L2 0.07562437653541565\n",
      "training 2.4336187839508057 relative L2 0.07527270168066025\n",
      "training 2.411053419113159 relative L2 0.07491598278284073\n",
      "training 2.388284683227539 relative L2 0.07458273321390152\n",
      "training 2.3671085834503174 relative L2 0.07431986182928085\n",
      "training 2.350459098815918 relative L2 0.07405189424753189\n",
      "training 2.3335280418395996 relative L2 0.07377618551254272\n",
      "training 2.316187620162964 relative L2 0.07349283248186111\n",
      "training 2.2984657287597656 relative L2 0.0732041597366333\n",
      "training 2.280484914779663 relative L2 0.07291009277105331\n",
      "training 2.262223958969116 relative L2 0.0726093128323555\n",
      "training 2.243608236312866 relative L2 0.07230040431022644\n",
      "training 2.224571704864502 relative L2 0.07198232412338257\n",
      "training 2.2050726413726807 relative L2 0.07165712863206863\n",
      "training 2.185232162475586 relative L2 0.07132899761199951\n",
      "training 2.165295124053955 relative L2 0.07101183384656906\n",
      "training 2.146104574203491 relative L2 0.0706963837146759\n",
      "training 2.1271064281463623 relative L2 0.07037966698408127\n",
      "training 2.108121156692505 relative L2 0.07006638497114182\n",
      "training 2.0894289016723633 relative L2 0.06977342069149017\n",
      "training 2.0720317363739014 relative L2 0.06948290020227432\n",
      "training 2.054856538772583 relative L2 0.0691843256354332\n",
      "training 2.0372636318206787 relative L2 0.06887712329626083\n",
      "training 2.019227981567383 relative L2 0.06856361776590347\n",
      "training 2.0009055137634277 relative L2 0.0682477056980133\n",
      "training 1.9825316667556763 relative L2 0.06793569773435593\n",
      "training 1.9644758701324463 relative L2 0.06763065606355667\n",
      "training 1.9468958377838135 relative L2 0.06732886284589767\n",
      "training 1.9295706748962402 relative L2 0.06702578067779541\n",
      "training 1.9122532606124878 relative L2 0.06672045588493347\n",
      "training 1.8949010372161865 relative L2 0.06641430407762527\n",
      "training 1.8775839805603027 relative L2 0.06610732525587082\n",
      "training 1.8602889776229858 relative L2 0.06579916179180145\n",
      "training 1.8429993391036987 relative L2 0.06549309939146042\n",
      "training 1.8259077072143555 relative L2 0.0651896670460701\n",
      "training 1.8090543746948242 relative L2 0.06488678604364395\n",
      "training 1.792319655418396 relative L2 0.06458256393671036\n",
      "training 1.7755768299102783 relative L2 0.06427586078643799\n",
      "training 1.7587649822235107 relative L2 0.0639672502875328\n",
      "training 1.741927146911621 relative L2 0.06365741044282913\n",
      "training 1.7251118421554565 relative L2 0.06334764510393143\n",
      "training 1.7083895206451416 relative L2 0.06303863227367401\n",
      "training 1.6917829513549805 relative L2 0.06272955983877182\n",
      "training 1.6752402782440186 relative L2 0.0624198317527771\n",
      "training 1.6587430238723755 relative L2 0.062108516693115234\n",
      "training 1.6422524452209473 relative L2 0.06179524213075638\n",
      "training 1.6257483959197998 relative L2 0.06148066744208336\n",
      "training 1.6092559099197388 relative L2 0.061164967715740204\n",
      "training 1.592782974243164 relative L2 0.06084814667701721\n",
      "training 1.5763369798660278 relative L2 0.060529738664627075\n",
      "training 1.5599008798599243 relative L2 0.06020926684141159\n",
      "training 1.5434495210647583 relative L2 0.059886544942855835\n",
      "training 1.5269695520401 relative L2 0.05956115946173668\n",
      "training 1.5104329586029053 relative L2 0.059232890605926514\n",
      "training 1.4938366413116455 relative L2 0.05890124291181564\n",
      "training 1.477168083190918 relative L2 0.05856585130095482\n",
      "training 1.4604135751724243 relative L2 0.05822636932134628\n",
      "training 1.4435490369796753 relative L2 0.057882003486156464\n",
      "training 1.4265371561050415 relative L2 0.05753157287836075\n",
      "training 1.409331202507019 relative L2 0.05717397853732109\n",
      "training 1.3918787240982056 relative L2 0.0568075105547905\n",
      "training 1.374106526374817 relative L2 0.05643098056316376\n",
      "training 1.355960488319397 relative L2 0.05604439973831177\n",
      "training 1.3374565839767456 relative L2 0.055676065385341644\n",
      "training 1.3199470043182373 relative L2 0.05536019802093506\n",
      "training 1.3050123453140259 relative L2 0.05503873899579048\n",
      "training 1.2898977994918823 relative L2 0.054713401943445206\n",
      "training 1.2747011184692383 relative L2 0.054385408759117126\n",
      "training 1.2594821453094482 relative L2 0.054057925939559937\n",
      "training 1.244372010231018 relative L2 0.05373226851224899\n",
      "training 1.2294269800186157 relative L2 0.05340873450040817\n",
      "training 1.2146694660186768 relative L2 0.05308568477630615\n",
      "training 1.2000311613082886 relative L2 0.052760567516088486\n",
      "training 1.185391902923584 relative L2 0.052429310977458954\n",
      "training 1.1705670356750488 relative L2 0.052091822028160095\n",
      "training 1.1555489301681519 relative L2 0.05175461992621422\n",
      "training 1.1406413316726685 relative L2 0.051429152488708496\n",
      "training 1.126360535621643 relative L2 0.05110238865017891\n",
      "training 1.1121156215667725 relative L2 0.050773750990629196\n",
      "training 1.097866415977478 relative L2 0.05045248568058014\n",
      "training 1.084006428718567 relative L2 0.0501541793346405\n",
      "training 1.0712271928787231 relative L2 0.049875423312187195\n",
      "training 1.059353232383728 relative L2 0.049602311104536057\n",
      "training 1.0477919578552246 relative L2 0.04933207109570503\n",
      "training 1.0364128351211548 relative L2 0.04905889928340912\n",
      "training 1.0249699354171753 relative L2 0.0487816259264946\n",
      "training 1.0134227275848389 relative L2 0.048501282930374146\n",
      "training 1.0018173456192017 relative L2 0.04821890592575073\n",
      "training 0.9901959300041199 relative L2 0.047937750816345215\n",
      "training 0.9786874651908875 relative L2 0.047660693526268005\n",
      "training 0.9674131274223328 relative L2 0.04738490283489227\n",
      "training 0.9562607407569885 relative L2 0.0471087247133255\n",
      "training 0.945155143737793 relative L2 0.0468306690454483\n",
      "training 0.9340354204177856 relative L2 0.046550214290618896\n",
      "training 0.9228855967521667 relative L2 0.046267423778772354\n",
      "training 0.9117137789726257 relative L2 0.045983780175447464\n",
      "training 0.9005787372589111 relative L2 0.045703284442424774\n",
      "training 0.8896321654319763 relative L2 0.045436978340148926\n",
      "training 0.8792998790740967 relative L2 0.04517354071140289\n",
      "training 0.869135320186615 relative L2 0.04490919038653374\n",
      "training 0.8590001463890076 relative L2 0.04464384913444519\n",
      "training 0.8488906621932983 relative L2 0.0443778857588768\n",
      "training 0.838814377784729 relative L2 0.0441119484603405\n",
      "training 0.8287943601608276 relative L2 0.043847233057022095\n",
      "training 0.8188809752464294 relative L2 0.04358319938182831\n",
      "training 0.8090579509735107 relative L2 0.043320123106241226\n",
      "training 0.7993282675743103 relative L2 0.04305810108780861\n",
      "training 0.7896921038627625 relative L2 0.04279690235853195\n",
      "training 0.7801438570022583 relative L2 0.042536262422800064\n",
      "training 0.7706776857376099 relative L2 0.04227863997220993\n",
      "training 0.761378824710846 relative L2 0.04202476143836975\n",
      "training 0.7522674798965454 relative L2 0.041777417063713074\n",
      "training 0.7434425354003906 relative L2 0.0415327250957489\n",
      "training 0.7347660660743713 relative L2 0.04128829762339592\n",
      "training 0.7261504530906677 relative L2 0.04104304686188698\n",
      "training 0.7175559401512146 relative L2 0.04079524427652359\n",
      "training 0.7089231014251709 relative L2 0.040547147393226624\n",
      "training 0.7003322243690491 relative L2 0.04030004143714905\n",
      "training 0.6918279528617859 relative L2 0.04005555436015129\n",
      "training 0.6834637522697449 relative L2 0.03981461003422737\n",
      "training 0.6752696633338928 relative L2 0.03957463800907135\n",
      "training 0.6671571731567383 relative L2 0.03933507949113846\n",
      "training 0.6591101288795471 relative L2 0.039095938205718994\n",
      "training 0.651124894618988 relative L2 0.03885699436068535\n",
      "training 0.6431926488876343 relative L2 0.03861814737319946\n",
      "training 0.6353117227554321 relative L2 0.038379423320293427\n",
      "training 0.6274858713150024 relative L2 0.038141749799251556\n",
      "training 0.6197435855865479 relative L2 0.03790416941046715\n",
      "training 0.6120515465736389 relative L2 0.037665653973817825\n",
      "training 0.6043751835823059 relative L2 0.03742542117834091\n",
      "training 0.5966921448707581 relative L2 0.03718255087733269\n",
      "training 0.5889748930931091 relative L2 0.0369366891682148\n",
      "training 0.5812131762504578 relative L2 0.036688972264528275\n",
      "training 0.573445737361908 relative L2 0.03645128756761551\n",
      "training 0.5660403966903687 relative L2 0.03624345734715462\n",
      "training 0.5596029162406921 relative L2 0.03603549301624298\n",
      "training 0.5531942248344421 relative L2 0.035824526101350784\n",
      "training 0.5467378497123718 relative L2 0.0356113575398922\n",
      "training 0.5402581691741943 relative L2 0.03539703041315079\n",
      "training 0.5337773561477661 relative L2 0.035182222723960876\n",
      "training 0.5273165106773376 relative L2 0.034967292100191116\n",
      "training 0.5208951830863953 relative L2 0.03475220128893852\n",
      "training 0.5145140886306763 relative L2 0.03453749418258667\n",
      "training 0.5081812143325806 relative L2 0.034323371946811676\n",
      "training 0.5018995404243469 relative L2 0.03411022201180458\n",
      "training 0.4956865608692169 relative L2 0.03390638157725334\n",
      "training 0.48978471755981445 relative L2 0.0337093323469162\n",
      "training 0.4841168522834778 relative L2 0.03351252153515816\n",
      "training 0.478480726480484 relative L2 0.03331442549824715\n",
      "training 0.4728381931781769 relative L2 0.03311464935541153\n",
      "training 0.46718716621398926 relative L2 0.03291440010070801\n",
      "training 0.4615590572357178 relative L2 0.03271510824561119\n",
      "training 0.4559861123561859 relative L2 0.03251891955733299\n",
      "training 0.45052963495254517 relative L2 0.03232726454734802\n",
      "training 0.44523659348487854 relative L2 0.03213943913578987\n",
      "training 0.44008177518844604 relative L2 0.03195235878229141\n",
      "training 0.4349746108055115 relative L2 0.03176533803343773\n",
      "training 0.42989659309387207 relative L2 0.03157845139503479\n",
      "training 0.42485398054122925 relative L2 0.03139176219701767\n",
      "training 0.41984912753105164 relative L2 0.031206803396344185\n",
      "training 0.4149181842803955 relative L2 0.03102482296526432\n",
      "training 0.41009271144866943 relative L2 0.03084554150700569\n",
      "training 0.4053681194782257 relative L2 0.03066812828183174\n",
      "training 0.4007221460342407 relative L2 0.030491897836327553\n",
      "training 0.39613255858421326 relative L2 0.030316490679979324\n",
      "training 0.39158838987350464 relative L2 0.03014189563691616\n",
      "training 0.38709187507629395 relative L2 0.02996850199997425\n",
      "training 0.38265371322631836 relative L2 0.02979675494134426\n",
      "training 0.37828218936920166 relative L2 0.029627105221152306\n",
      "training 0.3739876449108124 relative L2 0.02945937216281891\n",
      "training 0.36976608633995056 relative L2 0.02929326705634594\n",
      "training 0.3656105101108551 relative L2 0.02912856638431549\n",
      "training 0.36151280999183655 relative L2 0.02896515093743801\n",
      "training 0.3574683964252472 relative L2 0.028802907094359398\n",
      "training 0.3534758985042572 relative L2 0.028641877695918083\n",
      "training 0.3495369553565979 relative L2 0.028482405468821526\n",
      "training 0.3456573486328125 relative L2 0.028324643149971962\n",
      "training 0.34183916449546814 relative L2 0.028168480843305588\n",
      "training 0.33808082342147827 relative L2 0.028013909235596657\n",
      "training 0.33438220620155334 relative L2 0.02786078304052353\n",
      "training 0.3307381868362427 relative L2 0.027709059417247772\n",
      "training 0.32714611291885376 relative L2 0.027558691799640656\n",
      "training 0.323605477809906 relative L2 0.027409590780735016\n",
      "training 0.32011452317237854 relative L2 0.027261901646852493\n",
      "training 0.3166753649711609 relative L2 0.02711573801934719\n",
      "training 0.3132893741130829 relative L2 0.026971066370606422\n",
      "training 0.3099558651447296 relative L2 0.026827819645404816\n",
      "training 0.30667340755462646 relative L2 0.026686089113354683\n",
      "training 0.30344292521476746 relative L2 0.026545707136392593\n",
      "training 0.3002592921257019 relative L2 0.026406707242131233\n",
      "training 0.2971234619617462 relative L2 0.026269009336829185\n",
      "training 0.29403358697891235 relative L2 0.026132702827453613\n",
      "training 0.2909907102584839 relative L2 0.025997774675488472\n",
      "training 0.2879936695098877 relative L2 0.025864258408546448\n",
      "training 0.28504297137260437 relative L2 0.025732126086950302\n",
      "training 0.28213822841644287 relative L2 0.025601373985409737\n",
      "training 0.279278427362442 relative L2 0.02547195926308632\n",
      "training 0.2764618396759033 relative L2 0.025343889370560646\n",
      "training 0.27368876338005066 relative L2 0.025217141956090927\n",
      "training 0.2709583044052124 relative L2 0.02509172260761261\n",
      "training 0.2682698965072632 relative L2 0.024967597797513008\n",
      "training 0.2656220495700836 relative L2 0.024844786152243614\n",
      "training 0.2630152404308319 relative L2 0.024723239243030548\n",
      "training 0.260448157787323 relative L2 0.024603046476840973\n",
      "training 0.2579217255115509 relative L2 0.024484144523739815\n",
      "training 0.2554343640804291 relative L2 0.024366529658436775\n",
      "training 0.2529859244823456 relative L2 0.02425018884241581\n",
      "training 0.25057581067085266 relative L2 0.02413507178425789\n",
      "training 0.24820217490196228 relative L2 0.02402123250067234\n",
      "training 0.2458658367395401 relative L2 0.023908594623208046\n",
      "training 0.24356494843959808 relative L2 0.023797227069735527\n",
      "training 0.2413007766008377 relative L2 0.023687060922384262\n",
      "training 0.2390713095664978 relative L2 0.02357812412083149\n",
      "training 0.23687662184238434 relative L2 0.023470420390367508\n",
      "training 0.23471671342849731 relative L2 0.023363914340734482\n",
      "training 0.2325906604528427 relative L2 0.023258546367287636\n",
      "training 0.23049667477607727 relative L2 0.023154394701123238\n",
      "training 0.22843606770038605 relative L2 0.023051366209983826\n",
      "training 0.22640684247016907 relative L2 0.022949479520320892\n",
      "training 0.2244090884923935 relative L2 0.02284880541265011\n",
      "training 0.2224438190460205 relative L2 0.022749200463294983\n",
      "training 0.22050786018371582 relative L2 0.02265079878270626\n",
      "training 0.21860362589359283 relative L2 0.022553451359272003\n",
      "training 0.21672795712947845 relative L2 0.022457245737314224\n",
      "training 0.21488229930400848 relative L2 0.022362103685736656\n",
      "training 0.21306483447551727 relative L2 0.02226804383099079\n",
      "training 0.21127569675445557 relative L2 0.022175097838044167\n",
      "training 0.20951524376869202 relative L2 0.022083183750510216\n",
      "training 0.2077813595533371 relative L2 0.021992351859807968\n",
      "training 0.20607493817806244 relative L2 0.021902550011873245\n",
      "training 0.20439483225345612 relative L2 0.02181379310786724\n",
      "training 0.20274099707603455 relative L2 0.021726064383983612\n",
      "training 0.20111292600631714 relative L2 0.02163935825228691\n",
      "training 0.19951015710830688 relative L2 0.021553661674261093\n",
      "training 0.1979323774576187 relative L2 0.021468928083777428\n",
      "training 0.19637846946716309 relative L2 0.021385174244642258\n",
      "training 0.1948486715555191 relative L2 0.02130240574479103\n",
      "training 0.19334256649017334 relative L2 0.021220628172159195\n",
      "training 0.19186028838157654 relative L2 0.021139763295650482\n",
      "training 0.19040018320083618 relative L2 0.02105986326932907\n",
      "training 0.1889631152153015 relative L2 0.020980842411518097\n",
      "training 0.18754732608795166 relative L2 0.020902782678604126\n",
      "training 0.18615397810935974 relative L2 0.020825624465942383\n",
      "training 0.1847817599773407 relative L2 0.020749354735016823\n",
      "training 0.18343031406402588 relative L2 0.020673982799053192\n",
      "training 0.18209968507289886 relative L2 0.02059948816895485\n",
      "training 0.18078917264938354 relative L2 0.020525861531496048\n",
      "training 0.17949852347373962 relative L2 0.020453082397580147\n",
      "training 0.17822739481925964 relative L2 0.02038114331662655\n",
      "training 0.17697542905807495 relative L2 0.02031008154153824\n",
      "training 0.175742968916893 relative L2 0.02023984119296074\n",
      "training 0.1745288074016571 relative L2 0.020170366391539574\n",
      "training 0.17333205044269562 relative L2 0.020101727917790413\n",
      "training 0.17215383052825928 relative L2 0.020033853128552437\n",
      "training 0.17099256813526154 relative L2 0.01996677741408348\n",
      "training 0.16984882950782776 relative L2 0.019900474697351456\n",
      "training 0.16872204840183258 relative L2 0.01983492076396942\n",
      "training 0.16761155426502228 relative L2 0.019770126789808273\n",
      "training 0.166517436504364 relative L2 0.019706055521965027\n",
      "training 0.16543883085250854 relative L2 0.019642731174826622\n",
      "training 0.1643761843442917 relative L2 0.019580086693167686\n",
      "training 0.16332842409610748 relative L2 0.019518226385116577\n",
      "training 0.16229701042175293 relative L2 0.01945699006319046\n",
      "training 0.16127926111221313 relative L2 0.019396452233195305\n",
      "training 0.16027624905109406 relative L2 0.019336601719260216\n",
      "training 0.15928779542446136 relative L2 0.019277423620224\n",
      "training 0.15831324458122253 relative L2 0.019218888133764267\n",
      "training 0.1573522388935089 relative L2 0.019161049276590347\n",
      "training 0.15640558302402496 relative L2 0.019103823229670525\n",
      "training 0.1554717719554901 relative L2 0.01904730126261711\n",
      "training 0.15455207228660583 relative L2 0.01899135671555996\n",
      "training 0.15364451706409454 relative L2 0.018936049193143845\n",
      "training 0.15275000035762787 relative L2 0.018881339579820633\n",
      "training 0.15186768770217896 relative L2 0.018827242776751518\n",
      "training 0.15099778771400452 relative L2 0.018773773685097694\n",
      "training 0.15014046430587769 relative L2 0.01872086524963379\n",
      "training 0.1492943912744522 relative L2 0.018668554723262787\n",
      "training 0.1484602838754654 relative L2 0.018616797402501106\n",
      "training 0.14763730764389038 relative L2 0.018565606325864792\n",
      "training 0.14682556688785553 relative L2 0.018514975905418396\n",
      "training 0.14602483808994293 relative L2 0.01846490427851677\n",
      "training 0.14523504674434662 relative L2 0.01841534860432148\n",
      "training 0.14445561170578003 relative L2 0.018366334959864616\n",
      "training 0.14368677139282227 relative L2 0.018317844718694687\n",
      "training 0.1429281085729599 relative L2 0.01826987974345684\n",
      "training 0.14217963814735413 relative L2 0.018222413957118988\n",
      "training 0.14144092798233032 relative L2 0.01817544735968113\n",
      "training 0.14071188867092133 relative L2 0.018128959462046623\n",
      "training 0.13999220728874207 relative L2 0.018082981929183006\n",
      "training 0.1392821967601776 relative L2 0.018037475645542145\n",
      "training 0.1385812908411026 relative L2 0.017992423847317696\n",
      "training 0.13788913190364838 relative L2 0.017947828397154808\n",
      "training 0.13720564544200897 relative L2 0.017903698608279228\n",
      "training 0.13653089106082916 relative L2 0.01786000467836857\n",
      "training 0.13586458563804626 relative L2 0.017816733568906784\n",
      "training 0.13520628213882446 relative L2 0.01777392253279686\n",
      "training 0.13455656170845032 relative L2 0.017731502652168274\n",
      "training 0.1339143067598343 relative L2 0.01768951304256916\n",
      "training 0.13328009843826294 relative L2 0.017647933214902878\n",
      "training 0.1326535940170288 relative L2 0.017606742680072784\n",
      "training 0.13203434646129608 relative L2 0.017565932124853134\n",
      "training 0.13142217695713043 relative L2 0.017525477334856987\n",
      "training 0.1308167725801468 relative L2 0.017485400661826134\n",
      "training 0.13021834194660187 relative L2 0.017445607110857964\n",
      "training 0.12962540984153748 relative L2 0.017406154423952103\n",
      "training 0.1290389746427536 relative L2 0.01736699976027012\n",
      "training 0.12845833599567413 relative L2 0.01732822321355343\n",
      "training 0.12788444757461548 relative L2 0.017289776355028152\n",
      "training 0.12731678783893585 relative L2 0.01725166104733944\n",
      "training 0.12675534188747406 relative L2 0.01721382886171341\n",
      "training 0.12619921565055847 relative L2 0.01717633195221424\n",
      "training 0.12564915418624878 relative L2 0.01713910885155201\n",
      "training 0.1251041740179062 relative L2 0.017102202400565147\n",
      "training 0.12456502765417099 relative L2 0.017065562307834625\n",
      "training 0.12403088808059692 relative L2 0.0170291755348444\n",
      "training 0.12350165843963623 relative L2 0.016993019729852676\n",
      "training 0.12297695875167847 relative L2 0.016957147046923637\n",
      "training 0.12245746701955795 relative L2 0.016921525821089745\n",
      "training 0.1219426691532135 relative L2 0.016886118799448013\n",
      "training 0.12143213301897049 relative L2 0.016850927844643593\n",
      "training 0.12092584371566772 relative L2 0.016815952956676483\n",
      "training 0.12042378634214401 relative L2 0.016781171783804893\n",
      "training 0.11992556601762772 relative L2 0.016746625304222107\n",
      "training 0.11943169683218002 relative L2 0.01671220362186432\n",
      "training 0.11894078552722931 relative L2 0.01667794957756996\n",
      "training 0.11845341324806213 relative L2 0.01664379984140396\n",
      "training 0.11796845495700836 relative L2 0.016609802842140198\n",
      "training 0.11748652160167694 relative L2 0.016575898975133896\n",
      "training 0.11700643599033356 relative L2 0.0165421012789011\n",
      "training 0.11652865260839462 relative L2 0.016508376225829124\n",
      "training 0.11605263501405716 relative L2 0.01647469587624073\n",
      "training 0.1155780628323555 relative L2 0.016440991312265396\n",
      "training 0.11510364711284637 relative L2 0.016407210379838943\n",
      "training 0.11462957412004471 relative L2 0.01637331210076809\n",
      "training 0.11415521055459976 relative L2 0.01633922941982746\n",
      "training 0.11367965489625931 relative L2 0.016304921358823776\n",
      "training 0.11320193856954575 relative L2 0.016270259395241737\n",
      "training 0.11272074282169342 relative L2 0.01623525284230709\n",
      "training 0.11223620921373367 relative L2 0.016199888661503792\n",
      "training 0.11174724251031876 relative L2 0.01616406999528408\n",
      "training 0.11125354468822479 relative L2 0.016127999871969223\n",
      "training 0.11075728386640549 relative L2 0.016092270612716675\n",
      "training 0.11026514321565628 relative L2 0.01605830527842045\n",
      "training 0.1097991019487381 relative L2 0.016030538827180862\n",
      "training 0.10941924899816513 relative L2 0.01600617542862892\n",
      "training 0.10908375680446625 relative L2 0.015980981290340424\n",
      "training 0.10873907804489136 relative L2 0.015954941511154175\n",
      "training 0.10838599503040314 relative L2 0.015928540378808975\n",
      "training 0.10802629590034485 relative L2 0.015901856124401093\n",
      "training 0.10766230523586273 relative L2 0.01587463915348053\n",
      "training 0.10729441791772842 relative L2 0.015847206115722656\n",
      "training 0.10692417621612549 relative L2 0.015819741412997246\n",
      "training 0.10655193030834198 relative L2 0.01579206995666027\n",
      "training 0.10617886483669281 relative L2 0.015764236450195312\n",
      "training 0.10580579936504364 relative L2 0.015736503526568413\n",
      "training 0.10543306171894073 relative L2 0.015708865597844124\n",
      "training 0.10506172478199005 relative L2 0.015681061893701553\n",
      "training 0.10469050705432892 relative L2 0.01565505377948284\n",
      "training 0.10434382408857346 relative L2 0.015630580484867096\n",
      "training 0.10401661694049835 relative L2 0.015606452710926533\n",
      "training 0.10369480401277542 relative L2 0.015582084655761719\n",
      "training 0.10337167978286743 relative L2 0.015557480044662952\n",
      "training 0.10304474085569382 relative L2 0.015532595105469227\n",
      "training 0.10271397978067398 relative L2 0.015507389791309834\n",
      "training 0.10238084942102432 relative L2 0.015482082962989807\n",
      "training 0.1020469143986702 relative L2 0.015456941910088062\n",
      "training 0.10171439498662949 relative L2 0.015431968495249748\n",
      "training 0.1013852134346962 relative L2 0.015407292172312737\n",
      "training 0.10106109082698822 relative L2 0.015383200719952583\n",
      "training 0.10074437409639359 relative L2 0.015359781682491302\n",
      "training 0.100436732172966 relative L2 0.015336669981479645\n",
      "training 0.10013460367918015 relative L2 0.015313277021050453\n",
      "training 0.09982927143573761 relative L2 0.015289765782654285\n",
      "training 0.0995221883058548 relative L2 0.015266155824065208\n",
      "training 0.09921448677778244 relative L2 0.015242325142025948\n",
      "training 0.09890476614236832 relative L2 0.015218871645629406\n",
      "training 0.09860020875930786 relative L2 0.015195795334875584\n",
      "training 0.09830101579427719 relative L2 0.01517298724502325\n",
      "training 0.09800597280263901 relative L2 0.01515029463917017\n",
      "training 0.09771301597356796 relative L2 0.01512770913541317\n",
      "training 0.09742164611816406 relative L2 0.01510517206043005\n",
      "training 0.09713122993707657 relative L2 0.015082670375704765\n",
      "training 0.09684184938669205 relative L2 0.015060210600495338\n",
      "training 0.0965532585978508 relative L2 0.015037809498608112\n",
      "training 0.09626568108797073 relative L2 0.015015500597655773\n",
      "training 0.09597986191511154 relative L2 0.014993312768638134\n",
      "training 0.09569594264030457 relative L2 0.01497124508023262\n",
      "training 0.09541388601064682 relative L2 0.014949335716664791\n",
      "training 0.09513441473245621 relative L2 0.014927552081644535\n",
      "training 0.09485693275928497 relative L2 0.014905868098139763\n",
      "training 0.09458097070455551 relative L2 0.014884249307215214\n",
      "training 0.09430637210607529 relative L2 0.014862682670354843\n",
      "training 0.0940328985452652 relative L2 0.014841176569461823\n",
      "training 0.09376048296689987 relative L2 0.014819754287600517\n",
      "training 0.09348953515291214 relative L2 0.014798375777900219\n",
      "training 0.09321969747543335 relative L2 0.014777095057070255\n",
      "training 0.09295143932104111 relative L2 0.014755931682884693\n",
      "training 0.09268496185541153 relative L2 0.014734853059053421\n",
      "training 0.09241995960474014 relative L2 0.01471385732293129\n",
      "training 0.09215644001960754 relative L2 0.014692950993776321\n",
      "training 0.09189429134130478 relative L2 0.01467210240662098\n",
      "training 0.09163332730531693 relative L2 0.01465130876749754\n",
      "training 0.09137342870235443 relative L2 0.014630578458309174\n",
      "training 0.09111462533473969 relative L2 0.014609934762120247\n",
      "training 0.09085725247859955 relative L2 0.014589322730898857\n",
      "training 0.09060079604387283 relative L2 0.014568784274160862\n",
      "training 0.09034552425146103 relative L2 0.01454832311719656\n",
      "training 0.0900915190577507 relative L2 0.014527938328683376\n",
      "training 0.08983887732028961 relative L2 0.014507601037621498\n",
      "training 0.08958722651004791 relative L2 0.014487345702946186\n",
      "training 0.0893368124961853 relative L2 0.014467151835560799\n",
      "training 0.089087575674057 relative L2 0.01444702222943306\n",
      "training 0.08883944898843765 relative L2 0.014426935464143753\n",
      "training 0.08859218657016754 relative L2 0.01440691202878952\n",
      "training 0.08834603428840637 relative L2 0.01438692957162857\n",
      "training 0.08810080587863922 relative L2 0.014367010444402695\n",
      "training 0.08785669505596161 relative L2 0.014347139745950699\n",
      "training 0.0876135304570198 relative L2 0.014327319338917732\n",
      "training 0.0873713567852974 relative L2 0.01430756226181984\n",
      "training 0.08713020384311676 relative L2 0.014287863858044147\n",
      "training 0.08689015358686447 relative L2 0.014268208295106888\n",
      "training 0.08665095269680023 relative L2 0.014248594641685486\n",
      "training 0.08641256392002106 relative L2 0.014229046180844307\n",
      "training 0.08617532253265381 relative L2 0.014209539629518986\n",
      "training 0.08593887835741043 relative L2 0.014190064743161201\n",
      "training 0.08570314198732376 relative L2 0.014170646667480469\n",
      "training 0.08546841889619827 relative L2 0.014151288196444511\n",
      "training 0.08523474633693695 relative L2 0.014131959527730942\n",
      "training 0.08500175923109055 relative L2 0.01411268301308155\n",
      "training 0.08476970344781876 relative L2 0.014093422330915928\n",
      "training 0.08453822135925293 relative L2 0.01407423336058855\n",
      "training 0.08430792391300201 relative L2 0.014055085368454456\n",
      "training 0.08407840877771378 relative L2 0.014035973697900772\n",
      "training 0.08384962379932404 relative L2 0.014016911387443542\n",
      "training 0.08362175524234772 relative L2 0.013997934758663177\n",
      "training 0.08339512348175049 relative L2 0.013979003764688969\n",
      "training 0.08316931128501892 relative L2 0.013960112817585468\n",
      "training 0.08294431120157242 relative L2 0.013941296376287937\n",
      "training 0.08272052556276321 relative L2 0.01392251718789339\n",
      "training 0.08249751478433609 relative L2 0.013903776183724403\n",
      "training 0.08227530121803284 relative L2 0.013885065913200378\n",
      "training 0.0820537805557251 relative L2 0.013866403140127659\n",
      "training 0.08183309435844421 relative L2 0.013847759924829006\n",
      "training 0.08161301910877228 relative L2 0.013829166069626808\n",
      "training 0.0813937559723854 relative L2 0.013810591772198677\n",
      "training 0.08117502182722092 relative L2 0.013792052865028381\n",
      "training 0.08095700293779373 relative L2 0.013773546554148197\n",
      "training 0.08073970675468445 relative L2 0.013755077496170998\n",
      "training 0.08052308112382889 relative L2 0.013736647553741932\n",
      "training 0.080307237803936 relative L2 0.013718241825699806\n",
      "training 0.08009199798107147 relative L2 0.01369987241923809\n",
      "training 0.07987746596336365 relative L2 0.013681535609066486\n",
      "training 0.07966357469558716 relative L2 0.013663249090313911\n",
      "training 0.07945062965154648 relative L2 0.01364497933536768\n",
      "training 0.07923812419176102 relative L2 0.013626751489937305\n",
      "training 0.07902639359235764 relative L2 0.013608541339635849\n",
      "training 0.07881517708301544 relative L2 0.013590382412075996\n",
      "training 0.0786048024892807 relative L2 0.013572248630225658\n",
      "training 0.07839497923851013 relative L2 0.013554124161601067\n",
      "training 0.07818564027547836 relative L2 0.013536052778363228\n",
      "training 0.07797718793153763 relative L2 0.013518022373318672\n",
      "training 0.07776941359043121 relative L2 0.013500007800757885\n",
      "training 0.07756214588880539 relative L2 0.013482017442584038\n",
      "training 0.07735544443130493 relative L2 0.013464055024087429\n",
      "training 0.07714928686618805 relative L2 0.013446137309074402\n",
      "training 0.07694391161203384 relative L2 0.013428225181996822\n",
      "training 0.07673894613981247 relative L2 0.013410339131951332\n",
      "training 0.0765344575047493 relative L2 0.013392500579357147\n",
      "training 0.07633081078529358 relative L2 0.013374684378504753\n",
      "training 0.07612768560647964 relative L2 0.013356887735426426\n",
      "training 0.0759250745177269 relative L2 0.013339114375412464\n",
      "training 0.07572298496961594 relative L2 0.01332137081772089\n",
      "training 0.07552149146795273 relative L2 0.013303660787642002\n",
      "training 0.07532065361738205 relative L2 0.013285965658724308\n",
      "training 0.07512025535106659 relative L2 0.013268312439322472\n",
      "training 0.07492058724164963 relative L2 0.013250666670501232\n",
      "training 0.07472126930952072 relative L2 0.013233054429292679\n",
      "training 0.07452256232500076 relative L2 0.013215478509664536\n",
      "training 0.07432451844215393 relative L2 0.013197907246649265\n",
      "training 0.07412684708833694 relative L2 0.013180368579924107\n",
      "training 0.07392976433038712 relative L2 0.013162853196263313\n",
      "training 0.07373321056365967 relative L2 0.01314536202698946\n",
      "training 0.07353722304105759 relative L2 0.013127893209457397\n",
      "training 0.07334169000387192 relative L2 0.013110455125570297\n",
      "training 0.07314678281545639 relative L2 0.013093022629618645\n",
      "training 0.07295221090316772 relative L2 0.01307561807334423\n",
      "training 0.07275824248790741 relative L2 0.013058244250714779\n",
      "training 0.07256483286619186 relative L2 0.013040886260569096\n",
      "training 0.07237192243337631 relative L2 0.01302353572100401\n",
      "training 0.07217936962842941 relative L2 0.01300622895359993\n",
      "training 0.07198755443096161 relative L2 0.012988932430744171\n",
      "training 0.07179608196020126 relative L2 0.012971652671694756\n",
      "training 0.07160509377717972 relative L2 0.012954405508935452\n",
      "training 0.07141467183828354 relative L2 0.012937173247337341\n",
      "training 0.07122471183538437 relative L2 0.012919951230287552\n",
      "training 0.07103513181209564 relative L2 0.012902770191431046\n",
      "training 0.07084625214338303 relative L2 0.012885591015219688\n",
      "training 0.07065761834383011 relative L2 0.012868440710008144\n",
      "training 0.07046958059072495 relative L2 0.012851313687860966\n",
      "training 0.07028205692768097 relative L2 0.012834192253649235\n",
      "training 0.07009480148553848 relative L2 0.012817098759114742\n",
      "training 0.06990818679332733 relative L2 0.012800004333257675\n",
      "training 0.06972182542085648 relative L2 0.012782962061464787\n",
      "training 0.06953629851341248 relative L2 0.012765920720994473\n",
      "training 0.06935103237628937 relative L2 0.012748881243169308\n",
      "training 0.06916604936122894 relative L2 0.012731878086924553\n",
      "training 0.06898164749145508 relative L2 0.012714880518615246\n",
      "training 0.06879755854606628 relative L2 0.012697904370725155\n",
      "training 0.06861400604248047 relative L2 0.012680945917963982\n",
      "training 0.0684308409690857 relative L2 0.01266399770975113\n",
      "training 0.06824803352355957 relative L2 0.012647057883441448\n",
      "training 0.06806554645299911 relative L2 0.012630131095647812\n",
      "training 0.0678834617137909 relative L2 0.012613212689757347\n",
      "training 0.06770170480012894 relative L2 0.01259632222354412\n",
      "training 0.06752042472362518 relative L2 0.012579419650137424\n",
      "training 0.06733930855989456 relative L2 0.0125625254586339\n",
      "training 0.06715840101242065 relative L2 0.012545622885227203\n",
      "training 0.0669775977730751 relative L2 0.012528680264949799\n",
      "training 0.06679660826921463 relative L2 0.01251166220754385\n",
      "training 0.06661508232355118 relative L2 0.012494457885622978\n",
      "training 0.06643202900886536 relative L2 0.012476785108447075\n",
      "training 0.06624427437782288 relative L2 0.01245727390050888\n",
      "training 0.06603776663541794 relative L2 0.012435697950422764\n",
      "training 0.06580810993909836 relative L2 0.012418948113918304\n",
      "training 0.06563404202461243 relative L2 0.012402546592056751\n",
      "training 0.06545883417129517 relative L2 0.012386324815452099\n",
      "training 0.06528571248054504 relative L2 0.012369401752948761\n",
      "training 0.06511017680168152 relative L2 0.01235265750437975\n",
      "training 0.0649324581027031 relative L2 0.012336010113358498\n",
      "training 0.06475596874952316 relative L2 0.01231878437101841\n",
      "training 0.0645776242017746 relative L2 0.012301811017096043\n",
      "training 0.06439854949712753 relative L2 0.012284963391721249\n",
      "training 0.06422096490859985 relative L2 0.01226772740483284\n",
      "training 0.06404294818639755 relative L2 0.01225074753165245\n",
      "training 0.06386474519968033 relative L2 0.01223387848585844\n",
      "training 0.06368792057037354 relative L2 0.01221670862287283\n",
      "training 0.06351116299629211 relative L2 0.012199806049466133\n",
      "training 0.06333468854427338 relative L2 0.012183016166090965\n",
      "training 0.06315965950489044 relative L2 0.012165996246039867\n",
      "training 0.06298501044511795 relative L2 0.012149236164987087\n",
      "training 0.06281081587076187 relative L2 0.012132542207837105\n",
      "training 0.0626378133893013 relative L2 0.012115681543946266\n",
      "training 0.06246526166796684 relative L2 0.012099042534828186\n",
      "training 0.06229301914572716 relative L2 0.012082461267709732\n",
      "training 0.06212200969457626 relative L2 0.012066179886460304\n",
      "training 0.06195544824004173 relative L2 0.012049508281052113\n",
      "training 0.061782106757164 relative L2 0.012032628059387207\n",
      "training 0.06161217764019966 relative L2 0.012016158550977707\n",
      "training 0.061441950500011444 relative L2 0.01199994795024395\n",
      "training 0.061274606734514236 relative L2 0.011983216740190983\n",
      "training 0.061106424778699875 relative L2 0.011966831050813198\n",
      "training 0.06093782186508179 relative L2 0.011950600892305374\n",
      "training 0.0607713982462883 relative L2 0.011933977715671062\n",
      "training 0.060604676604270935 relative L2 0.011917666532099247\n",
      "training 0.060437604784965515 relative L2 0.011901389807462692\n",
      "training 0.06027182564139366 relative L2 0.01188484113663435\n",
      "training 0.06010622903704643 relative L2 0.011868556961417198\n",
      "training 0.059940237551927567 relative L2 0.011852252297103405\n",
      "training 0.05977519229054451 relative L2 0.011835770681500435\n",
      "training 0.05961061269044876 relative L2 0.011819533072412014\n",
      "training 0.05944589525461197 relative L2 0.011803237721323967\n",
      "training 0.059281960129737854 relative L2 0.011786838993430138\n",
      "training 0.05911857262253761 relative L2 0.011770659126341343\n",
      "training 0.058955203741788864 relative L2 0.011754392646253109\n",
      "training 0.058792468160390854 relative L2 0.011738107539713383\n",
      "training 0.05863061547279358 relative L2 0.011722002178430557\n",
      "training 0.05846874788403511 relative L2 0.011705782264471054\n",
      "training 0.05830737203359604 relative L2 0.011689573526382446\n",
      "training 0.05814661830663681 relative L2 0.011673541739583015\n",
      "training 0.05798623338341713 relative L2 0.01165736187249422\n",
      "training 0.05782603845000267 relative L2 0.011641230434179306\n",
      "training 0.057666484266519547 relative L2 0.011625227518379688\n",
      "training 0.05750722438097 relative L2 0.011609102599322796\n",
      "training 0.05734835937619209 relative L2 0.01159304566681385\n",
      "training 0.0571899451315403 relative L2 0.011577095836400986\n",
      "training 0.05703200027346611 relative L2 0.011561024934053421\n",
      "training 0.05687433108687401 relative L2 0.011545040644705296\n",
      "training 0.056717097759246826 relative L2 0.011529132723808289\n",
      "training 0.05656040832400322 relative L2 0.011513136327266693\n",
      "training 0.05640409514307976 relative L2 0.011497230269014835\n",
      "training 0.056248102337121964 relative L2 0.01148135680705309\n",
      "training 0.05609258636832237 relative L2 0.011465453542768955\n",
      "training 0.055937327444553375 relative L2 0.011449539102613926\n",
      "training 0.05578265339136124 relative L2 0.011433754116296768\n",
      "training 0.055628202855587006 relative L2 0.011417890898883343\n",
      "training 0.0554741770029068 relative L2 0.011402052827179432\n",
      "training 0.05532079562544823 relative L2 0.011386396363377571\n",
      "training 0.055167850106954575 relative L2 0.01137052197009325\n",
      "training 0.05501483380794525 relative L2 0.011354763060808182\n",
      "training 0.054862409830093384 relative L2 0.011339119635522366\n",
      "training 0.0547105148434639 relative L2 0.011323317885398865\n",
      "training 0.0545588843524456 relative L2 0.011307635344564915\n",
      "training 0.0544075071811676 relative L2 0.011292008683085442\n",
      "training 0.05425672605633736 relative L2 0.011276268400251865\n",
      "training 0.05410625785589218 relative L2 0.011260653845965862\n",
      "training 0.053955983370542526 relative L2 0.011245034635066986\n",
      "training 0.05380621552467346 relative L2 0.011229367926716805\n",
      "training 0.05365694314241409 relative L2 0.011213820427656174\n",
      "training 0.05350792780518532 relative L2 0.01119821984320879\n",
      "training 0.05335921794176102 relative L2 0.01118263229727745\n",
      "training 0.05321100354194641 relative L2 0.011167132295668125\n",
      "training 0.05306310951709747 relative L2 0.01115155965089798\n",
      "training 0.05291547626256943 relative L2 0.011136040091514587\n",
      "training 0.05276830494403839 relative L2 0.0111205680295825\n",
      "training 0.05262142792344093 relative L2 0.011105048470199108\n",
      "training 0.052474990487098694 relative L2 0.011089587584137917\n",
      "training 0.05232880637049675 relative L2 0.011074146255850792\n",
      "training 0.05218299850821495 relative L2 0.011058668605983257\n",
      "training 0.05203752964735031 relative L2 0.011043264530599117\n",
      "training 0.05189241096377373 relative L2 0.011027859523892403\n",
      "training 0.05174769461154938 relative L2 0.011012440547347069\n",
      "training 0.05160323902964592 relative L2 0.010997083969414234\n",
      "training 0.05145915225148201 relative L2 0.010981710627675056\n",
      "training 0.05131545290350914 relative L2 0.010966354049742222\n",
      "training 0.051172077655792236 relative L2 0.010951039381325245\n",
      "training 0.05102904140949249 relative L2 0.010935713537037373\n",
      "training 0.05088641121983528 relative L2 0.01092039979994297\n",
      "training 0.0507439523935318 relative L2 0.010905119590461254\n",
      "training 0.05060192197561264 relative L2 0.010889831930398941\n",
      "training 0.05046027526259422 relative L2 0.010874586179852486\n",
      "training 0.05031893029808998 relative L2 0.010859346017241478\n",
      "training 0.05017793923616409 relative L2 0.010844101198017597\n",
      "training 0.05003722384572029 relative L2 0.010828900150954723\n",
      "training 0.04989686235785484 relative L2 0.010813688859343529\n",
      "training 0.04975682869553566 relative L2 0.010798501782119274\n",
      "training 0.049617137759923935 relative L2 0.010783337987959385\n",
      "training 0.049477752298116684 relative L2 0.010768166743218899\n",
      "training 0.049338676035404205 relative L2 0.010753030888736248\n",
      "training 0.04920000955462456 relative L2 0.010737903416156769\n",
      "training 0.0490616150200367 relative L2 0.010722784325480461\n",
      "training 0.04892360046505928 relative L2 0.01070769689977169\n",
      "training 0.04878589138388634 relative L2 0.010692602023482323\n",
      "training 0.04864842817187309 relative L2 0.010677527636289597\n",
      "training 0.048511359840631485 relative L2 0.010662478394806385\n",
      "training 0.04837460443377495 relative L2 0.010647427290678024\n",
      "training 0.048238176852464676 relative L2 0.010632392950356007\n",
      "training 0.04810198023915291 relative L2 0.010617385618388653\n",
      "training 0.04796624928712845 relative L2 0.010602371767163277\n",
      "training 0.0478307269513607 relative L2 0.010587404482066631\n",
      "training 0.04769567400217056 relative L2 0.010572421364486217\n",
      "training 0.04756078124046326 relative L2 0.010557456873357296\n",
      "training 0.04742623493075371 relative L2 0.010542522184550762\n",
      "training 0.047292064875364304 relative L2 0.01052758377045393\n",
      "training 0.047158174216747284 relative L2 0.010512669570744038\n",
      "training 0.04702460765838623 relative L2 0.01049777027219534\n",
      "training 0.04689134657382965 relative L2 0.010482873767614365\n",
      "training 0.04675838723778725 relative L2 0.010468012653291225\n",
      "training 0.04662583768367767 relative L2 0.010453145019710064\n",
      "training 0.04649348184466362 relative L2 0.01043829694390297\n",
      "training 0.046361472457647324 relative L2 0.010423465631902218\n",
      "training 0.046229783445596695 relative L2 0.01040863897651434\n",
      "training 0.046098366379737854 relative L2 0.010393839329481125\n",
      "training 0.04596731439232826 relative L2 0.010379048064351082\n",
      "training 0.04583657160401344 relative L2 0.01036426518112421\n",
      "training 0.04570608213543892 relative L2 0.010349506512284279\n",
      "training 0.045575983822345734 relative L2 0.010334755294024944\n",
      "training 0.04544615000486374 relative L2 0.010320024564862251\n",
      "training 0.045316677540540695 relative L2 0.010305292904376984\n",
      "training 0.04518740251660347 relative L2 0.010290592908859253\n",
      "training 0.04505857452750206 relative L2 0.010275891982018948\n",
      "training 0.04492991045117378 relative L2 0.010261215269565582\n",
      "training 0.04480165243148804 relative L2 0.010246544145047665\n",
      "training 0.0446736104786396 relative L2 0.01023188978433609\n",
      "training 0.04454594478011131 relative L2 0.010217268019914627\n",
      "training 0.044418688863515854 relative L2 0.01020263135433197\n",
      "training 0.04429152235388756 relative L2 0.010188024491071701\n",
      "training 0.04416479542851448 relative L2 0.010173429735004902\n",
      "training 0.04403834044933319 relative L2 0.01015884056687355\n",
      "training 0.04391210898756981 relative L2 0.01014427375048399\n",
      "training 0.04378625750541687 relative L2 0.010129719972610474\n",
      "training 0.04366070032119751 relative L2 0.010115173645317554\n",
      "training 0.04353538900613785 relative L2 0.010100645944476128\n",
      "training 0.04341043904423714 relative L2 0.01008613407611847\n",
      "training 0.04328574985265732 relative L2 0.010071630589663982\n",
      "training 0.04316135495901108 relative L2 0.010057141073048115\n",
      "training 0.043037258088588715 relative L2 0.010042673908174038\n",
      "training 0.04291350394487381 relative L2 0.010028209537267685\n",
      "training 0.042789969593286514 relative L2 0.0100137609988451\n",
      "training 0.042666781693696976 relative L2 0.009999323636293411\n",
      "training 0.0425439327955246 relative L2 0.009984966367483139\n",
      "training 0.04242141544818878 relative L2 0.009970498271286488\n",
      "training 0.042299091815948486 relative L2 0.009956153109669685\n",
      "training 0.04217703640460968 relative L2 0.009941767901182175\n",
      "training 0.04205537959933281 relative L2 0.009927382692694664\n",
      "training 0.04193390533328056 relative L2 0.009913078509271145\n",
      "training 0.04181278124451637 relative L2 0.009898689575493336\n",
      "training 0.04169190302491188 relative L2 0.009884407743811607\n",
      "training 0.04157136753201485 relative L2 0.009870074689388275\n",
      "training 0.041450947523117065 relative L2 0.009855765849351883\n",
      "training 0.041331034153699875 relative L2 0.009841524995863438\n",
      "training 0.0412113182246685 relative L2 0.009827205911278725\n",
      "training 0.041091855615377426 relative L2 0.009812983684241772\n",
      "training 0.040972717106342316 relative L2 0.00979873351752758\n",
      "training 0.04085387662053108 relative L2 0.009784482419490814\n",
      "training 0.04073525220155716 relative L2 0.009770308621227741\n",
      "training 0.04061706364154816 relative L2 0.009756059385836124\n",
      "training 0.04049898311495781 relative L2 0.009741898626089096\n",
      "training 0.04038125276565552 relative L2 0.009727717377245426\n",
      "training 0.04026388004422188 relative L2 0.00971353892236948\n",
      "training 0.04014673829078674 relative L2 0.009699422866106033\n",
      "training 0.04002988338470459 relative L2 0.009685249999165535\n",
      "training 0.03991326317191124 relative L2 0.009671161882579327\n",
      "training 0.03979696333408356 relative L2 0.009657032787799835\n",
      "training 0.039680905640125275 relative L2 0.00964293908327818\n",
      "training 0.03956519067287445 relative L2 0.00962887518107891\n",
      "training 0.03944971412420273 relative L2 0.009614779613912106\n",
      "training 0.039334531873464584 relative L2 0.009600741788744926\n",
      "training 0.03921954333782196 relative L2 0.009586679749190807\n",
      "training 0.0391048900783062 relative L2 0.009572655893862247\n",
      "training 0.03899051249027252 relative L2 0.009558651596307755\n",
      "training 0.03887645900249481 relative L2 0.009544623084366322\n",
      "training 0.03876258805394173 relative L2 0.009530656971037388\n",
      "training 0.0386490561068058 relative L2 0.009516660124063492\n",
      "training 0.038535770028829575 relative L2 0.009502707049250603\n",
      "training 0.038422781974077225 relative L2 0.009488753043115139\n",
      "training 0.038310013711452484 relative L2 0.009474806487560272\n",
      "training 0.03819755092263222 relative L2 0.009460903704166412\n",
      "training 0.03808540478348732 relative L2 0.009446969255805016\n",
      "training 0.03797345235943794 relative L2 0.0094330795109272\n",
      "training 0.037861768156290054 relative L2 0.00941919069737196\n",
      "training 0.037750426679849625 relative L2 0.009405315853655338\n",
      "training 0.0376393124461174 relative L2 0.00939145591109991\n",
      "training 0.037528425455093384 relative L2 0.00937760341912508\n",
      "training 0.03741788864135742 relative L2 0.009363780729472637\n",
      "training 0.03730756416916847 relative L2 0.009349939413368702\n",
      "training 0.03719747066497803 relative L2 0.009336145594716072\n",
      "training 0.037087734788656235 relative L2 0.00932234525680542\n",
      "training 0.03697820380330086 relative L2 0.009308556094765663\n",
      "training 0.036868926137685776 relative L2 0.009294797666370869\n",
      "training 0.036759961396455765 relative L2 0.009281015954911709\n",
      "training 0.036651164293289185 relative L2 0.009267296642065048\n",
      "training 0.03654278814792633 relative L2 0.00925354938954115\n",
      "training 0.036434564739465714 relative L2 0.00923983845859766\n",
      "training 0.03632661700248718 relative L2 0.009226121939718723\n",
      "training 0.03621889650821686 relative L2 0.009212436154484749\n",
      "training 0.03611154854297638 relative L2 0.00919876154512167\n",
      "training 0.03600439056754112 relative L2 0.009185095317661762\n",
      "training 0.03589756414294243 relative L2 0.0091714384034276\n",
      "training 0.03579083830118179 relative L2 0.009157797321677208\n",
      "training 0.035684529691934586 relative L2 0.00914416741579771\n",
      "training 0.03557836264371872 relative L2 0.009130553342401981\n",
      "training 0.035472527146339417 relative L2 0.009116949513554573\n",
      "training 0.03536691889166832 relative L2 0.00910335872322321\n",
      "training 0.03526155650615692 relative L2 0.009089771658182144\n",
      "training 0.03515642508864403 relative L2 0.00907620694488287\n",
      "training 0.03505155071616173 relative L2 0.009062645025551319\n",
      "training 0.03494691848754883 relative L2 0.009049099870026112\n",
      "training 0.03484252840280533 relative L2 0.0090355658903718\n",
      "training 0.03473839536309242 relative L2 0.00902204867452383\n",
      "training 0.034634530544281006 relative L2 0.009008537977933884\n",
      "training 0.0345308743417263 relative L2 0.008995024487376213\n",
      "training 0.03442738577723503 relative L2 0.008981551043689251\n",
      "training 0.0343242809176445 relative L2 0.008968060836195946\n",
      "training 0.034221306443214417 relative L2 0.00895459670573473\n",
      "training 0.03411854803562164 relative L2 0.008941133506596088\n",
      "training 0.03401609882712364 relative L2 0.008927691727876663\n",
      "training 0.033913854509592056 relative L2 0.00891425646841526\n",
      "training 0.03381187841296196 relative L2 0.008900828659534454\n",
      "training 0.033710092306137085 relative L2 0.00888743158429861\n",
      "training 0.03360865265130997 relative L2 0.0088740149512887\n",
      "training 0.03350730240345001 relative L2 0.008860648609697819\n",
      "training 0.033406373113393784 relative L2 0.008847272023558617\n",
      "training 0.03330564498901367 relative L2 0.008833919651806355\n",
      "training 0.033205125480890274 relative L2 0.008820570074021816\n",
      "training 0.03310488536953926 relative L2 0.008807240054011345\n",
      "training 0.033004846423864365 relative L2 0.008793925866484642\n",
      "training 0.03290517255663872 relative L2 0.008780636824667454\n",
      "training 0.03280572593212128 relative L2 0.008767345920205116\n",
      "training 0.03270651400089264 relative L2 0.008754085749387741\n",
      "training 0.03260761499404907 relative L2 0.008740832097828388\n",
      "training 0.03250895440578461 relative L2 0.008727587759494781\n",
      "training 0.03241052106022835 relative L2 0.00871436856687069\n",
      "training 0.03231237828731537 relative L2 0.008701160550117493\n",
      "training 0.032214511185884476 relative L2 0.008687973022460938\n",
      "training 0.03211689367890358 relative L2 0.0086748031899333\n",
      "training 0.03201960772275925 relative L2 0.008661643601953983\n",
      "training 0.03192249312996864 relative L2 0.008648499846458435\n",
      "training 0.031825702637434006 relative L2 0.00863537099212408\n",
      "training 0.03172910213470459 relative L2 0.008622251451015472\n",
      "training 0.031632788479328156 relative L2 0.008609162643551826\n",
      "training 0.0315367728471756 relative L2 0.008596065454185009\n",
      "training 0.03144092112779617 relative L2 0.008583005517721176\n",
      "training 0.031345367431640625 relative L2 0.008569941855967045\n",
      "training 0.031250085681676865 relative L2 0.008556914515793324\n",
      "training 0.0311550535261631 relative L2 0.008543871343135834\n",
      "training 0.031060241162776947 relative L2 0.00853087566792965\n",
      "training 0.030965693295001984 relative L2 0.00851785484701395\n",
      "training 0.030871355906128883 relative L2 0.008504890836775303\n",
      "training 0.030777307227253914 relative L2 0.00849190168082714\n",
      "training 0.030683480203151703 relative L2 0.008478966541588306\n",
      "training 0.030589940026402473 relative L2 0.008466002531349659\n",
      "training 0.030496591702103615 relative L2 0.008453081361949444\n",
      "training 0.030403438955545425 relative L2 0.008440150879323483\n",
      "training 0.030310602858662605 relative L2 0.008427265100181103\n",
      "training 0.030218007043004036 relative L2 0.008414363488554955\n",
      "training 0.030125651508569717 relative L2 0.008401496335864067\n",
      "training 0.03003346361219883 relative L2 0.008388619869947433\n",
      "training 0.029941586777567863 relative L2 0.008375789038836956\n",
      "training 0.029849926009774208 relative L2 0.008362926542758942\n",
      "training 0.029758457094430923 relative L2 0.008350134827196598\n",
      "training 0.029667317867279053 relative L2 0.00833729188889265\n",
      "training 0.029576297849416733 relative L2 0.008324519731104374\n",
      "training 0.02948555164039135 relative L2 0.008311707526445389\n",
      "training 0.029395058751106262 relative L2 0.00829896330833435\n",
      "training 0.029304733499884605 relative L2 0.00828616600483656\n",
      "training 0.029214706271886826 relative L2 0.008273481391370296\n",
      "training 0.029124923050403595 relative L2 0.008260686881840229\n",
      "training 0.029035406187176704 relative L2 0.008248076774179935\n",
      "training 0.02894613705575466 relative L2 0.008235263638198376\n",
      "training 0.028857162222266197 relative L2 0.008222770877182484\n",
      "training 0.028768479824066162 relative L2 0.00820993259549141\n",
      "training 0.02868025377392769 relative L2 0.008197644725441933\n",
      "training 0.028592441231012344 relative L2 0.008184791542589664\n",
      "training 0.028505489230155945 relative L2 0.008172968402504921\n",
      "training 0.028419671580195427 relative L2 0.008160199970006943\n",
      "training 0.02833554707467556 relative L2 0.00814932119101286\n",
      "training 0.02825399860739708 relative L2 0.008136861957609653\n",
      "training 0.02817535027861595 relative L2 0.008127226494252682\n",
      "training 0.028099149465560913 relative L2 0.008114499039947987\n",
      "training 0.02802225574851036 relative L2 0.00810441467911005\n",
      "training 0.027940548956394196 relative L2 0.008089406415820122\n",
      "training 0.02784915640950203 relative L2 0.008076478727161884\n",
      "training 0.027749577537178993 relative L2 0.008060441352427006\n",
      "training 0.027647512033581734 relative L2 0.008047030307352543\n",
      "training 0.02755112387239933 relative L2 0.008034282363951206\n",
      "training 0.027464520186185837 relative L2 0.008022326976060867\n",
      "training 0.027385732159018517 relative L2 0.008011961355805397\n",
      "training 0.02730931155383587 relative L2 0.007999187335371971\n",
      "training 0.02722953073680401 relative L2 0.007987706921994686\n",
      "training 0.027143822982907295 relative L2 0.007973389700055122\n",
      "training 0.02705327235162258 relative L2 0.007960675284266472\n",
      "training 0.026962298899888992 relative L2 0.007947404868900776\n",
      "training 0.02687477506697178 relative L2 0.007935106754302979\n",
      "training 0.026791943237185478 relative L2 0.007923703640699387\n",
      "training 0.026712391525506973 relative L2 0.007911316119134426\n",
      "training 0.026633119210600853 relative L2 0.00789998285472393\n",
      "training 0.026551801711320877 relative L2 0.007886739447712898\n",
      "training 0.026467835530638695 relative L2 0.00787462666630745\n",
      "training 0.026382407173514366 relative L2 0.00786152109503746\n",
      "training 0.0262974314391613 relative L2 0.007849258370697498\n",
      "training 0.026214340701699257 relative L2 0.007837233133614063\n",
      "training 0.026133470237255096 relative L2 0.007824994623661041\n",
      "training 0.02605387382209301 relative L2 0.007813500240445137\n",
      "training 0.025974376127123833 relative L2 0.0078008840791881084\n",
      "training 0.025893984362483025 relative L2 0.007789123337715864\n",
      "training 0.025812586769461632 relative L2 0.007776340469717979\n",
      "training 0.025730717927217484 relative L2 0.007764313369989395\n",
      "training 0.025649232789874077 relative L2 0.007751972880214453\n",
      "training 0.025568615645170212 relative L2 0.007739882450550795\n",
      "training 0.02548910118639469 relative L2 0.007728084921836853\n",
      "training 0.02541026845574379 relative L2 0.007715853862464428\n",
      "training 0.02533174678683281 relative L2 0.007704215589910746\n",
      "training 0.025253085419535637 relative L2 0.0076918043196201324\n",
      "training 0.02517414651811123 relative L2 0.007680047303438187\n",
      "training 0.025095036253333092 relative L2 0.007667676545679569\n",
      "training 0.025016000494360924 relative L2 0.007655781228095293\n",
      "training 0.02493724599480629 relative L2 0.007643694989383221\n",
      "training 0.024859067052602768 relative L2 0.007631720043718815\n",
      "training 0.024781327694654465 relative L2 0.007619911804795265\n",
      "training 0.02470400743186474 relative L2 0.007607852108776569\n",
      "training 0.024626940488815308 relative L2 0.00759617006406188\n",
      "training 0.02454998530447483 relative L2 0.007584036327898502\n",
      "training 0.024473046883940697 relative L2 0.00757233751937747\n",
      "training 0.02439616620540619 relative L2 0.007560236845165491\n",
      "training 0.02431948110461235 relative L2 0.007548486813902855\n",
      "training 0.024242931976914406 relative L2 0.0075365048833191395\n",
      "training 0.024166671559214592 relative L2 0.007524699904024601\n",
      "training 0.024090653285384178 relative L2 0.007512868382036686\n",
      "training 0.02401493676006794 relative L2 0.007501016370952129\n",
      "training 0.02393949218094349 relative L2 0.007489314302802086\n",
      "training 0.023864297196269035 relative L2 0.007477425038814545\n",
      "training 0.023789256811141968 relative L2 0.007465796545147896\n",
      "training 0.023714477196335793 relative L2 0.00745390122756362\n",
      "training 0.02363981120288372 relative L2 0.0074422904290258884\n",
      "training 0.02356531471014023 relative L2 0.007430423051118851\n",
      "training 0.023491015657782555 relative L2 0.007418807130306959\n",
      "training 0.023416850715875626 relative L2 0.0074069919064641\n",
      "training 0.023342931643128395 relative L2 0.007395375985652208\n",
      "training 0.02326921373605728 relative L2 0.007383617572486401\n",
      "training 0.023195646703243256 relative L2 0.0073719993233680725\n",
      "training 0.0231223963201046 relative L2 0.007360317278653383\n",
      "training 0.02304929308593273 relative L2 0.007348679471760988\n",
      "training 0.022976398468017578 relative L2 0.007337067276239395\n",
      "training 0.02290372923016548 relative L2 0.007325423415750265\n",
      "training 0.022831235080957413 relative L2 0.007313868496567011\n",
      "training 0.022758958861231804 relative L2 0.007302229758352041\n",
      "training 0.022686874493956566 relative L2 0.007290722336620092\n",
      "training 0.02261500433087349 relative L2 0.007279091514647007\n",
      "training 0.022543299943208694 relative L2 0.007267622742801905\n",
      "training 0.022471822798252106 relative L2 0.007256004959344864\n",
      "training 0.022400476038455963 relative L2 0.007244568318128586\n",
      "training 0.022329378873109818 relative L2 0.00723297568038106\n",
      "training 0.022258471697568893 relative L2 0.007221573032438755\n",
      "training 0.02218773402273655 relative L2 0.00720999063923955\n",
      "training 0.02211719937622547 relative L2 0.007198633626103401\n",
      "training 0.022046884521842003 relative L2 0.007187065202742815\n",
      "training 0.021976757794618607 relative L2 0.007175753824412823\n",
      "training 0.021906815469264984 relative L2 0.00716419005766511\n",
      "training 0.02183709479868412 relative L2 0.007152940612286329\n",
      "training 0.021767569705843925 relative L2 0.007141378242522478\n",
      "training 0.02169829234480858 relative L2 0.007130203302949667\n",
      "training 0.021629195660352707 relative L2 0.0071186344139277935\n",
      "training 0.021560363471508026 relative L2 0.0071075535379350185\n",
      "training 0.0214917603880167 relative L2 0.007095973938703537\n",
      "training 0.021423427388072014 relative L2 0.007085021119564772\n",
      "training 0.021355386823415756 relative L2 0.007073426153510809\n",
      "training 0.02128770388662815 relative L2 0.007062680087983608\n",
      "training 0.021220482885837555 relative L2 0.007051099557429552\n",
      "training 0.021153844892978668 relative L2 0.007040688768029213\n",
      "training 0.021087931469082832 relative L2 0.007029163651168346\n",
      "training 0.021022949367761612 relative L2 0.0070193144492805\n",
      "training 0.02095923200249672 relative L2 0.007007969543337822\n",
      "training 0.020897123962640762 relative L2 0.006999044679105282\n",
      "training 0.02083718776702881 relative L2 0.006988042965531349\n",
      "training 0.020779523998498917 relative L2 0.006980374455451965\n",
      "training 0.020724736154079437 relative L2 0.006969613488763571\n",
      "training 0.020671289414167404 relative L2 0.006962681654840708\n",
      "training 0.02061839960515499 relative L2 0.0069508906453847885\n",
      "training 0.020561235025525093 relative L2 0.006942419800907373\n",
      "training 0.020497992634773254 relative L2 0.006927662994712591\n",
      "training 0.020423706620931625 relative L2 0.006915672682225704\n",
      "training 0.020341429859399796 relative L2 0.006899202708154917\n",
      "training 0.02025419846177101 relative L2 0.0068860179744660854\n",
      "training 0.020170003175735474 relative L2 0.006872430443763733\n",
      "training 0.020094135776162148 relative L2 0.006861275993287563\n",
      "training 0.020028404891490936 relative L2 0.006851819343864918\n",
      "training 0.019970737397670746 relative L2 0.006841704715043306\n",
      "training 0.019916635006666183 relative L2 0.006833359133452177\n",
      "training 0.01986144669353962 relative L2 0.006821762770414352\n",
      "training 0.019801534712314606 relative L2 0.006811817642301321\n",
      "training 0.019736194983124733 relative L2 0.006798534654080868\n",
      "training 0.019666176289319992 relative L2 0.006787153892219067\n",
      "training 0.019594760611653328 relative L2 0.00677439896389842\n",
      "training 0.019524963572621346 relative L2 0.006763251032680273\n",
      "training 0.01945890486240387 relative L2 0.006752497982233763\n",
      "training 0.01939687505364418 relative L2 0.0067418660037219524\n",
      "training 0.019337719306349754 relative L2 0.00673231016844511\n",
      "training 0.019279565662145615 relative L2 0.006721285171806812\n",
      "training 0.01922070048749447 relative L2 0.006711503025144339\n",
      "training 0.01916002668440342 relative L2 0.006699705496430397\n",
      "training 0.019097380340099335 relative L2 0.006689229980111122\n",
      "training 0.019033443182706833 relative L2 0.006677329074591398\n",
      "training 0.018969157710671425 relative L2 0.006666559260338545\n",
      "training 0.018905607983469963 relative L2 0.006655380129814148\n",
      "training 0.018843451514840126 relative L2 0.006644680164754391\n",
      "training 0.01878277212381363 relative L2 0.006634320132434368\n",
      "training 0.018723268061876297 relative L2 0.006623564753681421\n",
      "training 0.01866433210670948 relative L2 0.006613535340875387\n",
      "training 0.018605437129735947 relative L2 0.006602522451430559\n",
      "training 0.018546205013990402 relative L2 0.0065924073569476604\n",
      "training 0.01848652958869934 relative L2 0.006581193767488003\n",
      "training 0.018426401540637016 relative L2 0.006570867728441954\n",
      "training 0.018366094678640366 relative L2 0.006559726782143116\n",
      "training 0.018305910751223564 relative L2 0.0065492745488882065\n",
      "training 0.018245983868837357 relative L2 0.006538407877087593\n",
      "training 0.018186496570706367 relative L2 0.00652789231389761\n",
      "training 0.018127484247088432 relative L2 0.006517342757433653\n",
      "training 0.01806889846920967 relative L2 0.006506767589598894\n",
      "training 0.018010715022683144 relative L2 0.006496461108326912\n",
      "training 0.01795280911028385 relative L2 0.006485798396170139\n",
      "training 0.017895039170980453 relative L2 0.006475604604929686\n",
      "training 0.017837410792708397 relative L2 0.0064648776315152645\n",
      "training 0.017779821529984474 relative L2 0.006454702466726303\n",
      "training 0.01772228628396988 relative L2 0.006443954072892666\n",
      "training 0.017664814367890358 relative L2 0.006433755159378052\n",
      "training 0.017607413232326508 relative L2 0.006423033308237791\n",
      "training 0.017550094053149223 relative L2 0.006412802264094353\n",
      "training 0.017492972314357758 relative L2 0.006402173079550266\n",
      "training 0.01743602566421032 relative L2 0.006391909904778004\n",
      "training 0.017379265278577805 relative L2 0.006381379906088114\n",
      "training 0.017322665080428123 relative L2 0.006371090654283762\n",
      "training 0.01726631261408329 relative L2 0.006360662169754505\n",
      "training 0.017210114747285843 relative L2 0.006350349634885788\n",
      "training 0.01715412363409996 relative L2 0.006340015679597855\n",
      "training 0.017098328098654747 relative L2 0.006329687312245369\n",
      "training 0.01704270765185356 relative L2 0.006319420412182808\n",
      "training 0.01698722131550312 relative L2 0.0063090818002820015\n",
      "training 0.0169319286942482 relative L2 0.006298885215073824\n",
      "training 0.01687680557370186 relative L2 0.006288540083914995\n",
      "training 0.016821840777993202 relative L2 0.00627839844673872\n",
      "training 0.01676701195538044 relative L2 0.006268046796321869\n",
      "training 0.016712382435798645 relative L2 0.0062579745426774025\n",
      "training 0.01665790192782879 relative L2 0.006247620563954115\n",
      "training 0.01660361886024475 relative L2 0.0062376284040510654\n",
      "training 0.01654953882098198 relative L2 0.0062272618524730206\n",
      "training 0.016495611518621445 relative L2 0.006217348389327526\n",
      "training 0.016441872343420982 relative L2 0.0062069701962172985\n",
      "training 0.016388339921832085 relative L2 0.006197182461619377\n",
      "training 0.01633508689701557 relative L2 0.0061867982149124146\n",
      "training 0.016282113268971443 relative L2 0.006177193485200405\n",
      "training 0.01622951216995716 relative L2 0.0061668166890740395\n",
      "training 0.016177326440811157 relative L2 0.006157493218779564\n",
      "training 0.01612566038966179 relative L2 0.006147164851427078\n",
      "training 0.016074752435088158 relative L2 0.006138348951935768\n",
      "training 0.01602485030889511 relative L2 0.00612820778042078\n",
      "training 0.015976352617144585 relative L2 0.006120305974036455\n",
      "training 0.01592981442809105 relative L2 0.006110639777034521\n",
      "training 0.015885774046182632 relative L2 0.00610432168468833\n",
      "training 0.01584537886083126 relative L2 0.006095643620938063\n",
      "training 0.015809141099452972 relative L2 0.00609173346310854\n",
      "training 0.015778381377458572 relative L2 0.006084141321480274\n",
      "training 0.015751030296087265 relative L2 0.006082063540816307\n",
      "training 0.015726519748568535 relative L2 0.006073161028325558\n",
      "training 0.015695415437221527 relative L2 0.006068159826099873\n",
      "training 0.01565377227962017 relative L2 0.006052570883184671\n",
      "training 0.015588955022394657 relative L2 0.006039185915142298\n",
      "training 0.015505844727158546 relative L2 0.006018224637955427\n",
      "training 0.015410050749778748 relative L2 0.006002307403832674\n",
      "training 0.015320556238293648 relative L2 0.005987680517137051\n",
      "training 0.015249887481331825 relative L2 0.005978187080472708\n",
      "training 0.015201777219772339 relative L2 0.005972631741315126\n",
      "training 0.015169433318078518 relative L2 0.005965643562376499\n",
      "training 0.01514080073684454 relative L2 0.005960340145975351\n",
      "training 0.015105107799172401 relative L2 0.005948662757873535\n",
      "training 0.015055200085043907 relative L2 0.005938136950135231\n",
      "training 0.014993321150541306 relative L2 0.00592338340356946\n",
      "training 0.014925776049494743 relative L2 0.0059116254560649395\n",
      "training 0.014862089417874813 relative L2 0.005900490563362837\n",
      "training 0.01480777096003294 relative L2 0.005891397595405579\n",
      "training 0.01476318296045065 relative L2 0.005884265527129173\n",
      "training 0.014724005945026875 relative L2 0.0058753276243805885\n",
      "training 0.014684299938380718 relative L2 0.005867621395736933\n",
      "training 0.01463986188173294 relative L2 0.005856299307197332\n",
      "training 0.014589269645512104 relative L2 0.005846391431987286\n",
      "training 0.014534741640090942 relative L2 0.005834572017192841\n",
      "training 0.014479818753898144 relative L2 0.005824513733386993\n",
      "training 0.01442774012684822 relative L2 0.005814776755869389\n",
      "training 0.014379870146512985 relative L2 0.005805545020848513\n",
      "training 0.014335458166897297 relative L2 0.005797382444143295\n",
      "training 0.014292542822659016 relative L2 0.005787829402834177\n",
      "training 0.014248874969780445 relative L2 0.005779340397566557\n",
      "training 0.014203212223947048 relative L2 0.005768843926489353\n",
      "training 0.014155371114611626 relative L2 0.005759506952017546\n",
      "training 0.014106291346251965 relative L2 0.00574899697676301\n",
      "training 0.01405723113566637 relative L2 0.005739497486501932\n",
      "training 0.014009342528879642 relative L2 0.00572994677349925\n",
      "training 0.01396307721734047 relative L2 0.005720623768866062\n",
      "training 0.013918237760663033 relative L2 0.005711864680051804\n",
      "training 0.01387414988130331 relative L2 0.005702374503016472\n",
      "training 0.013830067589879036 relative L2 0.005693687126040459\n",
      "training 0.013785529881715775 relative L2 0.005683842580765486\n",
      "training 0.013740288093686104 relative L2 0.005674857646226883\n",
      "training 0.01369454525411129 relative L2 0.00566493347287178\n",
      "training 0.01364857517182827 relative L2 0.0056557441130280495\n",
      "training 0.013602831400930882 relative L2 0.005646157544106245\n",
      "training 0.013557580299675465 relative L2 0.005636926274746656\n",
      "training 0.013512938283383846 relative L2 0.005627783481031656\n",
      "training 0.013468831777572632 relative L2 0.0056184930726885796\n",
      "training 0.013425065204501152 relative L2 0.005609608255326748\n",
      "training 0.013381506316363811 relative L2 0.005600210279226303\n",
      "training 0.013337978161871433 relative L2 0.005591363646090031\n",
      "training 0.013294359669089317 relative L2 0.00558185949921608\n",
      "training 0.01325062196701765 relative L2 0.005572941619902849\n",
      "training 0.013206876814365387 relative L2 0.005563466809689999\n",
      "training 0.013163178227841854 relative L2 0.0055544693022966385\n",
      "training 0.013119574636220932 relative L2 0.005545124411582947\n",
      "training 0.013076169416308403 relative L2 0.005536074284464121\n",
      "training 0.01303299143910408 relative L2 0.0055269161239266396\n",
      "training 0.012990065850317478 relative L2 0.005517825949937105\n",
      "training 0.012947351671755314 relative L2 0.005508821457624435\n",
      "training 0.012904818169772625 relative L2 0.005499689374119043\n",
      "training 0.012862457893788815 relative L2 0.005490787327289581\n",
      "training 0.01282020378857851 relative L2 0.005481616128236055\n",
      "training 0.012778069823980331 relative L2 0.005472772289067507\n",
      "training 0.012736025266349316 relative L2 0.005463591311126947\n",
      "training 0.01269411388784647 relative L2 0.005454775411635637\n",
      "training 0.012652267701923847 relative L2 0.0054455953650176525\n",
      "training 0.012610526755452156 relative L2 0.005436791107058525\n",
      "training 0.012568897567689419 relative L2 0.005427640397101641\n",
      "training 0.012527363374829292 relative L2 0.005418836604803801\n",
      "training 0.012485961429774761 relative L2 0.005409723613411188\n",
      "training 0.012444623745977879 relative L2 0.005400918889790773\n",
      "training 0.012403490021824837 relative L2 0.005391870159655809\n",
      "training 0.012362457811832428 relative L2 0.005383053794503212\n",
      "training 0.012321528047323227 relative L2 0.005374051630496979\n",
      "training 0.012280725874006748 relative L2 0.005365242250263691\n",
      "training 0.012240082956850529 relative L2 0.00535630201920867\n",
      "training 0.012199576944112778 relative L2 0.0053474921733140945\n",
      "training 0.012159168720245361 relative L2 0.005338593851774931\n",
      "training 0.012118909507989883 relative L2 0.005329793784767389\n",
      "training 0.012078751809895039 relative L2 0.005320931319147348\n",
      "training 0.012038730084896088 relative L2 0.005312152206897736\n",
      "training 0.011998845264315605 relative L2 0.005303320474922657\n",
      "training 0.011959048919379711 relative L2 0.005294561851769686\n",
      "training 0.011919420212507248 relative L2 0.005285762716084719\n",
      "training 0.011879891157150269 relative L2 0.005277021322399378\n",
      "training 0.011840482242405415 relative L2 0.005268245004117489\n",
      "training 0.011801169253885746 relative L2 0.005259523633867502\n",
      "training 0.01176199410110712 relative L2 0.0052507841028273106\n",
      "training 0.011722958646714687 relative L2 0.005242085084319115\n",
      "training 0.011684015393257141 relative L2 0.005233364179730415\n",
      "training 0.011645199730992317 relative L2 0.005224692635238171\n",
      "training 0.011606493033468723 relative L2 0.005215988028794527\n",
      "training 0.011567899957299232 relative L2 0.005207352805882692\n",
      "training 0.011529461480677128 relative L2 0.005198669619858265\n",
      "training 0.01149111706763506 relative L2 0.00519006559625268\n",
      "training 0.011452912352979183 relative L2 0.005181395448744297\n",
      "training 0.011414793320000172 relative L2 0.005172821693122387\n",
      "training 0.011376788839697838 relative L2 0.005164165515452623\n",
      "training 0.011338932439684868 relative L2 0.005155645776540041\n",
      "training 0.011301198042929173 relative L2 0.005146991927176714\n",
      "training 0.011263594962656498 relative L2 0.005138539243489504\n",
      "training 0.011226129718124866 relative L2 0.0051298863254487514\n",
      "training 0.01118885912001133 relative L2 0.005121544003486633\n",
      "training 0.011151734739542007 relative L2 0.005112874787300825\n",
      "training 0.01111485157161951 relative L2 0.005104735493659973\n",
      "training 0.011078297160565853 relative L2 0.005096086300909519\n",
      "training 0.01104217953979969 relative L2 0.005088340491056442\n",
      "training 0.011006738059222698 relative L2 0.005079860333353281\n",
      "training 0.010972387157380581 relative L2 0.005073041655123234\n",
      "training 0.010939838364720345 relative L2 0.0050652590580284595\n",
      "training 0.010910175740718842 relative L2 0.005060744937509298\n",
      "training 0.010885551571846008 relative L2 0.005055364686995745\n",
      "training 0.010868961922824383 relative L2 0.0050567458383738995\n",
      "training 0.010866200551390648 relative L2 0.005058049689978361\n",
      "training 0.010882762260735035 relative L2 0.0050721135921776295\n",
      "training 0.010929091833531857 relative L2 0.005084239412099123\n",
      "training 0.010998698882758617 relative L2 0.005108735058456659\n",
      "training 0.011083888821303844 relative L2 0.0051089986227452755\n",
      "training 0.011107967235147953 relative L2 0.005097075365483761\n",
      "training 0.01103297434747219 relative L2 0.005042803939431906\n",
      "training 0.010819457471370697 relative L2 0.004992516245692968\n",
      "training 0.010591220110654831 relative L2 0.0049625881947577\n",
      "training 0.010469919070601463 relative L2 0.004966869484633207\n",
      "training 0.010491495952010155 relative L2 0.004988550208508968\n",
      "training 0.010572199709713459 relative L2 0.004989183507859707\n",
      "training 0.010589485988020897 relative L2 0.004971740301698446\n",
      "training 0.010501020587980747 relative L2 0.0049352203495800495\n",
      "training 0.010358056053519249 relative L2 0.004914804827421904\n",
      "training 0.01026828307658434 relative L2 0.004915258847177029\n",
      "training 0.010267416946589947 relative L2 0.004920839797705412\n",
      "training 0.01029875222593546 relative L2 0.004920539446175098\n",
      "training 0.010286600328981876 relative L2 0.004899467341601849\n",
      "training 0.01020873337984085 relative L2 0.004879414103925228\n",
      "training 0.010118900798261166 relative L2 0.004867863841354847\n",
      "training 0.010072383098304272 relative L2 0.004866633098572493\n",
      "training 0.010070821270346642 relative L2 0.00486811064183712\n",
      "training 0.010069763287901878 relative L2 0.004856984131038189\n",
      "training 0.01003175787627697 relative L2 0.004842844326049089\n",
      "training 0.009966909885406494 relative L2 0.004828407894819975\n",
      "training 0.009910753928124905 relative L2 0.004821784794330597\n",
      "training 0.009884247556328773 relative L2 0.004820188507437706\n",
      "training 0.009873560629785061 relative L2 0.004813316278159618\n",
      "training 0.009851168841123581 relative L2 0.004804095719009638\n",
      "training 0.009807742200791836 relative L2 0.004790787119418383\n",
      "training 0.00975731946527958 relative L2 0.004781737923622131\n",
      "training 0.009719334542751312 relative L2 0.004776594694703817\n",
      "training 0.009696663357317448 relative L2 0.004770624917000532\n",
      "training 0.009676087647676468 relative L2 0.004764123819768429\n",
      "training 0.009645216166973114 relative L2 0.00475311977788806\n",
      "training 0.009604490362107754 relative L2 0.004743814002722502\n",
      "training 0.009564721025526524 relative L2 0.004736132919788361\n",
      "training 0.009533739648759365 relative L2 0.0047295838594436646\n",
      "training 0.009509203024208546 relative L2 0.004723843652755022\n",
      "training 0.00948301237076521 relative L2 0.004714908078312874\n",
      "training 0.009450364857912064 relative L2 0.004706480540335178\n",
      "training 0.009414040483534336 relative L2 0.00469767302274704\n",
      "training 0.009379943832755089 relative L2 0.004690299741923809\n",
      "training 0.009350836277008057 relative L2 0.0046840510331094265\n",
      "training 0.00932422187179327 relative L2 0.004676335491240025\n",
      "training 0.009295818395912647 relative L2 0.004668955225497484\n",
      "training 0.009264097549021244 relative L2 0.004660188220441341\n",
      "training 0.00923099834471941 relative L2 0.004652430769056082\n",
      "training 0.009199541062116623 relative L2 0.004645259585231543\n",
      "training 0.009170730598270893 relative L2 0.004637911915779114\n",
      "training 0.009142966009676456 relative L2 0.004631060175597668\n",
      "training 0.009114176034927368 relative L2 0.0046228948049247265\n",
      "training 0.009083699434995651 relative L2 0.004615313373506069\n",
      "training 0.009052643552422523 relative L2 0.004607522394508123\n",
      "training 0.009022536687552929 relative L2 0.004600120708346367\n",
      "training 0.008993802592158318 relative L2 0.00459319120272994\n",
      "training 0.008965740911662579 relative L2 0.004585559014230967\n",
      "training 0.008937169797718525 relative L2 0.004578352905809879\n",
      "training 0.00890779122710228 relative L2 0.00457047950476408\n",
      "training 0.008878069929778576 relative L2 0.004563063383102417\n",
      "training 0.008848770521581173 relative L2 0.004555741790682077\n",
      "training 0.008820224553346634 relative L2 0.004548363387584686\n",
      "training 0.008792195469141006 relative L2 0.004541325382888317\n",
      "training 0.00876407977193594 relative L2 0.004533703438937664\n",
      "training 0.008735556155443192 relative L2 0.004526461008936167\n",
      "training 0.00870683416724205 relative L2 0.004518922418355942\n",
      "training 0.008678250946104527 relative L2 0.004511613864451647\n",
      "training 0.00865007285028696 relative L2 0.004504440817981958\n",
      "training 0.008622241206467152 relative L2 0.004497066605836153\n",
      "training 0.008594522252678871 relative L2 0.004489982035011053\n",
      "training 0.008566731587052345 relative L2 0.004482502583414316\n",
      "training 0.008538787253201008 relative L2 0.004475309979170561\n",
      "training 0.00851088110357523 relative L2 0.004467974416911602\n",
      "training 0.008483153767883778 relative L2 0.004460727330297232\n",
      "training 0.008455644361674786 relative L2 0.00445362227037549\n",
      "training 0.00842839851975441 relative L2 0.004446336068212986\n",
      "training 0.008401267230510712 relative L2 0.004439358599483967\n",
      "training 0.008374192751944065 relative L2 0.0044319648295640945\n",
      "training 0.008346954360604286 relative L2 0.004424879793077707\n",
      "training 0.008319636806845665 relative L2 0.004417549818754196\n",
      "training 0.008292412385344505 relative L2 0.004410408902913332\n",
      "training 0.008265450596809387 relative L2 0.00440331781283021\n",
      "training 0.008238689973950386 relative L2 0.004396121017634869\n",
      "training 0.008212066255509853 relative L2 0.004389143083244562\n",
      "training 0.008185489103198051 relative L2 0.004381885752081871\n",
      "training 0.008158865384757519 relative L2 0.004374867305159569\n",
      "training 0.008132265880703926 relative L2 0.00436767004430294\n",
      "training 0.008105757646262646 relative L2 0.004360601771622896\n",
      "training 0.00807936117053032 relative L2 0.004353545606136322\n",
      "training 0.008053116500377655 relative L2 0.0043464465998113155\n",
      "training 0.00802700500935316 relative L2 0.004339488688856363\n",
      "training 0.008000973612070084 relative L2 0.004332364536821842\n",
      "training 0.007974982261657715 relative L2 0.004325405694544315\n",
      "training 0.00794900767505169 relative L2 0.004318303894251585\n",
      "training 0.00792311504483223 relative L2 0.004311330150812864\n",
      "training 0.007897311821579933 relative L2 0.004304307512938976\n",
      "training 0.007871599867939949 relative L2 0.004297312814742327\n",
      "training 0.00784602016210556 relative L2 0.004290373995900154\n",
      "training 0.007820508442819118 relative L2 0.004283369518816471\n",
      "training 0.007795112207531929 relative L2 0.004276471212506294\n",
      "training 0.007769744843244553 relative L2 0.004269466735422611\n",
      "training 0.007744454313069582 relative L2 0.004262573551386595\n",
      "training 0.007719232700765133 relative L2 0.0042556156404316425\n",
      "training 0.007694091647863388 relative L2 0.0042487094178795815\n",
      "training 0.007669033482670784 relative L2 0.004241817630827427\n",
      "training 0.007644082885235548 relative L2 0.0042349109426140785\n",
      "training 0.007619230076670647 relative L2 0.004228073637932539\n",
      "training 0.007594458293169737 relative L2 0.0042211636900901794\n",
      "training 0.00756973959505558 relative L2 0.004214346408843994\n",
      "training 0.007545080501586199 relative L2 0.004207459744066\n",
      "training 0.00752051780000329 relative L2 0.004200662020593882\n",
      "training 0.007496048230677843 relative L2 0.004193801898509264\n",
      "training 0.007471608929336071 relative L2 0.004187007900327444\n",
      "training 0.0074472976848483086 relative L2 0.004180197604000568\n",
      "training 0.007423041854053736 relative L2 0.004173399414867163\n",
      "training 0.0073988731019198895 relative L2 0.004166636615991592\n",
      "training 0.007374787237495184 relative L2 0.004159840755164623\n",
      "training 0.007350771222263575 relative L2 0.004153112415224314\n",
      "training 0.007326847407966852 relative L2 0.004146330524235964\n",
      "training 0.007302971091121435 relative L2 0.004139619879424572\n",
      "training 0.00727919302880764 relative L2 0.004132870119065046\n",
      "training 0.007255484815686941 relative L2 0.004126173909753561\n",
      "training 0.007231869734823704 relative L2 0.004119452554732561\n",
      "training 0.007208307273685932 relative L2 0.004112761002033949\n",
      "training 0.007184816524386406 relative L2 0.004106077831238508\n",
      "training 0.007161423098295927 relative L2 0.0040993946604430676\n",
      "training 0.007138075772672892 relative L2 0.0040927487425506115\n",
      "training 0.007114858366549015 relative L2 0.004086083732545376\n",
      "training 0.007091675419360399 relative L2 0.004079457838088274\n",
      "training 0.007068587467074394 relative L2 0.004072813782840967\n",
      "training 0.007045553997159004 relative L2 0.004066209774464369\n",
      "training 0.0070226192474365234 relative L2 0.004059594124555588\n",
      "training 0.006999752018600702 relative L2 0.004053001757711172\n",
      "training 0.0069769504480063915 relative L2 0.004046410787850618\n",
      "training 0.0069542196579277515 relative L2 0.004039835650473833\n",
      "training 0.006931572686880827 relative L2 0.004033275414258242\n",
      "training 0.00690900394693017 relative L2 0.0040267170406877995\n",
      "training 0.006886507384479046 relative L2 0.004020181018859148\n",
      "training 0.006864072289317846 relative L2 0.004013642203062773\n",
      "training 0.006841728929430246 relative L2 0.004007131792604923\n",
      "training 0.006819445639848709 relative L2 0.004000607877969742\n",
      "training 0.006797239184379578 relative L2 0.00399412028491497\n",
      "training 0.006775092799216509 relative L2 0.0039876168593764305\n",
      "training 0.006753034424036741 relative L2 0.003981160931289196\n",
      "training 0.006731067784130573 relative L2 0.003974673803895712\n",
      "training 0.0067091332748532295 relative L2 0.003968235105276108\n",
      "training 0.0066872937604784966 relative L2 0.003961772657930851\n",
      "training 0.00666552409529686 relative L2 0.00395535584539175\n",
      "training 0.006643822882324457 relative L2 0.003948916681110859\n",
      "training 0.006622196640819311 relative L2 0.003942523617297411\n",
      "training 0.006600646767765284 relative L2 0.003936105873435736\n",
      "training 0.006579164415597916 relative L2 0.0039297291077673435\n",
      "training 0.006557746324688196 relative L2 0.003923330456018448\n",
      "training 0.006536394357681274 relative L2 0.003916977439075708\n",
      "training 0.006515122950077057 relative L2 0.003910605330020189\n",
      "training 0.006493923719972372 relative L2 0.0039042711723595858\n",
      "training 0.0064727929420769215 relative L2 0.003897920483723283\n",
      "training 0.006451727356761694 relative L2 0.00389160611666739\n",
      "training 0.006430727895349264 relative L2 0.003885275684297085\n",
      "training 0.006409808062016964 relative L2 0.003878987394273281\n",
      "training 0.006388950627297163 relative L2 0.003872671164572239\n",
      "training 0.006368162576109171 relative L2 0.0038664177991449833\n",
      "training 0.006347470451146364 relative L2 0.003860117169097066\n",
      "training 0.006326821632683277 relative L2 0.0038538880180567503\n",
      "training 0.006306252907961607 relative L2 0.0038476004265248775\n",
      "training 0.006285731215029955 relative L2 0.003841396886855364\n",
      "training 0.006265293341130018 relative L2 0.0038351300172507763\n",
      "training 0.006244930438697338 relative L2 0.003828959073871374\n",
      "training 0.006224633194506168 relative L2 0.003822700120508671\n",
      "training 0.006204394157975912 relative L2 0.003816557116806507\n",
      "training 0.006184225901961327 relative L2 0.0038103200495243073\n",
      "training 0.006164159160107374 relative L2 0.003804221283644438\n",
      "training 0.006144149694591761 relative L2 0.0037979844491928816\n",
      "training 0.006124219857156277 relative L2 0.003791948314756155\n",
      "training 0.006104385945945978 relative L2 0.003785711247473955\n",
      "training 0.006084631662815809 relative L2 0.0037797600962221622\n",
      "training 0.006064988672733307 relative L2 0.0037735255900770426\n",
      "training 0.006045490503311157 relative L2 0.0037677159998565912\n",
      "training 0.006026145536452532 relative L2 0.003761494532227516\n",
      "training 0.006007032468914986 relative L2 0.00375595735386014\n",
      "training 0.005988261662423611 relative L2 0.003749832510948181\n",
      "training 0.005969953257590532 relative L2 0.0037448310758918524\n",
      "training 0.0059523917734622955 relative L2 0.0037390629295259714\n",
      "training 0.005935979541391134 relative L2 0.0037352796643972397\n",
      "training 0.005921419709920883 relative L2 0.003730642143636942\n",
      "training 0.0059098051860928535 relative L2 0.0037297564558684826\n",
      "training 0.005902967881411314 relative L2 0.003728350857272744\n",
      "training 0.005903478711843491 relative L2 0.0037343634758144617\n",
      "training 0.005916128866374493 relative L2 0.0037409979850053787\n",
      "training 0.005945104639977217 relative L2 0.003761160420253873\n",
      "training 0.005999342538416386 relative L2 0.00378104904666543\n",
      "training 0.006075146142393351 relative L2 0.003815848147496581\n",
      "training 0.006172890309244394 relative L2 0.003831532085314393\n",
      "training 0.0062401373870670795 relative L2 0.003838503733277321\n",
      "training 0.006245466880500317 relative L2 0.0037934435531497\n",
      "training 0.006115886848419905 relative L2 0.0037334535736590624\n",
      "training 0.005910591688007116 relative L2 0.003670379053801298\n",
      "training 0.005720732267946005 relative L2 0.0036485714372247458\n",
      "training 0.0056505040265619755 relative L2 0.0036660442128777504\n",
      "training 0.005701174959540367 relative L2 0.0036913431249558926\n",
      "training 0.005788328591734171 relative L2 0.0037048328667879105\n",
      "training 0.005819994956254959 relative L2 0.003678917186334729\n",
      "training 0.005749249830842018 relative L2 0.003643712727352977\n",
      "training 0.005631634965538979 relative L2 0.0036154945846647024\n",
      "training 0.005548517219722271 relative L2 0.003613707609474659\n",
      "training 0.005543729290366173 relative L2 0.0036286632530391216\n",
      "training 0.0055848946794867516 relative L2 0.003632877254858613\n",
      "training 0.005605051293969154 relative L2 0.003624275093898177\n",
      "training 0.005570866633206606 relative L2 0.003599181305617094\n",
      "training 0.005499871913343668 relative L2 0.0035820978228002787\n",
      "training 0.005444639828056097 relative L2 0.0035782798659056425\n",
      "training 0.005432587116956711 relative L2 0.0035820133052766323\n",
      "training 0.005447283387184143 relative L2 0.003585279220715165\n",
      "training 0.005451935343444347 relative L2 0.003574579954147339\n",
      "training 0.005424883682280779 relative L2 0.0035610490012913942\n",
      "training 0.005379439331591129 relative L2 0.0035480656661093235\n",
      "training 0.005342383868992329 relative L2 0.0035434921737760305\n",
      "training 0.005328885279595852 relative L2 0.00354459835216403\n",
      "training 0.005329641047865152 relative L2 0.003541573416441679\n",
      "training 0.005324332043528557 relative L2 0.0035357370506972075\n",
      "training 0.005302672740072012 relative L2 0.0035239921417087317\n",
      "training 0.0052706291899085045 relative L2 0.0035154023207724094\n",
      "training 0.0052432469092309475 relative L2 0.003510473994538188\n",
      "training 0.0052282968536019325 relative L2 0.003507512854412198\n",
      "training 0.005221211817115545 relative L2 0.0035052290186285973\n",
      "training 0.0052115763537585735 relative L2 0.0034980957861989737\n",
      "training 0.005193300079554319 relative L2 0.0034908384550362825\n",
      "training 0.005169256590306759 relative L2 0.0034827934578061104\n",
      "training 0.005146741401404142 relative L2 0.0034773428924381733\n",
      "training 0.005130510777235031 relative L2 0.003473868826404214\n",
      "training 0.005119092762470245 relative L2 0.0034692552872002125\n",
      "training 0.005107355769723654 relative L2 0.0034646550193428993\n",
      "training 0.005091570783406496 relative L2 0.003457385580986738\n",
      "training 0.0050720698200166225 relative L2 0.00345113268122077\n",
      "training 0.00505243381485343 relative L2 0.0034453023690730333\n",
      "training 0.005035642068833113 relative L2 0.0034404147882014513\n",
      "training 0.0050218417309224606 relative L2 0.0034363986924290657\n",
      "training 0.005008844658732414 relative L2 0.003430894808843732\n",
      "training 0.004994295071810484 relative L2 0.0034257122315466404\n",
      "training 0.004977636970579624 relative L2 0.003419288666918874\n",
      "training 0.004960063379257917 relative L2 0.0034137496259063482\n",
      "training 0.004943367559462786 relative L2 0.0034086003433912992\n",
      "training 0.0049283262342214584 relative L2 0.0034035800490528345\n",
      "training 0.004914387129247189 relative L2 0.003399076871573925\n",
      "training 0.004900312051177025 relative L2 0.003393476363271475\n",
      "training 0.004885254427790642 relative L2 0.0033883233554661274\n",
      "training 0.004869330208748579 relative L2 0.0033825028222054243\n",
      "training 0.004853264894336462 relative L2 0.003377246670424938\n",
      "training 0.004837838467210531 relative L2 0.0033722249791026115\n",
      "training 0.004823239520192146 relative L2 0.0033671108540147543\n",
      "training 0.004809086211025715 relative L2 0.0033623932395130396\n",
      "training 0.004794788546860218 relative L2 0.003356939647346735\n",
      "training 0.0047800117172300816 relative L2 0.0033518935088068247\n",
      "training 0.004764856770634651 relative L2 0.0033464054577052593\n",
      "training 0.004749701824039221 relative L2 0.0033412757329642773\n",
      "training 0.0047348798252642155 relative L2 0.0033362475223839283\n",
      "training 0.004720476921647787 relative L2 0.00333114480599761\n",
      "training 0.004706343170255423 relative L2 0.003326336620375514\n",
      "training 0.004692175891250372 relative L2 0.0033210611436516047\n",
      "training 0.004677819553762674 relative L2 0.003316115355119109\n",
      "training 0.0046632965095341206 relative L2 0.0033108198549598455\n",
      "training 0.004648752044886351 relative L2 0.0033057916443794966\n",
      "training 0.0046343677677214146 relative L2 0.003300758544355631\n",
      "training 0.004620207007974386 relative L2 0.0032957119401544333\n",
      "training 0.004606205504387617 relative L2 0.003290858818218112\n",
      "training 0.004592270590364933 relative L2 0.0032857346814125776\n",
      "training 0.004578305874019861 relative L2 0.00328086712397635\n",
      "training 0.004564292263239622 relative L2 0.0032757206354290247\n",
      "training 0.0045502581633627415 relative L2 0.003270791145041585\n",
      "training 0.004536270629614592 relative L2 0.0032657587435096502\n",
      "training 0.004522369243204594 relative L2 0.0032607996836304665\n",
      "training 0.004508605692535639 relative L2 0.0032559256069362164\n",
      "training 0.004494931548833847 relative L2 0.0032509309239685535\n",
      "training 0.004481312353163958 relative L2 0.003246109001338482\n",
      "training 0.004467692691832781 relative L2 0.0032410782296210527\n",
      "training 0.004454069305211306 relative L2 0.0032362393103539944\n",
      "training 0.0044404747895896435 relative L2 0.0032312546391040087\n",
      "training 0.004426918923854828 relative L2 0.003226393135264516\n",
      "training 0.004413445480167866 relative L2 0.00322150276042521\n",
      "training 0.004400024190545082 relative L2 0.0032166142482310534\n",
      "training 0.004386677406728268 relative L2 0.003211809555068612\n",
      "training 0.004373394884169102 relative L2 0.0032069000881165266\n",
      "training 0.004360137041658163 relative L2 0.0032021289225667715\n",
      "training 0.004346924368292093 relative L2 0.003197228070348501\n",
      "training 0.00433374522253871 relative L2 0.003192456206306815\n",
      "training 0.0043206000700592995 relative L2 0.003187591442838311\n",
      "training 0.004307485651224852 relative L2 0.0031827976927161217\n",
      "training 0.004294422920793295 relative L2 0.0031779988203197718\n",
      "training 0.004281417932361364 relative L2 0.0031732050701975822\n",
      "training 0.004268493037670851 relative L2 0.003168470924720168\n",
      "training 0.004255608655512333 relative L2 0.0031636590138077736\n",
      "training 0.0042427536100149155 relative L2 0.003158955369144678\n",
      "training 0.004229946061968803 relative L2 0.003154153935611248\n",
      "training 0.004217164590954781 relative L2 0.0031494616996496916\n",
      "training 0.004204444121569395 relative L2 0.003144689602777362\n",
      "training 0.004191739484667778 relative L2 0.0031399885192513466\n",
      "training 0.004179090261459351 relative L2 0.0031352615915238857\n",
      "training 0.004166482016444206 relative L2 0.0031305626034736633\n",
      "training 0.004153942223638296 relative L2 0.003125887829810381\n",
      "training 0.004141448065638542 relative L2 0.0031211907044053078\n",
      "training 0.004129000008106232 relative L2 0.0031165494583547115\n",
      "training 0.004116587806493044 relative L2 0.003111852565780282\n",
      "training 0.004104212392121553 relative L2 0.0031072297133505344\n",
      "training 0.0040918686427176 relative L2 0.0031025484204292297\n",
      "training 0.004079570062458515 relative L2 0.003097944427281618\n",
      "training 0.004067333415150642 relative L2 0.003093298524618149\n",
      "training 0.004055137280374765 relative L2 0.0030886949971318245\n",
      "training 0.004042964428663254 relative L2 0.0030840777326375246\n",
      "training 0.00403085071593523 relative L2 0.0030794853810220957\n",
      "training 0.004018773324787617 relative L2 0.0030749018769711256\n",
      "training 0.004006756003946066 relative L2 0.003070320002734661\n",
      "training 0.003994770813733339 relative L2 0.0030657583847641945\n",
      "training 0.0039828261360526085 relative L2 0.003061190713196993\n",
      "training 0.0039709266275167465 relative L2 0.0030566537752747536\n",
      "training 0.003959064371883869 relative L2 0.003052100073546171\n",
      "training 0.003947248682379723 relative L2 0.0030475808307528496\n",
      "training 0.003935466520488262 relative L2 0.0030430511105805635\n",
      "training 0.003923743963241577 relative L2 0.003038544673472643\n",
      "training 0.003912035841494799 relative L2 0.0030340293888002634\n",
      "training 0.003900382202118635 relative L2 0.003029546933248639\n",
      "training 0.0038887718692421913 relative L2 0.0030250565614551306\n",
      "training 0.0038772111292928457 relative L2 0.0030205941293388605\n",
      "training 0.003865692066028714 relative L2 0.0030161188915371895\n",
      "training 0.0038542046677321196 relative L2 0.003011672990396619\n",
      "training 0.0038427598774433136 relative L2 0.0030072168447077274\n",
      "training 0.003831355832517147 relative L2 0.0030027918983250856\n",
      "training 0.0038199962582439184 relative L2 0.0029983518179506063\n",
      "training 0.003808674868196249 relative L2 0.002993948757648468\n",
      "training 0.0037973939906805754 relative L2 0.002989522647112608\n",
      "training 0.003786153392866254 relative L2 0.0029851405415683985\n",
      "training 0.0037749509792774916 relative L2 0.002980733523145318\n",
      "training 0.0037637946661561728 relative L2 0.0029763728380203247\n",
      "training 0.0037526721134781837 relative L2 0.0029719790909439325\n",
      "training 0.003741591703146696 relative L2 0.002967644250020385\n",
      "training 0.0037305618170648813 relative L2 0.0029632668010890484\n",
      "training 0.003719557309523225 relative L2 0.00295894593000412\n",
      "training 0.0037085970398038626 relative L2 0.002954586409032345\n",
      "training 0.0036976716946810484 relative L2 0.002950291382148862\n",
      "training 0.0036868019960820675 relative L2 0.002945950720459223\n",
      "training 0.003675957443192601 relative L2 0.002941668964922428\n",
      "training 0.003665150608867407 relative L2 0.0029373459983617067\n",
      "training 0.0036543873138725758 relative L2 0.00293308706022799\n",
      "training 0.0036436649970710278 relative L2 0.0029287768993526697\n",
      "training 0.00363297201693058 relative L2 0.0029245370533317327\n",
      "training 0.0036223188508301973 relative L2 0.0029202434234321117\n",
      "training 0.0036117094568908215 relative L2 0.002916035009548068\n",
      "training 0.003601150820031762 relative L2 0.0029117546509951353\n",
      "training 0.0035906212870031595 relative L2 0.002907563466578722\n",
      "training 0.0035801217891275883 relative L2 0.0029032952152192593\n",
      "training 0.0035696730483323336 relative L2 0.0028991440776735544\n",
      "training 0.003559267381206155 relative L2 0.0028948832768946886\n",
      "training 0.0035489120054990053 relative L2 0.0028907805681228638\n",
      "training 0.003538601566106081 relative L2 0.0028865179046988487\n",
      "training 0.0035283376928418875 relative L2 0.0028824815526604652\n",
      "training 0.0035181413404643536 relative L2 0.0028782228473573923\n",
      "training 0.0035080164670944214 relative L2 0.0028742842841893435\n",
      "training 0.003497966332361102 relative L2 0.002870036056265235\n",
      "training 0.0034880382008850574 relative L2 0.002866276539862156\n",
      "training 0.0034782744478434324 relative L2 0.002862077672034502\n",
      "training 0.003468720708042383 relative L2 0.0028586529660969973\n",
      "training 0.0034595143515616655 relative L2 0.0028546501416713\n",
      "training 0.003450822550803423 relative L2 0.002851950004696846\n",
      "training 0.003442933317273855 relative L2 0.002848559059202671\n",
      "training 0.003436322556808591 relative L2 0.0028475429862737656\n",
      "training 0.0034317895770072937 relative L2 0.0028459562454372644\n",
      "training 0.0034304868895560503 relative L2 0.002849034732207656\n",
      "training 0.003434655023738742 relative L2 0.0028524429071694613\n",
      "training 0.0034469610545784235 relative L2 0.0028652925975620747\n",
      "training 0.003472962649539113 relative L2 0.0028807444032281637\n",
      "training 0.0035170873161405325 relative L2 0.0029135181102901697\n",
      "training 0.0035896424669772387 relative L2 0.0029485258273780346\n",
      "training 0.0036865398287773132 relative L2 0.0030018154066056013\n",
      "training 0.0038094634655863047 relative L2 0.00303209968842566\n",
      "training 0.003900257870554924 relative L2 0.0030472774524241686\n",
      "training 0.003925340250134468 relative L2 0.002992279129102826\n",
      "training 0.0037977842148393393 relative L2 0.002909549046307802\n",
      "training 0.0035792591515928507 relative L2 0.0028193192556500435\n",
      "training 0.0033670831471681595 relative L2 0.002785510616376996\n",
      "training 0.00328416982665658 relative L2 0.0028108269907534122\n",
      "training 0.003341849660500884 relative L2 0.002854300895705819\n",
      "training 0.003452680306509137 relative L2 0.0028815960977226496\n",
      "training 0.0035106127616018057 relative L2 0.0028545702807605267\n",
      "training 0.003453438635915518 relative L2 0.0028073994908481836\n",
      "training 0.0033331243321299553 relative L2 0.0027665586676448584\n",
      "training 0.003240140387788415 relative L2 0.0027638650499284267\n",
      "training 0.0032338935416191816 relative L2 0.0027883092407137156\n",
      "training 0.003287977771833539 relative L2 0.0028030555695295334\n",
      "training 0.003328610211610794 relative L2 0.0027979209553450346\n",
      "training 0.0033101330045610666 relative L2 0.002767419209703803\n",
      "training 0.0032432284206151962 relative L2 0.0027439859695732594\n",
      "training 0.0031854314729571342 relative L2 0.0027391240000724792\n",
      "training 0.0031742274295538664 relative L2 0.0027486681938171387\n",
      "training 0.0031988262198865414 relative L2 0.002758984686806798\n",
      "training 0.0032188505865633488 relative L2 0.00275106611661613\n",
      "training 0.003204810433089733 relative L2 0.0027358760125935078\n",
      "training 0.003165619447827339 relative L2 0.002719840034842491\n",
      "training 0.0031305758748203516 relative L2 0.0027156721334904432\n",
      "training 0.003120823297649622 relative L2 0.0027205816004425287\n",
      "training 0.003130367724224925 relative L2 0.002722199773415923\n",
      "training 0.003137029241770506 relative L2 0.002718949457630515\n",
      "training 0.003126198658719659 relative L2 0.0027069873176515102\n",
      "training 0.003101368434727192 relative L2 0.002697824267670512\n",
      "training 0.003078728448599577 relative L2 0.0026936661452054977\n",
      "training 0.003069300437346101 relative L2 0.002693652408197522\n",
      "training 0.0030704541131854057 relative L2 0.002694777213037014\n",
      "training 0.0030708739068359137 relative L2 0.0026898651849478483\n",
      "training 0.0030620242469012737 relative L2 0.0026835643220692873\n",
      "training 0.003045537043362856 relative L2 0.00267584016546607\n",
      "training 0.003029200714081526 relative L2 0.0026716699358075857\n",
      "training 0.003019459778442979 relative L2 0.00267043337225914\n",
      "training 0.003015868365764618 relative L2 0.002668483881279826\n",
      "training 0.00301288696937263 relative L2 0.0026660976000130177\n",
      "training 0.003005690174177289 relative L2 0.0026601736899465322\n",
      "training 0.002993874019011855 relative L2 0.00265505351126194\n",
      "training 0.002981186378747225 relative L2 0.002650510286912322\n",
      "training 0.0029713104013353586 relative L2 0.0026475677732378244\n",
      "training 0.002964999293908477 relative L2 0.0026457728818058968\n",
      "training 0.002960014156997204 relative L2 0.002642343519255519\n",
      "training 0.0029535142239183187 relative L2 0.0026389071717858315\n",
      "training 0.002944542793557048 relative L2 0.002633894793689251\n",
      "training 0.002934259595349431 relative L2 0.002629873575642705\n",
      "training 0.0029247261118143797 relative L2 0.0026264451444149017\n",
      "training 0.002917039208114147 relative L2 0.002623401116579771\n",
      "training 0.0029106945730745792 relative L2 0.0026208902709186077\n",
      "training 0.0029042840469628572 relative L2 0.0026171107310801744\n",
      "training 0.0028967582620680332 relative L2 0.0026136357337236404\n",
      "training 0.0028881810139864683 relative L2 0.002609411021694541\n",
      "training 0.0028793688397854567 relative L2 0.0026058503426611423\n",
      "training 0.0028712088242173195 relative L2 0.0026026342529803514\n",
      "training 0.0028639587108045816 relative L2 0.0025994018651545048\n",
      "training 0.0028571956790983677 relative L2 0.0025965452659875154\n",
      "training 0.002850267803296447 relative L2 0.0025928665418177843\n",
      "training 0.0028427846264094114 relative L2 0.0025895293802022934\n",
      "training 0.0028348350897431374 relative L2 0.0025857286527752876\n",
      "training 0.0028268585447221994 relative L2 0.0025823423638939857\n",
      "training 0.0028192326426506042 relative L2 0.002579106017947197\n",
      "training 0.002812035847455263 relative L2 0.0025758203119039536\n",
      "training 0.0028050958644598722 relative L2 0.0025728207547217607\n",
      "training 0.0027981011662632227 relative L2 0.0025693129282444715\n",
      "training 0.0027908715419471264 relative L2 0.0025660961400717497\n",
      "training 0.002783413976430893 relative L2 0.0025625312700867653\n",
      "training 0.002775925910100341 relative L2 0.002559239976108074\n",
      "training 0.0027685933746397495 relative L2 0.0025559873320162296\n",
      "training 0.0027614848222583532 relative L2 0.002552720019593835\n",
      "training 0.00275453575886786 relative L2 0.002549664815887809\n",
      "training 0.0027476190589368343 relative L2 0.0025463008787482977\n",
      "training 0.0027406311128288507 relative L2 0.0025431702379137278\n",
      "training 0.0027335304766893387 relative L2 0.002539756940677762\n",
      "training 0.002726389328017831 relative L2 0.002536555752158165\n",
      "training 0.0027192968409508467 relative L2 0.0025332903023809195\n",
      "training 0.0027123058680444956 relative L2 0.002530081430450082\n",
      "training 0.002705418039113283 relative L2 0.002526969648897648\n",
      "training 0.0026985860895365477 relative L2 0.0025237207300961018\n",
      "training 0.0026917699724435806 relative L2 0.002520641079172492\n",
      "training 0.0026849457062780857 relative L2 0.0025173646863549948\n",
      "training 0.0026781035121530294 relative L2 0.0025142489466816187\n",
      "training 0.0026712657418102026 relative L2 0.0025110188871622086\n",
      "training 0.002664448693394661 relative L2 0.0025078793987631798\n",
      "training 0.0026576921809464693 relative L2 0.0025047543458640575\n",
      "training 0.0026509908493608236 relative L2 0.0025015990249812603\n",
      "training 0.002644333988428116 relative L2 0.002498538000509143\n",
      "training 0.0026376955211162567 relative L2 0.00249536638148129\n",
      "training 0.0026310719549655914 relative L2 0.002492310246452689\n",
      "training 0.002624449785798788 relative L2 0.0024891470093280077\n",
      "training 0.0026178420521318913 relative L2 0.0024860824923962355\n",
      "training 0.00261126272380352 relative L2 0.002482970245182514\n",
      "training 0.0026047166902571917 relative L2 0.002479895018041134\n",
      "training 0.0025982018560171127 relative L2 0.002476838883012533\n",
      "training 0.0025917203165590763 relative L2 0.0024737559724599123\n",
      "training 0.0025852671824395657 relative L2 0.0024707408156245947\n",
      "training 0.002578840823844075 relative L2 0.0024676593020558357\n",
      "training 0.0025724368169903755 relative L2 0.0024646574165672064\n",
      "training 0.0025660402607172728 relative L2 0.002461586147546768\n",
      "training 0.002559665823355317 relative L2 0.002458584262058139\n",
      "training 0.0025533156003803015 relative L2 0.0024555528070777655\n",
      "training 0.0025470012333244085 relative L2 0.0024525506887584925\n",
      "training 0.0025407031644135714 relative L2 0.0024495546240359545\n",
      "training 0.0025344390887767076 relative L2 0.0024465564638376236\n",
      "training 0.002528201323002577 relative L2 0.0024435888044536114\n",
      "training 0.002521983813494444 relative L2 0.002440591109916568\n",
      "training 0.0025157846976071596 relative L2 0.0024376437067985535\n",
      "training 0.0025096095632761717 relative L2 0.00243466068059206\n",
      "training 0.0025034623686224222 relative L2 0.00243173073977232\n",
      "training 0.0024973354302346706 relative L2 0.0024287612177431583\n",
      "training 0.0024912261869758368 relative L2 0.002425840590149164\n",
      "training 0.0024851446505635977 relative L2 0.002422899706289172\n",
      "training 0.0024790947791188955 relative L2 0.0024199874605983496\n",
      "training 0.002473066095262766 relative L2 0.0024170721881091595\n",
      "training 0.0024670620914548635 relative L2 0.002414161339402199\n",
      "training 0.0024610685650259256 relative L2 0.002411266090348363\n",
      "training 0.0024551027454435825 relative L2 0.002408368745818734\n",
      "training 0.002449162770062685 relative L2 0.0024054935202002525\n",
      "training 0.002443246776238084 relative L2 0.002402607351541519\n",
      "training 0.002437349408864975 relative L2 0.0023997535463422537\n",
      "training 0.0024314839392900467 relative L2 0.002396879019215703\n",
      "training 0.0024256310425698757 relative L2 0.002394036389887333\n",
      "training 0.002419800264760852 relative L2 0.0023911793250590563\n",
      "training 0.002413992304354906 relative L2 0.0023883492685854435\n",
      "training 0.002408206695690751 relative L2 0.002385510131716728\n",
      "training 0.0024024436715990305 relative L2 0.002382690319791436\n",
      "training 0.0023967018350958824 relative L2 0.002379871206358075\n",
      "training 0.0023909842129796743 relative L2 0.0023770665284246206\n",
      "training 0.0023852947633713484 relative L2 0.0023742688354104757\n",
      "training 0.0023796241730451584 relative L2 0.0023714753333479166\n",
      "training 0.00237397663295269 relative L2 0.0023686904460191727\n",
      "training 0.002368341665714979 relative L2 0.002365906722843647\n",
      "training 0.002362733008340001 relative L2 0.0023631423246115446\n",
      "training 0.00235714646987617 relative L2 0.0023603723384439945\n",
      "training 0.0023515818174928427 relative L2 0.0023576219100505114\n",
      "training 0.0023460378870368004 relative L2 0.002354871015995741\n",
      "training 0.002340523526072502 relative L2 0.0023521347902715206\n",
      "training 0.002335019176825881 relative L2 0.002349394140765071\n",
      "training 0.0023295385763049126 relative L2 0.0023466714192181826\n",
      "training 0.0023240777663886547 relative L2 0.0023439473006874323\n",
      "training 0.0023186388425529003 relative L2 0.002341238083317876\n",
      "training 0.0023132211063057184 relative L2 0.0023385302629321814\n",
      "training 0.002307820599526167 relative L2 0.002335835713893175\n",
      "training 0.002302449196577072 relative L2 0.0023331495467573404\n",
      "training 0.0022971013095229864 relative L2 0.002330466639250517\n",
      "training 0.0022917683236300945 relative L2 0.002327793510630727\n",
      "training 0.002286456758156419 relative L2 0.0023251224774867296\n",
      "training 0.002281159395352006 relative L2 0.002322465181350708\n",
      "training 0.00227589113637805 relative L2 0.0023198125418275595\n",
      "training 0.0022706417366862297 relative L2 0.0023171654902398586\n",
      "training 0.002265408169478178 relative L2 0.002314526354894042\n",
      "training 0.002260196255519986 relative L2 0.0023119000252336264\n",
      "training 0.00225501274690032 relative L2 0.002309272764250636\n",
      "training 0.002249838085845113 relative L2 0.002306657610461116\n",
      "training 0.0022446883376687765 relative L2 0.0023040485102683306\n",
      "training 0.0022395602427423 relative L2 0.0023014473263174295\n",
      "training 0.0022344496101140976 relative L2 0.002298851730301976\n",
      "training 0.0022293590009212494 relative L2 0.002296262886375189\n",
      "training 0.00222428934648633 relative L2 0.002293687080964446\n",
      "training 0.002219245070591569 relative L2 0.0022911145351827145\n",
      "training 0.0022142173256725073 relative L2 0.002288548741489649\n",
      "training 0.0022092030849307775 relative L2 0.0022859869059175253\n",
      "training 0.002204212825745344 relative L2 0.0022834409028291702\n",
      "training 0.0021992416586726904 relative L2 0.0022808900102972984\n",
      "training 0.0021942860912531614 relative L2 0.0022783614695072174\n",
      "training 0.002189359162002802 relative L2 0.0022758247796446085\n",
      "training 0.002184445969760418 relative L2 0.0022733116056770086\n",
      "training 0.002179553732275963 relative L2 0.0022707879543304443\n",
      "training 0.0021746857091784477 relative L2 0.0022682936396449804\n",
      "training 0.002169831423088908 relative L2 0.0022657776717096567\n",
      "training 0.0021650008857250214 relative L2 0.002263307338580489\n",
      "training 0.002160186180844903 relative L2 0.0022607948631048203\n",
      "training 0.0021553938277065754 relative L2 0.0022583557292819023\n",
      "training 0.0021506263874471188 relative L2 0.0022558465134352446\n",
      "training 0.0021458796691149473 relative L2 0.002253448124974966\n",
      "training 0.002141163684427738 relative L2 0.0022509435657411814\n",
      "training 0.002136481925845146 relative L2 0.002248608274385333\n",
      "training 0.002131841378286481 relative L2 0.0022461076732724905\n",
      "training 0.0021272466983646154 relative L2 0.0022438792511820793\n",
      "training 0.002122728154063225 relative L2 0.0022414049599319696\n",
      "training 0.002118312055245042 relative L2 0.0022393965627998114\n",
      "training 0.002114069415256381 relative L2 0.002237049862742424\n",
      "training 0.002110099419951439 relative L2 0.0022355581168085337\n",
      "training 0.0021065978799015284 relative L2 0.0022336854599416256\n",
      "training 0.0021038646809756756 relative L2 0.0022335308603942394\n",
      "training 0.0021024621091783047 relative L2 0.0022332321386784315\n",
      "training 0.002103289822116494 relative L2 0.0022367192432284355\n",
      "training 0.0021080486476421356 relative L2 0.002241382608190179\n",
      "training 0.0021192904096096754 relative L2 0.002254974329844117\n",
      "training 0.0021420938428491354 relative L2 0.0022741323336958885\n",
      "training 0.002182982163503766 relative L2 0.002313733333721757\n",
      "training 0.002254834631457925 relative L2 0.0023667491041123867\n",
      "training 0.0023669027723371983 relative L2 0.002454000059515238\n",
      "training 0.0025370081420987844 relative L2 0.0025413932744413614\n",
      "training 0.002732588443905115 relative L2 0.002630712930113077\n",
      "training 0.002916886005550623 relative L2 0.002624831860885024\n",
      "training 0.002916246885433793 relative L2 0.0025329296477138996\n",
      "training 0.002703292528167367 relative L2 0.00234623602591455\n",
      "training 0.002325601875782013 relative L2 0.0022132934536784887\n",
      "training 0.002063440391793847 relative L2 0.002215525833889842\n",
      "training 0.0020674760453402996 relative L2 0.002311168937012553\n",
      "training 0.002255806000903249 relative L2 0.002392573282122612\n",
      "training 0.002410994144156575 relative L2 0.002363481791689992\n",
      "training 0.002360275713726878 relative L2 0.002268972108140588\n",
      "training 0.0021678670309484005 relative L2 0.0021891086362302303\n",
      "training 0.0020200007129460573 relative L2 0.0021998623851686716\n",
      "training 0.002040577121078968 relative L2 0.002263623056933284\n",
      "training 0.0021575225982815027 relative L2 0.0022879249881953\n",
      "training 0.002210118342190981 relative L2 0.0022540513891726732\n",
      "training 0.002139223972335458 relative L2 0.0021913840901106596\n",
      "training 0.0020246836356818676 relative L2 0.0021719629876315594\n",
      "training 0.001987760653719306 relative L2 0.002201469149440527\n",
      "training 0.002040622290223837 relative L2 0.0022279571276158094\n",
      "training 0.0020942387636750937 relative L2 0.0022213722113519907\n",
      "training 0.002077446784824133 relative L2 0.0021830801852047443\n",
      "training 0.0020092222839593887 relative L2 0.0021608283277601004\n",
      "training 0.001966733019798994 relative L2 0.0021701515652239323\n",
      "training 0.001983060035854578 relative L2 0.0021882299333810806\n",
      "training 0.002019026782363653 relative L2 0.002191380597651005\n",
      "training 0.002021587686613202 relative L2 0.0021700498182326555\n",
      "training 0.0019849492236971855 relative L2 0.0021521879825741053\n",
      "training 0.0019505864474922419 relative L2 0.002151516731828451\n",
      "training 0.0019492239225655794 relative L2 0.002161372685804963\n",
      "training 0.0019688422325998545 relative L2 0.0021668991539627314\n",
      "training 0.001976576866582036 relative L2 0.0021560394670814276\n",
      "training 0.001958982553333044 relative L2 0.002143499907106161\n",
      "training 0.0019345288164913654 relative L2 0.002138485200703144\n",
      "training 0.0019257180392742157 relative L2 0.0021423189900815487\n",
      "training 0.0019335520919412374 relative L2 0.0021470829378813505\n",
      "training 0.0019405231578275561 relative L2 0.0021421832498162985\n",
      "training 0.0019334520911797881 relative L2 0.002134277019649744\n",
      "training 0.0019176399800926447 relative L2 0.0021280357614159584\n",
      "training 0.001906929537653923 relative L2 0.0021279621869325638\n",
      "training 0.0019071224378421903 relative L2 0.0021307538263499737\n",
      "training 0.0019110681023448706 relative L2 0.002128847409039736\n",
      "training 0.0019090173300355673 relative L2 0.0021244485396891832\n",
      "training 0.0018997823353856802 relative L2 0.0021186773665249348\n",
      "training 0.0018901267321780324 relative L2 0.0021164752542972565\n",
      "training 0.0018860913114622235 relative L2 0.0021171036642044783\n",
      "training 0.0018866168102249503 relative L2 0.00211626710370183\n",
      "training 0.0018860885174944997 relative L2 0.002114120637997985\n",
      "training 0.0018811599584296346 relative L2 0.002109564607962966\n",
      "training 0.001873773755505681 relative L2 0.002106613712385297\n",
      "training 0.001868144841864705 relative L2 0.002105457242578268\n",
      "training 0.001865869970060885 relative L2 0.002104546409100294\n",
      "training 0.0018648257246240973 relative L2 0.002103486331179738\n",
      "training 0.0018621213966980577 relative L2 0.0021002760622650385\n",
      "training 0.0018571175169199705 relative L2 0.002097504213452339\n",
      "training 0.0018516768468543887 relative L2 0.0020952310878783464\n",
      "training 0.0018477297853678465 relative L2 0.0020937917288392782\n",
      "training 0.001845399267040193 relative L2 0.0020928613375872374\n",
      "training 0.0018432238139212132 relative L2 0.002090665977448225\n",
      "training 0.0018399064429104328 relative L2 0.0020885050762444735\n",
      "training 0.0018355390056967735 relative L2 0.0020859036594629288\n",
      "training 0.001831216854043305 relative L2 0.0020840091165155172\n",
      "training 0.0018278165953233838 relative L2 0.0020826561376452446\n",
      "training 0.001825184328481555 relative L2 0.0020809087436646223\n",
      "training 0.001822470803745091 relative L2 0.0020792619325220585\n",
      "training 0.0018191089620813727 relative L2 0.0020768868271261454\n",
      "training 0.0018152641132473946 relative L2 0.002074891934171319\n",
      "training 0.0018115313723683357 relative L2 0.0020730711985379457\n",
      "training 0.0018083106260746717 relative L2 0.002071363851428032\n",
      "training 0.0018054703250527382 relative L2 0.002069897251203656\n",
      "training 0.0018025810131803155 relative L2 0.0020678751170635223\n",
      "training 0.00179934105835855 relative L2 0.0020660480950027704\n",
      "training 0.0017958444077521563 relative L2 0.002064005471765995\n",
      "training 0.0017924049170687795 relative L2 0.0020622138399630785\n",
      "training 0.0017892385367304087 relative L2 0.002060609171167016\n",
      "training 0.001786293345503509 relative L2 0.0020588121842592955\n",
      "training 0.0017833460588008165 relative L2 0.0020571791101247072\n",
      "training 0.0017802404472604394 relative L2 0.002055203774943948\n",
      "training 0.0017769841942936182 relative L2 0.0020534268114715815\n",
      "training 0.0017737390007823706 relative L2 0.0020516326185315847\n",
      "training 0.0017706266371533275 relative L2 0.002049896167591214\n",
      "training 0.0017676567658782005 relative L2 0.002048296155408025\n",
      "training 0.0017647187924012542 relative L2 0.0020464749541133642\n",
      "training 0.001761711435392499 relative L2 0.0020448036957532167\n",
      "training 0.0017586307367309928 relative L2 0.0020429538562893867\n",
      "training 0.0017555239610373974 relative L2 0.002041235566139221\n",
      "training 0.0017524792347103357 relative L2 0.0020395454484969378\n",
      "training 0.001749515999108553 relative L2 0.0020378234330564737\n",
      "training 0.0017466045683249831 relative L2 0.002036211546510458\n",
      "training 0.001743682543747127 relative L2 0.0020344338845461607\n",
      "training 0.0017407232662662864 relative L2 0.0020327738020569086\n",
      "training 0.001737737562507391 relative L2 0.002031025243923068\n",
      "training 0.0017347707180306315 relative L2 0.0020293465349823236\n",
      "training 0.001731851720251143 relative L2 0.0020277018193155527\n",
      "training 0.0017289700917899609 relative L2 0.0020260054152458906\n",
      "training 0.0017261095345020294 relative L2 0.002024396089836955\n",
      "training 0.0017232394311577082 relative L2 0.0020226803608238697\n",
      "training 0.0017203646712005138 relative L2 0.002021052176132798\n",
      "training 0.0017174879321828485 relative L2 0.0020193690434098244\n",
      "training 0.001714626676402986 relative L2 0.0020177275873720646\n",
      "training 0.0017117949901148677 relative L2 0.002016106154769659\n",
      "training 0.0017089812317863107 relative L2 0.002014452824369073\n",
      "training 0.001706186798401177 relative L2 0.002012864453718066\n",
      "training 0.0017033962067216635 relative L2 0.002011201810091734\n",
      "training 0.0017006045673042536 relative L2 0.002009605523198843\n",
      "training 0.0016978183994069695 relative L2 0.002007971052080393\n",
      "training 0.0016950457356870174 relative L2 0.0020063661504536867\n",
      "training 0.0016922912327572703 relative L2 0.0020047735888510942\n",
      "training 0.0016895529115572572 relative L2 0.0020031628664582968\n",
      "training 0.001686830073595047 relative L2 0.002001596614718437\n",
      "training 0.0016841137548908591 relative L2 0.0019999847281724215\n",
      "training 0.0016814065165817738 relative L2 0.0019984215032309294\n",
      "training 0.0016787017229944468 relative L2 0.001996823353692889\n",
      "training 0.0016760091530159116 relative L2 0.0019952598959207535\n",
      "training 0.0016733313677832484 relative L2 0.001993694109842181\n",
      "training 0.0016706661554053426 relative L2 0.001992125529795885\n",
      "training 0.0016680158441886306 relative L2 0.001990586519241333\n",
      "training 0.001665376708842814 relative L2 0.0019890174735337496\n",
      "training 0.001662742579355836 relative L2 0.0019874905701726675\n",
      "training 0.0016601216048002243 relative L2 0.001985935727134347\n",
      "training 0.00165751320309937 relative L2 0.0019844085909426212\n",
      "training 0.0016549094580113888 relative L2 0.001982872374355793\n",
      "training 0.0016523171216249466 relative L2 0.0019813464023172855\n",
      "training 0.0016497408505529165 relative L2 0.001979831838980317\n",
      "training 0.0016471694689244032 relative L2 0.0019783047027885914\n",
      "training 0.0016446099616587162 relative L2 0.0019768080674111843\n",
      "training 0.0016420632600784302 relative L2 0.0019752897787839174\n",
      "training 0.0016395244747400284 relative L2 0.0019737996626645327\n",
      "training 0.0016369952354580164 relative L2 0.0019722976721823215\n",
      "training 0.0016344814794138074 relative L2 0.0019708105828613043\n",
      "training 0.001631973427720368 relative L2 0.0019693239592015743\n",
      "training 0.001629476435482502 relative L2 0.001967842224985361\n",
      "training 0.0016269916668534279 relative L2 0.0019663716666400433\n",
      "training 0.001624514115974307 relative L2 0.0019648929592221975\n",
      "training 0.0016220479737967253 relative L2 0.0019634352065622807\n",
      "training 0.0016195919597521424 relative L2 0.001961967209354043\n",
      "training 0.001617147121578455 relative L2 0.0019605166744440794\n",
      "training 0.0016147101996466517 relative L2 0.0019590614829212427\n",
      "training 0.0016122831730172038 relative L2 0.0019576179329305887\n",
      "training 0.0016098696505650878 relative L2 0.001956176944077015\n",
      "training 0.0016074625309556723 relative L2 0.001954738749191165\n",
      "training 0.0016050670528784394 relative L2 0.0019533117301762104\n",
      "training 0.0016026824014261365 relative L2 0.0019518802873790264\n",
      "training 0.0016003069467842579 relative L2 0.001950463280081749\n",
      "training 0.0015979409217834473 relative L2 0.0019490428967401385\n",
      "training 0.00159558467566967 relative L2 0.0019476338056847453\n",
      "training 0.001593237742781639 relative L2 0.0019462233176454902\n",
      "training 0.0015908993082121015 relative L2 0.0019448248203843832\n",
      "training 0.0015885749598965049 relative L2 0.0019434259738773108\n",
      "training 0.0015862557338550687 relative L2 0.0019420308526605368\n",
      "training 0.0015839465195313096 relative L2 0.0019406442297622561\n",
      "training 0.0015816462691873312 relative L2 0.001939257956109941\n",
      "training 0.0015793554484844208 relative L2 0.001937882974743843\n",
      "training 0.0015770784812048078 relative L2 0.0019365055486559868\n",
      "training 0.0015748051228001714 relative L2 0.0019351361552253366\n",
      "training 0.001572543173097074 relative L2 0.0019337728153914213\n",
      "training 0.0015702926320955157 relative L2 0.0019324107561260462\n",
      "training 0.0015680495416745543 relative L2 0.0019310584757477045\n",
      "training 0.0015658176271244884 relative L2 0.0019297057297080755\n",
      "training 0.0015635924646630883 relative L2 0.0019283632282167673\n",
      "training 0.001561379642225802 relative L2 0.0019270183984190226\n",
      "training 0.001559172640554607 relative L2 0.0019256832310929894\n",
      "training 0.0015569747192785144 relative L2 0.0019243517890572548\n",
      "training 0.001554789487272501 relative L2 0.0019230236066505313\n",
      "training 0.0015526097267866135 relative L2 0.0019217000808566809\n",
      "training 0.0015504382317885756 relative L2 0.0019203800475224853\n",
      "training 0.001548276748508215 relative L2 0.0019190663006156683\n",
      "training 0.0015461246948689222 relative L2 0.001917756744660437\n",
      "training 0.0015439832350239158 relative L2 0.0019164528930559754\n",
      "training 0.0015418489929288626 relative L2 0.001915152184665203\n",
      "training 0.0015397249953821301 relative L2 0.001913859392516315\n",
      "training 0.0015376124065369368 relative L2 0.0019125700928270817\n",
      "training 0.0015355077339336276 relative L2 0.0019112828886136413\n",
      "training 0.001533410046249628 relative L2 0.0019100015051662922\n",
      "training 0.0015313206240534782 relative L2 0.0019087226828560233\n",
      "training 0.0015292373718693852 relative L2 0.001907450263388455\n",
      "training 0.0015271661104634404 relative L2 0.0019061806378886104\n",
      "training 0.001525101251900196 relative L2 0.0019049166003242135\n",
      "training 0.0015230460558086634 relative L2 0.0019036580342799425\n",
      "training 0.0015210029669106007 relative L2 0.0019024034263566136\n",
      "training 0.001518964534625411 relative L2 0.001901153358630836\n",
      "training 0.0015169368125498295 relative L2 0.0018999045714735985\n",
      "training 0.0015149136306717992 relative L2 0.001898663816973567\n",
      "training 0.0015128999948501587 relative L2 0.001897426089271903\n",
      "training 0.0015108982333913445 relative L2 0.0018961922032758594\n",
      "training 0.0015089013613760471 relative L2 0.0018949643708765507\n",
      "training 0.001506916480138898 relative L2 0.001893740613013506\n",
      "training 0.0015049375360831618 relative L2 0.0018925213953480124\n",
      "training 0.001502967905253172 relative L2 0.0018913060193881392\n",
      "training 0.0015010058414191008 relative L2 0.0018900951836258173\n",
      "training 0.0014990512281656265 relative L2 0.00188888655975461\n",
      "training 0.0014971039490774274 relative L2 0.0018876836402341723\n",
      "training 0.0014951664488762617 relative L2 0.001886484562419355\n",
      "training 0.0014932345366105437 relative L2 0.0018852896755561233\n",
      "training 0.0014913155464455485 relative L2 0.0018841053824871778\n",
      "training 0.0014894087798893452 relative L2 0.0018829184118658304\n",
      "training 0.0014875037595629692 relative L2 0.0018817392410710454\n",
      "training 0.0014856057241559029 relative L2 0.0018805599538609385\n",
      "training 0.0014837168855592608 relative L2 0.0018793863710016012\n",
      "training 0.0014818322379142046 relative L2 0.0018782163970172405\n",
      "training 0.0014799595810472965 relative L2 0.0018770527094602585\n",
      "training 0.0014780944911763072 relative L2 0.0018758923979476094\n",
      "training 0.0014762377832084894 relative L2 0.0018747377907857299\n",
      "training 0.001474389573559165 relative L2 0.0018735837657004595\n",
      "training 0.0014725463697686791 relative L2 0.0018724354449659586\n",
      "training 0.0014707109658047557 relative L2 0.0018712930614128709\n",
      "training 0.0014688863884657621 relative L2 0.0018701528897508979\n",
      "training 0.001467068330384791 relative L2 0.0018690171418711543\n",
      "training 0.00146525539457798 relative L2 0.0018678853521123528\n",
      "training 0.00146345270331949 relative L2 0.0018667601980268955\n",
      "training 0.0014616597909480333 relative L2 0.0018656402826309204\n",
      "training 0.0014598764246329665 relative L2 0.001864518621005118\n",
      "training 0.0014580936403945088 relative L2 0.0018634038278833032\n",
      "training 0.00145631970372051 relative L2 0.0018622922943904996\n",
      "training 0.0014545543817803264 relative L2 0.0018611858831718564\n",
      "training 0.0014527966268360615 relative L2 0.001860081567429006\n",
      "training 0.001451043994165957 relative L2 0.0018589848186820745\n",
      "training 0.0014493028866127133 relative L2 0.0018578876042738557\n",
      "training 0.0014475671341642737 relative L2 0.0018568022642284632\n",
      "training 0.001445842208340764 relative L2 0.0018557094736024737\n",
      "training 0.0014441183302551508 relative L2 0.001854632282629609\n",
      "training 0.0014424053952097893 relative L2 0.001853547408245504\n",
      "training 0.0014406985137611628 relative L2 0.0018524803454056382\n",
      "training 0.001439001178368926 relative L2 0.0018514003604650497\n",
      "training 0.0014373110607266426 relative L2 0.0018503498286008835\n",
      "training 0.0014356310712173581 relative L2 0.0018492735689505935\n",
      "training 0.0014339598128572106 relative L2 0.0018482450395822525\n",
      "training 0.0014323024079203606 relative L2 0.0018471663352102041\n",
      "training 0.0014306462835520506 relative L2 0.0018461584113538265\n",
      "training 0.0014290038961917162 relative L2 0.0018450807547196746\n",
      "training 0.0014273773413151503 relative L2 0.001844119979068637\n",
      "training 0.0014257754664868116 relative L2 0.0018430502386763692\n",
      "training 0.001424208632670343 relative L2 0.0018421836430206895\n",
      "training 0.001422696397639811 relative L2 0.0018411572091281414\n",
      "training 0.001421277760528028 relative L2 0.001840517157688737\n",
      "training 0.0014200161676853895 relative L2 0.0018396716332063079\n",
      "training 0.0014190222136676311 relative L2 0.0018396388040855527\n",
      "training 0.0014185217441990972 relative L2 0.0018395038787275553\n",
      "training 0.0014188883360475302 relative L2 0.0018412411445751786\n",
      "training 0.0014208188513293862 relative L2 0.0018435785314068198\n",
      "training 0.0014255017740651965 relative L2 0.001850669737905264\n",
      "training 0.0014352366561070085 relative L2 0.0018612382700666785\n",
      "training 0.0014537191018462181 relative L2 0.001884561963379383\n",
      "training 0.0014883511466905475 relative L2 0.0019204369746148586\n",
      "training 0.0015495498664677143 relative L2 0.001988281263038516\n",
      "training 0.001657796325162053 relative L2 0.0020841346122324467\n",
      "training 0.0018290920415893197 relative L2 0.0022305340971797705\n",
      "training 0.002090075518935919 relative L2 0.002373957773670554\n",
      "training 0.0023790509440004826 relative L2 0.0024900641292333603\n",
      "training 0.002608910668641329 relative L2 0.0024333088658750057\n",
      "training 0.002500387607142329 relative L2 0.0022227740846574306\n",
      "training 0.0020754446741193533 relative L2 0.0019317276310175657\n",
      "training 0.0015681053046137094 relative L2 0.0018239152850583196\n",
      "training 0.0013944680104032159 relative L2 0.001956205815076828\n",
      "training 0.0016042807837948203 relative L2 0.0021156116854399443\n",
      "training 0.0018853036453947425 relative L2 0.002136483322829008\n",
      "training 0.001916242646984756 relative L2 0.001980287954211235\n",
      "training 0.0016491165151819587 relative L2 0.0018338370136916637\n",
      "training 0.0014088560128584504 relative L2 0.0018481977749615908\n",
      "training 0.0014309987891465425 relative L2 0.0019590684678405523\n",
      "training 0.0016134436009451747 relative L2 0.002007587580010295\n",
      "training 0.001690294244326651 relative L2 0.0019254721701145172\n",
      "training 0.0015577520243823528 relative L2 0.001828637789003551\n",
      "training 0.0014007673598825932 relative L2 0.0018273090245202184\n",
      "training 0.0013987099518999457 relative L2 0.0018948358483612537\n",
      "training 0.0015077672433108091 relative L2 0.0019261804409325123\n",
      "training 0.001554948277771473 relative L2 0.001873886794783175\n",
      "training 0.0014740255428478122 relative L2 0.0018163151107728481\n",
      "training 0.0013818966690450907 relative L2 0.0018196776509284973\n",
      "training 0.0013869521208107471 relative L2 0.001860802760347724\n",
      "training 0.0014531246852129698 relative L2 0.0018754700431600213\n",
      "training 0.0014735468430444598 relative L2 0.0018396193627268076\n",
      "training 0.0014195628464221954 relative L2 0.0018072730163112283\n",
      "training 0.001368161872960627 relative L2 0.0018138443119823933\n",
      "training 0.0013779696309939027 relative L2 0.0018381840782240033\n",
      "training 0.0014173127710819244 relative L2 0.0018431254429742694\n",
      "training 0.0014228177024051547 relative L2 0.0018188266549259424\n",
      "training 0.0013869324466213584 relative L2 0.0018012962536886334\n",
      "training 0.0013591150054708123 relative L2 0.0018078447319567204\n",
      "training 0.0013687702594324946 relative L2 0.0018216155003756285\n",
      "training 0.0013913256116211414 relative L2 0.0018222933867946267\n",
      "training 0.0013906479580327868 relative L2 0.0018063451861962676\n",
      "training 0.0013674813089892268 relative L2 0.0017967437161132693\n",
      "training 0.0013522061053663492 relative L2 0.0018015805399045348\n",
      "training 0.0013592172181233764 relative L2 0.0018090445082634687\n",
      "training 0.001371739897876978 relative L2 0.0018084204057231545\n",
      "training 0.001369440695270896 relative L2 0.0017981880810111761\n",
      "training 0.0013548271963372827 relative L2 0.0017926363507285714\n",
      "training 0.001345947734080255 relative L2 0.0017955834046006203\n",
      "training 0.0013501123758032918 relative L2 0.0017994862282648683\n",
      "training 0.0013569204602390528 relative L2 0.0017987820319831371\n",
      "training 0.001354805426672101 relative L2 0.0017922365805134177\n",
      "training 0.0013456369051709771 relative L2 0.001788682653568685\n",
      "training 0.0013399096205830574 relative L2 0.0017900889506563544\n",
      "training 0.0013418017188087106 relative L2 0.0017920136451721191\n",
      "training 0.001345376716926694 relative L2 0.0017915721982717514\n",
      "training 0.001343902898952365 relative L2 0.001787357497960329\n",
      "training 0.0013381378958001733 relative L2 0.001784832333214581\n",
      "training 0.001334026106633246 relative L2 0.0017851528245955706\n",
      "training 0.0013343568425625563 relative L2 0.0017859713407233357\n",
      "training 0.0013360707089304924 relative L2 0.001785755972377956\n",
      "training 0.0013351284433156252 relative L2 0.0017829922726377845\n",
      "training 0.0013314541429281235 relative L2 0.0017810824792832136\n",
      "training 0.0013283032458275557 relative L2 0.0017807261319831014\n",
      "training 0.0013276919489726424 relative L2 0.0017808974953368306\n",
      "training 0.0013282799627631903 relative L2 0.0017807760741561651\n",
      "training 0.0013276285026222467 relative L2 0.001778916222974658\n",
      "training 0.0013252361677587032 relative L2 0.0017774270381778479\n",
      "training 0.0013227364979684353 relative L2 0.0017766710370779037\n",
      "training 0.0013215943472459912 relative L2 0.0017764344811439514\n",
      "training 0.0013214483624324203 relative L2 0.001776298857294023\n",
      "training 0.0013208971358835697 relative L2 0.0017750018741935492\n",
      "training 0.0013192812912166119 relative L2 0.0017738371388986707\n",
      "training 0.00131728604901582 relative L2 0.0017728785751387477\n",
      "training 0.0013159014051780105 relative L2 0.0017723821802064776\n",
      "training 0.001315262750722468 relative L2 0.0017721413169056177\n",
      "training 0.0013146601850166917 relative L2 0.001771189272403717\n",
      "training 0.0013134957989677787 relative L2 0.0017702856566756964\n",
      "training 0.001311912783421576 relative L2 0.0017692741239443421\n",
      "training 0.0013104915851727128 relative L2 0.0017686166102066636\n",
      "training 0.00130953430198133 relative L2 0.0017682226607576013\n",
      "training 0.001308791572228074 relative L2 0.0017674558330327272\n",
      "training 0.0013078382471576333 relative L2 0.0017667405772954226\n",
      "training 0.0013065712992101908 relative L2 0.001765781664289534\n",
      "training 0.0013052498688921332 relative L2 0.001765062683261931\n",
      "training 0.0013041442725807428 relative L2 0.0017645178595557809\n",
      "training 0.0013032526476308703 relative L2 0.0017638331046327949\n",
      "training 0.0013023574138060212 relative L2 0.0017632243689149618\n",
      "training 0.0013012902345508337 relative L2 0.0017623569583520293\n",
      "training 0.0013001093175262213 relative L2 0.00176163949072361\n",
      "training 0.001298973336815834 relative L2 0.0017609890783205628\n",
      "training 0.0012979814782738686 relative L2 0.0017603280721232295\n",
      "training 0.0012970665702596307 relative L2 0.0017597596161067486\n",
      "training 0.00129610113799572 relative L2 0.001758976373821497\n",
      "training 0.0012950374512001872 relative L2 0.0017582997679710388\n",
      "training 0.001293946523219347 relative L2 0.0017575898673385382\n",
      "training 0.0012929090298712254 relative L2 0.001756930025294423\n",
      "training 0.0012919459259137511 relative L2 0.0017563446890562773\n",
      "training 0.0012910043587908149 relative L2 0.0017556376988068223\n",
      "training 0.0012900292640551925 relative L2 0.001755008241161704\n",
      "training 0.0012890107464045286 relative L2 0.0017542924033477902\n",
      "training 0.001287989434786141 relative L2 0.0017536409432068467\n",
      "training 0.0012870040955021977 relative L2 0.0017530224286019802\n",
      "training 0.0012860544957220554 relative L2 0.0017523588612675667\n",
      "training 0.0012851135106757283 relative L2 0.0017517585074529052\n",
      "training 0.0012841550633311272 relative L2 0.001751065836288035\n",
      "training 0.0012831765925511718 relative L2 0.0017504357965663075\n",
      "training 0.0012822014978155494 relative L2 0.0017497898079454899\n",
      "training 0.0012812453787773848 relative L2 0.001749152084812522\n",
      "training 0.001280311495065689 relative L2 0.0017485558055341244\n",
      "training 0.0012793848291039467 relative L2 0.0017478978261351585\n",
      "training 0.0012784507125616074 relative L2 0.0017472911858931184\n",
      "training 0.0012775042559951544 relative L2 0.001746639027260244\n",
      "training 0.0012765610590577126 relative L2 0.0017460185335949063\n",
      "training 0.0012756295036524534 relative L2 0.0017454121261835098\n",
      "training 0.0012747145956382155 relative L2 0.0017447835998609662\n",
      "training 0.001273806905373931 relative L2 0.0017441934905946255\n",
      "training 0.0012728949077427387 relative L2 0.0017435553018003702\n",
      "training 0.0012719780206680298 relative L2 0.0017429555300623178\n",
      "training 0.0012710649752989411 relative L2 0.0017423388781026006\n",
      "training 0.0012701564701274037 relative L2 0.0017417308408766985\n",
      "training 0.00126925902441144 relative L2 0.0017411417793482542\n",
      "training 0.0012683671666309237 relative L2 0.0017405260587111115\n",
      "training 0.001267478452064097 relative L2 0.0017399439821019769\n",
      "training 0.0012665900867432356 relative L2 0.001739333150908351\n",
      "training 0.0012657020706683397 relative L2 0.001738742459565401\n",
      "training 0.001264819409698248 relative L2 0.001738153863698244\n",
      "training 0.0012639439664781094 relative L2 0.0017375570023432374\n",
      "training 0.0012630716664716601 relative L2 0.0017369830748066306\n",
      "training 0.0012622043723240495 relative L2 0.001736384816467762\n",
      "training 0.0012613371945917606 relative L2 0.0017358122859150171\n",
      "training 0.001260474557057023 relative L2 0.0017352259019389749\n",
      "training 0.0012596172746270895 relative L2 0.0017346504610031843\n",
      "training 0.001258763251826167 relative L2 0.0017340794438496232\n",
      "training 0.0012579140020534396 relative L2 0.0017335007432848215\n",
      "training 0.0012570673134177923 relative L2 0.0017329368274658918\n",
      "training 0.0012562230695039034 relative L2 0.001732360920868814\n",
      "training 0.001255383132956922 relative L2 0.0017317988676950336\n",
      "training 0.0012545454083010554 relative L2 0.001731231575831771\n",
      "training 0.0012537126895040274 relative L2 0.0017306716181337833\n",
      "training 0.0012528854422271252 relative L2 0.0017301146872341633\n",
      "training 0.001252059475518763 relative L2 0.0017295513534918427\n",
      "training 0.0012512370012700558 relative L2 0.0017290015239268541\n",
      "training 0.0012504173209890723 relative L2 0.0017284416826441884\n",
      "training 0.0012496009003371 relative L2 0.0017278968589380383\n",
      "training 0.0012487908825278282 relative L2 0.0017273447010666132\n",
      "training 0.0012479821452870965 relative L2 0.0017267989460378885\n",
      "training 0.0012471777154132724 relative L2 0.0017262561013922095\n",
      "training 0.0012463771272450686 relative L2 0.0017257112776860595\n",
      "training 0.0012455793330445886 relative L2 0.0017251739045605063\n",
      "training 0.0012447835179045796 relative L2 0.001724632573314011\n",
      "training 0.0012439930578693748 relative L2 0.0017240989254787564\n",
      "training 0.00124320387840271 relative L2 0.001723561785183847\n",
      "training 0.0012424192391335964 relative L2 0.0017230309313163161\n",
      "training 0.0012416364625096321 relative L2 0.00172250100877136\n",
      "training 0.0012408588081598282 relative L2 0.0017219720175489783\n",
      "training 0.00124008406419307 relative L2 0.001721449545584619\n",
      "training 0.0012393139768391848 relative L2 0.0017209226498380303\n",
      "training 0.0012385452864691615 relative L2 0.0017204047180712223\n",
      "training 0.0012377811362966895 relative L2 0.001719881547614932\n",
      "training 0.0012370178010314703 relative L2 0.0017193673411384225\n",
      "training 0.001236260519362986 relative L2 0.001718850457109511\n",
      "training 0.0012355046346783638 relative L2 0.0017183372983708978\n",
      "training 0.001234753872267902 relative L2 0.0017178269336000085\n",
      "training 0.001234003808349371 relative L2 0.0017173165688291192\n",
      "training 0.0012332592159509659 relative L2 0.0017168117919936776\n",
      "training 0.001232517883181572 relative L2 0.0017163052689284086\n",
      "training 0.001231778645887971 relative L2 0.0017158028203994036\n",
      "training 0.0012310417369008064 relative L2 0.0017153013031929731\n",
      "training 0.001230308786034584 relative L2 0.0017148012993857265\n",
      "training 0.0012295786291360855 relative L2 0.0017143051372841\n",
      "training 0.0012288523139432073 relative L2 0.00171380874235183\n",
      "training 0.001228129374794662 relative L2 0.0017133178189396858\n",
      "training 0.0012274092296138406 relative L2 0.001712824683636427\n",
      "training 0.0012266935082152486 relative L2 0.0017123394645750523\n",
      "training 0.0012259805807843804 relative L2 0.0017118501709774137\n",
      "training 0.0012252706801518798 relative L2 0.0017113659996539354\n",
      "training 0.0012245617108419538 relative L2 0.0017108804313465953\n",
      "training 0.0012238554190844297 relative L2 0.0017103997524827719\n",
      "training 0.0012231539003551006 relative L2 0.001709920121356845\n",
      "training 0.001222454127855599 relative L2 0.001709440490230918\n",
      "training 0.0012217574985697865 relative L2 0.0017089671455323696\n",
      "training 0.0012210654094815254 relative L2 0.0017084931023418903\n",
      "training 0.0012203763471916318 relative L2 0.0017080195248126984\n",
      "training 0.0012196868192404509 relative L2 0.0017075504874810576\n",
      "training 0.001219003228470683 relative L2 0.0017070797039195895\n",
      "training 0.0012183204526081681 relative L2 0.0017066150903701782\n",
      "training 0.0012176423333585262 relative L2 0.001706149778328836\n",
      "training 0.0012169667752459645 relative L2 0.0017056871438398957\n",
      "training 0.0012162936618551612 relative L2 0.0017052257899194956\n",
      "training 0.0012156247394159436 relative L2 0.0017047671135514975\n",
      "training 0.0012149580288678408 relative L2 0.0017043097177520394\n",
      "training 0.0012142934137955308 relative L2 0.001703853951767087\n",
      "training 0.0012136319419369102 relative L2 0.0017034007469192147\n",
      "training 0.0012129723327234387 relative L2 0.001702948589809239\n",
      "training 0.0012123179621994495 relative L2 0.001702499226666987\n",
      "training 0.0012116635916754603 relative L2 0.0017020503291860223\n",
      "training 0.0012110141105949879 relative L2 0.0017016044585034251\n",
      "training 0.0012103667249903083 relative L2 0.0017011594027280807\n",
      "training 0.001209721900522709 relative L2 0.0017007160931825638\n",
      "training 0.0012090795207768679 relative L2 0.0017002769745886326\n",
      "training 0.0012084413319826126 relative L2 0.0016998333157971501\n",
      "training 0.0012078030267730355 relative L2 0.0016993999015539885\n",
      "training 0.0012071701930835843 relative L2 0.0016989618306979537\n",
      "training 0.0012065399205312133 relative L2 0.0016985313268378377\n",
      "training 0.0012059103464707732 relative L2 0.001698095933534205\n",
      "training 0.001205284963361919 relative L2 0.0016976686893031001\n",
      "training 0.0012046635383740067 relative L2 0.001697239000350237\n",
      "training 0.0012040442088618875 relative L2 0.0016968125710263848\n",
      "training 0.0012034261599183083 relative L2 0.0016963859088718891\n",
      "training 0.0012028104392811656 relative L2 0.001695962157100439\n",
      "training 0.0012021977454423904 relative L2 0.0016955413157120347\n",
      "training 0.0012015876127406955 relative L2 0.001695119310170412\n",
      "training 0.0012009800411760807 relative L2 0.0016947026597335935\n",
      "training 0.001200375845655799 relative L2 0.0016942844958975911\n",
      "training 0.00119977502617985 relative L2 0.0016938723856583238\n",
      "training 0.0011991758365184069 relative L2 0.0016934563172981143\n",
      "training 0.0011985795572400093 relative L2 0.001693046884611249\n",
      "training 0.0011979833943769336 relative L2 0.0016926336102187634\n",
      "training 0.001197391888126731 relative L2 0.0016922281356528401\n",
      "training 0.0011968024773523211 relative L2 0.001691818586550653\n",
      "training 0.0011962150456383824 relative L2 0.0016914157895371318\n",
      "training 0.0011956315720453858 relative L2 0.0016910110134631395\n",
      "training 0.0011950501939281821 relative L2 0.0016906105447560549\n",
      "training 0.0011944713769480586 relative L2 0.0016902103088796139\n",
      "training 0.001193894655443728 relative L2 0.0016898103058338165\n",
      "training 0.001193319563753903 relative L2 0.0016894147265702486\n",
      "training 0.00119274843018502 relative L2 0.0016890179831534624\n",
      "training 0.0011921798577532172 relative L2 0.0016886269440874457\n",
      "training 0.0011916121002286673 relative L2 0.0016882321797311306\n",
      "training 0.0011910483008250594 relative L2 0.001687844400294125\n",
      "training 0.0011904863640666008 relative L2 0.0016874532448127866\n",
      "training 0.0011899274541065097 relative L2 0.0016870703548192978\n",
      "training 0.001189370988868177 relative L2 0.0016866783844307065\n",
      "training 0.0011888141743838787 relative L2 0.0016862995689734817\n",
      "training 0.0011882620165124536 relative L2 0.0016859122551977634\n",
      "training 0.0011877139331772923 relative L2 0.001685536466538906\n",
      "training 0.0011871655005961657 relative L2 0.00168515311088413\n",
      "training 0.0011866213753819466 relative L2 0.0016847801161929965\n",
      "training 0.001186079578474164 relative L2 0.0016844007186591625\n",
      "training 0.0011855404591187835 relative L2 0.0016840288881212473\n",
      "training 0.0011850005248561502 relative L2 0.0016836515860632062\n",
      "training 0.0011844646651297808 relative L2 0.001683284412138164\n",
      "training 0.0011839320650324225 relative L2 0.0016829099040478468\n",
      "training 0.0011833999305963516 relative L2 0.0016825434286147356\n",
      "training 0.00118286965880543 relative L2 0.0016821734607219696\n",
      "training 0.0011823427630588412 relative L2 0.0016818101285025477\n",
      "training 0.0011818180792033672 relative L2 0.00168144260533154\n",
      "training 0.0011812954908236861 relative L2 0.0016810832312330604\n",
      "training 0.001180774997919798 relative L2 0.0016807176871225238\n",
      "training 0.001180256367661059 relative L2 0.0016803608741611242\n",
      "training 0.0011797402985394 relative L2 0.0016800001030787826\n",
      "training 0.001179227139800787 relative L2 0.001679645269177854\n",
      "training 0.0011787150288000703 relative L2 0.001679286826401949\n",
      "training 0.001178205944597721 relative L2 0.001678934902884066\n",
      "training 0.0011776991887018085 relative L2 0.001678580534644425\n",
      "training 0.0011771952267736197 relative L2 0.001678231987170875\n",
      "training 0.0011766924289986491 relative L2 0.0016778786666691303\n",
      "training 0.0011761907953768969 relative L2 0.0016775339609012008\n",
      "training 0.0011756933527067304 relative L2 0.0016771832015365362\n",
      "training 0.0011751966085284948 relative L2 0.0016768414061516523\n",
      "training 0.0011747018434107304 relative L2 0.0016764921601861715\n",
      "training 0.0011742092901840806 relative L2 0.0016761558363214135\n",
      "training 0.0011737198801711202 relative L2 0.0016758088022470474\n",
      "training 0.0011732324492186308 relative L2 0.0016754763200879097\n",
      "training 0.0011727475794032216 relative L2 0.001675129751674831\n",
      "training 0.0011722641065716743 relative L2 0.0016748033231124282\n",
      "training 0.0011717830784618855 relative L2 0.0016744588501751423\n",
      "training 0.0011713070562109351 relative L2 0.0016741377767175436\n",
      "training 0.0011708310339599848 relative L2 0.001673793070949614\n",
      "training 0.0011703597847372293 relative L2 0.0016734839882701635\n",
      "training 0.001169891795143485 relative L2 0.0016731367213651538\n",
      "training 0.001169427763670683 relative L2 0.0016728421906009316\n",
      "training 0.0011689687380567193 relative L2 0.0016724929446354508\n",
      "training 0.0011685186764225364 relative L2 0.0016722276341170073\n",
      "training 0.0011680807219818234 relative L2 0.001671884674578905\n",
      "training 0.0011676643043756485 relative L2 0.0016716811805963516\n",
      "training 0.001167281181551516 relative L2 0.0016713683726266026\n",
      "training 0.0011669525410979986 relative L2 0.0016713112127035856\n",
      "training 0.0011667180806398392 relative L2 0.0016711201751604676\n",
      "training 0.0011666386853903532 relative L2 0.001671446138061583\n",
      "training 0.001166846021078527 relative L2 0.0016717045800760388\n",
      "training 0.001167540904134512 relative L2 0.0016731215873733163\n",
      "training 0.0011691158870235085 relative L2 0.0016749091446399689\n",
      "training 0.0011722255731001496 relative L2 0.0016795687843114138\n",
      "training 0.0011781015200540423 relative L2 0.001686340430751443\n",
      "training 0.001188767608255148 relative L2 0.0017008132999762893\n",
      "training 0.0012082498287782073 relative L2 0.0017232658574357629\n",
      "training 0.0012426116736605763 relative L2 0.0017661394085735083\n",
      "training 0.0013038007309660316 relative L2 0.0018313205800950527\n",
      "training 0.0014063045382499695 relative L2 0.0019401968456804752\n",
      "training 0.0015766943106427789 relative L2 0.002079844241961837\n",
      "training 0.00181980908382684 relative L2 0.002254566876217723\n",
      "training 0.00213532499037683 relative L2 0.002370672533288598\n",
      "training 0.002369981724768877 relative L2 0.0023814640007913113\n",
      "training 0.002384669380262494 relative L2 0.0021751150488853455\n",
      "training 0.0019921218045055866 relative L2 0.001868521561846137\n",
      "training 0.0014611523365601897 relative L2 0.0016702419379726052\n",
      "training 0.0011655444977805018 relative L2 0.0017496144864708185\n",
      "training 0.0012816233793273568 relative L2 0.0019496483728289604\n",
      "training 0.001592265092767775 relative L2 0.0020254177507013083\n",
      "training 0.0017247284995391965 relative L2 0.0019227191805839539\n",
      "training 0.0015481188893318176 relative L2 0.0017304403008893132\n",
      "training 0.0012531536631286144 relative L2 0.0016667649615556002\n",
      "training 0.001160541782155633 relative L2 0.001768902875483036\n",
      "training 0.001307899714447558 relative L2 0.0018646074458956718\n",
      "training 0.0014585874741896987 relative L2 0.0018422608263790607\n",
      "training 0.0014199160505086184 relative L2 0.0017251846147701144\n",
      "training 0.001245390041731298 relative L2 0.0016622128896415234\n",
      "training 0.001153839286416769 relative L2 0.0017108330503106117\n",
      "training 0.0012225667014718056 relative L2 0.0017769386759027839\n",
      "training 0.00132264185231179 relative L2 0.0017730123363435268\n",
      "training 0.001314037013798952 relative L2 0.0017028403235599399\n",
      "training 0.0012126803630962968 relative L2 0.0016611203318461776\n",
      "training 0.0011522481217980385 relative L2 0.001687890267930925\n",
      "training 0.0011896926444023848 relative L2 0.0017286393558606505\n",
      "training 0.0012504603946581483 relative L2 0.001727495575323701\n",
      "training 0.001246715779416263 relative L2 0.0016848794184625149\n",
      "training 0.0011866616550832987 relative L2 0.0016597608337178826\n",
      "training 0.0011503314599394798 relative L2 0.0016757387202233076\n",
      "training 0.0011724885553121567 relative L2 0.001700078253634274\n",
      "training 0.0012086571659892797 relative L2 0.0016994463512673974\n",
      "training 0.0012061495799571276 relative L2 0.0016735055251047015\n",
      "training 0.0011702958727255464 relative L2 0.0016584458062425256\n",
      "training 0.001148477429524064 relative L2 0.0016678045503795147\n",
      "training 0.0011613384122028947 relative L2 0.001682148315012455\n",
      "training 0.001182727050036192 relative L2 0.0016820807941257954\n",
      "training 0.0011814023600891232 relative L2 0.0016664760187268257\n",
      "training 0.0011602145386859775 relative L2 0.0016572072636336088\n",
      "training 0.0011467232834547758 relative L2 0.0016623210394755006\n",
      "training 0.001153676537796855 relative L2 0.0016708375187590718\n",
      "training 0.0011664776830002666 relative L2 0.0016714428784325719\n",
      "training 0.0011663868790492415 relative L2 0.001662126393057406\n",
      "training 0.001153989345766604 relative L2 0.0016560639487579465\n",
      "training 0.001145093352533877 relative L2 0.0016583859687671065\n",
      "training 0.0011482061818242073 relative L2 0.0016633878694847226\n",
      "training 0.0011558086844161153 relative L2 0.0016644473653286695\n",
      "training 0.0011565773747861385 relative L2 0.0016590649029240012\n",
      "training 0.0011496130609884858 relative L2 0.0016549999127164483\n",
      "training 0.0011435713386163116 relative L2 0.0016555889742448926\n",
      "training 0.0011443343246355653 relative L2 0.0016583887627348304\n",
      "training 0.0011486544972285628 relative L2 0.0016596066998317838\n",
      "training 0.0011498233070597053 relative L2 0.0016566769918426871\n",
      "training 0.0011462016263976693 relative L2 0.0016539825592190027\n",
      "training 0.0011421156814321876 relative L2 0.0016535986214876175\n",
      "training 0.001141589367762208 relative L2 0.001655014930292964\n",
      "training 0.0011438207002356648 relative L2 0.0016561747761443257\n",
      "training 0.00114504958037287 relative L2 0.00165470689535141\n",
      "training 0.0011433906620368361 relative L2 0.001652971957810223\n",
      "training 0.0011406749254092574 relative L2 0.0016521377256140113\n",
      "training 0.0011395759647712111 relative L2 0.0016526663675904274\n",
      "training 0.0011404493125155568 relative L2 0.0016535442555323243\n",
      "training 0.0011414006585255265 relative L2 0.001652913517318666\n",
      "training 0.0011408313876017928 relative L2 0.0016519103664904833\n",
      "training 0.0011391685111448169 relative L2 0.0016509752022102475\n",
      "training 0.0011379698989912868 relative L2 0.0016509650740772486\n",
      "training 0.0011379997013136744 relative L2 0.001651484752073884\n",
      "training 0.001138548832386732 relative L2 0.0016512757865712047\n",
      "training 0.0011384921381250024 relative L2 0.0016507890541106462\n",
      "training 0.001137584913522005 relative L2 0.0016499529592692852\n",
      "training 0.0011365496320649981 relative L2 0.001649659825488925\n",
      "training 0.0011361150536686182 relative L2 0.0016498189652338624\n",
      "training 0.0011362432269379497 relative L2 0.0016497517935931683\n",
      "training 0.001136312959715724 relative L2 0.001649586483836174\n",
      "training 0.0011358946794643998 relative L2 0.00164895411580801\n",
      "training 0.001135150552727282 relative L2 0.0016485806554555893\n",
      "training 0.0011345581151545048 relative L2 0.001648452365770936\n",
      "training 0.0011343526421114802 relative L2 0.0016483648214489222\n",
      "training 0.001134323887526989 relative L2 0.0016483236104249954\n",
      "training 0.001134128076955676 relative L2 0.0016478996258229017\n",
      "training 0.0011336614843457937 relative L2 0.001647580647841096\n",
      "training 0.0011331242276355624 relative L2 0.001647302182391286\n",
      "training 0.0011327542597427964 relative L2 0.0016471464186906815\n",
      "training 0.0011325750965625048 relative L2 0.0016470991540700197\n",
      "training 0.0011324158404022455 relative L2 0.001646815799176693\n",
      "training 0.001132121542468667 relative L2 0.0016465822700411081\n",
      "training 0.0011316994205117226 relative L2 0.0016462492058053613\n",
      "training 0.001131282770074904 relative L2 0.0016460365150123835\n",
      "training 0.0011309782275930047 relative L2 0.0016459134640172124\n",
      "training 0.00113075936678797 relative L2 0.0016456980956718326\n",
      "training 0.0011305263033136725 relative L2 0.0016455301083624363\n",
      "training 0.0011302083730697632 relative L2 0.0016452184645459056\n",
      "training 0.0011298323515802622 relative L2 0.0016449891263619065\n",
      "training 0.001129474607296288 relative L2 0.0016447851667180657\n",
      "training 0.0011291810078546405 relative L2 0.0016445793444290757\n",
      "training 0.0011289259418845177 relative L2 0.0016444260254502296\n",
      "training 0.001128649921156466 relative L2 0.0016441562911495566\n",
      "training 0.0011283275671303272 relative L2 0.0016439406899735332\n",
      "training 0.00112797727342695 relative L2 0.0016436870209872723\n",
      "training 0.0011276403674855828 relative L2 0.0016434694407507777\n",
      "training 0.0011273372219875455 relative L2 0.0016432885313406587\n",
      "training 0.0011270528193563223 relative L2 0.0016430453397333622\n",
      "training 0.0011267540976405144 relative L2 0.0016428460367023945\n",
      "training 0.0011264302302151918 relative L2 0.0016425803769379854\n",
      "training 0.0011260913452133536 relative L2 0.001642354647628963\n",
      "training 0.001125761424191296 relative L2 0.0016421332256868482\n",
      "training 0.0011254504788666964 relative L2 0.001641900627873838\n",
      "training 0.0011251495452597737 relative L2 0.0016417033039033413\n",
      "training 0.0011248427908867598 relative L2 0.0016414490528404713\n",
      "training 0.001124523114413023 relative L2 0.001641232636757195\n",
      "training 0.0011241961037740111 relative L2 0.0016409882809966803\n",
      "training 0.001123872702009976 relative L2 0.0016407618531957269\n",
      "training 0.0011235607089474797 relative L2 0.001640551257878542\n",
      "training 0.0011232569813728333 relative L2 0.0016403166810050607\n",
      "training 0.0011229534866288304 relative L2 0.001640109228901565\n",
      "training 0.0011226431233808398 relative L2 0.0016398682491853833\n",
      "training 0.0011223303154110909 relative L2 0.0016396526480093598\n",
      "training 0.0011220215819776058 relative L2 0.0016394334379583597\n",
      "training 0.0011217208812013268 relative L2 0.001639215275645256\n",
      "training 0.0011214306578040123 relative L2 0.0016390149248763919\n",
      "training 0.001121142297051847 relative L2 0.0016387926880270243\n",
      "training 0.0011208555661141872 relative L2 0.001638592453673482\n",
      "training 0.0011205680202692747 relative L2 0.0016383766196668148\n",
      "training 0.0011202831519767642 relative L2 0.0016381755704060197\n",
      "training 0.0011200058506801724 relative L2 0.001637978246435523\n",
      "training 0.0011197328567504883 relative L2 0.0016377760330215096\n",
      "training 0.0011194649850949645 relative L2 0.001637589535675943\n",
      "training 0.0011191992089152336 relative L2 0.0016373886028304696\n",
      "training 0.0011189341312274337 relative L2 0.0016372036188840866\n",
      "training 0.0011186726624146104 relative L2 0.0016370133962482214\n",
      "training 0.0011184148024767637 relative L2 0.0016368271317332983\n",
      "training 0.0011181595036759973 relative L2 0.0016366482013836503\n",
      "training 0.0011179078137502074 relative L2 0.0016364606563001871\n",
      "training 0.0011176567059010267 relative L2 0.0016362861497327685\n",
      "training 0.0011174079263582826 relative L2 0.00163610081654042\n",
      "training 0.0011171590304002166 relative L2 0.0016359257278963923\n",
      "training 0.0011169119970872998 relative L2 0.0016357476124539971\n",
      "training 0.0011166669428348541 relative L2 0.0016355690313503146\n",
      "training 0.0011164225870743394 relative L2 0.0016353968530893326\n",
      "training 0.001116178696975112 relative L2 0.001635217689909041\n",
      "training 0.001115935854613781 relative L2 0.0016350472578778863\n",
      "training 0.0011156935943290591 relative L2 0.0016348707722499967\n",
      "training 0.0011154531966894865 relative L2 0.0016347002238035202\n",
      "training 0.0011152132647112012 relative L2 0.0016345282783731818\n",
      "training 0.0011149741476401687 relative L2 0.0016343561001121998\n",
      "training 0.0011147368932142854 relative L2 0.0016341892769560218\n",
      "training 0.0011145004536956549 relative L2 0.0016340167494490743\n",
      "training 0.0011142652947455645 relative L2 0.0016338513232767582\n",
      "training 0.0011140304850414395 relative L2 0.0016336808912456036\n",
      "training 0.0011137966066598892 relative L2 0.0016335148829966784\n",
      "training 0.0011135637760162354 relative L2 0.0016333481762558222\n",
      "training 0.0011133317602798343 relative L2 0.001633180072531104\n",
      "training 0.0011131000937893987 relative L2 0.0016330176731571555\n",
      "training 0.001112870522774756 relative L2 0.0016328503843396902\n",
      "training 0.0011126408353447914 relative L2 0.0016326879849657416\n",
      "training 0.0011124114971607924 relative L2 0.0016325225587934256\n",
      "training 0.0011121833231300116 relative L2 0.0016323582967743278\n",
      "training 0.0011119545670226216 relative L2 0.0016321964794769883\n",
      "training 0.00111172697506845 relative L2 0.001632031169719994\n",
      "training 0.0011114991502836347 relative L2 0.0016318692360073328\n",
      "training 0.001111270859837532 relative L2 0.0016317050904035568\n",
      "training 0.0011110440827906132 relative L2 0.0016315433895215392\n",
      "training 0.0011108178878203034 relative L2 0.0016313806409016252\n",
      "training 0.0011105905286967754 relative L2 0.0016312174266204238\n",
      "training 0.0011103642173111439 relative L2 0.0016310552600771189\n",
      "training 0.0011101362761110067 relative L2 0.00163089029956609\n",
      "training 0.0011099083349108696 relative L2 0.0016307279001921415\n",
      "training 0.0011096798116341233 relative L2 0.0016305615426972508\n",
      "training 0.0011094495421275496 relative L2 0.0016303970478475094\n",
      "training 0.0011092191562056541 relative L2 0.0016302292933687568\n",
      "training 0.0011089869076386094 relative L2 0.0016300620045512915\n",
      "training 0.0011087525635957718 relative L2 0.001629893435165286\n",
      "training 0.0011085172882303596 relative L2 0.0016297245165333152\n",
      "training 0.0011082811979576945 relative L2 0.0016295560635626316\n",
      "training 0.0011080442927777767 relative L2 0.0016293834196403623\n",
      "training 0.0011078049428761005 relative L2 0.0016292125219479203\n",
      "training 0.0011075640795752406 relative L2 0.0016290381317958236\n",
      "training 0.0011073230998590589 relative L2 0.0016288652550429106\n",
      "training 0.0011070810724049807 relative L2 0.0016286892350763083\n",
      "training 0.0011068364838138223 relative L2 0.001628513797186315\n",
      "training 0.0011065916623920202 relative L2 0.0016283383592963219\n",
      "training 0.0011063468409702182 relative L2 0.0016281622229143977\n",
      "training 0.0011061020195484161 relative L2 0.0016279863193631172\n",
      "training 0.0011058564996346831 relative L2 0.0016278092516586185\n",
      "training 0.0011056114453822374 relative L2 0.0016276354435831308\n",
      "training 0.0011053687194362283 relative L2 0.001627459772862494\n",
      "training 0.0011051257606595755 relative L2 0.0016272806096822023\n",
      "training 0.0011048762826249003 relative L2 0.0016270967898890376\n",
      "training 0.0011046221479773521 relative L2 0.0016269143670797348\n",
      "training 0.0011043688282370567 relative L2 0.0016267327591776848\n",
      "training 0.0011041158577427268 relative L2 0.0016265508020296693\n",
      "training 0.0011038638185709715 relative L2 0.0016263712896034122\n",
      "training 0.0011036142241209745 relative L2 0.0016261925920844078\n",
      "training 0.0011033674236387014 relative L2 0.0016260159900411963\n",
      "training 0.0011031229514628649 relative L2 0.0016258404357358813\n",
      "training 0.0011028808075934649 relative L2 0.0016256660455837846\n",
      "training 0.0011026414576917887 relative L2 0.0016254950314760208\n",
      "training 0.0011024049017578363 relative L2 0.0016253252979367971\n",
      "training 0.001102173002436757 relative L2 0.0016251608030870557\n",
      "training 0.0011019458761438727 relative L2 0.0016250016633421183\n",
      "training 0.0011017295764759183 relative L2 0.0016248580068349838\n",
      "training 0.0011015330674126744 relative L2 0.0016247176099568605\n",
      "training 0.0011013455223292112 relative L2 0.0016245811711996794\n",
      "training 0.0011011605383828282 relative L2 0.0016244424041360617\n",
      "training 0.0011009755544364452 relative L2 0.0016243044519796968\n",
      "training 0.0011007912689819932 relative L2 0.0016241662669926882\n",
      "training 0.0011006069835275412 relative L2 0.001624028547666967\n",
      "training 0.0011004232801496983 relative L2 0.0016238904790952802\n",
      "training 0.0011002393439412117 relative L2 0.001623755320906639\n",
      "training 0.001100056921131909 relative L2 0.0016236198134720325\n",
      "training 0.001099875895306468 relative L2 0.001623486983589828\n",
      "training 0.00109969568438828 relative L2 0.0016233534552156925\n",
      "training 0.0010995157063007355 relative L2 0.0016232230700552464\n",
      "training 0.0010993374744430184 relative L2 0.001623092801310122\n",
      "training 0.001099158776924014 relative L2 0.0016229639295488596\n",
      "training 0.001098981243558228 relative L2 0.0016228362219408154\n",
      "training 0.001098804292269051 relative L2 0.001622708747163415\n",
      "training 0.001098628155887127 relative L2 0.001622584299184382\n",
      "training 0.0010984530672430992 relative L2 0.0016224585706368089\n",
      "training 0.0010982789099216461 relative L2 0.0016223356360569596\n",
      "training 0.0010981055675074458 relative L2 0.0016222115373238921\n",
      "training 0.0010979322250932455 relative L2 0.0016220902325585485\n",
      "training 0.0010977599304169416 relative L2 0.001621967414394021\n",
      "training 0.001097588799893856 relative L2 0.0016218471573665738\n",
      "training 0.0010974176693707705 relative L2 0.001621723989956081\n",
      "training 0.0010972472373396158 relative L2 0.0016216057119891047\n",
      "training 0.001097077620215714 relative L2 0.0016214823117479682\n",
      "training 0.001096908701583743 relative L2 0.0016213650815188885\n",
      "training 0.001096739899367094 relative L2 0.0016212421469390392\n",
      "training 0.0010965722613036633 relative L2 0.0016211248002946377\n",
      "training 0.001096404273994267 relative L2 0.0016210025642067194\n",
      "training 0.0010962378000840545 relative L2 0.0016208846354857087\n",
      "training 0.0010960708605125546 relative L2 0.0016207635635510087\n",
      "training 0.0010959053179249167 relative L2 0.0016206459840759635\n",
      "training 0.001095740357413888 relative L2 0.0016205268912017345\n",
      "training 0.0010955760953947902 relative L2 0.0016204086132347584\n",
      "training 0.0010954121826216578 relative L2 0.001620291150175035\n",
      "training 0.0010952496668323874 relative L2 0.001620174152776599\n",
      "training 0.0010950879659503698 relative L2 0.0016200578538700938\n",
      "training 0.0010949262650683522 relative L2 0.0016199405072256923\n",
      "training 0.001094765611924231 relative L2 0.0016198260709643364\n",
      "training 0.0010946050751954317 relative L2 0.001619708607904613\n",
      "training 0.0010944451205432415 relative L2 0.0016195967327803373\n",
      "training 0.0010942857479676604 relative L2 0.0016194789204746485\n",
      "training 0.0010941269574686885 relative L2 0.0016193691408261657\n",
      "training 0.0010939693311229348 relative L2 0.0016192514449357986\n",
      "training 0.0010938106570392847 relative L2 0.0016191420145332813\n",
      "training 0.001093653729185462 relative L2 0.0016190257156267762\n",
      "training 0.0010934965685009956 relative L2 0.001618916285224259\n",
      "training 0.001093340222723782 relative L2 0.0016188016161322594\n",
      "training 0.0010931846918538213 relative L2 0.0016186930006369948\n",
      "training 0.0010930290445685387 relative L2 0.0016185790300369263\n",
      "training 0.0010928751435130835 relative L2 0.0016184718115255237\n",
      "training 0.0010927217081189156 relative L2 0.0016183594707399607\n",
      "training 0.0010925688548013568 relative L2 0.001618251670151949\n",
      "training 0.0010924162343144417 relative L2 0.0016181392129510641\n",
      "training 0.0010922634974122047 relative L2 0.0016180326929315925\n",
      "training 0.0010921118082478642 relative L2 0.001617921399883926\n",
      "training 0.0010919609339907765 relative L2 0.0016178152291104198\n",
      "training 0.0010918101761490107 relative L2 0.0016177038196474314\n",
      "training 0.001091660000383854 relative L2 0.0016176000935956836\n",
      "training 0.00109151063952595 relative L2 0.0016174878692254424\n",
      "training 0.0010913613950833678 relative L2 0.0016173854237422347\n",
      "training 0.001091212616302073 relative L2 0.0016172730829566717\n",
      "training 0.0010910645360127091 relative L2 0.0016171723837032914\n",
      "training 0.0010909171542152762 relative L2 0.0016170607414096594\n",
      "training 0.001090771402232349 relative L2 0.0016169616719707847\n",
      "training 0.0010906249517574906 relative L2 0.0016168496804311872\n",
      "training 0.0010904803639277816 relative L2 0.00161675363779068\n",
      "training 0.0010903355432674289 relative L2 0.001616642577573657\n",
      "training 0.001090195495635271 relative L2 0.0016165523556992412\n",
      "training 0.001090053003281355 relative L2 0.0016164357075467706\n",
      "training 0.0010899119079113007 relative L2 0.0016163530526682734\n",
      "training 0.001089773722924292 relative L2 0.0016162351239472628\n",
      "training 0.0010896383319050074 relative L2 0.0016161655075848103\n",
      "training 0.001089508063159883 relative L2 0.0016160475788637996\n",
      "training 0.0010893854778259993 relative L2 0.0016160063678398728\n",
      "training 0.001089276745915413 relative L2 0.0016159008955582976\n",
      "training 0.0010891956044360995 relative L2 0.0016159372171387076\n",
      "training 0.0010891600977629423 relative L2 0.0016158927464857697\n",
      "training 0.001089206780306995 relative L2 0.0016161443199962378\n",
      "training 0.0010894056176766753 relative L2 0.001616347930394113\n",
      "training 0.0010898764012381434 relative L2 0.0016172544565051794\n",
      "training 0.0010908602271229029 relative L2 0.001618394162505865\n",
      "training 0.0010927703697234392 relative L2 0.001621388248167932\n",
      "training 0.0010964173125103116 relative L2 0.00162588432431221\n",
      "training 0.0011032418115064502 relative L2 0.0016357314307242632\n",
      "training 0.001116013154387474 relative L2 0.0016517119947820902\n",
      "training 0.0011394558241590858 relative L2 0.001683597918599844\n",
      "training 0.0011830282164737582 relative L2 0.0017358888871967793\n",
      "training 0.0012609813129529357 relative L2 0.0018301502568647265\n",
      "training 0.0014008631696924567 relative L2 0.0019691905472427607\n",
      "training 0.0016286331228911877 relative L2 0.002172216074541211\n",
      "training 0.001980949891731143 relative L2 0.0023770148400217295\n",
      "training 0.002381737809628248 relative L2 0.0025282909628003836\n",
      "training 0.0026905739214271307 relative L2 0.0024451669305562973\n",
      "training 0.0025213195476680994 relative L2 0.002133588306605816\n",
      "training 0.0019104533130303025 relative L2 0.001730755320750177\n",
      "training 0.0012533834669739008 relative L2 0.0016299131093546748\n",
      "training 0.0011088565224781632 relative L2 0.0018681887304410338\n",
      "training 0.001460438477806747 relative L2 0.0020711547695100307\n",
      "training 0.001803680555894971 relative L2 0.0020317980088293552\n",
      "training 0.0017307628877460957 relative L2 0.0017760245827957988\n",
      "training 0.0013209342723712325 relative L2 0.0016141408123075962\n",
      "training 0.0010866249212995172 relative L2 0.0017211787635460496\n",
      "training 0.0012371102347970009 relative L2 0.0018762152176350355\n",
      "training 0.0014765708474442363 relative L2 0.0018669317942112684\n",
      "training 0.0014584591845050454 relative L2 0.0017040139064192772\n",
      "training 0.0012142385821789503 relative L2 0.001613027649000287\n",
      "training 0.0010852008126676083 relative L2 0.0016923670191317797\n",
      "training 0.0011955342488363385 relative L2 0.0017813568701967597\n",
      "training 0.0013289846247062087 relative L2 0.0017523282440379262\n",
      "training 0.0012828776380047202 relative L2 0.0016466134693473577\n",
      "training 0.0011322366772219539 relative L2 0.0016165570123121142\n",
      "training 0.001090224483050406 relative L2 0.0016810110537335277\n",
      "training 0.0011793454177677631 relative L2 0.001719664316624403\n",
      "training 0.0012370437616482377 relative L2 0.0016799465520307422\n",
      "training 0.0011778323678299785 relative L2 0.0016196025535464287\n",
      "training 0.0010944739915430546 relative L2 0.0016232706839218736\n",
      "training 0.001099583227187395 relative L2 0.0016676407540217042\n",
      "training 0.001160430139862001 relative L2 0.001675084000453353\n",
      "training 0.0011725685326382518 relative L2 0.001639762194827199\n",
      "training 0.0011215282138437033 relative L2 0.0016121530206874013\n",
      "training 0.001084027229808271 relative L2 0.0016268863109871745\n",
      "training 0.0011046187719330192 relative L2 0.001651821774430573\n",
      "training 0.001138258376158774 relative L2 0.0016452025156468153\n",
      "training 0.0011302415514364839 relative L2 0.0016209144378080964\n",
      "training 0.0010956729529425502 relative L2 0.0016121759545058012\n",
      "training 0.001083914889022708 relative L2 0.0016262243734672666\n",
      "training 0.001103694667108357 relative L2 0.0016375469276681542\n",
      "training 0.0011184532195329666 relative L2 0.0016276028472930193\n",
      "training 0.001105615170672536 relative L2 0.0016135921468958259\n",
      "training 0.0010857721790671349 relative L2 0.0016132234595716\n",
      "training 0.0010852761333808303 relative L2 0.0016228940803557634\n",
      "training 0.0010990560986101627 relative L2 0.0016267758328467607\n",
      "training 0.0011036443756893277 relative L2 0.0016183058032765985\n",
      "training 0.0010926708346232772 relative L2 0.0016111898003146052\n",
      "training 0.0010825740173459053 relative L2 0.0016133602475747466\n",
      "training 0.0010854392312467098 relative L2 0.0016189463203772902\n",
      "training 0.0010935632744804025 relative L2 0.001619571354240179\n",
      "training 0.001093815779313445 relative L2 0.0016136814374476671\n",
      "training 0.0010862286435440183 relative L2 0.0016104138921946287\n",
      "training 0.0010815603891387582 relative L2 0.0016127415001392365\n",
      "training 0.0010845944052562118 relative L2 0.0016156643396243453\n",
      "training 0.0010889979312196374 relative L2 0.0016152366297319531\n",
      "training 0.0010879375040531158 relative L2 0.0016114868922159076\n",
      "training 0.001083158073015511 relative L2 0.0016100270440801978\n",
      "training 0.0010810497915372252 relative L2 0.001611788640730083\n",
      "training 0.0010833049891516566 relative L2 0.001613245694898069\n",
      "training 0.0010856291046366096 relative L2 0.0016127062262967229\n",
      "training 0.0010845217620953918 relative L2 0.0016103691887110472\n",
      "training 0.0010815863497555256 relative L2 0.0016096666222438216\n",
      "training 0.0010805599158629775 relative L2 0.0016108467243611813\n",
      "training 0.0010820337338373065 relative L2 0.0016115567414090037\n",
      "training 0.0010832713451236486 relative L2 0.0016111258883029222\n",
      "training 0.001082396018318832 relative L2 0.001609667669981718\n",
      "training 0.0010805984493345022 relative L2 0.0016092899022623897\n",
      "training 0.0010800397722050548 relative L2 0.0016100163338705897\n",
      "training 0.0010809187078848481 relative L2 0.001610351842828095\n",
      "training 0.0010815857676789165 relative L2 0.0016100865323096514\n",
      "training 0.0010810006642714143 relative L2 0.00160916184540838\n",
      "training 0.0010798892471939325 relative L2 0.00160890631377697\n",
      "training 0.0010795047273859382 relative L2 0.0016093191225081682\n",
      "training 0.0010799826122820377 relative L2 0.0016094850143417716\n",
      "training 0.0010803702753037214 relative L2 0.0016093543963506818\n",
      "training 0.0010800169548019767 relative L2 0.001608742051757872\n",
      "training 0.0010793039109557867 relative L2 0.001608532969839871\n",
      "training 0.0010789822554215789 relative L2 0.0016087318072095513\n",
      "training 0.0010791956447064877 relative L2 0.0016088021220639348\n",
      "training 0.0010794112458825111 relative L2 0.0016087552066892385\n",
      "training 0.0010792106622830033 relative L2 0.0016083476366475224\n",
      "training 0.0010787560604512691 relative L2 0.001608175807632506\n",
      "training 0.0010784838814288378 relative L2 0.0016082437941804528\n",
      "training 0.001078540226444602 relative L2 0.001608257764019072\n",
      "training 0.0010786468628793955 relative L2 0.001608248450793326\n",
      "training 0.0010785284684970975 relative L2 0.0016079694032669067\n",
      "training 0.0010782304452732205 relative L2 0.0016078292392194271\n",
      "training 0.001078002154827118 relative L2 0.0016078187618404627\n",
      "training 0.0010779687436297536 relative L2 0.0016077974578365684\n",
      "training 0.001078000059351325 relative L2 0.0016078001353889704\n",
      "training 0.0010779231088235974 relative L2 0.0016076031606644392\n",
      "training 0.0010777234565466642 relative L2 0.0016074907034635544\n",
      "training 0.0010775316040962934 relative L2 0.0016074335435405374\n",
      "training 0.001077448483556509 relative L2 0.001607391401194036\n",
      "training 0.0010774305555969477 relative L2 0.0016073897713795304\n",
      "training 0.0010773686226457357 relative L2 0.0016072466969490051\n",
      "training 0.001077228575013578 relative L2 0.0016071574063971639\n",
      "training 0.0010770700173452497 relative L2 0.0016070767305791378\n",
      "training 0.0010769651271402836 relative L2 0.0016070217825472355\n",
      "training 0.0010769128566607833 relative L2 0.0016070094425231218\n",
      "training 0.001076852553524077 relative L2 0.0016068980330601335\n",
      "training 0.0010767463827505708 relative L2 0.00160682643763721\n",
      "training 0.0010766133200377226 relative L2 0.0016067356336861849\n",
      "training 0.0010765010956674814 relative L2 0.0016066726529970765\n",
      "training 0.001076425425708294 relative L2 0.0016066450625658035\n",
      "training 0.0010763586033135653 relative L2 0.0016065535601228476\n",
      "training 0.0010762682650238276 relative L2 0.0016064936062321067\n",
      "training 0.0010761565063148737 relative L2 0.0016064037336036563\n",
      "training 0.0010760482400655746 relative L2 0.001606338657438755\n",
      "training 0.0010759616270661354 relative L2 0.0016062967479228973\n",
      "training 0.0010758854914456606 relative L2 0.0016062160721048713\n",
      "training 0.0010758008575066924 relative L2 0.0016061612404882908\n",
      "training 0.001075701555237174 relative L2 0.0016060767229646444\n",
      "training 0.0010756001574918628 relative L2 0.0016060119960457087\n",
      "training 0.0010755100520327687 relative L2 0.001605961355380714\n",
      "training 0.001075429143384099 relative L2 0.0016058871988207102\n",
      "training 0.0010753460228443146 relative L2 0.001605834811925888\n",
      "training 0.0010752547532320023 relative L2 0.001605755416676402\n",
      "training 0.0010751591762527823 relative L2 0.0016056924359872937\n",
      "training 0.001075069303624332 relative L2 0.0016056351596489549\n",
      "training 0.001074984553270042 relative L2 0.0016055653104558587\n",
      "training 0.001074901083484292 relative L2 0.0016055124578997493\n",
      "training 0.0010748140048235655 relative L2 0.0016054378356784582\n",
      "training 0.0010747237829491496 relative L2 0.00160537741612643\n",
      "training 0.001074634725227952 relative L2 0.0016053151339292526\n",
      "training 0.0010745483450591564 relative L2 0.0016052491264417768\n",
      "training 0.0010744642931967974 relative L2 0.0016051947604864836\n",
      "training 0.0010743794264271855 relative L2 0.0016051243292167783\n",
      "training 0.0010742926970124245 relative L2 0.0016050648409873247\n",
      "training 0.0010742050362750888 relative L2 0.0016050017438828945\n",
      "training 0.0010741193545982242 relative L2 0.0016049372497946024\n",
      "training 0.0010740351863205433 relative L2 0.0016048820689320564\n",
      "training 0.0010739517165347934 relative L2 0.0016048140823841095\n",
      "training 0.001073867199011147 relative L2 0.001604756573215127\n",
      "training 0.0010737812845036387 relative L2 0.001604691962711513\n",
      "training 0.001073696301318705 relative L2 0.0016046302625909448\n",
      "training 0.0010736132971942425 relative L2 0.0016045731026679277\n",
      "training 0.0010735305259004235 relative L2 0.0016045086085796356\n",
      "training 0.0010734472889453173 relative L2 0.0016044508665800095\n",
      "training 0.00107336335349828 relative L2 0.0016043877694755793\n",
      "training 0.0010732795344665647 relative L2 0.001604327349923551\n",
      "training 0.0010731971124187112 relative L2 0.0016042692586779594\n",
      "training 0.0010731146903708577 relative L2 0.0016042060451582074\n",
      "training 0.001073032384738326 relative L2 0.0016041495837271214\n",
      "training 0.0010729506611824036 relative L2 0.0016040871851146221\n",
      "training 0.0010728684719651937 relative L2 0.0016040281625464559\n",
      "training 0.0010727865155786276 relative L2 0.0016039686743170023\n",
      "training 0.0010727052576839924 relative L2 0.0016039082547649741\n",
      "training 0.0010726245818659663 relative L2 0.0016038515605032444\n",
      "training 0.0010725435568019748 relative L2 0.0016037895111367106\n",
      "training 0.0010724621824920177 relative L2 0.001603731419891119\n",
      "training 0.0010723804589360952 relative L2 0.0016036716988310218\n",
      "training 0.001072299899533391 relative L2 0.0016036124434322119\n",
      "training 0.0010722194565460086 relative L2 0.0016035553999245167\n",
      "training 0.001072139828465879 relative L2 0.0016034954460337758\n",
      "training 0.0010720600839704275 relative L2 0.0016034384025260806\n",
      "training 0.001071980339474976 relative L2 0.0016033791471272707\n",
      "training 0.0010719005949795246 relative L2 0.0016033207066357136\n",
      "training 0.0010718214325606823 relative L2 0.0016032635467126966\n",
      "training 0.0010717420373111963 relative L2 0.001603204756975174\n",
      "training 0.0010716634569689631 relative L2 0.0016031479462981224\n",
      "training 0.0010715847602114081 relative L2 0.0016030900878831744\n",
      "training 0.0010715062962844968 relative L2 0.00160303246229887\n",
      "training 0.001071428065188229 relative L2 0.0016029761172831059\n",
      "training 0.0010713499505072832 relative L2 0.001602918142452836\n",
      "training 0.0010712723014876246 relative L2 0.0016028615646064281\n",
      "training 0.0010711943032220006 relative L2 0.0016028044046834111\n",
      "training 0.001071116654202342 relative L2 0.0016027475940063596\n",
      "training 0.001071039354428649 relative L2 0.0016026911325752735\n",
      "training 0.0010709617054089904 relative L2 0.001602634321898222\n",
      "training 0.0010708848712965846 relative L2 0.0016025783261284232\n",
      "training 0.001070807920768857 relative L2 0.0016025215154513717\n",
      "training 0.001070731203071773 relative L2 0.001602465403266251\n",
      "training 0.0010706543689593673 relative L2 0.0016024090582504869\n",
      "training 0.0010705777676776052 relative L2 0.0016023527132347226\n",
      "training 0.0010705013992264867 relative L2 0.0016022970667108893\n",
      "training 0.0010704254964366555 relative L2 0.0016022415366023779\n",
      "training 0.0010703495936468244 relative L2 0.0016021860064938664\n",
      "training 0.0010702742729336023 relative L2 0.001602131174877286\n",
      "training 0.0010701990686357021 relative L2 0.0016020749462768435\n",
      "training 0.001070123864337802 relative L2 0.0016020210459828377\n",
      "training 0.0010700490092858672 relative L2 0.0016019653994590044\n",
      "training 0.0010699743870645761 relative L2 0.001601911848410964\n",
      "training 0.001069899764843285 relative L2 0.0016018556198105216\n",
      "training 0.001069825142621994 relative L2 0.001601802185177803\n",
      "training 0.001069750520400703 relative L2 0.0016017465386539698\n",
      "training 0.001069676480256021 relative L2 0.0016016929876059294\n",
      "training 0.0010696019744500518 relative L2 0.0016016376903280616\n",
      "training 0.001069527817890048 relative L2 0.001601584255695343\n",
      "training 0.0010694541269913316 relative L2 0.0016015295404940844\n",
      "training 0.0010693802032619715 relative L2 0.0016014763386920094\n",
      "training 0.0010693068616092205 relative L2 0.001601422089152038\n",
      "training 0.0010692336363717914 relative L2 0.0016013688873499632\n",
      "training 0.0010691606439650059 relative L2 0.001601315219886601\n",
      "training 0.001069087884388864 relative L2 0.001601262018084526\n",
      "training 0.0010690151248127222 relative L2 0.0016012081177905202\n",
      "training 0.0010689421324059367 relative L2 0.001601155148819089\n",
      "training 0.0010688694892451167 relative L2 0.001601101947017014\n",
      "training 0.001068797311745584 relative L2 0.0016010486287996173\n",
      "training 0.0010687251342460513 relative L2 0.001600995659828186\n",
      "training 0.0010686523746699095 relative L2 0.0016009430401027203\n",
      "training 0.0010685805464163423 relative L2 0.00160089076962322\n",
      "training 0.0010685093002393842 relative L2 0.001600837567821145\n",
      "training 0.0010684373555704951 relative L2 0.0016007852973416448\n",
      "training 0.0010683658765628934 relative L2 0.00160073337610811\n",
      "training 0.001068294863216579 relative L2 0.0016006811056286097\n",
      "training 0.0010682236170396209 relative L2 0.0016006288351491094\n",
      "training 0.001068152952939272 relative L2 0.0016005769139155746\n",
      "training 0.0010680819395929575 relative L2 0.0016005245270207524\n",
      "training 0.0010680113919079304 relative L2 0.0016004735371097922\n",
      "training 0.001067941077053547 relative L2 0.001600421266630292\n",
      "training 0.0010678705293685198 relative L2 0.001600370043888688\n",
      "training 0.0010678000980988145 relative L2 0.0016003178898245096\n",
      "training 0.0010677298996597528 relative L2 0.001600267132744193\n",
      "training 0.0010676595848053694 relative L2 0.0016002149786800146\n",
      "training 0.0010675895027816296 relative L2 0.001600164221599698\n",
      "training 0.001067519886419177 relative L2 0.0016001129988580942\n",
      "training 0.0010674503864720464 relative L2 0.0016000623581930995\n",
      "training 0.001067380653694272 relative L2 0.0016000110190361738\n",
      "training 0.0010673117358237505 relative L2 0.0015999602619558573\n",
      "training 0.0010672423522919416 relative L2 0.001599909388460219\n",
      "training 0.00106717343442142 relative L2 0.0015998590970411897\n",
      "training 0.0010671047493815422 relative L2 0.0015998083399608731\n",
      "training 0.001067035598680377 relative L2 0.0015997585142031312\n",
      "training 0.001066967030055821 relative L2 0.0015997077571228147\n",
      "training 0.0010668988106772304 relative L2 0.0015996585134416819\n",
      "training 0.0010668305912986398 relative L2 0.0015996074071153998\n",
      "training 0.00106676178984344 relative L2 0.0015995578141883016\n",
      "training 0.0010666935704648495 relative L2 0.0015995072899386287\n",
      "training 0.0010666255839169025 relative L2 0.0015994574641808867\n",
      "training 0.001066558063030243 relative L2 0.0015994078712537885\n",
      "training 0.0010664901928976178 relative L2 0.0015993585111573339\n",
      "training 0.0010664229048416018 relative L2 0.0015993089182302356\n",
      "training 0.0010663556167855859 relative L2 0.0015992594417184591\n",
      "training 0.00106628832872957 relative L2 0.0015992105472832918\n",
      "training 0.0010662211570888758 relative L2 0.0015991602558642626\n",
      "training 0.0010661543346941471 relative L2 0.001599112874828279\n",
      "training 0.0010660875122994184 relative L2 0.0015990622341632843\n",
      "training 0.0010660209227353334 relative L2 0.0015990148531273007\n",
      "training 0.0010659542167559266 relative L2 0.0015989644452929497\n",
      "training 0.0010658876271918416 relative L2 0.001598917180672288\n",
      "training 0.0010658211540430784 relative L2 0.0015988665400072932\n",
      "training 0.001065754913724959 relative L2 0.0015988195082172751\n",
      "training 0.0010656885569915175 relative L2 0.0015987696824595332\n",
      "training 0.0010656226659193635 relative L2 0.001598722767084837\n",
      "training 0.0010655566584318876 relative L2 0.0015986730577424169\n",
      "training 0.0010654910001903772 relative L2 0.0015986270736902952\n",
      "training 0.001065425225533545 relative L2 0.0015985764330253005\n",
      "training 0.0010653596837073565 relative L2 0.0015985309146344662\n",
      "training 0.0010652942582964897 relative L2 0.0015984809724614024\n",
      "training 0.0010652291821315885 relative L2 0.00159843557048589\n",
      "training 0.0010651639895513654 relative L2 0.0015983852790668607\n",
      "training 0.0010650986805558205 relative L2 0.0015983398770913482\n",
      "training 0.0010650339536368847 relative L2 0.0015982905169948936\n",
      "training 0.0010649689938873053 relative L2 0.0015982446493580937\n",
      "training 0.0010649041505530477 relative L2 0.0015981948236003518\n",
      "training 0.00106483930721879 relative L2 0.001598149654455483\n",
      "training 0.001064774813130498 relative L2 0.0015981009928509593\n",
      "training 0.0010647107847034931 relative L2 0.0015980555908754468\n",
      "training 0.0010646464070305228 relative L2 0.0015980068128556013\n",
      "training 0.0010645820293575525 relative L2 0.0015979621093720198\n",
      "training 0.0010645183501765132 relative L2 0.0015979130985215306\n",
      "training 0.0010644546709954739 relative L2 0.0015978689771145582\n",
      "training 0.0010643911082297564 relative L2 0.00159781938418746\n",
      "training 0.0010643273126333952 relative L2 0.0015977758448570967\n",
      "training 0.0010642636334523559 relative L2 0.001597726484760642\n",
      "training 0.001064200303517282 relative L2 0.0015976823633536696\n",
      "training 0.00106413708999753 relative L2 0.0015976334689185023\n",
      "training 0.0010640737600624561 relative L2 0.001597589929588139\n",
      "training 0.0010640110122039914 relative L2 0.0015975412679836154\n",
      "training 0.0010639481479302049 relative L2 0.0015974969137459993\n",
      "training 0.0010638850508257747 relative L2 0.0015974482521414757\n",
      "training 0.0010638221865519881 relative L2 0.0015974044799804688\n",
      "training 0.0010637594386935234 relative L2 0.0015973564004525542\n",
      "training 0.001063697156496346 relative L2 0.0015973129775375128\n",
      "training 0.001063634641468525 relative L2 0.0015972646651789546\n",
      "training 0.0010635722428560257 relative L2 0.0015972214750945568\n",
      "training 0.0010635100770741701 relative L2 0.0015971726970747113\n",
      "training 0.001063447562046349 relative L2 0.0015971303218975663\n",
      "training 0.0010633856290951371 relative L2 0.0015970815438777208\n",
      "training 0.0010633236961439252 relative L2 0.001597039052285254\n",
      "training 0.001063261879608035 relative L2 0.0015969905070960522\n",
      "training 0.0010632001794874668 relative L2 0.0015969483647495508\n",
      "training 0.0010631383629515767 relative L2 0.0015968995867297053\n",
      "training 0.0010630764300003648 relative L2 0.0015968575607985258\n",
      "training 0.001063015079125762 relative L2 0.0015968093648552895\n",
      "training 0.0010629541939124465 relative L2 0.0015967676881700754\n",
      "training 0.0010628930758684874 relative L2 0.0015967190265655518\n",
      "training 0.0010628317249938846 relative L2 0.0015966783976182342\n",
      "training 0.0010627713054418564 relative L2 0.0015966297360137105\n",
      "training 0.001062710303813219 relative L2 0.0015965894563123584\n",
      "training 0.0010626496514305472 relative L2 0.0015965404454618692\n",
      "training 0.0010625888826325536 relative L2 0.001596500282175839\n",
      "training 0.001062528695911169 relative L2 0.0015964512713253498\n",
      "training 0.0010624680435284972 relative L2 0.0015964115737006068\n",
      "training 0.0010624079732224345 relative L2 0.0015963624464347959\n",
      "training 0.0010623474372550845 relative L2 0.0015963232144713402\n",
      "training 0.0010622873669490218 relative L2 0.0015962739707902074\n",
      "training 0.0010622271802276373 relative L2 0.0015962349716573954\n",
      "training 0.0010621672263368964 relative L2 0.001596185378730297\n",
      "training 0.0010621073888614774 relative L2 0.001596147078089416\n",
      "training 0.001062047784216702 relative L2 0.0015960976015776396\n",
      "training 0.0010619880631566048 relative L2 0.0015960595337674022\n",
      "training 0.001061928109265864 relative L2 0.0015960095915943384\n",
      "training 0.0010618686210364103 relative L2 0.0015959721058607101\n",
      "training 0.0010618093656376004 relative L2 0.0015959222801029682\n",
      "training 0.0010617499938234687 relative L2 0.0015958851436153054\n",
      "training 0.0010616910876706243 relative L2 0.0015958347357809544\n",
      "training 0.0010616319486871362 relative L2 0.0015957995783537626\n",
      "training 0.0010615736246109009 relative L2 0.0015957484720274806\n",
      "training 0.001061515766195953 relative L2 0.001595715875737369\n",
      "training 0.0010614583734422922 relative L2 0.0015956626739352942\n",
      "training 0.0010614014463499188 relative L2 0.001595633919350803\n",
      "training 0.0010613453341647983 relative L2 0.0015955790877342224\n",
      "training 0.0010612908517941833 relative L2 0.0015955574344843626\n",
      "training 0.0010612384648993611 relative L2 0.0015955012058839202\n",
      "training 0.0010611893376335502 relative L2 0.0015954934060573578\n",
      "training 0.001061145798303187 relative L2 0.001595438807271421\n",
      "training 0.0010611119214445353 relative L2 0.0015954652335494757\n",
      "training 0.0010610970202833414 relative L2 0.0015954304253682494\n",
      "training 0.0010611149482429028 relative L2 0.001595552428625524\n",
      "training 0.001061194809153676 relative L2 0.0015956147108227015\n",
      "training 0.0010613917838782072 relative L2 0.001596037414856255\n",
      "training 0.0010618133237585425 relative L2 0.0015965065686032176\n",
      "training 0.0010626536095514894 relative L2 0.0015979234594851732\n",
      "training 0.0010642948327586055 relative L2 0.001599967828951776\n",
      "training 0.0010674498043954372 relative L2 0.0016048250254243612\n",
      "training 0.0010735318064689636 relative L2 0.0016127706039696932\n",
      "training 0.0010850956896319985 relative L2 0.0016296367393806577\n",
      "training 0.0011073151836171746 relative L2 0.0016586618730798364\n",
      "training 0.0011491631157696247 relative L2 0.0017155588138848543\n",
      "training 0.0012287743156775832 relative L2 0.0018099643057212234\n",
      "training 0.0013724911259487271 relative L2 0.001971784047782421\n",
      "training 0.0016288681654259562 relative L2 0.0021927866619080305\n",
      "training 0.002023655455559492 relative L2 0.002465908881276846\n",
      "training 0.0025584709364920855 relative L2 0.0026451789308339357\n",
      "training 0.0029531707987189293 relative L2 0.0026223312597721815\n",
      "training 0.0028959906194359064 relative L2 0.002243662951514125\n",
      "training 0.002119518583640456 relative L2 0.001751383882947266\n",
      "training 0.0012813287321478128 relative L2 0.0016162621323019266\n",
      "training 0.0010890106204897165 relative L2 0.001920410431921482\n",
      "training 0.0015476146945729852 relative L2 0.0021734207402914762\n",
      "training 0.001983129885047674 relative L2 0.0020703482441604137\n",
      "training 0.001801873091608286 relative L2 0.0017466343706473708\n",
      "training 0.0012743007391691208 relative L2 0.0015984894707798958\n",
      "training 0.0010650212643668056 relative L2 0.0017886542482301593\n",
      "training 0.0013398488517850637 relative L2 0.0019547392148524523\n",
      "training 0.0016005199868232012 relative L2 0.0018530035158619285\n",
      "training 0.0014394802274182439 relative L2 0.0016420894535258412\n",
      "training 0.0011245043715462089 relative L2 0.0016179820522665977\n",
      "training 0.0010913530131801963 relative L2 0.0017640924779698253\n",
      "training 0.0013027304084971547 relative L2 0.0018170573748648167\n",
      "training 0.0013805540511384606 relative L2 0.001696994062513113\n",
      "training 0.0012039177818223834 relative L2 0.0015956780407577753\n",
      "training 0.0010612887563183904 relative L2 0.0016499576158821583\n",
      "training 0.0011354474117979407 relative L2 0.001731454161927104\n",
      "training 0.0012541962787508965 relative L2 0.0017058785306289792\n",
      "training 0.0012147797970101237 relative L2 0.0016152026364579797\n",
      "training 0.0010884449584409595 relative L2 0.0016021751798689365\n",
      "training 0.0010704974411055446 relative L2 0.0016637247754260898\n",
      "training 0.00115472381003201 relative L2 0.001681271125562489\n",
      "training 0.0011813021264970303 relative L2 0.0016322318697348237\n",
      "training 0.001110886805690825 relative L2 0.001593888271600008\n",
      "training 0.0010590211022645235 relative L2 0.001618737238459289\n",
      "training 0.0010933316079899669 relative L2 0.0016516519244760275\n",
      "training 0.0011378130875527859 relative L2 0.0016345074400305748\n",
      "training 0.0011152320075780153 relative L2 0.001599899842403829\n",
      "training 0.001066899043507874 relative L2 0.0015988468658179045\n",
      "training 0.001065489836037159 relative L2 0.0016230802284553647\n",
      "training 0.0010993467876687646 relative L2 0.0016287051839753985\n",
      "training 0.0011060346150770783 relative L2 0.0016062723007053137\n",
      "training 0.0010761419543996453 relative L2 0.001593463122844696\n",
      "training 0.001058421446941793 relative L2 0.0016051462152972817\n",
      "training 0.0010739470599219203 relative L2 0.0016162487445399165\n",
      "training 0.0010898979380726814 relative L2 0.0016093045705929399\n",
      "training 0.0010795652633532882 relative L2 0.0015951914247125387\n",
      "training 0.0010608843294903636 relative L2 0.0015955149428918958\n",
      "training 0.0010613345075398684 relative L2 0.0016056221211329103\n",
      "training 0.0010745872277766466 relative L2 0.001606576144695282\n",
      "training 0.001076567335985601 relative L2 0.0015984171768650413\n",
      "training 0.0010649103205651045 relative L2 0.0015931400703266263\n",
      "training 0.0010579742956906557 relative L2 0.001597318216226995\n",
      "training 0.0010638319654390216 relative L2 0.0016023869393393397\n",
      "training 0.0010702289873734117 relative L2 0.001599273644387722\n",
      "training 0.0010665268637239933 relative L2 0.0015940535813570023\n",
      "training 0.0010591201717033982 relative L2 0.0015936914132907987\n",
      "training 0.0010586465941742063 relative L2 0.0015972224064171314\n",
      "training 0.0010637049563229084 relative L2 0.0015986397629603744\n",
      "training 0.00106520252302289 relative L2 0.001595234964042902\n",
      "training 0.0010609631426632404 relative L2 0.001592865213751793\n",
      "training 0.0010575891938060522 relative L2 0.0015941208694130182\n",
      "training 0.0010591986356303096 relative L2 0.0015960121527314186\n",
      "training 0.0010620409157127142 relative L2 0.001595783862285316\n",
      "training 0.0010613928316161036 relative L2 0.00159339455422014\n",
      "training 0.0010584085248410702 relative L2 0.0015926698688417673\n",
      "training 0.0010573662584647536 relative L2 0.0015939924633130431\n",
      "training 0.0010590230813249946 relative L2 0.0015946893254294991\n",
      "training 0.0010602177353575826 relative L2 0.0015940397279337049\n",
      "training 0.0010590821038931608 relative L2 0.0015926496125757694\n",
      "training 0.0010573583422228694 relative L2 0.0015926080523058772\n",
      "training 0.0010573016479611397 relative L2 0.0015935584669932723\n",
      "training 0.0010584468254819512 relative L2 0.0015936372801661491\n",
      "training 0.001058762427419424 relative L2 0.0015930462395772338\n",
      "training 0.0010577765060588717 relative L2 0.001592327724210918\n",
      "training 0.0010568966390565038 relative L2 0.0015924829058349133\n",
      "training 0.0010571383172646165 relative L2 0.001593065564520657\n",
      "training 0.0010577953653410077 relative L2 0.0015929046785458922\n",
      "training 0.0010577451903373003 relative L2 0.0015924866311252117\n",
      "training 0.0010570462327450514 relative L2 0.0015921401791274548\n",
      "training 0.0010566286509856582 relative L2 0.001592292101122439\n",
      "training 0.001056878943927586 relative L2 0.001592625747434795\n",
      "training 0.0010572164319455624 relative L2 0.0015924136387184262\n",
      "training 0.0010570603189989924 relative L2 0.0015921450685709715\n",
      "training 0.001056601875461638 relative L2 0.0015919777797535062\n",
      "training 0.0010564024560153484 relative L2 0.0015920762671157718\n",
      "training 0.001056581735610962 relative L2 0.0015922621823847294\n",
      "training 0.001056738430634141 relative L2 0.0015920692821964622\n",
      "training 0.0010565797565504909 relative L2 0.0015919014113023877\n",
      "training 0.001056283712387085 relative L2 0.0015918121207505465\n",
      "training 0.0010561763774603605 relative L2 0.0015918589197099209\n",
      "training 0.001056281616911292 relative L2 0.0015919605502858758\n",
      "training 0.0010563427349552512 relative L2 0.0015918046701699495\n",
      "training 0.0010562101379036903 relative L2 0.001591697451658547\n",
      "training 0.0010560147929936647 relative L2 0.0015916440170258284\n",
      "training 0.0010559490183368325 relative L2 0.001591655076481402\n",
      "training 0.0010560002410784364 relative L2 0.0015917073469609022\n",
      "training 0.0010560096707195044 relative L2 0.0015915845287963748\n",
      "training 0.0010559041984379292 relative L2 0.0015915132826194167\n",
      "training 0.0010557710193097591 relative L2 0.0015914738178253174\n",
      "training 0.0010557199129834771 relative L2 0.001591463340446353\n",
      "training 0.0010557364439591765 relative L2 0.0015914874384179711\n",
      "training 0.0010557201458141208 relative L2 0.0015913896495476365\n",
      "training 0.0010556348133832216 relative L2 0.001591338193975389\n",
      "training 0.001055537723004818 relative L2 0.0015913047827780247\n",
      "training 0.001055492670275271 relative L2 0.0015912812668830156\n",
      "training 0.0010554856853559613 relative L2 0.001591287786141038\n",
      "training 0.0010554565815255046 relative L2 0.0015912085073068738\n",
      "training 0.0010553855681791902 relative L2 0.0015911678783595562\n",
      "training 0.0010553100146353245 relative L2 0.0015911378432065248\n",
      "training 0.0010552681051194668 relative L2 0.0015911068767309189\n",
      "training 0.0010552469175308943 relative L2 0.0015911016380414367\n",
      "training 0.0010552096646279097 relative L2 0.0015910352813079953\n",
      "training 0.001055147498846054 relative L2 0.0015910000074654818\n",
      "training 0.001055085682310164 relative L2 0.0015909719513729215\n",
      "training 0.0010550449369475245 relative L2 0.001590937259607017\n",
      "training 0.001055015716701746 relative L2 0.001590924453921616\n",
      "training 0.0010549743892624974 relative L2 0.0015908670611679554\n",
      "training 0.0010549178114160895 relative L2 0.0015908345812931657\n",
      "training 0.0010548637947067618 relative L2 0.001590808155015111\n",
      "training 0.0010548238642513752 relative L2 0.0015907709021121264\n",
      "training 0.001054789056070149 relative L2 0.0015907532069832087\n",
      "training 0.0010547454003244638 relative L2 0.0015907026827335358\n",
      "training 0.0010546931298449636 relative L2 0.0015906706685200334\n",
      "training 0.0010546438861638308 relative L2 0.0015906441258266568\n",
      "training 0.0010546036064624786 relative L2 0.0015906061744317412\n",
      "training 0.0010545651894062757 relative L2 0.0015905840555205941\n",
      "training 0.0010545202530920506 relative L2 0.0015905392356216908\n",
      "training 0.001054470892995596 relative L2 0.0015905075706541538\n",
      "training 0.0010544250253587961 relative L2 0.0015904811443760991\n",
      "training 0.0010543842799961567 relative L2 0.0015904431929811835\n",
      "training 0.0010543435346335173 relative L2 0.0015904187457635999\n",
      "training 0.0010542989475652575 relative L2 0.0015903775347396731\n",
      "training 0.0010542520321905613 relative L2 0.0015903464518487453\n",
      "training 0.0010542073287069798 relative L2 0.001590318395756185\n",
      "training 0.001054166117683053 relative L2 0.0015902817249298096\n",
      "training 0.0010541246738284826 relative L2 0.0015902559971436858\n",
      "training 0.001054080668836832 relative L2 0.0015902176965028048\n",
      "training 0.0010540353832766414 relative L2 0.0015901861479505897\n",
      "training 0.001053991261869669 relative L2 0.001590156927704811\n",
      "training 0.0010539491195231676 relative L2 0.0015901210717856884\n",
      "training 0.0010539074428379536 relative L2 0.0015900942962616682\n",
      "training 0.0010538640199229121 relative L2 0.0015900579746812582\n",
      "training 0.0010538200149312615 relative L2 0.0015900266589596868\n",
      "training 0.001053776708431542 relative L2 0.0015899976715445518\n",
      "training 0.001053734216839075 relative L2 0.0015899620484560728\n",
      "training 0.0010536921909078956 relative L2 0.0015899342251941562\n",
      "training 0.0010536487679928541 relative L2 0.0015898984856903553\n",
      "training 0.0010536049958318472 relative L2 0.0015898672863841057\n",
      "training 0.0010535621549934149 relative L2 0.0015898378333076835\n",
      "training 0.0010535197798162699 relative L2 0.001589803141541779\n",
      "training 0.0010534776374697685 relative L2 0.0015897747362032533\n",
      "training 0.0010534346802160144 relative L2 0.0015897400444373488\n",
      "training 0.0010533913737162948 relative L2 0.0015897093107923865\n",
      "training 0.0010533485328778625 relative L2 0.0015896792756393552\n",
      "training 0.001053306390531361 relative L2 0.0015896448167040944\n",
      "training 0.001053264131769538 relative L2 0.0015896162949502468\n",
      "training 0.0010532212909311056 relative L2 0.0015895823016762733\n",
      "training 0.0010531784500926733 relative L2 0.0015895512187853456\n",
      "training 0.0010531360749155283 relative L2 0.0015895218821242452\n",
      "training 0.0010530942818149924 relative L2 0.0015894875396043062\n",
      "training 0.001053052255883813 relative L2 0.001589459367096424\n",
      "training 0.001053009880706668 relative L2 0.0015894252574071288\n",
      "training 0.0010529672726988792 relative L2 0.0015893945237621665\n",
      "training 0.0010529247811064124 relative L2 0.0015893649542704225\n",
      "training 0.0010528831044211984 relative L2 0.0015893313102424145\n",
      "training 0.0010528414277359843 relative L2 0.0015893029049038887\n",
      "training 0.0010527991689741611 relative L2 0.0015892692608758807\n",
      "training 0.0010527567937970161 relative L2 0.0015892385272309184\n",
      "training 0.0010527147678658366 relative L2 0.0015892088413238525\n",
      "training 0.0010526732075959444 relative L2 0.00158917554654181\n",
      "training 0.0010526315309107304 relative L2 0.0015891470247879624\n",
      "training 0.0010525895049795508 relative L2 0.0015891138464212418\n",
      "training 0.0010525474790483713 relative L2 0.0015890831127762794\n",
      "training 0.0010525055695325136 relative L2 0.0015890534268692136\n",
      "training 0.001052464242093265 relative L2 0.0015890205977484584\n",
      "training 0.0010524229146540165 relative L2 0.0015889918431639671\n",
      "training 0.0010523810051381588 relative L2 0.0015889592468738556\n",
      "training 0.001052339095622301 relative L2 0.0015889290953055024\n",
      "training 0.0010522973025217652 relative L2 0.0015888982452452183\n",
      "training 0.0010522556258365512 relative L2 0.0015888666966930032\n",
      "training 0.0010522142983973026 relative L2 0.0015888371272012591\n",
      "training 0.0010521726217120886 relative L2 0.0015888051129877567\n",
      "training 0.00105213129427284 relative L2 0.001588775310665369\n",
      "training 0.001052089617587626 relative L2 0.0015887442277744412\n",
      "training 0.0010520481737330556 relative L2 0.0015887136105448008\n",
      "training 0.001052006846293807 relative L2 0.0015886835753917694\n",
      "training 0.0010519655188545585 relative L2 0.001588651561178267\n",
      "training 0.001051924074999988 relative L2 0.0015886223409324884\n",
      "training 0.0010518828639760613 relative L2 0.0015885906759649515\n",
      "training 0.001051841420121491 relative L2 0.0015885609900578856\n",
      "training 0.001051800325512886 relative L2 0.0015885302564129233\n",
      "training 0.001051759347319603 relative L2 0.0015884996391832829\n",
      "training 0.0010517179034650326 relative L2 0.0015884697204455733\n",
      "training 0.0010516769252717495 relative L2 0.0015884381718933582\n",
      "training 0.0010516357142478228 relative L2 0.0015884089516475797\n",
      "training 0.0010515943868085742 relative L2 0.0015883776359260082\n",
      "training 0.0010515532921999693 relative L2 0.0015883479500189424\n",
      "training 0.001051512430422008 relative L2 0.0015883173327893019\n",
      "training 0.0010514712193980813 relative L2 0.0015882865991443396\n",
      "training 0.0010514301247894764 relative L2 0.0015882571460679173\n",
      "training 0.0010513891465961933 relative L2 0.001588225830346346\n",
      "training 0.001051348284818232 relative L2 0.0015881967265158892\n",
      "training 0.001051307306624949 relative L2 0.001588165294378996\n",
      "training 0.001051266328431666 relative L2 0.0015881359577178955\n",
      "training 0.0010512255830690265 relative L2 0.0015881051076576114\n",
      "training 0.0010511843720450997 relative L2 0.00158807507250458\n",
      "training 0.0010511437430977821 relative L2 0.0015880450373515487\n",
      "training 0.0010511029977351427 relative L2 0.0015880144201219082\n",
      "training 0.0010510620195418596 relative L2 0.0015879846177995205\n",
      "training 0.0010510215070098639 relative L2 0.00158795400056988\n",
      "training 0.0010509807616472244 relative L2 0.0015879241982474923\n",
      "training 0.001050940016284585 relative L2 0.0015878940466791391\n",
      "training 0.0010508992709219456 relative L2 0.0015878640115261078\n",
      "training 0.0010508588748052716 relative L2 0.0015878339763730764\n",
      "training 0.0010508187115192413 relative L2 0.0015878037083894014\n",
      "training 0.0010507780825719237 relative L2 0.0015877733239904046\n",
      "training 0.0010507376864552498 relative L2 0.0015877437544986606\n",
      "training 0.0010506972903385758 relative L2 0.0015877134865149856\n",
      "training 0.0010506566613912582 relative L2 0.0015876834513619542\n",
      "training 0.0010506163816899061 relative L2 0.0015876534162089229\n",
      "training 0.001050576101988554 relative L2 0.0015876239631325006\n",
      "training 0.0010505358222872019 relative L2 0.0015875935787335038\n",
      "training 0.0010504957754164934 relative L2 0.001587564474903047\n",
      "training 0.001050455728545785 relative L2 0.0015875347889959812\n",
      "training 0.0010504154488444328 relative L2 0.0015875047538429499\n",
      "training 0.0010503755183890462 relative L2 0.0015874755335971713\n",
      "training 0.0010503357043489814 relative L2 0.0015874451491981745\n",
      "training 0.001050295541062951 relative L2 0.0015874155797064304\n",
      "training 0.0010502556106075644 relative L2 0.0015873860102146864\n",
      "training 0.0010502154473215342 relative L2 0.0015873564407229424\n",
      "training 0.0010501756332814693 relative L2 0.0015873265219852328\n",
      "training 0.0010501357028260827 relative L2 0.0015872973017394543\n",
      "training 0.001050095772370696 relative L2 0.0015872674994170666\n",
      "training 0.0010500559583306313 relative L2 0.0015872380463406444\n",
      "training 0.0010500161442905664 relative L2 0.001587208011187613\n",
      "training 0.0010499762138351798 relative L2 0.0015871785581111908\n",
      "training 0.001049936399795115 relative L2 0.0015871485229581594\n",
      "training 0.0010498962365090847 relative L2 0.0015871194191277027\n",
      "training 0.0010498566552996635 relative L2 0.0015870895003899932\n",
      "training 0.0010498170740902424 relative L2 0.0015870602801442146\n",
      "training 0.001049777609296143 relative L2 0.0015870308270677924\n",
      "training 0.001049738028086722 relative L2 0.0015870016068220139\n",
      "training 0.0010496985632926226 relative L2 0.0015869718044996262\n",
      "training 0.0010496588656678796 relative L2 0.0015869427006691694\n",
      "training 0.0010496194008737803 relative L2 0.0015869130147621036\n",
      "training 0.001049579819664359 relative L2 0.0015868839109316468\n",
      "training 0.0010495403548702598 relative L2 0.0015868544578552246\n",
      "training 0.0010495008900761604 relative L2 0.0015868253540247679\n",
      "training 0.001049461541697383 relative L2 0.001586795668117702\n",
      "training 0.0010494221933186054 relative L2 0.0015867663314566016\n",
      "training 0.0010493826121091843 relative L2 0.0015867374604567885\n",
      "training 0.001049343147315085 relative L2 0.0015867078909650445\n",
      "training 0.0010493036825209856 relative L2 0.0015866784378886223\n",
      "training 0.0010492645669728518 relative L2 0.0015866493340581656\n",
      "training 0.001049225451424718 relative L2 0.0015866204630583525\n",
      "training 0.0010491862194612622 relative L2 0.001586591242812574\n",
      "training 0.0010491469874978065 relative L2 0.0015865620225667953\n",
      "training 0.0010491077555343509 relative L2 0.0015865329187363386\n",
      "training 0.0010490685235708952 relative L2 0.0015865035820752382\n",
      "training 0.0010490294080227613 relative L2 0.0015864744782447815\n",
      "training 0.0010489904088899493 relative L2 0.0015864454908296466\n",
      "training 0.0010489512933418155 relative L2 0.0015864160377532244\n",
      "training 0.0010489120613783598 relative L2 0.0015863872831687331\n",
      "training 0.001048872945830226 relative L2 0.0015863579465076327\n",
      "training 0.0010488341795280576 relative L2 0.0015863287262618542\n",
      "training 0.00104879483114928 relative L2 0.001586299971677363\n",
      "training 0.0010487557156011462 relative L2 0.0015862708678469062\n",
      "training 0.0010487166000530124 relative L2 0.001586242113262415\n",
      "training 0.001048677833750844 relative L2 0.0015862127766013145\n",
      "training 0.0010486389510333538 relative L2 0.0015861837891861796\n",
      "training 0.0010485999519005418 relative L2 0.0015861549181863666\n",
      "training 0.0010485611855983734 relative L2 0.0015861259307712317\n",
      "training 0.0010485220700502396 relative L2 0.0015860970597714186\n",
      "training 0.0010484833037480712 relative L2 0.0015860683051869273\n",
      "training 0.0010484445374459028 relative L2 0.0015860390849411488\n",
      "training 0.0010484055383130908 relative L2 0.0015860103303566575\n",
      "training 0.0010483667720109224 relative L2 0.0015859815757721663\n",
      "training 0.0010483278892934322 relative L2 0.001585952821187675\n",
      "training 0.0010482893558219075 relative L2 0.0015859240666031837\n",
      "training 0.0010482505895197392 relative L2 0.0015858953120186925\n",
      "training 0.0010482118232175708 relative L2 0.0015858664410188794\n",
      "training 0.001048173289746046 relative L2 0.00158583780284971\n",
      "training 0.0010481345234438777 relative L2 0.001585808931849897\n",
      "training 0.0010480958735570312 relative L2 0.0015857802936807275\n",
      "training 0.0010480571072548628 relative L2 0.0015857517719268799\n",
      "training 0.001048018573783338 relative L2 0.001585722784511745\n",
      "training 0.0010479801567271352 relative L2 0.0015856943791732192\n",
      "training 0.0010479413904249668 relative L2 0.001585665624588728\n",
      "training 0.001047902973368764 relative L2 0.0015856368700042367\n",
      "training 0.0010478644398972392 relative L2 0.001585608464665711\n",
      "training 0.0010478260228410363 relative L2 0.0015855795936658978\n",
      "training 0.001047787256538868 relative L2 0.0015855509554967284\n",
      "training 0.001047748839482665 relative L2 0.0015855226665735245\n",
      "training 0.001047710538841784 relative L2 0.0015854937955737114\n",
      "training 0.0010476721217855811 relative L2 0.0015854655066505075\n",
      "training 0.0010476335883140564 relative L2 0.0015854367520660162\n",
      "training 0.0010475954040884972 relative L2 0.0015854083467274904\n",
      "training 0.0010475568706169724 relative L2 0.001585379708558321\n",
      "training 0.0010475186863914132 relative L2 0.0015853511868044734\n",
      "training 0.0010474803857505322 relative L2 0.0015853227814659476\n",
      "training 0.001047442085109651 relative L2 0.0015852944925427437\n",
      "training 0.0010474040172994137 relative L2 0.0015852656215429306\n",
      "training 0.0010473658330738544 relative L2 0.0015852375654503703\n",
      "training 0.0010473276488482952 relative L2 0.0015852093929424882\n",
      "training 0.0010472895810380578 relative L2 0.0015851811040192842\n",
      "training 0.0010472515132278204 relative L2 0.0015851526986807585\n",
      "training 0.001047213445417583 relative L2 0.0015851248754188418\n",
      "training 0.0010471756104379892 relative L2 0.001585096470080316\n",
      "training 0.0010471376590430737 relative L2 0.001585068297572434\n",
      "training 0.0010470994748175144 relative L2 0.0015850404743105173\n",
      "training 0.0010470616398379207 relative L2 0.0015850118361413479\n",
      "training 0.001047023688443005 relative L2 0.001584984245710075\n",
      "training 0.0010469858534634113 relative L2 0.001584955956786871\n",
      "training 0.001046947785653174 relative L2 0.0015849281335249543\n",
      "training 0.0010469099506735802 relative L2 0.0015848999610170722\n",
      "training 0.0010468718828633428 relative L2 0.001584871904924512\n",
      "training 0.0010468342807143927 relative L2 0.0015848438488319516\n",
      "training 0.0010467962129041553 relative L2 0.0015848157927393913\n",
      "training 0.0010467584943398833 relative L2 0.0015847879694774747\n",
      "training 0.0010467206593602896 relative L2 0.001584759564138949\n",
      "training 0.0010466830572113395 relative L2 0.0015847315080463886\n",
      "training 0.0010466452222317457 relative L2 0.001584703684784472\n",
      "training 0.0010466075036674738 relative L2 0.0015846756286919117\n",
      "training 0.00104656966868788 relative L2 0.0015846480382606387\n",
      "training 0.0010465322993695736 relative L2 0.0015846199821680784\n",
      "training 0.0010464944643899798 relative L2 0.0015845921589061618\n",
      "training 0.0010464569786563516 relative L2 0.0015845642192289233\n",
      "training 0.0010464193765074015 relative L2 0.001584536163136363\n",
      "training 0.0010463815415278077 relative L2 0.0015845083398744464\n",
      "training 0.0010463438229635358 relative L2 0.0015844807494431734\n",
      "training 0.0010463062208145857 relative L2 0.0015844526933506131\n",
      "training 0.0010462685022503138 relative L2 0.0015844246372580528\n",
      "training 0.0010462311329320073 relative L2 0.0015843971632421017\n",
      "training 0.001046193647198379 relative L2 0.0015843691071495414\n",
      "training 0.0010461561614647508 relative L2 0.0015843414003029466\n",
      "training 0.0010461184429004788 relative L2 0.0015843133442103863\n",
      "training 0.0010460810735821724 relative L2 0.0015842861030250788\n",
      "training 0.001046043704263866 relative L2 0.001584257697686553\n",
      "training 0.0010460061021149158 relative L2 0.0015842302236706018\n",
      "training 0.0010459687327966094 relative L2 0.0015842021675780416\n",
      "training 0.001045931363478303 relative L2 0.0015841748099774122\n",
      "training 0.0010458939941599965 relative L2 0.00158414663746953\n",
      "training 0.00104585662484169 relative L2 0.001584119745530188\n",
      "training 0.0010458192555233836 relative L2 0.0015840913401916623\n",
      "training 0.001045782002620399 relative L2 0.0015840642154216766\n",
      "training 0.0010457446333020926 relative L2 0.0015840359264984727\n",
      "training 0.001045707263983786 relative L2 0.001584008801728487\n",
      "training 0.0010456701274961233 relative L2 0.001583980629220605\n",
      "training 0.001045632641762495 relative L2 0.001583953620865941\n",
      "training 0.001045595621690154 relative L2 0.001583925448358059\n",
      "training 0.0010455582523718476 relative L2 0.001583898556418717\n",
      "training 0.0010455212322995067 relative L2 0.0015838699182495475\n",
      "training 0.0010454840958118439 relative L2 0.0015838434919714928\n",
      "training 0.001045446959324181 relative L2 0.0015838149702176452\n",
      "training 0.0010454097064211965 relative L2 0.0015837884275242686\n",
      "training 0.00104537233710289 relative L2 0.0015837597893550992\n",
      "training 0.0010453355498611927 relative L2 0.0015837333630770445\n",
      "training 0.00104529841337353 relative L2 0.0015837048413231969\n",
      "training 0.001045261393301189 relative L2 0.0015836784150451422\n",
      "training 0.001045224373228848 relative L2 0.001583649660460651\n",
      "training 0.0010451872367411852 relative L2 0.0015836234670132399\n",
      "training 0.001045150333084166 relative L2 0.0015835947124287486\n",
      "training 0.001045113429427147 relative L2 0.0015835684025660157\n",
      "training 0.0010450762929394841 relative L2 0.0015835395315662026\n",
      "training 0.0010450396221131086 relative L2 0.001583513687364757\n",
      "training 0.0010450026020407677 relative L2 0.0015834846999496222\n",
      "training 0.0010449659312143922 relative L2 0.0015834590885788202\n",
      "training 0.0010449291439726949 relative L2 0.0015834291698411107\n",
      "training 0.0010448925895616412 relative L2 0.001583404024131596\n",
      "training 0.001044855802319944 relative L2 0.0015833743382245302\n",
      "training 0.0010448190150782466 relative L2 0.0015833494253456593\n",
      "training 0.0010447819950059056 relative L2 0.001583319390192628\n",
      "training 0.0010447452077642083 relative L2 0.0015832938952371478\n",
      "training 0.001044708420522511 relative L2 0.0015832645585760474\n",
      "training 0.0010446718661114573 relative L2 0.0015832390636205673\n",
      "training 0.00104463507886976 relative L2 0.0015832098433747888\n",
      "training 0.0010445984080433846 relative L2 0.0015831844648346305\n",
      "training 0.001044561737217009 relative L2 0.0015831550117582083\n",
      "training 0.0010445250663906336 relative L2 0.0015831298660486937\n",
      "training 0.0010444886283949018 relative L2 0.0015831004129722714\n",
      "training 0.001044452073983848 relative L2 0.0015830752672627568\n",
      "training 0.0010444154031574726 relative L2 0.0015830458141863346\n",
      "training 0.0010443789651617408 relative L2 0.0015830204356461763\n",
      "training 0.001044342527166009 relative L2 0.0015829912154003978\n",
      "training 0.0010443059727549553 relative L2 0.001582966186106205\n",
      "training 0.0010442695347592235 relative L2 0.0015829367330297828\n",
      "training 0.0010442332131788135 relative L2 0.0015829115873202682\n",
      "training 0.001044196542352438 relative L2 0.001582882134243846\n",
      "training 0.0010441601043567061 relative L2 0.0015828571049496531\n",
      "training 0.0010441240156069398 relative L2 0.001582827535457909\n",
      "training 0.001044087577611208 relative L2 0.0015828035539016128\n",
      "training 0.0010440513724461198 relative L2 0.0015827728202566504\n",
      "training 0.001044015632942319 relative L2 0.0015827504685148597\n",
      "training 0.0010439796606078744 relative L2 0.001582718570716679\n",
      "training 0.0010439441539347172 relative L2 0.0015826985472813249\n",
      "training 0.00104390864726156 relative L2 0.0015826649032533169\n",
      "training 0.0010438739554956555 relative L2 0.0015826496528461576\n",
      "training 0.001043840660713613 relative L2 0.0015826142625883222\n",
      "training 0.001043809694238007 relative L2 0.001582611701451242\n",
      "training 0.0010437844321131706 relative L2 0.0015825792215764523\n",
      "training 0.0010437719756737351 relative L2 0.0015826249727979302\n",
      "training 0.0010437886230647564 relative L2 0.0015826374292373657\n",
      "training 0.001043871627189219 relative L2 0.001582894241437316\n",
      "training 0.0010441181948408484 relative L2 0.001583262113854289\n",
      "training 0.0010447627864778042 relative L2 0.001584628946147859\n",
      "training 0.0010463680373504758 relative L2 0.0015872801886871457\n",
      "training 0.0010502944933250546 relative L2 0.0015947791980579495\n",
      "training 0.0010598612716421485 relative L2 0.0016111250733956695\n",
      "training 0.0010829546954482794 relative L2 0.001652601407840848\n",
      "training 0.0011390579165890813 relative L2 0.0017435557674616575\n",
      "training 0.0012721531093120575 relative L2 0.001946001430042088\n",
      "training 0.0015860283747315407 relative L2 0.0023177287075668573\n",
      "training 0.002263015601783991 relative L2 0.0029116589576005936\n",
      "training 0.003574956674128771 relative L2 0.0034963127691298723\n",
      "training 0.0051722838543355465 relative L2 0.0036894080694764853\n",
      "training 0.005752348341047764 relative L2 0.002844173926860094\n",
      "training 0.00341690331697464 relative L2 0.0016651269979774952\n",
      "training 0.0011566183529794216 relative L2 0.002103011356666684\n",
      "training 0.001855456386692822 relative L2 0.0028741019777953625\n",
      "training 0.003489549970254302 relative L2 0.0025839509908109903\n",
      "training 0.0028111538849771023 relative L2 0.0016511967405676842\n",
      "training 0.0011386775877326727 relative L2 0.0020037454087287188\n",
      "training 0.0016865823417901993 relative L2 0.0025445292703807354\n",
      "training 0.002725380240008235 relative L2 0.0020595313981175423\n",
      "training 0.0017828901764005423 relative L2 0.001592285931110382\n",
      "training 0.0010571111924946308 relative L2 0.002116745337843895\n",
      "training 0.001879998715594411 relative L2 0.0021712242159992456\n",
      "training 0.0019836288411170244 relative L2 0.0016574696637690067\n",
      "training 0.0011458672815933824 relative L2 0.0017595026874914765\n",
      "training 0.001293357228860259 relative L2 0.002063416875898838\n",
      "training 0.0017896801000460982 relative L2 0.001790480106137693\n",
      "training 0.0013399135787039995 relative L2 0.0016018353635445237\n",
      "training 0.0010693382937461138 relative L2 0.0018900169525295496\n",
      "training 0.0014983684523031116 relative L2 0.0018455443205311894\n",
      "training 0.0014246978098526597 relative L2 0.0015866088215261698\n",
      "training 0.001049356535077095 relative L2 0.001737059443257749\n",
      "training 0.0012624786468222737 relative L2 0.0018274190369993448\n",
      "training 0.001396508188918233 relative L2 0.00162145821377635\n",
      "training 0.0010971735464408994 relative L2 0.001638071029447019\n",
      "training 0.0011202594032511115 relative L2 0.0017707280348986387\n",
      "training 0.0013101446675136685 relative L2 0.0016540546203032136\n",
      "training 0.0011426787823438644 relative L2 0.0015920082805678248\n",
      "training 0.00105673645157367 relative L2 0.0017050749156624079\n",
      "training 0.0012135718716308475 relative L2 0.0016681523993611336\n",
      "training 0.0011626239866018295 relative L2 0.0015820441767573357\n",
      "training 0.0010429979301989079 relative L2 0.001648738980293274\n",
      "training 0.001133684883825481 relative L2 0.0016641168622300029\n",
      "training 0.0011568997288122773 relative L2 0.0015895712422206998\n",
      "training 0.0010528976563364267 relative L2 0.0016100018983706832\n",
      "training 0.001080397516489029 relative L2 0.0016482402570545673\n",
      "training 0.0011345031671226025 relative L2 0.001600860501639545\n",
      "training 0.0010680342093110085 relative L2 0.0015891464427113533\n",
      "training 0.0010523335076868534 relative L2 0.0016276792157441378\n",
      "training 0.0011058016680181026 relative L2 0.0016084745293483138\n",
      "training 0.0010783292818814516 relative L2 0.0015820121625438333\n",
      "training 0.0010429373942315578 relative L2 0.0016082031652331352\n",
      "training 0.0010789161315187812 relative L2 0.0016098606865853071\n",
      "training 0.0010802119504660368 relative L2 0.0015828815521672368\n",
      "training 0.0010442616185173392 relative L2 0.0015935755800455809\n",
      "training 0.001058892928995192 relative L2 0.0016059429617598653\n",
      "training 0.0010749038774520159 relative L2 0.001586620812304318\n",
      "training 0.0010493986774235964 relative L2 0.001584964687936008\n",
      "training 0.0010471342829987407 relative L2 0.0015990555984899402\n",
      "training 0.0010656104423105717 relative L2 0.001589783001691103\n",
      "training 0.0010537212947383523 relative L2 0.0015816345112398267\n",
      "training 0.0010425327345728874 relative L2 0.0015917265554890037\n",
      "training 0.0010557809146121144 relative L2 0.0015907683409750462\n",
      "training 0.0010550692677497864 relative L2 0.0015817454550415277\n",
      "training 0.0010425819782540202 relative L2 0.001585929887369275\n",
      "training 0.0010480663040652871 relative L2 0.001589587307535112\n",
      "training 0.00105346052441746 relative L2 0.001583234639838338\n",
      "training 0.0010445115622133017 relative L2 0.0015824702568352222\n",
      "training 0.001043512485921383 relative L2 0.0015870671486482024\n",
      "training 0.001050022430717945 relative L2 0.0015845216112211347\n",
      "training 0.0010462020291015506 relative L2 0.0015811839839443564\n",
      "training 0.0010418700985610485 relative L2 0.0015843501314520836\n",
      "training 0.0010463104117661715 relative L2 0.0015848653856664896\n",
      "training 0.0010466554667800665 relative L2 0.0015812802594155073\n",
      "training 0.0010420650942251086 relative L2 0.0015822413843125105\n",
      "training 0.0010434137657284737 relative L2 0.0015841826098039746\n",
      "training 0.0010457532480359077 relative L2 0.0015818412648513913\n",
      "training 0.0010428621899336576 relative L2 0.0015811390476301312\n",
      "training 0.0010418746387585998 relative L2 0.0015830249758437276\n",
      "training 0.0010442300699651241 relative L2 0.0015822355635464191\n",
      "training 0.0010434146970510483 relative L2 0.001580883632414043\n",
      "training 0.001041480340063572 relative L2 0.0015818418469280005\n",
      "training 0.001042685005813837 relative L2 0.0015821607084944844\n",
      "training 0.0010433163261041045 relative L2 0.0015810836339369416\n",
      "training 0.001041711657308042 relative L2 0.0015810426557436585\n",
      "training 0.0010416579898446798 relative L2 0.0015817112289369106\n",
      "training 0.0010426982771605253 relative L2 0.0015813129721209407\n",
      "training 0.00104199827183038 relative L2 0.0015807022573426366\n",
      "training 0.001041242154315114 relative L2 0.0015811691991984844\n",
      "training 0.001041945768520236 relative L2 0.0015813400968909264\n",
      "training 0.001042029820382595 relative L2 0.0015806680312380195\n",
      "training 0.0010412285337224603 relative L2 0.0015807427698746324\n",
      "training 0.0010413440177217126 relative L2 0.0015811414923518896\n",
      "training 0.0010417713783681393 relative L2 0.0015807270538061857\n",
      "training 0.0010413277195766568 relative L2 0.001580529846251011\n",
      "training 0.00104103097692132 relative L2 0.0015808449825271964\n",
      "training 0.0010413902346044779 relative L2 0.0015807327581569552\n",
      "training 0.0010413442505523562 relative L2 0.0015804831637069583\n",
      "training 0.0010409449459984899 relative L2 0.0015805577859282494\n",
      "training 0.0010410274844616652 relative L2 0.001580619951710105\n",
      "training 0.00104118965100497 relative L2 0.0015804932918399572\n",
      "training 0.001040944247506559 relative L2 0.0015803799033164978\n",
      "training 0.001040810951963067 relative L2 0.001580459182150662\n",
      "training 0.0010409647366032004 relative L2 0.0015804738504812121\n",
      "training 0.001040912582539022 relative L2 0.0015802948037162423\n",
      "training 0.0010407157242298126 relative L2 0.0015803063288331032\n",
      "training 0.0010407454101368785 relative L2 0.0015803882852196693\n",
      "training 0.00104080094024539 relative L2 0.0015802503330633044\n",
      "training 0.0010406678775325418 relative L2 0.0015802050475031137\n",
      "training 0.00104059639852494 relative L2 0.001580278156325221\n",
      "training 0.0010406604269519448 relative L2 0.0015802044654265046\n",
      "training 0.0010406133951619267 relative L2 0.001580142299644649\n",
      "training 0.0010405018692836165 relative L2 0.00158015638589859\n",
      "training 0.0010405088542029262 relative L2 0.001580132870003581\n",
      "training 0.0010405171196907759 relative L2 0.0015800971304997802\n",
      "training 0.001040433649905026 relative L2 0.0015800574328750372\n",
      "training 0.001040387200191617 relative L2 0.0015800491673871875\n",
      "training 0.0010404010536149144 relative L2 0.0015800456749275327\n",
      "training 0.0010403618216514587 relative L2 0.0015799812972545624\n",
      "training 0.0010402958141639829 relative L2 0.0015799662796780467\n",
      "training 0.0010402839398011565 relative L2 0.001579976873472333\n",
      "training 0.001040271483361721 relative L2 0.0015799184329807758\n",
      "training 0.0010402172338217497 relative L2 0.0015798945678398013\n",
      "training 0.0010401819599792361 relative L2 0.00157990085426718\n",
      "training 0.0010401734616607428 relative L2 0.0015798567328602076\n",
      "training 0.0010401379549875855 relative L2 0.0015798312379047275\n",
      "training 0.0010400927858427167 relative L2 0.0015798230888321996\n",
      "training 0.0010400746250525117 relative L2 0.0015797916566953063\n",
      "training 0.0010400516912341118 relative L2 0.001579771749675274\n",
      "training 0.0010400101309642196 relative L2 0.0015797489322721958\n",
      "training 0.0010399807943031192 relative L2 0.0015797241358086467\n",
      "training 0.001039960770867765 relative L2 0.0015797107480466366\n",
      "training 0.0010399274760857224 relative L2 0.0015796800144016743\n",
      "training 0.0010398933663964272 relative L2 0.001579658011905849\n",
      "training 0.0010398700833320618 relative L2 0.0015796470688655972\n",
      "training 0.001039843074977398 relative L2 0.0015796145889908075\n",
      "training 0.0010398091981187463 relative L2 0.0015795930521562696\n",
      "training 0.001039782422594726 relative L2 0.0015795816434547305\n",
      "training 0.0010397573933005333 relative L2 0.001579550327733159\n",
      "training 0.0010397257283329964 relative L2 0.0015795293729752302\n",
      "training 0.0010396961588412523 relative L2 0.001579515403136611\n",
      "training 0.0010396713623777032 relative L2 0.001579486532136798\n",
      "training 0.0010396422585472465 relative L2 0.001579466974362731\n",
      "training 0.001039611641317606 relative L2 0.0015794492792338133\n",
      "training 0.0010395852150395513 relative L2 0.0015794228529557586\n",
      "training 0.001039557857438922 relative L2 0.0015794042265042663\n",
      "training 0.0010395278222858906 relative L2 0.001579383620992303\n",
      "training 0.0010395002318546176 relative L2 0.0015793591737747192\n",
      "training 0.0010394734563305974 relative L2 0.0015793421771377325\n",
      "training 0.0010394443525001407 relative L2 0.0015793186612427235\n",
      "training 0.001039415830746293 relative L2 0.001579296076670289\n",
      "training 0.0010393890552222729 relative L2 0.0015792794292792678\n",
      "training 0.0010393612319603562 relative L2 0.0015792549820616841\n",
      "training 0.0010393323609605432 relative L2 0.0015792333288118243\n",
      "training 0.001039304886944592 relative L2 0.0015792157500982285\n",
      "training 0.0010392776457592845 relative L2 0.0015791915357112885\n",
      "training 0.001039249007590115 relative L2 0.0015791706973686814\n",
      "training 0.0010392211843281984 relative L2 0.0015791523037478328\n",
      "training 0.0010391941759735346 relative L2 0.0015791284386068583\n",
      "training 0.0010391661198809743 relative L2 0.0015791081823408604\n",
      "training 0.0010391379473730922 relative L2 0.001579088973812759\n",
      "training 0.0010391107061877847 relative L2 0.0015790654579177499\n",
      "training 0.0010390829993411899 relative L2 0.0015790455508977175\n",
      "training 0.0010390549432486296 relative L2 0.0015790255274623632\n",
      "training 0.0010390273528173566 relative L2 0.0015790029428899288\n",
      "training 0.0010389999952167273 relative L2 0.0015789831522852182\n",
      "training 0.0010389720555394888 relative L2 0.001578962430357933\n",
      "training 0.0010389444651082158 relative L2 0.001578940311446786\n",
      "training 0.0010389171075075865 relative L2 0.0015789204044267535\n",
      "training 0.0010388892842456698 relative L2 0.0015788996824994683\n",
      "training 0.0010388619266450405 relative L2 0.0015788779128342867\n",
      "training 0.0010388344526290894 relative L2 0.0015788578893989325\n",
      "training 0.0010388068621978164 relative L2 0.0015788369346410036\n",
      "training 0.0010387793881818652 relative L2 0.0015788155142217875\n",
      "training 0.0010387521469965577 relative L2 0.0015787954907864332\n",
      "training 0.0010387245565652847 relative L2 0.0015787743031978607\n",
      "training 0.0010386971989646554 relative L2 0.0015787533484399319\n",
      "training 0.0010386697249487042 relative L2 0.0015787334414198995\n",
      "training 0.001038642367348075 relative L2 0.0015787121374160051\n",
      "training 0.0010386148933321238 relative L2 0.001578690716996789\n",
      "training 0.0010385876521468163 relative L2 0.0015786708099767566\n",
      "training 0.001038560178130865 relative L2 0.0015786500880494714\n",
      "training 0.001038532704114914 relative L2 0.0015786286676302552\n",
      "training 0.0010385053465142846 relative L2 0.0015786088770255446\n",
      "training 0.001038478221744299 relative L2 0.0015785876894369721\n",
      "training 0.0010384507477283478 relative L2 0.0015785668510943651\n",
      "training 0.0010384233901277184 relative L2 0.0015785465948283672\n",
      "training 0.001038396148942411 relative L2 0.0015785254072397947\n",
      "training 0.0010383690241724253 relative L2 0.0015785044524818659\n",
      "training 0.0010383415501564741 relative L2 0.0015784844290465117\n",
      "training 0.0010383144253864884 relative L2 0.0015784638235345483\n",
      "training 0.001038287067785859 relative L2 0.0015784426359459758\n",
      "training 0.0010382598266005516 relative L2 0.001578422379679978\n",
      "training 0.0010382324689999223 relative L2 0.0015784018905833364\n",
      "training 0.0010382054606452584 relative L2 0.0015783809358254075\n",
      "training 0.001038178219459951 relative L2 0.0015783605631440878\n",
      "training 0.0010381508618593216 relative L2 0.0015783398412168026\n",
      "training 0.0010381238535046577 relative L2 0.001578318770043552\n",
      "training 0.0010380966123193502 relative L2 0.0015782989794388413\n",
      "training 0.0010380694875493646 relative L2 0.001578278373926878\n",
      "training 0.001038042246364057 relative L2 0.0015782570699229836\n",
      "training 0.0010380152380093932 relative L2 0.0015782369300723076\n",
      "training 0.0010379879968240857 relative L2 0.0015782162081450224\n",
      "training 0.0010379609884694219 relative L2 0.0015781952533870935\n",
      "training 0.0010379338636994362 relative L2 0.0015781749971210957\n",
      "training 0.0010379067389294505 relative L2 0.0015781545080244541\n",
      "training 0.0010378797305747867 relative L2 0.0015781339025124907\n",
      "training 0.0010378527222201228 relative L2 0.0015781134134158492\n",
      "training 0.001037825713865459 relative L2 0.0015780931571498513\n",
      "training 0.0010377985890954733 relative L2 0.0015780722023919225\n",
      "training 0.0010377716971561313 relative L2 0.0015780518297106028\n",
      "training 0.0010377446888014674 relative L2 0.0015780313406139612\n",
      "training 0.00103771744761616 relative L2 0.0015780103858560324\n",
      "training 0.0010376906720921397 relative L2 0.0015779901295900345\n",
      "training 0.0010376634309068322 relative L2 0.0015779698733240366\n",
      "training 0.0010376364225521684 relative L2 0.0015779489185661077\n",
      "training 0.0010376095306128263 relative L2 0.0015779286623001099\n",
      "training 0.0010375824058428407 relative L2 0.001577908406034112\n",
      "training 0.0010375553974881768 relative L2 0.0015778873348608613\n",
      "training 0.001037528389133513 relative L2 0.0015778670785948634\n",
      "training 0.0010375013807788491 relative L2 0.0015778464730829\n",
      "training 0.001037474488839507 relative L2 0.0015778257511556149\n",
      "training 0.0010374474804848433 relative L2 0.0015778052620589733\n",
      "training 0.0010374205885455012 relative L2 0.0015777850057929754\n",
      "training 0.001037393813021481 relative L2 0.001577764400281012\n",
      "training 0.001037366921082139 relative L2 0.0015777439111843705\n",
      "training 0.0010373399127274752 relative L2 0.0015777235385030508\n",
      "training 0.0010373130207881331 relative L2 0.001577703282237053\n",
      "training 0.0010372863616794348 relative L2 0.0015776826767250896\n",
      "training 0.001037259353324771 relative L2 0.0015776620712131262\n",
      "training 0.0010372326942160726 relative L2 0.0015776419313624501\n",
      "training 0.0010372058022767305 relative L2 0.0015776213258504868\n",
      "training 0.0010371789103373885 relative L2 0.0015776007203385234\n",
      "training 0.0010371520183980465 relative L2 0.0015775804640725255\n",
      "training 0.0010371253592893481 relative L2 0.0015775602078065276\n",
      "training 0.001037098583765328 relative L2 0.0015775396022945642\n",
      "training 0.001037071691825986 relative L2 0.0015775191131979227\n",
      "training 0.0010370449163019657 relative L2 0.0015774988569319248\n",
      "training 0.0010370182571932673 relative L2 0.001577478600665927\n",
      "training 0.0010369913652539253 relative L2 0.0015774579951539636\n",
      "training 0.0010369645897299051 relative L2 0.0015774376224726439\n",
      "training 0.001036937814205885 relative L2 0.0015774171333760023\n",
      "training 0.0010369112715125084 relative L2 0.0015773967606946826\n",
      "training 0.0010368842631578445 relative L2 0.001577376271598041\n",
      "training 0.001036857720464468 relative L2 0.0015773558989167213\n",
      "training 0.0010368310613557696 relative L2 0.0015773356426507235\n",
      "training 0.0010368042858317494 relative L2 0.001577315153554082\n",
      "training 0.0010367775103077292 relative L2 0.001577294315211475\n",
      "training 0.0010367509676143527 relative L2 0.0015772742917761207\n",
      "training 0.0010367241920903325 relative L2 0.0015772538026794791\n",
      "training 0.001036697649396956 relative L2 0.0015772331971675158\n",
      "training 0.001036670757457614 relative L2 0.001577212824486196\n",
      "training 0.0010366442147642374 relative L2 0.0015771929174661636\n",
      "training 0.0010366176720708609 relative L2 0.0015771723119542003\n",
      "training 0.0010365908965468407 relative L2 0.0015771521721035242\n",
      "training 0.001036564470268786 relative L2 0.0015771317994222045\n",
      "training 0.0010365378111600876 relative L2 0.0015771111939102411\n",
      "training 0.0010365111520513892 relative L2 0.0015770909376442432\n",
      "training 0.0010364846093580127 relative L2 0.0015770707977935672\n",
      "training 0.0010364580666646361 relative L2 0.0015770498430356383\n",
      "training 0.0010364314075559378 relative L2 0.0015770301688462496\n",
      "training 0.0010364047484472394 relative L2 0.0015770099125802517\n",
      "training 0.0010363783221691847 relative L2 0.0015769890742376447\n",
      "training 0.0010363517794758081 relative L2 0.0015769691672176123\n",
      "training 0.001036325003951788 relative L2 0.0015769487945362926\n",
      "training 0.0010362984612584114 relative L2 0.001576928305439651\n",
      "training 0.0010362722678110003 relative L2 0.0015769080491736531\n",
      "training 0.0010362453758716583 relative L2 0.001576888025738299\n",
      "training 0.0010362190660089254 relative L2 0.0015768671873956919\n",
      "training 0.001036192406900227 relative L2 0.0015768475132063031\n",
      "training 0.0010361659806221724 relative L2 0.001576827373355627\n",
      "training 0.0010361395543441176 relative L2 0.0015768064185976982\n",
      "training 0.0010361127788200974 relative L2 0.0015767867444083095\n",
      "training 0.0010360863525420427 relative L2 0.001576766837388277\n",
      "training 0.0010360601590946317 relative L2 0.00157674599904567\n",
      "training 0.001036033732816577 relative L2 0.001576726441271603\n",
      "training 0.0010360070737078786 relative L2 0.0015767060685902834\n",
      "training 0.0010359804145991802 relative L2 0.0015766852302476764\n",
      "training 0.0010359542211517692 relative L2 0.001576665905304253\n",
      "training 0.0010359277948737144 relative L2 0.0015766454162076116\n",
      "training 0.0010359013685956597 relative L2 0.0015766246942803264\n",
      "training 0.0010358748259022832 relative L2 0.0015766052529215813\n",
      "training 0.0010358485160395503 relative L2 0.0015765849966555834\n",
      "training 0.0010358220897614956 relative L2 0.0015765642747282982\n",
      "training 0.0010357958963140845 relative L2 0.0015765452990308404\n",
      "training 0.0010357692372053862 relative L2 0.0015765245771035552\n",
      "training 0.0010357428109273314 relative L2 0.00157650385517627\n",
      "training 0.0010357163846492767 relative L2 0.0015764845302328467\n",
      "training 0.0010356898419559002 relative L2 0.0015764638083055615\n",
      "training 0.0010356632992625237 relative L2 0.0015764434356242418\n",
      "training 0.0010356369893997908 relative L2 0.001576423761434853\n",
      "training 0.0010356104467064142 relative L2 0.0015764035051688552\n",
      "training 0.0010355839040130377 relative L2 0.00157638278324157\n",
      "training 0.0010355578269809484 relative L2 0.001576363341882825\n",
      "training 0.001035531284287572 relative L2 0.001576343085616827\n",
      "training 0.0010355048580095172 relative L2 0.0015763225965201855\n",
      "training 0.0010354785481467843 relative L2 0.0015763030387461185\n",
      "training 0.0010354521218687296 relative L2 0.0015762826660647988\n",
      "training 0.0010354258120059967 relative L2 0.001576262409798801\n",
      "training 0.0010353992693126202 relative L2 0.0015762426191940904\n",
      "training 0.001035373075865209 relative L2 0.0015762225957587361\n",
      "training 0.0010353466495871544 relative L2 0.0015762019902467728\n",
      "training 0.0010353203397244215 relative L2 0.0015761824324727058\n",
      "training 0.0010352941462770104 relative L2 0.001576162176206708\n",
      "training 0.0010352678364142776 relative L2 0.0015761418035253882\n",
      "training 0.0010352411773055792 relative L2 0.0015761225949972868\n",
      "training 0.00103521510027349 relative L2 0.001576102338731289\n",
      "training 0.001035188906826079 relative L2 0.0015760818496346474\n",
      "training 0.001035162596963346 relative L2 0.0015760627575218678\n",
      "training 0.0010351361706852913 relative L2 0.0015760421520099044\n",
      "training 0.0010351098608225584 relative L2 0.0015760218957439065\n",
      "training 0.0010350837837904692 relative L2 0.0015760024543851614\n",
      "training 0.0010350573575124145 relative L2 0.0015759820817038417\n",
      "training 0.0010350310476496816 relative L2 0.0015759621746838093\n",
      "training 0.0010350049706175923 relative L2 0.0015759426169097424\n",
      "training 0.0010349786607548594 relative L2 0.001575922011397779\n",
      "training 0.0010349524673074484 relative L2 0.0015759026864543557\n",
      "training 0.0010349260410293937 relative L2 0.0015758825466036797\n",
      "training 0.0010348999639973044 relative L2 0.0015758622903376818\n",
      "training 0.0010348738869652152 relative L2 0.001575842616148293\n",
      "training 0.0010348475771024823 relative L2 0.0015758225927129388\n",
      "training 0.0010348212672397494 relative L2 0.0015758026856929064\n",
      "training 0.0010347950737923384 relative L2 0.0015757828950881958\n",
      "training 0.0010347689967602491 relative L2 0.001575762522406876\n",
      "training 0.00103474291972816 relative L2 0.0015757429646328092\n",
      "training 0.001034716609865427 relative L2 0.001575722941197455\n",
      "training 0.001034690416418016 relative L2 0.001575702684931457\n",
      "training 0.001034664106555283 relative L2 0.0015756831271573901\n",
      "training 0.0010346380295231938 relative L2 0.0015756632201373577\n",
      "training 0.0010346120689064264 relative L2 0.0015756433131173253\n",
      "training 0.0010345858754590154 relative L2 0.0015756237553432584\n",
      "training 0.0010345596820116043 relative L2 0.0015756033826619387\n",
      "training 0.0010345334885641932 relative L2 0.0015755838248878717\n",
      "training 0.0010345072951167822 relative L2 0.0015755636850371957\n",
      "training 0.001034481218084693 relative L2 0.001575544010847807\n",
      "training 0.0010344553738832474 relative L2 0.0015755239874124527\n",
      "training 0.0010344291804358363 relative L2 0.0015755041968077421\n",
      "training 0.001034403103403747 relative L2 0.0015754845226183534\n",
      "training 0.0010343767935410142 relative L2 0.0015754644991829991\n",
      "training 0.0010343508329242468 relative L2 0.0015754447085782886\n",
      "training 0.0010343246394768357 relative L2 0.0015754246851429343\n",
      "training 0.0010342987952753901 relative L2 0.0015754051273688674\n",
      "training 0.001034272601827979 relative L2 0.0015753854531794786\n",
      "training 0.0010342466412112117 relative L2 0.0015753653133288026\n",
      "training 0.0010342205641791224 relative L2 0.0015753459883853793\n",
      "training 0.0010341944871470332 relative L2 0.001575325964950025\n",
      "training 0.0010341685265302658 relative L2 0.0015753064071759582\n",
      "training 0.0010341423330828547 relative L2 0.0015752866165712476\n",
      "training 0.0010341164888814092 relative L2 0.0015752669423818588\n",
      "training 0.00103409041184932 relative L2 0.001575247384607792\n",
      "training 0.0010340644512325525 relative L2 0.0015752273611724377\n",
      "training 0.0010340384906157851 relative L2 0.001575207686983049\n",
      "training 0.0010340125299990177 relative L2 0.0015751878963783383\n",
      "training 0.0010339866857975721 relative L2 0.0015751683386042714\n",
      "training 0.0010339607251808047 relative L2 0.0015751485479995608\n",
      "training 0.0010339347645640373 relative L2 0.0015751286409795284\n",
      "training 0.00103390880394727 relative L2 0.0015751088503748178\n",
      "training 0.0010338826104998589 relative L2 0.0015750895254313946\n",
      "training 0.0010338566498830914 relative L2 0.0015750696184113622\n",
      "training 0.001033830689266324 relative L2 0.0015750497113913298\n",
      "training 0.0010338046122342348 relative L2 0.0015750298043712974\n",
      "training 0.0010337785352021456 relative L2 0.0015750103630125523\n",
      "training 0.0010337524581700563 relative L2 0.0015749905724078417\n",
      "training 0.0010337266139686108 relative L2 0.001574970898218453\n",
      "training 0.0010337005369365215 relative L2 0.0015749512240290642\n",
      "training 0.001033674692735076 relative L2 0.0015749313170090318\n",
      "training 0.0010336486157029867 relative L2 0.0015749117592349648\n",
      "training 0.0010336227715015411 relative L2 0.0015748918522149324\n",
      "training 0.001033596694469452 relative L2 0.0015748721780255437\n",
      "training 0.0010335708502680063 relative L2 0.0015748526202514768\n",
      "training 0.001033544773235917 relative L2 0.001574832946062088\n",
      "training 0.0010335189290344715 relative L2 0.0015748130390420556\n",
      "training 0.0010334932012483478 relative L2 0.0015747934812679887\n",
      "training 0.0010334671242162585 relative L2 0.0015747738070786\n",
      "training 0.0010334411635994911 relative L2 0.001574754249304533\n",
      "training 0.0010334153193980455 relative L2 0.0015747344586998224\n",
      "training 0.0010333895916119218 relative L2 0.0015747146680951118\n",
      "training 0.0010333635145798326 relative L2 0.001574695110321045\n",
      "training 0.001033337670378387 relative L2 0.001574675552546978\n",
      "training 0.0010333117097616196 relative L2 0.0015746558783575892\n",
      "training 0.001033285865560174 relative L2 0.0015746363205835223\n",
      "training 0.0010332600213587284 relative L2 0.0015746166463941336\n",
      "training 0.0010332341771572828 relative L2 0.0015745970886200666\n",
      "training 0.001033208565786481 relative L2 0.0015745775308459997\n",
      "training 0.0010331826051697135 relative L2 0.0015745579730719328\n",
      "training 0.0010331568773835897 relative L2 0.001574538298882544\n",
      "training 0.0010331310331821442 relative L2 0.0015745187411084771\n",
      "training 0.0010331050725653768 relative L2 0.0015744990669190884\n",
      "training 0.0010330792283639312 relative L2 0.0015744795091450214\n",
      "training 0.001033053733408451 relative L2 0.0015744601842015982\n",
      "training 0.0010330276563763618 relative L2 0.0015744402771815658\n",
      "training 0.00103300204500556 relative L2 0.0015744207194074988\n",
      "training 0.0010329763172194362 relative L2 0.001574401161633432\n",
      "training 0.0010329505894333124 relative L2 0.0015743818366900086\n",
      "training 0.0010329247452318668 relative L2 0.0015743625117465854\n",
      "training 0.001032899017445743 relative L2 0.0015743429539725184\n",
      "training 0.0010328734060749412 relative L2 0.0015743233961984515\n",
      "training 0.0010328474454581738 relative L2 0.0015743037220090628\n",
      "training 0.0010328218340873718 relative L2 0.0015742841642349958\n",
      "training 0.0010327959898859262 relative L2 0.001574264606460929\n",
      "training 0.0010327703785151243 relative L2 0.0015742452815175056\n",
      "training 0.0010327446507290006 relative L2 0.0015742257237434387\n",
      "training 0.0010327190393581986 relative L2 0.0015742062823846936\n",
      "training 0.001032693195156753 relative L2 0.0015741870738565922\n",
      "training 0.0010326674673706293 relative L2 0.0015741671668365598\n",
      "training 0.0010326418559998274 relative L2 0.0015741478418931365\n",
      "training 0.0010326162446290255 relative L2 0.0015741282841190696\n",
      "training 0.0010325906332582235 relative L2 0.0015741089591756463\n",
      "training 0.0010325649054720998 relative L2 0.0015740894014015794\n",
      "training 0.001032539177685976 relative L2 0.0015740700764581561\n",
      "training 0.001032513566315174 relative L2 0.0015740505186840892\n",
      "training 0.001032488071359694 relative L2 0.0015740313101559877\n",
      "training 0.001032462459988892 relative L2 0.0015740118687972426\n",
      "training 0.001032436965033412 relative L2 0.0015739923110231757\n",
      "training 0.00103241135366261 relative L2 0.0015739728696644306\n",
      "training 0.0010323856258764863 relative L2 0.0015739535447210073\n",
      "training 0.0010323600145056844 relative L2 0.0015739339869469404\n",
      "training 0.0010323345195502043 relative L2 0.0015739146620035172\n",
      "training 0.0010323089081794024 relative L2 0.0015738951042294502\n",
      "training 0.001032283529639244 relative L2 0.001573875779286027\n",
      "training 0.0010322579182684422 relative L2 0.0015738564543426037\n",
      "training 0.001032232423312962 relative L2 0.0015738371293991804\n",
      "training 0.0010322068119421601 relative L2 0.0015738175716251135\n",
      "training 0.00103218131698668 relative L2 0.0015737982466816902\n",
      "training 0.0010321559384465218 relative L2 0.0015737786889076233\n",
      "training 0.0010321303270757198 relative L2 0.0015737593639642\n",
      "training 0.0010321048321202397 relative L2 0.001573739806190133\n",
      "training 0.0010320793371647596 relative L2 0.0015737204812467098\n",
      "training 0.0010320540750399232 relative L2 0.0015737011563032866\n",
      "training 0.0010320284636691213 relative L2 0.0015736815985292196\n",
      "training 0.0010320029687136412 relative L2 0.0015736621571704745\n",
      "training 0.001031977473758161 relative L2 0.0015736431814730167\n",
      "training 0.001031951978802681 relative L2 0.001573623507283628\n",
      "training 0.0010319266002625227 relative L2 0.0015736042987555265\n",
      "training 0.0010319011053070426 relative L2 0.0015735848573967814\n",
      "training 0.0010318756103515625 relative L2 0.00157356564886868\n",
      "training 0.0010318501153960824 relative L2 0.001573546091094613\n",
      "training 0.0010318247368559241 relative L2 0.0015735267661511898\n",
      "training 0.001031799241900444 relative L2 0.0015735075576230884\n",
      "training 0.0010317738633602858 relative L2 0.0015734884655103087\n",
      "training 0.0010317483684048057 relative L2 0.0015734690241515636\n",
      "training 0.0010317228734493256 relative L2 0.001573449932038784\n",
      "training 0.0010316973784938455 relative L2 0.001573430490680039\n",
      "training 0.0010316719999536872 relative L2 0.0015734111657366157\n",
      "training 0.001031646504998207 relative L2 0.0015733919572085142\n",
      "training 0.0010316212428733706 relative L2 0.0015733725158497691\n",
      "training 0.0010315958643332124 relative L2 0.0015733533073216677\n",
      "training 0.001031570602208376 relative L2 0.0015733339823782444\n",
      "training 0.0010315451072528958 relative L2 0.0015733150066807866\n",
      "training 0.0010315198451280594 relative L2 0.0015732954489067197\n",
      "training 0.0010314944665879011 relative L2 0.0015732761239632964\n",
      "training 0.0010314692044630647 relative L2 0.0015732571482658386\n",
      "training 0.0010314437095075846 relative L2 0.0015732375904917717\n",
      "training 0.0010314184473827481 relative L2 0.0015732187312096357\n",
      "training 0.0010313931852579117 relative L2 0.001573199057020247\n",
      "training 0.0010313675738871098 relative L2 0.001573180197738111\n",
      "training 0.0010313424281775951 relative L2 0.0015731608727946877\n",
      "training 0.0010313171660527587 relative L2 0.0015731415478512645\n",
      "training 0.0010312919039279222 relative L2 0.0015731222229078412\n",
      "training 0.0010312666418030858 relative L2 0.0015731030143797398\n",
      "training 0.0010312412632629275 relative L2 0.0015730843879282475\n",
      "training 0.001031216117553413 relative L2 0.0015730647137388587\n",
      "training 0.0010311909718438983 relative L2 0.0015730462037026882\n",
      "training 0.00103116559330374 relative L2 0.0015730266459286213\n",
      "training 0.0010311402147635818 relative L2 0.0015730075538158417\n",
      "training 0.0010311150690540671 relative L2 0.0015729883452877402\n",
      "training 0.0010310898069292307 relative L2 0.0015729691367596388\n",
      "training 0.0010310645448043942 relative L2 0.0015729500446468592\n",
      "training 0.0010310390498489141 relative L2 0.0015729304868727922\n",
      "training 0.0010310140205547214 relative L2 0.0015729116275906563\n",
      "training 0.0010309888748452067 relative L2 0.0015728920698165894\n",
      "training 0.001030963845551014 relative L2 0.001572873443365097\n",
      "training 0.0010309384670108557 relative L2 0.0015728537691757083\n",
      "training 0.001030913321301341 relative L2 0.0015728345606476068\n",
      "training 0.0010308881755918264 relative L2 0.0015728154685348272\n",
      "training 0.00103086291346699 relative L2 0.0015727962600067258\n",
      "training 0.0010308376513421535 relative L2 0.001572777284309268\n",
      "training 0.001030812505632639 relative L2 0.0015727580757811666\n",
      "training 0.0010307873599231243 relative L2 0.0015727394493296742\n",
      "training 0.0010307623306289315 relative L2 0.0015727195423096418\n",
      "training 0.001030737068504095 relative L2 0.0015727007994428277\n",
      "training 0.0010307119227945805 relative L2 0.0015726815909147263\n",
      "training 0.0010306865442544222 relative L2 0.0015726623823866248\n",
      "training 0.0010306615149602294 relative L2 0.0015726430574432015\n",
      "training 0.0010306364856660366 relative L2 0.0015726243145763874\n",
      "training 0.0010306114563718438 relative L2 0.001572605106048286\n",
      "training 0.001030586427077651 relative L2 0.0015725858975201845\n",
      "training 0.0010305611649528146 relative L2 0.0015725669218227267\n",
      "training 0.0010305361356586218 relative L2 0.0015725478297099471\n",
      "training 0.0010305113391950727 relative L2 0.0015725286211818457\n",
      "training 0.0010304863099008799 relative L2 0.0015725094126537442\n",
      "training 0.0010304611641913652 relative L2 0.0015724904369562864\n",
      "training 0.0010304362513124943 relative L2 0.0015724714612588286\n",
      "training 0.0010304112220183015 relative L2 0.0015724522527307272\n",
      "training 0.0010303861927241087 relative L2 0.0015724332770332694\n",
      "training 0.0010303612798452377 relative L2 0.0015724143013358116\n",
      "training 0.0010303364833816886 relative L2 0.0015723949763923883\n",
      "training 0.0010303114540874958 relative L2 0.0015723761171102524\n",
      "training 0.0010302865412086248 relative L2 0.0015723567921668291\n",
      "training 0.001030261511914432 relative L2 0.0015723378164693713\n",
      "training 0.001030236599035561 relative L2 0.0015723189571872354\n",
      "training 0.001030211802572012 relative L2 0.0015722999814897776\n",
      "training 0.0010301867732778192 relative L2 0.0015722806565463543\n",
      "training 0.0010301617439836264 relative L2 0.0015722617972642183\n",
      "training 0.0010301369475200772 relative L2 0.0015722427051514387\n",
      "training 0.0010301119182258844 relative L2 0.001572223729453981\n",
      "training 0.0010300871217623353 relative L2 0.001572204870171845\n",
      "training 0.0010300623252987862 relative L2 0.0015721856616437435\n",
      "training 0.0010300374124199152 relative L2 0.0015721666859462857\n",
      "training 0.0010300124995410442 relative L2 0.0015721479430794716\n",
      "training 0.001029987703077495 relative L2 0.0015721287345513701\n",
      "training 0.0010299627901986241 relative L2 0.0015721101080998778\n",
      "training 0.0010299381101503968 relative L2 0.0015720908995717764\n",
      "training 0.0010299131972715259 relative L2 0.0015720721567049623\n",
      "training 0.001029888167977333 relative L2 0.0015720531810075045\n",
      "training 0.001029863371513784 relative L2 0.0015720340888947248\n",
      "training 0.0010298385750502348 relative L2 0.0015720154624432325\n",
      "training 0.0010298137785866857 relative L2 0.001571996370330453\n",
      "training 0.0010297887492924929 relative L2 0.001571977511048317\n",
      "training 0.0010297639528289437 relative L2 0.0015719585353508592\n",
      "training 0.0010297393891960382 relative L2 0.0015719395596534014\n",
      "training 0.0010297144763171673 relative L2 0.0015719207003712654\n",
      "training 0.0010296896798536181 relative L2 0.0015719016082584858\n",
      "training 0.0010296647669747472 relative L2 0.0015718827489763498\n",
      "training 0.0010296398540958762 relative L2 0.0015718640061095357\n",
      "training 0.001029615174047649 relative L2 0.001571844913996756\n",
      "training 0.0010295903775840998 relative L2 0.001571826171129942\n",
      "training 0.0010295656975358725 relative L2 0.0015718071954324841\n",
      "training 0.0010295410174876451 relative L2 0.0015717882197350264\n",
      "training 0.001029516221024096 relative L2 0.0015717693604528904\n",
      "training 0.0010294914245605469 relative L2 0.0015717503847554326\n",
      "training 0.0010294666280969977 relative L2 0.0015717315254732966\n",
      "training 0.0010294418316334486 relative L2 0.0015717127826064825\n",
      "training 0.0010294170351698995 relative L2 0.0015716938069090247\n",
      "training 0.0010293922387063503 relative L2 0.0015716749476268888\n",
      "training 0.0010293677914887667 relative L2 0.0015716562047600746\n",
      "training 0.0010293428786098957 relative L2 0.0015716373454779387\n",
      "training 0.0010293181985616684 relative L2 0.0015716183697804809\n",
      "training 0.001029293518513441 relative L2 0.001571599394083023\n",
      "training 0.001029268722049892 relative L2 0.001571580534800887\n",
      "training 0.0010292441584169865 relative L2 0.001571561791934073\n",
      "training 0.001029219594784081 relative L2 0.001571542932651937\n",
      "training 0.0010291949147358537 relative L2 0.0015715245390310884\n",
      "training 0.0010291700018569827 relative L2 0.0015715054469183087\n",
      "training 0.0010291454382240772 relative L2 0.001571486471220851\n",
      "training 0.00102912075817585 relative L2 0.0015714678447693586\n",
      "training 0.0010290963109582663 relative L2 0.0015714488690719008\n",
      "training 0.0010290713980793953 relative L2 0.0015714302426204085\n",
      "training 0.0010290469508618116 relative L2 0.001571411150507629\n",
      "training 0.0010290221543982625 relative L2 0.0015713926404714584\n",
      "training 0.001028997590765357 relative L2 0.0015713740140199661\n",
      "training 0.0010289730271324515 relative L2 0.0015713551547378302\n",
      "training 0.0010289486963301897 relative L2 0.001571336411871016\n",
      "training 0.0010289240162819624 relative L2 0.0015713174361735582\n",
      "training 0.001028899336233735 relative L2 0.0015712989261373878\n",
      "training 0.0010288747726008296 relative L2 0.00157127995043993\n",
      "training 0.0010288502089679241 relative L2 0.0015712613239884377\n",
      "training 0.0010288257617503405 relative L2 0.0015712425811216235\n",
      "training 0.0010288010817021132 relative L2 0.0015712239546701312\n",
      "training 0.0010287766344845295 relative L2 0.0015712050953879952\n",
      "training 0.0010287521872669458 relative L2 0.0015711867017671466\n",
      "training 0.0010287275072187185 relative L2 0.0015711678424850106\n",
      "training 0.001028702943585813 relative L2 0.0015711489832028747\n",
      "training 0.0010286784963682294 relative L2 0.001571130589582026\n",
      "training 0.0010286541655659676 relative L2 0.00157111173029989\n",
      "training 0.0010286298347637057 relative L2 0.0015710931038483977\n",
      "training 0.0010286052711308002 relative L2 0.0015710744773969054\n",
      "training 0.0010285809403285384 relative L2 0.0015710560837760568\n",
      "training 0.0010285564931109548 relative L2 0.001571037108078599\n",
      "training 0.001028532162308693 relative L2 0.0015710185980424285\n",
      "training 0.0010285077150911093 relative L2 0.0015709995059296489\n",
      "training 0.0010284833842888474 relative L2 0.001570981228724122\n",
      "training 0.0010284589370712638 relative L2 0.0015709626022726297\n",
      "training 0.0010284344898536801 relative L2 0.0015709437429904938\n",
      "training 0.0010284102754667401 relative L2 0.0015709250001236796\n",
      "training 0.0010283858282491565 relative L2 0.0015709063736721873\n",
      "training 0.0010283617302775383 relative L2 0.001570887747220695\n",
      "training 0.0010283371666446328 relative L2 0.0015708692371845245\n",
      "training 0.0010283126030117273 relative L2 0.0015708504943177104\n",
      "training 0.0010282885050401092 relative L2 0.0015708322171121836\n",
      "training 0.0010282640578225255 relative L2 0.001570813124999404\n",
      "training 0.0010282397270202637 relative L2 0.001570794964209199\n",
      "training 0.0010282155126333237 relative L2 0.0015707758720964193\n",
      "training 0.0010281911818310618 relative L2 0.0015707578277215362\n",
      "training 0.0010281669674441218 relative L2 0.0015707392012700438\n",
      "training 0.00102814263664186 relative L2 0.0015707208076491952\n",
      "training 0.0010281185386702418 relative L2 0.0015707017155364156\n",
      "training 0.0010280943242833018 relative L2 0.0015706835547462106\n",
      "training 0.00102806999348104 relative L2 0.001570664462633431\n",
      "training 0.0010280456626787782 relative L2 0.0015706464182585478\n",
      "training 0.0010280216811224818 relative L2 0.0015706276753917336\n",
      "training 0.0010279974667355418 relative L2 0.0015706097474321723\n",
      "training 0.0010279730195179582 relative L2 0.0015705905389040709\n",
      "training 0.00102794892154634 relative L2 0.0015705724945291877\n",
      "training 0.0010279245907440782 relative L2 0.0015705536352470517\n",
      "training 0.0010279007256031036 relative L2 0.0015705355908721685\n",
      "training 0.00102787627838552 relative L2 0.0015705166151747108\n",
      "training 0.0010278521804139018 relative L2 0.001570498337969184\n",
      "training 0.0010278281988576055 relative L2 0.0015704797115176916\n",
      "training 0.0010278039844706655 relative L2 0.0015704612014815211\n",
      "training 0.0010277797700837255 relative L2 0.0015704428078606725\n",
      "training 0.0010277556721121073 relative L2 0.001570424297824502\n",
      "training 0.001027731574140489 relative L2 0.0015704055549576879\n",
      "training 0.001027707359753549 relative L2 0.0015703873941674829\n",
      "training 0.001027683145366609 relative L2 0.001570368418470025\n",
      "training 0.0010276591638103127 relative L2 0.0015703501412644982\n",
      "training 0.0010276350658386946 relative L2 0.001570331514813006\n",
      "training 0.0010276107350364327 relative L2 0.001570313237607479\n",
      "training 0.0010275867534801364 relative L2 0.0015702946111559868\n",
      "training 0.0010275626555085182 relative L2 0.0015702759847044945\n",
      "training 0.0010275383247062564 relative L2 0.0015702573582530022\n",
      "training 0.00102751434314996 relative L2 0.001570239313878119\n",
      "training 0.0010274903615936637 relative L2 0.0015702206874266267\n",
      "training 0.0010274663800373673 relative L2 0.001570202293805778\n",
      "training 0.0010274421656504273 relative L2 0.0015701836673542857\n",
      "training 0.0010274180676788092 relative L2 0.0015701653901487589\n",
      "training 0.0010273942025378346 relative L2 0.0015701467636972666\n",
      "training 0.0010273701045662165 relative L2 0.0015701284864917397\n",
      "training 0.0010273463558405638 relative L2 0.0015701098600402474\n",
      "training 0.0010273221414536238 relative L2 0.0015700916992500424\n",
      "training 0.0010272981598973274 relative L2 0.0015700727235525846\n",
      "training 0.0010272744111716747 relative L2 0.001570055028423667\n",
      "training 0.0010272505460307002 relative L2 0.001570036169141531\n",
      "training 0.0010272265644744039 relative L2 0.0015700181247666478\n",
      "training 0.0010272025829181075 relative L2 0.0015699993818998337\n",
      "training 0.0010271784849464893 relative L2 0.0015699811046943069\n",
      "training 0.001027154503390193 relative L2 0.0015699622454121709\n",
      "training 0.0010271304054185748 relative L2 0.0015699443174526095\n",
      "training 0.0010271064238622785 relative L2 0.0015699255745857954\n",
      "training 0.001027082558721304 relative L2 0.001569907646626234\n",
      "training 0.0010270586935803294 relative L2 0.0015698890201747417\n",
      "training 0.001027034712024033 relative L2 0.0015698710922151804\n",
      "training 0.0010270109632983804 relative L2 0.0015698522329330444\n",
      "training 0.0010269870981574059 relative L2 0.0015698341885581613\n",
      "training 0.0010269630001857877 relative L2 0.0015698154456913471\n",
      "training 0.001026939251460135 relative L2 0.0015697976341471076\n",
      "training 0.0010269152699038386 relative L2 0.0015697790076956153\n",
      "training 0.0010268912883475423 relative L2 0.0015697611961513758\n",
      "training 0.0010268676560372114 relative L2 0.0015697426861152053\n",
      "training 0.001026843674480915 relative L2 0.0015697246417403221\n",
      "training 0.0010268199257552624 relative L2 0.0015697061317041516\n",
      "training 0.001026795944198966 relative L2 0.001569687738083303\n",
      "training 0.0010267719626426697 relative L2 0.0015696694608777761\n",
      "training 0.001026748213917017 relative L2 0.001569651416502893\n",
      "training 0.0010267243487760425 relative L2 0.0015696326736360788\n",
      "training 0.0010267007164657116 relative L2 0.0015696148620918393\n",
      "training 0.001026676967740059 relative L2 0.001569596235640347\n",
      "training 0.0010266532190144062 relative L2 0.0015695792390033603\n",
      "training 0.0010266293538734317 relative L2 0.0015695597976446152\n",
      "training 0.001026605605147779 relative L2 0.0015695426845923066\n",
      "training 0.0010265819728374481 relative L2 0.0015695233596488833\n",
      "training 0.0010265583405271173 relative L2 0.001569506130181253\n",
      "training 0.0010265345918014646 relative L2 0.0015694871544837952\n",
      "training 0.0010265108430758119 relative L2 0.001569469808600843\n",
      "training 0.0010264870943501592 relative L2 0.0015694508329033852\n",
      "training 0.0010264633456245065 relative L2 0.0015694333706051111\n",
      "training 0.0010264397133141756 relative L2 0.0015694143949076533\n",
      "training 0.001026415964588523 relative L2 0.0015693969326093793\n",
      "training 0.0010263922158628702 relative L2 0.0015693780733272433\n",
      "training 0.0010263684671372175 relative L2 0.0015693604946136475\n",
      "training 0.0010263448348268867 relative L2 0.0015693415189161897\n",
      "training 0.0010263212025165558 relative L2 0.001569324522279203\n",
      "training 0.0010262978030368686 relative L2 0.0015693055465817451\n",
      "training 0.0010262740543112159 relative L2 0.0015692884335294366\n",
      "training 0.001026250422000885 relative L2 0.0015692692250013351\n",
      "training 0.001026226906105876 relative L2 0.0015692516462877393\n",
      "training 0.0010262031573802233 relative L2 0.001569233019836247\n",
      "training 0.0010261795250698924 relative L2 0.0015692159067839384\n",
      "training 0.0010261557763442397 relative L2 0.0015691968146711588\n",
      "training 0.0010261322604492307 relative L2 0.0015691797016188502\n",
      "training 0.0010261086281388998 relative L2 0.0015691607259213924\n",
      "training 0.001026084995828569 relative L2 0.001569143496453762\n",
      "training 0.00102606147993356 relative L2 0.0015691245207563043\n",
      "training 0.001026037847623229 relative L2 0.0015691072912886739\n",
      "training 0.0010260142153128982 relative L2 0.0015690879663452506\n",
      "training 0.0010259904665872455 relative L2 0.0015690712025389075\n",
      "training 0.0010259669506922364 relative L2 0.001569051994010806\n",
      "training 0.0010259435512125492 relative L2 0.0015690347645431757\n",
      "training 0.0010259199189022183 relative L2 0.0015690161380916834\n",
      "training 0.0010258964030072093 relative L2 0.0015689993742853403\n",
      "training 0.001025873119942844 relative L2 0.001568980049341917\n",
      "training 0.0010258496040478349 relative L2 0.0015689631691202521\n",
      "training 0.001025825971737504 relative L2 0.0015689441934227943\n",
      "training 0.0010258023394271731 relative L2 0.0015689270803704858\n",
      "training 0.001025778939947486 relative L2 0.0015689082210883498\n",
      "training 0.0010257554240524769 relative L2 0.0015688914572820067\n",
      "training 0.0010257321409881115 relative L2 0.0015688723651692271\n",
      "training 0.0010257086250931025 relative L2 0.001568855601362884\n",
      "training 0.001025685342028737 relative L2 0.0015688366256654263\n",
      "training 0.001025661826133728 relative L2 0.0015688193961977959\n",
      "training 0.0010256381938233972 relative L2 0.0015688006533309817\n",
      "training 0.0010256149107590318 relative L2 0.001568783656693995\n",
      "training 0.0010255915112793446 relative L2 0.001568764797411859\n",
      "training 0.0010255681117996573 relative L2 0.0015687478007748723\n",
      "training 0.0010255447123199701 relative L2 0.0015687289414927363\n",
      "training 0.0010255210800096393 relative L2 0.0015687119448557496\n",
      "training 0.0010254979133605957 relative L2 0.0015686932019889355\n",
      "training 0.0010254745138809085 relative L2 0.0015686763217672706\n",
      "training 0.001025451230816543 relative L2 0.0015686575789004564\n",
      "training 0.0010254278313368559 relative L2 0.001568641047924757\n",
      "training 0.0010254046646878123 relative L2 0.0015686219558119774\n",
      "training 0.0010253816144540906 relative L2 0.0015686053084209561\n",
      "training 0.0010253582149744034 relative L2 0.0015685860998928547\n",
      "training 0.0010253350483253598 relative L2 0.0015685694525018334\n",
      "training 0.0010253117652609944 relative L2 0.0015685507096350193\n",
      "training 0.0010252881329506636 relative L2 0.0015685338294133544\n",
      "training 0.0010252646170556545 relative L2 0.0015685150865465403\n",
      "training 0.0010252411011606455 relative L2 0.0015684980899095535\n",
      "training 0.0010252177016809583 relative L2 0.0015684794634580612\n",
      "training 0.0010251941857859492 relative L2 0.0015684623504057527\n",
      "training 0.0010251709027215838 relative L2 0.0015684433747082949\n",
      "training 0.0010251477360725403 relative L2 0.0015684269601479173\n",
      "training 0.0010251242201775312 relative L2 0.0015684079844504595\n",
      "training 0.0010251009371131659 relative L2 0.0015683913370594382\n",
      "training 0.0010250777704641223 relative L2 0.0015683724777773023\n",
      "training 0.0010250546038150787 relative L2 0.0015683562960475683\n",
      "training 0.0010250312043353915 relative L2 0.0015683367382735014\n",
      "training 0.001025008037686348 relative L2 0.0015683206729590893\n",
      "training 0.0010249846382066607 relative L2 0.0015683014644309878\n",
      "training 0.0010249617043882608 relative L2 0.0015682853991165757\n",
      "training 0.0010249385377392173 relative L2 0.0015682652592658997\n",
      "training 0.00102491513825953 relative L2 0.0015682508237659931\n",
      "training 0.0010248922044411302 relative L2 0.0015682290541008115\n",
      "training 0.0010248698526993394 relative L2 0.0015682204393669963\n",
      "training 0.001024849247187376 relative L2 0.0015681965742260218\n",
      "training 0.0010248336475342512 relative L2 0.0015682162484154105\n",
      "training 0.0010248336475342512 relative L2 0.0015682210214436054\n",
      "training 0.0010248865000903606 relative L2 0.0015684609534218907\n",
      "training 0.0010251224739477038 relative L2 0.0015689980937168002\n",
      "training 0.0010259912814944983 relative L2 0.0015715121990069747\n",
      "training 0.0010290569625794888 relative L2 0.0015790924662724137\n",
      "training 0.0010397139703854918 relative L2 0.0016072442522272468\n",
      "training 0.0010767977219074965 relative L2 0.0016970743890851736\n",
      "training 0.0012045125477015972 relative L2 0.0019795119296759367\n",
      "training 0.0016419080784544349 relative L2 0.002685312647372484\n",
      "training 0.0030444301664829254 relative L2 0.004059344530105591\n",
      "training 0.006968097761273384 relative L2 0.005493753124028444\n",
      "training 0.012793902307748795 relative L2 0.0054818554781377316\n",
      "training 0.012725419364869595 relative L2 0.0024282815866172314\n",
      "training 0.002486256882548332 relative L2 0.0029892337042838335\n",
      "training 0.003776589874178171 relative L2 0.004738204181194305\n",
      "training 0.009501288644969463 relative L2 0.0025329587515443563\n",
      "training 0.002706794301047921 relative L2 0.00265307305380702\n",
      "training 0.002971294801682234 relative L2 0.0040201651863753796\n",
      "training 0.006833667866885662 relative L2 0.0018820491386577487\n",
      "training 0.0014860180672258139 relative L2 0.002950491616502404\n",
      "training 0.0036788408178836107 relative L2 0.0032816093880683184\n",
      "training 0.0045463573187589645 relative L2 0.0015813299687579274\n",
      "training 0.0010420262115076184 relative L2 0.003163702553138137\n",
      "training 0.004232221748679876 relative L2 0.002358756260946393\n",
      "training 0.002339243656024337 relative L2 0.0020461620297282934\n",
      "training 0.0017556581879034638 relative L2 0.0028905898798257113\n",
      "training 0.0035302485339343548 relative L2 0.0016221570549532771\n",
      "training 0.0010971390875056386 relative L2 0.0024859136901795864\n",
      "training 0.002600268693640828 relative L2 0.0021704379469156265\n",
      "training 0.001982456538826227 relative L2 0.0017711272230371833\n",
      "training 0.0013135793851688504 relative L2 0.002413862617686391\n",
      "training 0.002450573490932584 relative L2 0.0015998537419363856\n",
      "training 0.0010677564423531294 relative L2 0.0021591803524643183\n",
      "training 0.0019617239013314247 relative L2 0.0019350131042301655\n",
      "training 0.0015681044897064567 relative L2 0.0017247619107365608\n",
      "training 0.0012423066655173898 relative L2 0.0021091164089739323\n",
      "training 0.001870920998044312 relative L2 0.0015816139057278633\n",
      "training 0.0010423790663480759 relative L2 0.001963783521205187\n",
      "training 0.001615615445189178 relative L2 0.0017541121924296021\n",
      "training 0.0012880309950560331 relative L2 0.0016930613201111555\n",
      "training 0.0011984981829300523 relative L2 0.0018888643244281411\n",
      "training 0.001493376330472529 relative L2 0.0015730044106021523\n",
      "training 0.001031109131872654 relative L2 0.0018342214170843363\n",
      "training 0.0014102532295510173 relative L2 0.0016550460131838918\n",
      "training 0.001142664346843958 relative L2 0.0016747362678870559\n",
      "training 0.0011703908676281571 relative L2 0.001750585506670177\n",
      "training 0.001282768207602203 relative L2 0.0015749791637063026\n",
      "training 0.0010339139262214303 relative L2 0.0017402012599632144\n",
      "training 0.0012649877462536097 relative L2 0.0016024817014113069\n",
      "training 0.0010713371448218822 relative L2 0.00164879416115582\n",
      "training 0.0011355166789144278 relative L2 0.0016675274819135666\n",
      "training 0.0011602160520851612 relative L2 0.0015786122530698776\n",
      "training 0.0010383946355432272 relative L2 0.0016752156661823392\n",
      "training 0.0011729194084182382 relative L2 0.0015822185669094324\n",
      "training 0.0010431755799800158 relative L2 0.0016270928317680955\n",
      "training 0.0011039103846997023 relative L2 0.001620098715648055\n",
      "training 0.0010955685283988714 relative L2 0.0015782705741003156\n",
      "training 0.0010384286288172007 relative L2 0.001633289037272334\n",
      "training 0.0011124456068500876 relative L2 0.0015733307227492332\n",
      "training 0.0010317377746105194 relative L2 0.001607221201993525\n",
      "training 0.0010778596624732018 relative L2 0.00159606768283993\n",
      "training 0.001061703311279416 relative L2 0.0015771266771480441\n",
      "training 0.0010364152258262038 relative L2 0.0016061979113146663\n",
      "training 0.0010764647740870714 relative L2 0.0015704846009612083\n",
      "training 0.0010276963002979755 relative L2 0.0015937644056975842\n",
      "training 0.001058605732396245 relative L2 0.0015825690934434533\n",
      "training 0.001044287346303463 relative L2 0.001574316411279142\n",
      "training 0.0010331138037145138 relative L2 0.001590874744579196\n",
      "training 0.0010547294514253736 relative L2 0.001569074229337275\n",
      "training 0.0010259663686156273 relative L2 0.001583519042469561\n",
      "training 0.0010455867741256952 relative L2 0.0015762944240123034\n",
      "training 0.0010353113757446408 relative L2 0.001572366338223219\n",
      "training 0.0010301367146894336 relative L2 0.0015810008626431227\n",
      "training 0.001042183837853372 relative L2 0.0015685204416513443\n",
      "training 0.0010251534404233098 relative L2 0.0015776101499795914\n",
      "training 0.001037048059515655 relative L2 0.0015724045224487782\n",
      "training 0.0010305512696504593 relative L2 0.0015703210374340415\n",
      "training 0.001027722260914743 relative L2 0.0015758582158014178\n",
      "training 0.001034727320075035 relative L2 0.0015680965734645724\n",
      "training 0.0010246483143419027 relative L2 0.0015732255997136235\n",
      "training 0.001031675492413342 relative L2 0.0015706585254520178\n",
      "training 0.0010278879199177027 relative L2 0.0015692500164732337\n",
      "training 0.0010260557755827904 relative L2 0.0015722198877483606\n",
      "training 0.0010303196031600237 relative L2 0.0015678791096433997\n",
      "training 0.0010243133874610066 relative L2 0.0015709942672401667\n",
      "training 0.0010283211013302207 relative L2 0.0015692721353843808\n",
      "training 0.0010263138683512807 relative L2 0.0015682870289310813\n",
      "training 0.0010249584447592497 relative L2 0.0015704781981185079\n",
      "training 0.0010276444954797626 relative L2 0.00156765419524163\n",
      "training 0.0010240651899948716 relative L2 0.0015692280139774084\n",
      "training 0.001026262529194355 relative L2 0.001568697509355843\n",
      "training 0.0010253303917124867 relative L2 0.0015678588533774018\n",
      "training 0.0010242610005661845 relative L2 0.0015690259169787169\n",
      "training 0.001025991514325142 relative L2 0.0015675494214519858\n",
      "training 0.0010238764807581902 relative L2 0.0015684531535953283\n",
      "training 0.0010250131599605083 relative L2 0.0015680741053074598\n",
      "training 0.0010246866149827838 relative L2 0.0015674621099606156\n",
      "training 0.0010238245595246553 relative L2 0.0015683936653658748\n",
      "training 0.0010249351616948843 relative L2 0.0015673830639570951\n",
      "training 0.0010237167589366436 relative L2 0.0015677397605031729\n",
      "training 0.0010242321295663714 relative L2 0.001567827071994543\n",
      "training 0.0010242110583931208 relative L2 0.0015672948211431503\n",
      "training 0.0010235545923933387 relative L2 0.0015677466290071607\n",
      "training 0.0010242495918646455 relative L2 0.0015673183370381594\n",
      "training 0.0010235764784738421 relative L2 0.0015674533788114786\n",
      "training 0.001023739343509078 relative L2 0.0015674552414566278\n",
      "training 0.0010238484246656299 relative L2 0.0015671364963054657\n",
      "training 0.001023374730721116 relative L2 0.0015674828318879008\n",
      "training 0.0010237738024443388 relative L2 0.001567163271829486\n",
      "training 0.0010234351502731442 relative L2 0.0015671586152166128\n",
      "training 0.0010234324727207422 relative L2 0.0015673217130824924\n",
      "training 0.0010235714726150036 relative L2 0.001567038125358522\n",
      "training 0.001023243647068739 relative L2 0.0015671661822125316\n",
      "training 0.0010234554065391421 relative L2 0.0015671021537855268\n",
      "training 0.001023301389068365 relative L2 0.0015670380089432001\n",
      "training 0.0010232253698632121 relative L2 0.0015670821303501725\n",
      "training 0.0010233426000922918 relative L2 0.0015669583808630705\n",
      "training 0.0010231343330815434 relative L2 0.0015670373104512691\n",
      "training 0.0010232183849439025 relative L2 0.0015669582644477487\n",
      "training 0.001023168209940195 relative L2 0.001566898892633617\n",
      "training 0.0010230749612674117 relative L2 0.0015669887652620673\n",
      "training 0.0010231557535007596 relative L2 0.001566865248605609\n",
      "training 0.0010230327025055885 relative L2 0.0015668701380491257\n",
      "training 0.001023049233481288 relative L2 0.0015668959822505713\n",
      "training 0.0010230421321466565 relative L2 0.0015668162377551198\n",
      "training 0.0010229564504697919 relative L2 0.0015668298583477736\n",
      "training 0.0010230004554614425 relative L2 0.0015668035484850407\n",
      "training 0.0010229325853288174 relative L2 0.0015667874831706285\n",
      "training 0.0010229118634015322 relative L2 0.001566768274642527\n",
      "training 0.001022917334921658 relative L2 0.0015667338157072663\n",
      "training 0.001022852724418044 relative L2 0.0015667540719732642\n",
      "training 0.0010228657629340887 relative L2 0.0015667061088606715\n",
      "training 0.0010228302562609315 relative L2 0.00156668561976403\n",
      "training 0.00102279894053936 relative L2 0.0015667048282921314\n",
      "training 0.0010228028986603022 relative L2 0.0015666540712118149\n",
      "training 0.0010227549355477095 relative L2 0.0015666442923247814\n",
      "training 0.0010227501625195146 relative L2 0.0015666449908167124\n",
      "training 0.001022729673422873 relative L2 0.0015666130930185318\n",
      "training 0.0010226955637335777 relative L2 0.0015665995888411999\n",
      "training 0.0010226930025964975 relative L2 0.0015665865503251553\n",
      "training 0.0010226594749838114 relative L2 0.0015665747923776507\n",
      "training 0.0010226428275927901 relative L2 0.0015665516257286072\n",
      "training 0.0010226285085082054 relative L2 0.001566534978337586\n",
      "training 0.0010225983569398522 relative L2 0.0015665336977690458\n",
      "training 0.0010225888108834624 relative L2 0.0015665044775232673\n",
      "training 0.001022563548758626 relative L2 0.0015664895763620734\n",
      "training 0.0010225441073998809 relative L2 0.0015664875973016024\n",
      "training 0.0010225300211459398 relative L2 0.0015664610546082258\n",
      "training 0.001022503711283207 relative L2 0.0015664462698623538\n",
      "training 0.0010224898578599095 relative L2 0.0015664391685277224\n",
      "training 0.001022469368763268 relative L2 0.001566419959999621\n",
      "training 0.00102244783192873 relative L2 0.0015664026141166687\n",
      "training 0.0010224331635981798 relative L2 0.0015663910889998078\n",
      "training 0.0010224104626104236 relative L2 0.0015663800295442343\n",
      "training 0.001022394048050046 relative L2 0.0015663591912016273\n",
      "training 0.0010223754215985537 relative L2 0.0015663458034396172\n",
      "training 0.0010223546996712685 relative L2 0.0015663376543670893\n",
      "training 0.0010223391000181437 relative L2 0.001566316932439804\n",
      "training 0.0010223182616755366 relative L2 0.0015663022641092539\n",
      "training 0.0010223006829619408 relative L2 0.0015662937657907605\n",
      "training 0.001022282987833023 relative L2 0.0015662758378311992\n",
      "training 0.0010222627315670252 relative L2 0.001566259772516787\n",
      "training 0.0010222463170066476 relative L2 0.001566249062307179\n",
      "training 0.0010222266428172588 relative L2 0.0015662347432225943\n",
      "training 0.0010222084820270538 relative L2 0.0015662177465856075\n",
      "training 0.0010221910197287798 relative L2 0.001566205290146172\n",
      "training 0.0010221718112006783 relative L2 0.001566193881444633\n",
      "training 0.001022154581733048 relative L2 0.0015661760699003935\n",
      "training 0.0010221360716968775 relative L2 0.0015661626821383834\n",
      "training 0.001022117561660707 relative L2 0.0015661517390981317\n",
      "training 0.0010221002157777548 relative L2 0.0015661349752917886\n",
      "training 0.0010220814729109406 relative L2 0.001566120539791882\n",
      "training 0.0010220640106126666 relative L2 0.0015661095967516303\n",
      "training 0.0010220459662377834 relative L2 0.0015660942299291492\n",
      "training 0.0010220275726169348 relative L2 0.0015660790959373116\n",
      "training 0.0010220101103186607 relative L2 0.001566067454405129\n",
      "training 0.001021991833113134 relative L2 0.001566053251735866\n",
      "training 0.0010219739051535726 relative L2 0.001566037884913385\n",
      "training 0.0010219563264399767 relative L2 0.0015660253120586276\n",
      "training 0.0010219382820650935 relative L2 0.0015660119242966175\n",
      "training 0.0010219203541055322 relative L2 0.0015659965574741364\n",
      "training 0.0010219027753919363 relative L2 0.0015659837517887354\n",
      "training 0.001021884847432375 relative L2 0.0015659709461033344\n",
      "training 0.0010218671523034573 relative L2 0.001565955695696175\n",
      "training 0.001021849224343896 relative L2 0.0015659426571801305\n",
      "training 0.0010218315292149782 relative L2 0.0015659304335713387\n",
      "training 0.0010218138340860605 relative L2 0.0015659151831641793\n",
      "training 0.0010217963717877865 relative L2 0.0015659021446481347\n",
      "training 0.0010217786766588688 relative L2 0.0015658895717933774\n",
      "training 0.001021761097945273 relative L2 0.0015658749034628272\n",
      "training 0.0010217432864010334 relative L2 0.001565861515700817\n",
      "training 0.0010217255912721157 relative L2 0.001565848826430738\n",
      "training 0.0010217082453891635 relative L2 0.0015658341581001878\n",
      "training 0.0010216905502602458 relative L2 0.0015658207703381777\n",
      "training 0.0010216730879619718 relative L2 0.0015658081974834204\n",
      "training 0.001021655509248376 relative L2 0.001565793645568192\n",
      "training 0.0010216380469501019 relative L2 0.0015657799085602164\n",
      "training 0.0010216205846518278 relative L2 0.0015657676849514246\n",
      "training 0.001021603005938232 relative L2 0.0015657531330361962\n",
      "training 0.0010215856600552797 relative L2 0.0015657396288588643\n",
      "training 0.001021567964926362 relative L2 0.0015657268231734633\n",
      "training 0.0010215506190434098 relative L2 0.0015657128533348441\n",
      "training 0.0010215331567451358 relative L2 0.0015656991163268685\n",
      "training 0.00102151557803154 relative L2 0.0015656864270567894\n",
      "training 0.0010214981157332659 relative L2 0.0015656721079722047\n",
      "training 0.0010214807698503137 relative L2 0.0015656587202101946\n",
      "training 0.0010214634239673615 relative L2 0.0015656461473554373\n",
      "training 0.0010214458452537656 relative L2 0.001565632177516818\n",
      "training 0.0010214284993708134 relative L2 0.0015656184405088425\n",
      "training 0.0010214111534878612 relative L2 0.0015656057512387633\n",
      "training 0.0010213934583589435 relative L2 0.0015655920142307878\n",
      "training 0.001021376228891313 relative L2 0.0015655782772228122\n",
      "training 0.0010213588830083609 relative L2 0.0015655652387067676\n",
      "training 0.0010213415371254086 relative L2 0.001565552200190723\n",
      "training 0.0010213240748271346 relative L2 0.0015655384631827474\n",
      "training 0.001021306961774826 relative L2 0.0015655249590054154\n",
      "training 0.0010212896158918738 relative L2 0.0015655116876587272\n",
      "training 0.0010212721535935998 relative L2 0.0015654979506507516\n",
      "training 0.0010212548077106476 relative L2 0.0015654847957193851\n",
      "training 0.0010212375782430172 relative L2 0.0015654717572033405\n",
      "training 0.0010212203487753868 relative L2 0.0015654582530260086\n",
      "training 0.0010212030028924346 relative L2 0.0015654448652639985\n",
      "training 0.0010211857734248042 relative L2 0.001565431826747954\n",
      "training 0.001021168427541852 relative L2 0.0015654182061553001\n",
      "training 0.0010211514309048653 relative L2 0.0015654051676392555\n",
      "training 0.001021134085021913 relative L2 0.0015653917798772454\n",
      "training 0.0010211169719696045 relative L2 0.001565378624945879\n",
      "training 0.0010210995096713305 relative L2 0.0015653654700145125\n",
      "training 0.0010210821637883782 relative L2 0.0015653520822525024\n",
      "training 0.0010210651671513915 relative L2 0.0015653390437364578\n",
      "training 0.0010210479376837611 relative L2 0.0015653256559744477\n",
      "training 0.0010210307082161307 relative L2 0.0015653122682124376\n",
      "training 0.0010210133623331785 relative L2 0.0015652989968657494\n",
      "training 0.0010209961328655481 relative L2 0.001565285841934383\n",
      "training 0.0010209790198132396 relative L2 0.0015652724541723728\n",
      "training 0.001020961906760931 relative L2 0.0015652592992410064\n",
      "training 0.0010209446772933006 relative L2 0.0015652462607249618\n",
      "training 0.001020927680656314 relative L2 0.0015652329893782735\n",
      "training 0.001020910800434649 relative L2 0.001565219834446907\n",
      "training 0.001020893338136375 relative L2 0.0015652066795155406\n",
      "training 0.0010208761086687446 relative L2 0.0015651934081688523\n",
      "training 0.0010208592284470797 relative L2 0.0015651803696528077\n",
      "training 0.0010208418825641274 relative L2 0.0015651672147214413\n",
      "training 0.0010208250023424625 relative L2 0.0015651541762053967\n",
      "training 0.0010208076564595103 relative L2 0.0015651410212740302\n",
      "training 0.0010207908926531672 relative L2 0.001565127749927342\n",
      "training 0.0010207737796008587 relative L2 0.0015651147114112973\n",
      "training 0.0010207566665485501 relative L2 0.0015651015564799309\n",
      "training 0.0010207396699115634 relative L2 0.0015650884015485644\n",
      "training 0.001020722440443933 relative L2 0.0015650753630325198\n",
      "training 0.0010207054438069463 relative L2 0.0015650619752705097\n",
      "training 0.001020688097923994 relative L2 0.0015650487039238214\n",
      "training 0.0010206709848716855 relative L2 0.001565035548992455\n",
      "training 0.0010206539882346988 relative L2 0.0015650222776457667\n",
      "training 0.001020636991597712 relative L2 0.001565009355545044\n",
      "training 0.0010206199949607253 relative L2 0.0015649963170289993\n",
      "training 0.0010206028819084167 relative L2 0.0015649832785129547\n",
      "training 0.0010205860016867518 relative L2 0.0015649698907509446\n",
      "training 0.0010205690050497651 relative L2 0.0015649568522349\n",
      "training 0.0010205520084127784 relative L2 0.0015649436973035336\n",
      "training 0.0010205350117757916 relative L2 0.001564930658787489\n",
      "training 0.0010205181315541267 relative L2 0.001564917853102088\n",
      "training 0.00102050113491714 relative L2 0.0015649045817553997\n",
      "training 0.001020484254695475 relative L2 0.001564891543239355\n",
      "training 0.0010204673744738102 relative L2 0.0015648783883079886\n",
      "training 0.0010204503778368235 relative L2 0.0015648655826225877\n",
      "training 0.0010204333811998367 relative L2 0.0015648523112758994\n",
      "training 0.0010204165009781718 relative L2 0.0015648392727598548\n",
      "training 0.0010203993879258633 relative L2 0.0015648262342438102\n",
      "training 0.0010203826241195202 relative L2 0.0015648134285584092\n",
      "training 0.0010203656274825335 relative L2 0.0015648002736270428\n",
      "training 0.0010203486308455467 relative L2 0.0015647872351109982\n",
      "training 0.0010203317506238818 relative L2 0.0015647739637643099\n",
      "training 0.0010203146375715733 relative L2 0.0015647608088329434\n",
      "training 0.0010202978737652302 relative L2 0.0015647477703168988\n",
      "training 0.0010202807607129216 relative L2 0.0015647347318008542\n",
      "training 0.0010202637640759349 relative L2 0.0015647219261154532\n",
      "training 0.0010202467674389482 relative L2 0.0015647091204300523\n",
      "training 0.0010202298872172832 relative L2 0.001564695849083364\n",
      "training 0.0010202130069956183 relative L2 0.0015646826941519976\n",
      "training 0.0010201960103586316 relative L2 0.0015646698884665966\n",
      "training 0.0010201790137216449 relative L2 0.0015646566171199083\n",
      "training 0.00102016213349998 relative L2 0.0015646435786038637\n",
      "training 0.0010201451368629932 relative L2 0.0015646307729184628\n",
      "training 0.0010201281402260065 relative L2 0.0015646177344024181\n",
      "training 0.0010201113764196634 relative L2 0.0015646046958863735\n",
      "training 0.0010200946126133204 relative L2 0.001564591657370329\n",
      "training 0.0010200776159763336 relative L2 0.0015645787352696061\n",
      "training 0.0010200608521699905 relative L2 0.0015645658131688833\n",
      "training 0.0010200438555330038 relative L2 0.0015645530074834824\n",
      "training 0.001020026975311339 relative L2 0.001564539852552116\n",
      "training 0.0010200102115049958 relative L2 0.0015645269304513931\n",
      "training 0.001019993214868009 relative L2 0.0015645137755200267\n",
      "training 0.001019976451061666 relative L2 0.0015645009698346257\n",
      "training 0.001019959687255323 relative L2 0.001564488047733903\n",
      "training 0.001019942807033658 relative L2 0.001564475242048502\n",
      "training 0.0010199259268119931 relative L2 0.0015644622035324574\n",
      "training 0.00101990916300565 relative L2 0.0015644491650164127\n",
      "training 0.0010198921663686633 relative L2 0.0015644363593310118\n",
      "training 0.0010198754025623202 relative L2 0.0015644235536456108\n",
      "training 0.0010198585223406553 relative L2 0.0015644105151295662\n",
      "training 0.0010198417585343122 relative L2 0.0015643977094441652\n",
      "training 0.0010198249947279692 relative L2 0.0015643847873434424\n",
      "training 0.0010198081145063043 relative L2 0.001564371632412076\n",
      "training 0.0010197913506999612 relative L2 0.0015643589431419969\n",
      "training 0.0010197744704782963 relative L2 0.0015643459046259522\n",
      "training 0.0010197577066719532 relative L2 0.001564333215355873\n",
      "training 0.0010197407100349665 relative L2 0.0015643202932551503\n",
      "training 0.0010197239462286234 relative L2 0.0015643073711544275\n",
      "training 0.0010197071824222803 relative L2 0.001564294332638383\n",
      "training 0.0010196904186159372 relative L2 0.001564281526952982\n",
      "training 0.0010196735383942723 relative L2 0.0015642684884369373\n",
      "training 0.001019656891003251 relative L2 0.0015642556827515364\n",
      "training 0.0010196400107815862 relative L2 0.0015642432263121009\n",
      "training 0.0010196231305599213 relative L2 0.0015642299549654126\n",
      "training 0.0010196062503382564 relative L2 0.0015642171492800117\n",
      "training 0.001019589719362557 relative L2 0.0015642043435946107\n",
      "training 0.001019572839140892 relative L2 0.0015641916543245316\n",
      "training 0.0010195561917498708 relative L2 0.0015641788486391306\n",
      "training 0.0010195393115282059 relative L2 0.0015641659265384078\n",
      "training 0.001019522431306541 relative L2 0.001564153004437685\n",
      "training 0.001019505551084876 relative L2 0.0015641400823369622\n",
      "training 0.001019488787278533 relative L2 0.0015641272766515613\n",
      "training 0.0010194721398875117 relative L2 0.0015641145873814821\n",
      "training 0.0010194550268352032 relative L2 0.0015641015488654375\n",
      "training 0.0010194384958595037 relative L2 0.0015640887431800365\n",
      "training 0.0010194218484684825 relative L2 0.0015640759374946356\n",
      "training 0.0010194052010774612 relative L2 0.0015640630153939128\n",
      "training 0.0010193884372711182 relative L2 0.0015640504425391555\n",
      "training 0.001019371673464775 relative L2 0.0015640374040231109\n",
      "training 0.0010193547932431102 relative L2 0.0015640247147530317\n",
      "training 0.001019338029436767 relative L2 0.0015640116762369871\n",
      "training 0.001019321265630424 relative L2 0.0015639991033822298\n",
      "training 0.001019304501824081 relative L2 0.0015639860648661852\n",
      "training 0.0010192878544330597 relative L2 0.0015639734920114279\n",
      "training 0.0010192710906267166 relative L2 0.001563960686326027\n",
      "training 0.0010192543268203735 relative L2 0.001563947880640626\n",
      "training 0.0010192374465987086 relative L2 0.001563935074955225\n",
      "training 0.0010192209156230092 relative L2 0.0015639221528545022\n",
      "training 0.0010192040354013443 relative L2 0.0015639096964150667\n",
      "training 0.0010191872715950012 relative L2 0.0015638965414837003\n",
      "training 0.00101917062420398 relative L2 0.0015638838522136211\n",
      "training 0.0010191538603976369 relative L2 0.0015638710465282202\n",
      "training 0.0010191372130066156 relative L2 0.001563858357258141\n",
      "training 0.0010191204492002726 relative L2 0.00156384555157274\n",
      "training 0.0010191038018092513 relative L2 0.0015638326294720173\n",
      "training 0.0010190868051722646 relative L2 0.0015638198237866163\n",
      "training 0.0010190701577812433 relative L2 0.0015638071345165372\n",
      "training 0.001019053510390222 relative L2 0.0015637939795851707\n",
      "training 0.0010190366301685572 relative L2 0.0015637812903150916\n",
      "training 0.001019019866362214 relative L2 0.0015637688338756561\n",
      "training 0.0010190033353865147 relative L2 0.0015637557953596115\n",
      "training 0.0010189865715801716 relative L2 0.001563743338920176\n",
      "training 0.0010189696913585067 relative L2 0.001563730649650097\n",
      "training 0.0010189531603828073 relative L2 0.0015637176111340523\n",
      "training 0.0010189363965764642 relative L2 0.0015637049218639731\n",
      "training 0.0010189198656007648 relative L2 0.0015636923490092158\n",
      "training 0.0010189032182097435 relative L2 0.0015636796597391367\n",
      "training 0.0010188865708187222 relative L2 0.001563666621223092\n",
      "training 0.0010188698070123792 relative L2 0.001563653931953013\n",
      "training 0.001018853159621358 relative L2 0.001563641126267612\n",
      "training 0.0010188365122303367 relative L2 0.001563628320582211\n",
      "training 0.0010188197484239936 relative L2 0.0015636158641427755\n",
      "training 0.0010188028682023287 relative L2 0.0015636029420420527\n",
      "training 0.0010187863372266293 relative L2 0.0015635901363566518\n",
      "training 0.0010187695734202862 relative L2 0.0015635776799172163\n",
      "training 0.0010187530424445868 relative L2 0.0015635647578164935\n",
      "training 0.0010187362786382437 relative L2 0.0015635519521310925\n",
      "training 0.0010187195148319006 relative L2 0.0015635392628610134\n",
      "training 0.0010187029838562012 relative L2 0.001563526690006256\n",
      "training 0.0010186864528805017 relative L2 0.001563514000736177\n",
      "training 0.0010186696890741587 relative L2 0.0015635013114660978\n",
      "training 0.0010186531580984592 relative L2 0.0015634886221960187\n",
      "training 0.0010186366271227598 relative L2 0.0015634760493412614\n",
      "training 0.0010186200961470604 relative L2 0.0015634631272405386\n",
      "training 0.0010186034487560391 relative L2 0.0015634505543857813\n",
      "training 0.0010185868013650179 relative L2 0.0015634378651157022\n",
      "training 0.0010185701539739966 relative L2 0.0015634250594303012\n",
      "training 0.0010185533901676536 relative L2 0.001563412370160222\n",
      "training 0.001018536975607276 relative L2 0.001563399680890143\n",
      "training 0.0010185204446315765 relative L2 0.0015633869916200638\n",
      "training 0.0010185037972405553 relative L2 0.0015633739531040192\n",
      "training 0.0010184873826801777 relative L2 0.0015633614966645837\n",
      "training 0.0010184706188738346 relative L2 0.0015633490402251482\n",
      "training 0.001018454204313457 relative L2 0.0015633362345397472\n",
      "training 0.0010184376733377576 relative L2 0.001563323545269668\n",
      "training 0.0010184209095314145 relative L2 0.0015633107395842671\n",
      "training 0.001018404494971037 relative L2 0.001563298050314188\n",
      "training 0.0010183879639953375 relative L2 0.0015632857102900743\n",
      "training 0.0010183715494349599 relative L2 0.0015632729046046734\n",
      "training 0.0010183551348745823 relative L2 0.001563260448165238\n",
      "training 0.0010183386038988829 relative L2 0.0015632477588951588\n",
      "training 0.0010183218400925398 relative L2 0.0015632350696250796\n",
      "training 0.0010183053091168404 relative L2 0.0015632224967703223\n",
      "training 0.0010182886617258191 relative L2 0.001563209923915565\n",
      "training 0.0010182720143347979 relative L2 0.001563197118230164\n",
      "training 0.0010182555997744203 relative L2 0.001563184428960085\n",
      "training 0.0010182390687987208 relative L2 0.0015631718561053276\n",
      "training 0.0010182226542383432 relative L2 0.0015631592832505703\n",
      "training 0.0010182061232626438 relative L2 0.001563146710395813\n",
      "training 0.0010181894758716226 relative L2 0.0015631340211257339\n",
      "training 0.0010181729448959231 relative L2 0.0015631212154403329\n",
      "training 0.0010181562975049019 relative L2 0.0015631085261702538\n",
      "training 0.0010181396501138806 relative L2 0.0015630958369001746\n",
      "training 0.0010181230027228594 relative L2 0.0015630833804607391\n",
      "training 0.00101810647174716 relative L2 0.0015630709240213037\n",
      "training 0.0010180899407714605 relative L2 0.0015630580019205809\n",
      "training 0.001018073526211083 relative L2 0.0015630455454811454\n",
      "training 0.0010180568788200617 relative L2 0.0015630328562110662\n",
      "training 0.0010180402314290404 relative L2 0.0015630199341103435\n",
      "training 0.0010180234676226974 relative L2 0.0015630072448402643\n",
      "training 0.001018006820231676 relative L2 0.0015629949048161507\n",
      "training 0.0010179904056712985 relative L2 0.0015629824483767152\n",
      "training 0.001017973874695599 relative L2 0.001562969759106636\n",
      "training 0.0010179572273045778 relative L2 0.0015629571862518787\n",
      "training 0.0010179410455748439 relative L2 0.0015629444969817996\n",
      "training 0.0010179243981838226 relative L2 0.0015629320405423641\n",
      "training 0.0010179076343774796 relative L2 0.001562919351272285\n",
      "training 0.001017891219817102 relative L2 0.0015629066620022058\n",
      "training 0.0010178746888414025 relative L2 0.0015628942055627704\n",
      "training 0.0010178580414503813 relative L2 0.0015628817491233349\n",
      "training 0.0010178416268900037 relative L2 0.0015628692926838994\n",
      "training 0.0010178250959143043 relative L2 0.0015628563705831766\n",
      "training 0.0010178086813539267 relative L2 0.0015628441469743848\n",
      "training 0.0010177920339629054 relative L2 0.001562831224873662\n",
      "training 0.001017775502987206 relative L2 0.0015628186520189047\n",
      "training 0.0010177589720115066 relative L2 0.001562806311994791\n",
      "training 0.0010177422082051635 relative L2 0.00156279350630939\n",
      "training 0.0010177257936447859 relative L2 0.001562780817039311\n",
      "training 0.0010177093790844083 relative L2 0.0015627684770151973\n",
      "training 0.001017692731693387 relative L2 0.0015627556713297963\n",
      "training 0.0010176760843023658 relative L2 0.0015627432148903608\n",
      "training 0.00101765978615731 relative L2 0.0015627308748662472\n",
      "training 0.0010176431387662888 relative L2 0.0015627178363502026\n",
      "training 0.0010176266077905893 relative L2 0.0015627051470801234\n",
      "training 0.0010176101932302117 relative L2 0.001562693272717297\n",
      "training 0.0010175935458391905 relative L2 0.0015626804670318961\n",
      "training 0.0010175771312788129 relative L2 0.0015626678941771388\n",
      "training 0.0010175608331337571 relative L2 0.0015626557869836688\n",
      "training 0.0010175443021580577 relative L2 0.0015626427484676242\n",
      "training 0.0010175277711823583 relative L2 0.0015626302920281887\n",
      "training 0.0010175112402066588 relative L2 0.0015626181848347187\n",
      "training 0.0010174947092309594 relative L2 0.001562605146318674\n",
      "training 0.00101747817825526 relative L2 0.0015625925734639168\n",
      "training 0.0010174616472795606 relative L2 0.0015625808155164123\n",
      "training 0.0010174454655498266 relative L2 0.0015625677770003676\n",
      "training 0.001017429050989449 relative L2 0.0015625553205609322\n",
      "training 0.0010174126364290714 relative L2 0.0015625430969521403\n",
      "training 0.0010173963382840157 relative L2 0.0015625302912667394\n",
      "training 0.001017379923723638 relative L2 0.0015625176019966602\n",
      "training 0.0010173633927479386 relative L2 0.0015625057276338339\n",
      "training 0.001017346978187561 relative L2 0.0015624926891177893\n",
      "training 0.0010173306800425053 relative L2 0.0015624803490936756\n",
      "training 0.0010173143818974495 relative L2 0.0015624684747308493\n",
      "training 0.0010172980837523937 relative L2 0.0015624556690454483\n",
      "training 0.0010172814363613725 relative L2 0.0015624429797753692\n",
      "training 0.0010172651382163167 relative L2 0.0015624306397512555\n",
      "training 0.0010172486072406173 relative L2 0.0015624178340658545\n",
      "training 0.0010172321926802397 relative L2 0.0015624051447957754\n",
      "training 0.0010172155452892184 relative L2 0.001562393270432949\n",
      "training 0.0010171991307288408 relative L2 0.00156238058116287\n",
      "training 0.0010171827161684632 relative L2 0.0015623681247234344\n",
      "training 0.0010171664180234075 relative L2 0.0015623555518686771\n",
      "training 0.001017149887047708 relative L2 0.0015623429790139198\n",
      "training 0.0010171335889026523 relative L2 0.0015623305225744843\n",
      "training 0.0010171171743422747 relative L2 0.0015623178333044052\n",
      "training 0.0010171005269512534 relative L2 0.0015623054932802916\n",
      "training 0.0010170842288061976 relative L2 0.001562293036840856\n",
      "training 0.00101706781424582 relative L2 0.0015622805804014206\n",
      "training 0.0010170515161007643 relative L2 0.0015622678911313415\n",
      "training 0.0010170349851250648 relative L2 0.001562255434691906\n",
      "training 0.0010170184541493654 relative L2 0.0015622427454218268\n",
      "training 0.0010170018067583442 relative L2 0.0015622300561517477\n",
      "training 0.0010169853921979666 relative L2 0.0015622174832969904\n",
      "training 0.0010169687448069453 relative L2 0.001562204910442233\n",
      "training 0.0010169519810006022 relative L2 0.0015621923375874758\n",
      "training 0.0010169354500249028 relative L2 0.0015621797647327185\n",
      "training 0.0010169189190492034 relative L2 0.0015621669590473175\n",
      "training 0.0010169021552428603 relative L2 0.0015621546190232038\n",
      "training 0.001016885624267161 relative L2 0.0015621421625837684\n",
      "training 0.0010168689768761396 relative L2 0.0015621297061443329\n",
      "training 0.0010168527951464057 relative L2 0.0015621172497048974\n",
      "training 0.0010168362641707063 relative L2 0.0015621045604348183\n",
      "training 0.0010168197331950068 relative L2 0.0015620918711647391\n",
      "training 0.0010168032022193074 relative L2 0.0015620796475559473\n",
      "training 0.001016786671243608 relative L2 0.0015620669582858682\n",
      "training 0.0010167702566832304 relative L2 0.0015620545018464327\n",
      "training 0.0010167536092922091 relative L2 0.001562042161822319\n",
      "training 0.0010167373111471534 relative L2 0.0015620297053828835\n",
      "training 0.0010167206637561321 relative L2 0.001562017248943448\n",
      "training 0.0010167041327804327 relative L2 0.001562004559673369\n",
      "training 0.0010166874853894114 relative L2 0.0015619922196492553\n",
      "training 0.0010166710708290339 relative L2 0.001561979646794498\n",
      "training 0.0010166546562686563 relative L2 0.0015619673067703843\n",
      "training 0.0010166381252929568 relative L2 0.0015619546175003052\n",
      "training 0.0010166215943172574 relative L2 0.0015619423938915133\n",
      "training 0.0010166051797568798 relative L2 0.001561929821036756\n",
      "training 0.0010165887651965022 relative L2 0.0015619173645973206\n",
      "training 0.001016572117805481 relative L2 0.0015619051409885287\n",
      "training 0.0010165557032451034 relative L2 0.001561892218887806\n",
      "training 0.0010165394051000476 relative L2 0.0015618799952790141\n",
      "training 0.0010165227577090263 relative L2 0.0015618674224242568\n",
      "training 0.0010165063431486487 relative L2 0.0015618548495694995\n",
      "training 0.001016490045003593 relative L2 0.0015618425095453858\n",
      "training 0.0010164735140278935 relative L2 0.0015618300531059504\n",
      "training 0.0010164569830521941 relative L2 0.0015618175966665149\n",
      "training 0.0010164404520764947 relative L2 0.0015618052566424012\n",
      "training 0.0010164238046854734 relative L2 0.0015617928002029657\n",
      "training 0.0010164073901250958 relative L2 0.0015617799945175648\n",
      "training 0.0010163909755647182 relative L2 0.0015617673052474856\n",
      "training 0.0010163745610043406 relative L2 0.001561754965223372\n",
      "training 0.001016358146443963 relative L2 0.0015617422759532928\n",
      "training 0.0010163417318835855 relative L2 0.0015617298195138574\n",
      "training 0.0010163249680772424 relative L2 0.0015617173630744219\n",
      "training 0.0010163086699321866 relative L2 0.0015617050230503082\n",
      "training 0.001016292255371809 relative L2 0.0015616927994415164\n",
      "training 0.0010162757243961096 relative L2 0.0015616804594174027\n",
      "training 0.0010162595426663756 relative L2 0.0015616676537320018\n",
      "training 0.0010162428952753544 relative L2 0.0015616551972925663\n",
      "training 0.0010162264807149768 relative L2 0.0015616428572684526\n",
      "training 0.0010162099497392774 relative L2 0.0015616304008290172\n",
      "training 0.0010161935351788998 relative L2 0.0015616179443895817\n",
      "training 0.0010161770042032003 relative L2 0.0015616053715348244\n",
      "training 0.0010161607060581446 relative L2 0.0015615931479260325\n",
      "training 0.0010161441750824451 relative L2 0.0015615804586559534\n",
      "training 0.0010161279933527112 relative L2 0.0015615681186318398\n",
      "training 0.0010161116952076554 relative L2 0.0015615556621924043\n",
      "training 0.0010160952806472778 relative L2 0.0015615433221682906\n",
      "training 0.0010160788660869002 relative L2 0.0015615308657288551\n",
      "training 0.0010160623351112008 relative L2 0.0015615186421200633\n",
      "training 0.0010160459205508232 relative L2 0.001561506069265306\n",
      "training 0.0010160297388210893 relative L2 0.0015614936128258705\n",
      "training 0.0010160135570913553 relative L2 0.0015614813892170787\n",
      "training 0.0010159972589462996 relative L2 0.0015614686999469995\n",
      "training 0.0010159807279706001 relative L2 0.001561456243507564\n",
      "training 0.0010159644298255444 relative L2 0.001561443554237485\n",
      "training 0.0010159476660192013 relative L2 0.0015614310977980494\n",
      "training 0.0010159314842894673 relative L2 0.001561418641358614\n",
      "training 0.001015914836898446 relative L2 0.0015614059520885348\n",
      "training 0.0010158984223380685 relative L2 0.0015613934956490993\n",
      "training 0.001015881891362369 relative L2 0.001561380922794342\n",
      "training 0.0010158654768019915 relative L2 0.0015613684663549066\n",
      "training 0.0010158492950722575 relative L2 0.0015613557770848274\n",
      "training 0.001015832764096558 relative L2 0.001561343320645392\n",
      "training 0.0010158163495361805 relative L2 0.0015613308642059565\n",
      "training 0.001015799934975803 relative L2 0.0015613181749358773\n",
      "training 0.0010157836368307471 relative L2 0.0015613057184964418\n",
      "training 0.0010157672222703695 relative L2 0.0015612936113029718\n",
      "training 0.0010157510405406356 relative L2 0.0015612808056175709\n",
      "training 0.0010157345095649362 relative L2 0.0015612684655934572\n",
      "training 0.0010157180950045586 relative L2 0.0015612560091540217\n",
      "training 0.001015701680444181 relative L2 0.0015612433198839426\n",
      "training 0.0010156852658838034 relative L2 0.001561230979859829\n",
      "training 0.0010156690841540694 relative L2 0.0015612184070050716\n",
      "training 0.00101565255317837 relative L2 0.001561206066980958\n",
      "training 0.0010156360222026706 relative L2 0.0015611936105415225\n",
      "training 0.0010156198404729366 relative L2 0.0015611809212714434\n",
      "training 0.001015603425912559 relative L2 0.0015611688140779734\n",
      "training 0.0010155870113521814 relative L2 0.0015611561248078942\n",
      "training 0.0010155705967918038 relative L2 0.0015611439011991024\n",
      "training 0.00101555441506207 relative L2 0.001561131328344345\n",
      "training 0.0010155378840863705 relative L2 0.0015611187554895878\n",
      "training 0.0010155214695259929 relative L2 0.0015611064154654741\n",
      "training 0.001015505171380937 relative L2 0.0015610939590260386\n",
      "training 0.0010154888732358813 relative L2 0.0015610812697559595\n",
      "training 0.001015472342260182 relative L2 0.0015610690461471677\n",
      "training 0.0010154560441151261 relative L2 0.001561056706123054\n",
      "training 0.0010154396295547485 relative L2 0.001561044016852975\n",
      "training 0.001015423214994371 relative L2 0.0015610315604135394\n",
      "training 0.001015407033264637 relative L2 0.001561019103974104\n",
      "training 0.001015390269458294 relative L2 0.0015610064147040248\n",
      "training 0.0010153736220672727 relative L2 0.0015609940746799111\n",
      "training 0.0010153574403375387 relative L2 0.0015609815018251538\n",
      "training 0.0010153410257771611 relative L2 0.0015609691618010402\n",
      "training 0.0010153248440474272 relative L2 0.0015609570546075702\n",
      "training 0.0010153084294870496 relative L2 0.0015609445981681347\n",
      "training 0.001015292014926672 relative L2 0.0015609319088980556\n",
      "training 0.001015275833196938 relative L2 0.001560919568873942\n",
      "training 0.0010152595350518823 relative L2 0.0015609071124345064\n",
      "training 0.0010152430040761828 relative L2 0.001560894655995071\n",
      "training 0.001015226822346449 relative L2 0.0015608821995556355\n",
      "training 0.001015210640616715 relative L2 0.0015608697431162\n",
      "training 0.0010151942260563374 relative L2 0.0015608574030920863\n",
      "training 0.0010151779279112816 relative L2 0.0015608447138220072\n",
      "training 0.0010151616297662258 relative L2 0.0015608323737978935\n",
      "training 0.0010151454480364919 relative L2 0.0015608201501891017\n",
      "training 0.0010151293827220798 relative L2 0.001560807810164988\n",
      "training 0.0010151128517463803 relative L2 0.0015607955865561962\n",
      "training 0.0010150966700166464 relative L2 0.0015607832465320826\n",
      "training 0.0010150804882869124 relative L2 0.0015607705572620034\n",
      "training 0.0010150641901418567 relative L2 0.001560758100822568\n",
      "training 0.0010150480084121227 relative L2 0.0015607457607984543\n",
      "training 0.0010150318266823888 relative L2 0.0015607329551130533\n",
      "training 0.0010150151792913675 relative L2 0.0015607204986736178\n",
      "training 0.0010149991139769554 relative L2 0.0015607081586495042\n",
      "training 0.0010149826994165778 relative L2 0.0015606957022100687\n",
      "training 0.0010149662848562002 relative L2 0.0015606830129399896\n",
      "training 0.0010149499867111444 relative L2 0.0015606709057465196\n",
      "training 0.0010149336885660887 relative L2 0.0015606586821377277\n",
      "training 0.001014917274005711 relative L2 0.0015606461092829704\n",
      "training 0.0010149009758606553 relative L2 0.0015606335364282131\n",
      "training 0.0010148845613002777 relative L2 0.001560620847158134\n",
      "training 0.0010148681467399001 relative L2 0.0015606085071340203\n",
      "training 0.0010148516157642007 relative L2 0.0015605958178639412\n",
      "training 0.001014835317619145 relative L2 0.0015605835942551494\n",
      "training 0.0010148190194740891 relative L2 0.0015605711378157139\n",
      "training 0.0010148027213290334 relative L2 0.0015605585649609566\n",
      "training 0.0010147863067686558 relative L2 0.0015605463413521647\n",
      "training 0.0010147700086236 relative L2 0.0015605336520820856\n",
      "training 0.001014753826893866 relative L2 0.0015605211956426501\n",
      "training 0.0010147371795028448 relative L2 0.0015605088556185365\n",
      "training 0.0010147209977731109 relative L2 0.001560496399179101\n",
      "training 0.001014704816043377 relative L2 0.0015604837099090219\n",
      "training 0.0010146882850676775 relative L2 0.0015604712534695864\n",
      "training 0.0010146721033379436 relative L2 0.0015604587970301509\n",
      "training 0.001014655688777566 relative L2 0.0015604463405907154\n",
      "training 0.001014639507047832 relative L2 0.001560433767735958\n",
      "training 0.00101462344173342 relative L2 0.0015604215441271663\n",
      "training 0.0010146069107577205 relative L2 0.0015604090876877308\n",
      "training 0.0010145906126126647 relative L2 0.0015603963984176517\n",
      "training 0.001014574314467609 relative L2 0.0015603839419782162\n",
      "training 0.0010145580163225532 relative L2 0.0015603713691234589\n",
      "training 0.001014541951008141 relative L2 0.0015603589126840234\n",
      "training 0.0010145256528630853 relative L2 0.0015603466890752316\n",
      "training 0.0010145093547180295 relative L2 0.0015603339998051524\n",
      "training 0.001014492940157652 relative L2 0.0015603218926116824\n",
      "training 0.0010144768748432398 relative L2 0.0015603090869262815\n",
      "training 0.001014460576698184 relative L2 0.0015602965140715241\n",
      "training 0.00101444439496845 relative L2 0.0015602840576320887\n",
      "training 0.0010144280968233943 relative L2 0.001560271717607975\n",
      "training 0.0010144120315089822 relative L2 0.0015602594939991832\n",
      "training 0.0010143956169486046 relative L2 0.001560246804729104\n",
      "training 0.001014379202388227 relative L2 0.001560234115459025\n",
      "training 0.0010143627878278494 relative L2 0.001560221891850233\n",
      "training 0.0010143464896827936 relative L2 0.001560209202580154\n",
      "training 0.001014330075122416 relative L2 0.0015601967461407185\n",
      "training 0.001014313893392682 relative L2 0.0015601840568706393\n",
      "training 0.00101429782807827 relative L2 0.0015601719496771693\n",
      "training 0.001014281646348536 relative L2 0.0015601592604070902\n",
      "training 0.001014265464618802 relative L2 0.0015601471532136202\n",
      "training 0.0010142495157197118 relative L2 0.001560134463943541\n",
      "training 0.0010142331011593342 relative L2 0.0015601220075041056\n",
      "training 0.0010142169194296002 relative L2 0.00156010955106467\n",
      "training 0.0010142006212845445 relative L2 0.0015600972110405564\n",
      "training 0.0010141843231394887 relative L2 0.001560084754601121\n",
      "training 0.0010141682578250766 relative L2 0.0015600722981616855\n",
      "training 0.0010141523089259863 relative L2 0.0015600599581375718\n",
      "training 0.001014135661534965 relative L2 0.0015600475016981363\n",
      "training 0.001014119479805231 relative L2 0.0015600350452587008\n",
      "training 0.0010141030652448535 relative L2 0.0015600222395732999\n",
      "training 0.0010140867670997977 relative L2 0.0015600095503032207\n",
      "training 0.0010140703525394201 relative L2 0.001559997210279107\n",
      "training 0.0010140541708096862 relative L2 0.0015599847538396716\n",
      "training 0.0010140379890799522 relative L2 0.0015599722974002361\n",
      "training 0.0010140218073502183 relative L2 0.0015599598409608006\n",
      "training 0.0010140053927898407 relative L2 0.001559947500936687\n",
      "training 0.0010139893274754286 relative L2 0.0015599351609125733\n",
      "training 0.0010139731457456946 relative L2 0.001559922588057816\n",
      "training 0.0010139570804312825 relative L2 0.0015599102480337024\n",
      "training 0.0010139407822862267 relative L2 0.0015598977915942669\n",
      "training 0.0010139247169718146 relative L2 0.0015598853351548314\n",
      "training 0.0010139085352420807 relative L2 0.001559872762300074\n",
      "training 0.0010138923535123467 relative L2 0.0015598603058606386\n",
      "training 0.0010138761717826128 relative L2 0.0015598476165905595\n",
      "training 0.001013859873637557 relative L2 0.001559835160151124\n",
      "training 0.001013843691907823 relative L2 0.0015598224708810449\n",
      "training 0.0010138271609321237 relative L2 0.001559810247272253\n",
      "training 0.001013810746371746 relative L2 0.0015597977908328176\n",
      "training 0.0010137945646420121 relative L2 0.0015597852179780602\n",
      "training 0.0010137781500816345 relative L2 0.0015597731107845902\n",
      "training 0.0010137619683519006 relative L2 0.0015597606543451548\n",
      "training 0.0010137459030374885 relative L2 0.0015597481979057193\n",
      "training 0.0010137294884771109 relative L2 0.0015597355086356401\n",
      "training 0.0010137136559933424 relative L2 0.0015597232850268483\n",
      "training 0.0010136973578482866 relative L2 0.0015597109450027347\n",
      "training 0.0010136811761185527 relative L2 0.0015596982557326555\n",
      "training 0.0010136649943888187 relative L2 0.0015596860321238637\n",
      "training 0.001013648696243763 relative L2 0.0015596733428537846\n",
      "training 0.0010136322816833854 relative L2 0.001559661002829671\n",
      "training 0.0010136160999536514 relative L2 0.00155964819714427\n",
      "training 0.0010136000346392393 relative L2 0.0015596358571201563\n",
      "training 0.0010135835036635399 relative L2 0.0015596234006807208\n",
      "training 0.0010135672055184841 relative L2 0.0015596110606566072\n",
      "training 0.0010135507909581065 relative L2 0.0015595986042171717\n",
      "training 0.0010135346092283726 relative L2 0.0015595861477777362\n",
      "training 0.0010135185439139605 relative L2 0.0015595736913383007\n",
      "training 0.0010135021293535829 relative L2 0.0015595612348988652\n",
      "training 0.001013485947623849 relative L2 0.001559548545628786\n",
      "training 0.0010134698823094368 relative L2 0.0015595362056046724\n",
      "training 0.0010134537005797029 relative L2 0.001559523749165237\n",
      "training 0.001013437402434647 relative L2 0.0015595114091411233\n",
      "training 0.001013421337120235 relative L2 0.0015594991855323315\n",
      "training 0.001013405155390501 relative L2 0.0015594864962622523\n",
      "training 0.0010133888572454453 relative L2 0.0015594738069921732\n",
      "training 0.0010133727919310331 relative L2 0.0015594613505527377\n",
      "training 0.0010133564937859774 relative L2 0.0015594485448673368\n",
      "training 0.0010133400792255998 relative L2 0.001559436204843223\n",
      "training 0.0010133241303265095 relative L2 0.001559423515573144\n",
      "training 0.0010133077157661319 relative L2 0.0015594109427183867\n",
      "training 0.0010132911847904325 relative L2 0.0015593988355249166\n",
      "training 0.0010132751194760203 relative L2 0.0015593861462548375\n",
      "training 0.0010132590541616082 relative L2 0.0015593738062307239\n",
      "training 0.001013243105262518 relative L2 0.0015593613497912884\n",
      "training 0.001013226923532784 relative L2 0.0015593486605212092\n",
      "training 0.0010132106253877282 relative L2 0.0015593365533277392\n",
      "training 0.0010131944436579943 relative L2 0.00155932386405766\n",
      "training 0.0010131782619282603 relative L2 0.0015593114076182246\n",
      "training 0.0010131620801985264 relative L2 0.0015592991840094328\n",
      "training 0.0010131460148841143 relative L2 0.0015592868439853191\n",
      "training 0.0010131299495697021 relative L2 0.00155927415471524\n",
      "training 0.00101311388425529 relative L2 0.00155926204752177\n",
      "training 0.0010130974696949124 relative L2 0.0015592493582516909\n",
      "training 0.0010130812879651785 relative L2 0.0015592369018122554\n",
      "training 0.0010130653390660882 relative L2 0.0015592245617881417\n",
      "training 0.0010130491573363543 relative L2 0.0015592119889333844\n",
      "training 0.0010130329756066203 relative L2 0.0015591996489092708\n",
      "training 0.0010130166774615645 relative L2 0.0015591871924698353\n",
      "training 0.0010130004957318306 relative L2 0.0015591747360303998\n",
      "training 0.0010129841975867748 relative L2 0.0015591623960062861\n",
      "training 0.0010129681322723627 relative L2 0.001559149706736207\n",
      "training 0.001012951834127307 relative L2 0.0015591372502967715\n",
      "training 0.0010129355359822512 relative L2 0.0015591245610266924\n",
      "training 0.0010129193542525172 relative L2 0.001559112104587257\n",
      "training 0.001012903288938105 relative L2 0.0015590994153171778\n",
      "training 0.0010128869907930493 relative L2 0.0015590869588777423\n",
      "training 0.0010128708090633154 relative L2 0.0015590747352689505\n",
      "training 0.0010128543945029378 relative L2 0.0015590621624141932\n",
      "training 0.0010128382127732038 relative L2 0.0015590495895594358\n",
      "training 0.001012821914628148 relative L2 0.0015590370167046785\n",
      "training 0.0010128059657290578 relative L2 0.001559024560265243\n",
      "training 0.0010127893183380365 relative L2 0.0015590121038258076\n",
      "training 0.0010127730201929808 relative L2 0.001558999647386372\n",
      "training 0.0010127566056326032 relative L2 0.0015589871909469366\n",
      "training 0.0010127404239028692 relative L2 0.0015589742688462138\n",
      "training 0.0010127242421731353 relative L2 0.001558962045237422\n",
      "training 0.0010127080604434013 relative L2 0.0015589493559673429\n",
      "training 0.0010126917622983456 relative L2 0.0015589368995279074\n",
      "training 0.0010126754641532898 relative L2 0.0015589247923344374\n",
      "training 0.0010126593988388777 relative L2 0.0015589121030643582\n",
      "training 0.0010126432171091437 relative L2 0.0015588996466249228\n",
      "training 0.0010126270353794098 relative L2 0.0015588871901854873\n",
      "training 0.0010126108536496758 relative L2 0.00155887461733073\n",
      "training 0.0010125949047505856 relative L2 0.0015588627429679036\n",
      "training 0.0010125788394361734 relative L2 0.0015588502865284681\n",
      "training 0.0010125625412911177 relative L2 0.001558837597258389\n",
      "training 0.0010125463595613837 relative L2 0.0015588252572342753\n",
      "training 0.0010125302942469716 relative L2 0.0015588125679641962\n",
      "training 0.001012513879686594 relative L2 0.0015588001115247607\n",
      "training 0.00101249769795686 relative L2 0.001558787887915969\n",
      "training 0.0010124815162271261 relative L2 0.001558774965815246\n",
      "training 0.001012465450912714 relative L2 0.001558762858621776\n",
      "training 0.0010124491527676582 relative L2 0.001558750751428306\n",
      "training 0.0010124330874532461 relative L2 0.001558738062158227\n",
      "training 0.001012417022138834 relative L2 0.001558725954964757\n",
      "training 0.0010124009568244219 relative L2 0.0015587133821099997\n",
      "training 0.0010123846586793661 relative L2 0.001558701042085886\n",
      "training 0.001012368593364954 relative L2 0.0015586885856464505\n",
      "training 0.0010123522952198982 relative L2 0.0015586762456223369\n",
      "training 0.0010123364627361298 relative L2 0.0015586639055982232\n",
      "training 0.0010123203974217176 relative L2 0.0015586516819894314\n",
      "training 0.0010123043321073055 relative L2 0.0015586389927193522\n",
      "training 0.001012287917546928 relative L2 0.0015586266526952386\n",
      "training 0.001012271735817194 relative L2 0.001558614196255803\n",
      "training 0.0010122553212568164 relative L2 0.001558601506985724\n",
      "training 0.0010122390231117606 relative L2 0.0015585890505462885\n",
      "training 0.0010122228413820267 relative L2 0.001558576594106853\n",
      "training 0.0010122067760676146 relative L2 0.0015585642540827394\n",
      "training 0.0010121904779225588 relative L2 0.0015585520304739475\n",
      "training 0.0010121744126081467 relative L2 0.0015585393412038684\n",
      "training 0.0010121582308784127 relative L2 0.001558526884764433\n",
      "training 0.0010121420491486788 relative L2 0.0015585144283249974\n",
      "training 0.001012125751003623 relative L2 0.0015585023211315274\n",
      "training 0.001012109685689211 relative L2 0.0015584899811074138\n",
      "training 0.001012093503959477 relative L2 0.0015584771754220128\n",
      "training 0.0010120774386450648 relative L2 0.0015584650682285428\n",
      "training 0.0010120612569153309 relative L2 0.0015584523789584637\n",
      "training 0.001012045075185597 relative L2 0.0015584399225190282\n",
      "training 0.0010120286606252193 relative L2 0.0015584273496642709\n",
      "training 0.0010120123624801636 relative L2 0.0015584148932248354\n",
      "training 0.0010119960643351078 relative L2 0.0015584022039547563\n",
      "training 0.0010119798826053739 relative L2 0.0015583899803459644\n",
      "training 0.0010119638172909617 relative L2 0.0015583772910758853\n",
      "training 0.001011947519145906 relative L2 0.0015583649510517716\n",
      "training 0.001011931337416172 relative L2 0.0015583528438583016\n",
      "training 0.00101191527210176 relative L2 0.0015583402710035443\n",
      "training 0.001011899090372026 relative L2 0.0015583279309794307\n",
      "training 0.001011882908642292 relative L2 0.0015583151252940297\n",
      "training 0.00101186684332788 relative L2 0.0015583025524392724\n",
      "training 0.0010118508944287896 relative L2 0.0015582904452458024\n",
      "training 0.0010118347126990557 relative L2 0.001558277872391045\n",
      "training 0.001011818414554 relative L2 0.0015582655323669314\n",
      "training 0.0010118024656549096 relative L2 0.0015582534251734614\n",
      "training 0.0010117862839251757 relative L2 0.0015582406194880605\n",
      "training 0.0010117701021954417 relative L2 0.0015582282794639468\n",
      "training 0.0010117539204657078 relative L2 0.0015582155901938677\n",
      "training 0.001011737622320652 relative L2 0.0015582034830003977\n",
      "training 0.001011721440590918 relative L2 0.0015581907937303185\n",
      "training 0.001011705375276506 relative L2 0.001558178337290883\n",
      "training 0.0010116893099620938 relative L2 0.001558166230097413\n",
      "training 0.0010116731282323599 relative L2 0.001558153540827334\n",
      "training 0.001011656946502626 relative L2 0.001558141317218542\n",
      "training 0.0010116411140188575 relative L2 0.001558129210025072\n",
      "training 0.0010116250487044454 relative L2 0.0015581166371703148\n",
      "training 0.0010116087505593896 relative L2 0.0015581041807308793\n",
      "training 0.0010115926852449775 relative L2 0.0015580917242914438\n",
      "training 0.0010115765035152435 relative L2 0.0015580796170979738\n",
      "training 0.0010115604382008314 relative L2 0.0015580669278278947\n",
      "training 0.0010115443728864193 relative L2 0.0015580548206344247\n",
      "training 0.0010115283075720072 relative L2 0.0015580427134409547\n",
      "training 0.0010115121258422732 relative L2 0.0015580300241708755\n",
      "training 0.001011496176943183 relative L2 0.00155801756773144\n",
      "training 0.001011479995213449 relative L2 0.00155800546053797\n",
      "training 0.0010114639298990369 relative L2 0.001557992771267891\n",
      "training 0.0010114479809999466 relative L2 0.001557980547659099\n",
      "training 0.0010114320321008563 relative L2 0.0015579682076349854\n",
      "training 0.001011416083201766 relative L2 0.00155795575119555\n",
      "training 0.0010113997850567102 relative L2 0.0015579434111714363\n",
      "training 0.00101138383615762 relative L2 0.0015579309547320008\n",
      "training 0.001011367654427886 relative L2 0.0015579186147078872\n",
      "training 0.001011351589113474 relative L2 0.0015579065075144172\n",
      "training 0.0010113355237990618 relative L2 0.0015578940510749817\n",
      "training 0.0010113194584846497 relative L2 0.0015578819438815117\n",
      "training 0.0010113033931702375 relative L2 0.0015578692546114326\n",
      "training 0.001011287560686469 relative L2 0.0015578573802486062\n",
      "training 0.0010112713789567351 relative L2 0.001557844690978527\n",
      "training 0.0010112554300576448 relative L2 0.0015578323509544134\n",
      "training 0.0010112390154972672 relative L2 0.0015578201273456216\n",
      "training 0.001011223066598177 relative L2 0.0015578075544908643\n",
      "training 0.0010112071176990867 relative L2 0.0015577954472973943\n",
      "training 0.0010111911687999964 relative L2 0.0015577832236886024\n",
      "training 0.001011175336316228 relative L2 0.0015577706508338451\n",
      "training 0.0010111592710018158 relative L2 0.0015577585436403751\n",
      "training 0.0010111430892720819 relative L2 0.0015577460872009397\n",
      "training 0.0010111271403729916 relative L2 0.0015577338635921478\n",
      "training 0.0010111111914739013 relative L2 0.0015577218728139997\n",
      "training 0.001011095242574811 relative L2 0.0015577091835439205\n",
      "training 0.0010110791772603989 relative L2 0.0015576969599351287\n",
      "training 0.0010110632283613086 relative L2 0.0015576843870803714\n",
      "training 0.0010110472794622183 relative L2 0.0015576722798869014\n",
      "training 0.0010110314469784498 relative L2 0.0015576601726934314\n",
      "training 0.0010110154980793595 relative L2 0.0015576480654999614\n",
      "training 0.0010109995491802692 relative L2 0.001557636191137135\n",
      "training 0.001010983600281179 relative L2 0.001557623501867056\n",
      "training 0.0010109676513820887 relative L2 0.001557611394673586\n",
      "training 0.001010951935313642 relative L2 0.0015575989382341504\n",
      "training 0.0010109362192451954 relative L2 0.0015575869474560022\n",
      "training 0.001010920270346105 relative L2 0.001557574258185923\n",
      "training 0.001010904205031693 relative L2 0.001557562150992453\n",
      "training 0.0010108884889632463 relative L2 0.0015575502766296268\n",
      "training 0.0010108723072335124 relative L2 0.001557537354528904\n",
      "training 0.0010108562419191003 relative L2 0.0015575254801660776\n",
      "training 0.0010108401766046882 relative L2 0.001557513140141964\n",
      "training 0.0010108242277055979 relative L2 0.0015575005672872066\n",
      "training 0.0010108082788065076 relative L2 0.0015574885765090585\n",
      "training 0.0010107923299074173 relative L2 0.0015574758872389793\n",
      "training 0.001010776381008327 relative L2 0.001557464012876153\n",
      "training 0.0010107605485245585 relative L2 0.0015574516728520393\n",
      "training 0.00101074471604079 relative L2 0.0015574392164126039\n",
      "training 0.0010107285343110561 relative L2 0.0015574271092191339\n",
      "training 0.001010712468996644 relative L2 0.0015574146527796984\n",
      "training 0.0010106966365128756 relative L2 0.0015574024291709065\n",
      "training 0.0010106806876137853 relative L2 0.0015573902055621147\n",
      "training 0.0010106648551300168 relative L2 0.001557377865538001\n",
      "training 0.001010649255476892 relative L2 0.0015573659911751747\n",
      "training 0.001010633073747158 relative L2 0.00155735295265913\n",
      "training 0.001010616892017424 relative L2 0.0015573411947116256\n",
      "training 0.0010106009431183338 relative L2 0.00155732873827219\n",
      "training 0.0010105849942192435 relative L2 0.00155731663107872\n",
      "training 0.001010569161735475 relative L2 0.0015573044074699283\n",
      "training 0.0010105532128363848 relative L2 0.0015572916017845273\n",
      "training 0.0010105373803526163 relative L2 0.0015572800766676664\n",
      "training 0.0010105216642841697 relative L2 0.0015572673873975873\n",
      "training 0.0010105055989697576 relative L2 0.0015572553966194391\n",
      "training 0.0010104896500706673 relative L2 0.0015572432894259691\n",
      "training 0.0010104739340022206 relative L2 0.0015572307165712118\n",
      "training 0.0010104578686878085 relative L2 0.0015572190750390291\n",
      "training 0.0010104419197887182 relative L2 0.0015572066185995936\n",
      "training 0.0010104262037202716 relative L2 0.0015571945114061236\n",
      "training 0.0010104106040671468 relative L2 0.0015571820549666882\n",
      "training 0.0010103946551680565 relative L2 0.0015571702970191836\n",
      "training 0.0010103787062689662 relative L2 0.0015571578405797482\n",
      "training 0.0010103629902005196 relative L2 0.0015571456169709563\n",
      "training 0.001010347157716751 relative L2 0.00155713374260813\n",
      "training 0.0010103312088176608 relative L2 0.00155712163541466\n",
      "training 0.0010103154927492142 relative L2 0.00155710952822119\n",
      "training 0.0010102997766807675 relative L2 0.00155709742102772\n",
      "training 0.001010284060612321 relative L2 0.00155708531383425\n",
      "training 0.0010102682281285524 relative L2 0.0015570730902254581\n",
      "training 0.0010102525120601058 relative L2 0.0015570606337860227\n",
      "training 0.0010102365631610155 relative L2 0.0015570484101772308\n",
      "training 0.001010220730677247 relative L2 0.0015570360701531172\n",
      "training 0.0010102047817781568 relative L2 0.001557024079374969\n",
      "training 0.0010101892985403538 relative L2 0.001557011972181499\n",
      "training 0.0010101733496412635 relative L2 0.0015569995157420635\n",
      "training 0.0010101574007421732 relative L2 0.0015569874085485935\n",
      "training 0.0010101416846737266 relative L2 0.0015569753013551235\n",
      "training 0.0010101257357746363 relative L2 0.0015569633105769753\n",
      "training 0.001010109786875546 relative L2 0.0015569510869681835\n",
      "training 0.0010100940708070993 relative L2 0.001556938630528748\n",
      "training 0.001010078121908009 relative L2 0.0015569269889965653\n",
      "training 0.001010062638670206 relative L2 0.0015569147653877735\n",
      "training 0.0010100466897711158 relative L2 0.0015569025417789817\n",
      "training 0.0010100307408720255 relative L2 0.0015568904345855117\n",
      "training 0.0010100150248035789 relative L2 0.0015568783273920417\n",
      "training 0.0010099991923198104 relative L2 0.0015568661037832499\n",
      "training 0.0010099834762513638 relative L2 0.0015568537637591362\n",
      "training 0.0010099675273522735 relative L2 0.0015568420058116317\n",
      "training 0.0010099518112838268 relative L2 0.0015568298986181617\n",
      "training 0.001009936211630702 relative L2 0.0015568181406706572\n",
      "training 0.0010099206119775772 relative L2 0.0015568058006465435\n",
      "training 0.0010099048959091306 relative L2 0.001556793344207108\n",
      "training 0.0010098890634253621 relative L2 0.0015567818190902472\n",
      "training 0.0010098733473569155 relative L2 0.0015567697118967772\n",
      "training 0.0010098578641191125 relative L2 0.001556757721118629\n",
      "training 0.001009842031635344 relative L2 0.0015567453810945153\n",
      "training 0.0010098260827362537 relative L2 0.0015567332739010453\n",
      "training 0.0010098102502524853 relative L2 0.0015567211667075753\n",
      "training 0.0010097944177687168 relative L2 0.0015567089430987835\n",
      "training 0.0010097787017002702 relative L2 0.001556697185151279\n",
      "training 0.0010097629856318235 relative L2 0.0015566848451271653\n",
      "training 0.0010097469203174114 relative L2 0.0015566727379336953\n",
      "training 0.0010097314370796084 relative L2 0.0015566606307402253\n",
      "training 0.0010097157210111618 relative L2 0.0015566485235467553\n",
      "training 0.0010096996556967497 relative L2 0.0015566365327686071\n",
      "training 0.001009683939628303 relative L2 0.0015566243091598153\n",
      "training 0.0010096682235598564 relative L2 0.0015566125512123108\n",
      "training 0.0010096527403220534 relative L2 0.0015566002111881971\n",
      "training 0.001009636907838285 relative L2 0.001556588220410049\n",
      "training 0.0010096213081851602 relative L2 0.0015565759968012571\n",
      "training 0.0010096054757013917 relative L2 0.0015565638896077871\n",
      "training 0.001009589759632945 relative L2 0.001556551898829639\n",
      "training 0.0010095740435644984 relative L2 0.0015565401408821344\n",
      "training 0.00100955821108073 relative L2 0.0015565280336886644\n",
      "training 0.0010095424950122833 relative L2 0.0015565156936645508\n",
      "training 0.0010095267789438367 relative L2 0.0015565038193017244\n",
      "training 0.0010095109464600682 relative L2 0.0015564917121082544\n",
      "training 0.001009495579637587 relative L2 0.00155647995416075\n",
      "training 0.0010094797471538186 relative L2 0.001556467148475349\n",
      "training 0.0010094636818394065 relative L2 0.0015564555069431663\n",
      "training 0.0010094477329403162 relative L2 0.0015564430505037308\n",
      "training 0.0010094320168718696 relative L2 0.0015564310597255826\n",
      "training 0.001009416184388101 relative L2 0.0015564189525321126\n",
      "training 0.0010094003519043326 relative L2 0.0015564068453386426\n",
      "training 0.001009384635835886 relative L2 0.001556395087391138\n",
      "training 0.0010093686869367957 relative L2 0.0015563826309517026\n",
      "training 0.0010093532036989927 relative L2 0.0015563711058348417\n",
      "training 0.001009337487630546 relative L2 0.0015563585329800844\n",
      "training 0.0010093217715620995 relative L2 0.00155634677503258\n",
      "training 0.0010093060554936528 relative L2 0.0015563349006697536\n",
      "training 0.0010092901065945625 relative L2 0.001556322444230318\n",
      "training 0.001009274274110794 relative L2 0.0015563108026981354\n",
      "training 0.0010092585580423474 relative L2 0.0015562985790893435\n",
      "training 0.0010092428419739008 relative L2 0.0015562869375571609\n",
      "training 0.001009227242320776 relative L2 0.001556274713948369\n",
      "training 0.0010092116426676512 relative L2 0.0015562623739242554\n",
      "training 0.0010091956937685609 relative L2 0.0015562506159767509\n",
      "training 0.001009180094115436 relative L2 0.0015562382759526372\n",
      "training 0.001009164610877633 relative L2 0.0015562265180051327\n",
      "training 0.0010091488948091865 relative L2 0.0015562140615656972\n",
      "training 0.0010091332951560616 relative L2 0.0015562023036181927\n",
      "training 0.001009117579087615 relative L2 0.0015561901964247227\n",
      "training 0.0010091018630191684 relative L2 0.0015561780892312527\n",
      "training 0.0010090863797813654 relative L2 0.0015561665641143918\n",
      "training 0.0010090706637129188 relative L2 0.0015561539912596345\n",
      "training 0.0010090551804751158 relative L2 0.00155614223331213\n",
      "training 0.0010090396972373128 relative L2 0.0015561300097033381\n",
      "training 0.0010090238647535443 relative L2 0.0015561182517558336\n",
      "training 0.0010090082651004195 relative L2 0.0015561062609776855\n",
      "training 0.0010089925490319729 relative L2 0.0015560941537842155\n",
      "training 0.0010089772986248136 relative L2 0.0015560826286673546\n",
      "training 0.001008961582556367 relative L2 0.001556070172227919\n",
      "training 0.0010089457500725985 relative L2 0.0015560585306957364\n",
      "training 0.0010089303832501173 relative L2 0.0015560458414256573\n",
      "training 0.0010089145507663488 relative L2 0.0015560340834781528\n",
      "training 0.0010088988346979022 relative L2 0.001556021859869361\n",
      "training 0.0010088830022141337 relative L2 0.0015560101019218564\n",
      "training 0.001008867286145687 relative L2 0.0015559979947283864\n",
      "training 0.0010088516864925623 relative L2 0.0015559858875349164\n",
      "training 0.0010088360868394375 relative L2 0.0015559738967567682\n",
      "training 0.0010088204871863127 relative L2 0.00155596190597862\n",
      "training 0.0010088048875331879 relative L2 0.00155594979878515\n",
      "training 0.001008789287880063 relative L2 0.0015559379244223237\n",
      "training 0.0010087735718116164 relative L2 0.00155592558439821\n",
      "training 0.0010087578557431698 relative L2 0.0015559138264507055\n",
      "training 0.0010087421396747231 relative L2 0.0015559019520878792\n",
      "training 0.0010087264236062765 relative L2 0.0015558896120637655\n",
      "training 0.0010087107075378299 relative L2 0.001555877854116261\n",
      "training 0.001008695224300027 relative L2 0.001555865746922791\n",
      "training 0.0010086795082315803 relative L2 0.0015558534068986773\n",
      "training 0.0010086640249937773 relative L2 0.001555841532535851\n",
      "training 0.0010086481925100088 relative L2 0.0015558295417577028\n",
      "training 0.001008632592856884 relative L2 0.0015558177838101983\n",
      "training 0.001008617109619081 relative L2 0.0015558056766167283\n",
      "training 0.0010086013935506344 relative L2 0.0015557939186692238\n",
      "training 0.0010085856774821877 relative L2 0.001555781695060432\n",
      "training 0.0010085701942443848 relative L2 0.0015557699371129274\n",
      "training 0.0010085548274219036 relative L2 0.0015557578299194574\n",
      "training 0.0010085392277687788 relative L2 0.0015557456063106656\n",
      "training 0.0010085235117003322 relative L2 0.001555733848363161\n",
      "training 0.0010085080284625292 relative L2 0.001555721741169691\n",
      "training 0.0010084924288094044 relative L2 0.0015557099832221866\n",
      "training 0.0010084769455716014 relative L2 0.0015556984581053257\n",
      "training 0.0010084613459184766 relative L2 0.0015556860016658902\n",
      "training 0.0010084457462653518 relative L2 0.0015556742437183857\n",
      "training 0.001008430146612227 relative L2 0.0015556622529402375\n",
      "training 0.001008414663374424 relative L2 0.0015556503785774112\n",
      "training 0.001008399180136621 relative L2 0.001555638387799263\n",
      "training 0.0010083838133141398 relative L2 0.001555626280605793\n",
      "training 0.0010083680972456932 relative L2 0.0015556147554889321\n",
      "training 0.0010083524975925684 relative L2 0.0015556026482954621\n",
      "training 0.001008337363600731 relative L2 0.0015555910067632794\n",
      "training 0.0010083216475322843 relative L2 0.0015555788995698094\n",
      "training 0.001008306280709803 relative L2 0.0015555667923763394\n",
      "training 0.0010082905646413565 relative L2 0.0015555548015981913\n",
      "training 0.0010082750814035535 relative L2 0.0015555426944047213\n",
      "training 0.0010082593653351068 relative L2 0.001555530820041895\n",
      "training 0.0010082438820973039 relative L2 0.0015555188292637467\n",
      "training 0.0010082283988595009 relative L2 0.0015555070713162422\n",
      "training 0.001008212799206376 relative L2 0.0015554949641227722\n",
      "training 0.0010081971995532513 relative L2 0.0015554835554212332\n",
      "training 0.0010081817163154483 relative L2 0.0015554713318124413\n",
      "training 0.0010081661166623235 relative L2 0.0015554595738649368\n",
      "training 0.0010081504005938768 relative L2 0.0015554474666714668\n",
      "training 0.0010081350337713957 relative L2 0.0015554353594779968\n",
      "training 0.001008119317702949 relative L2 0.0015554236015304923\n",
      "training 0.001008103950880468 relative L2 0.0015554112615063787\n",
      "training 0.0010080878855660558 relative L2 0.0015553992707282305\n",
      "training 0.0010080725187435746 relative L2 0.0015553879784420133\n",
      "training 0.0010080572683364153 relative L2 0.0015553757548332214\n",
      "training 0.0010080415522679687 relative L2 0.0015553636476397514\n",
      "training 0.001008025836199522 relative L2 0.001555351773276925\n",
      "training 0.0010080102365463972 relative L2 0.001555339782498777\n",
      "training 0.001007994869723916 relative L2 0.0015553279081359506\n",
      "training 0.001007979502901435 relative L2 0.0015553159173578024\n",
      "training 0.0010079637868329883 relative L2 0.0015553038101643324\n",
      "training 0.0010079480707645416 relative L2 0.0015552920522168279\n",
      "training 0.0010079328203573823 relative L2 0.0015552799450233579\n",
      "training 0.0010079173371195793 relative L2 0.0015552681870758533\n",
      "training 0.0010079016210511327 relative L2 0.0015552557306364179\n",
      "training 0.0010078860213980079 relative L2 0.0015552439726889133\n",
      "training 0.0010078706545755267 relative L2 0.0015552322147414088\n",
      "training 0.0010078551713377237 relative L2 0.0015552202239632607\n",
      "training 0.0010078398045152426 relative L2 0.0015552081167697906\n",
      "training 0.0010078244376927614 relative L2 0.0015551963588222861\n",
      "training 0.0010078090708702803 relative L2 0.0015551846008747816\n",
      "training 0.0010077935876324773 relative L2 0.0015551724936813116\n",
      "training 0.0010077778715640306 relative L2 0.001555160735733807\n",
      "training 0.001007762155495584 relative L2 0.0015551488613709807\n",
      "training 0.0010077470215037465 relative L2 0.0015551368705928326\n",
      "training 0.0010077314218506217 relative L2 0.0015551249962300062\n",
      "training 0.0010077161714434624 relative L2 0.0015551126562058926\n",
      "training 0.0010077004553750157 relative L2 0.001555100898258388\n",
      "training 0.0010076850885525346 relative L2 0.0015550891403108835\n",
      "training 0.001007669372484088 relative L2 0.0015550768002867699\n",
      "training 0.0010076540056616068 relative L2 0.001555065275169909\n",
      "training 0.0010076385224238038 relative L2 0.0015550529351457953\n",
      "training 0.0010076230391860008 relative L2 0.0015550414100289345\n",
      "training 0.0010076076723635197 relative L2 0.0015550293028354645\n",
      "training 0.001007591956295073 relative L2 0.0015550171956419945\n",
      "training 0.00100757647305727 relative L2 0.0015550052048638463\n",
      "training 0.001007560989819467 relative L2 0.0015549930976703763\n",
      "training 0.0010075457394123077 relative L2 0.0015549815725535154\n",
      "training 0.001007530023343861 relative L2 0.0015549694653600454\n",
      "training 0.0010075145401060581 relative L2 0.0015549574745818973\n",
      "training 0.0010074990568682551 relative L2 0.001554945600219071\n",
      "training 0.0010074839228764176 relative L2 0.0015549338422715664\n",
      "training 0.0010074679739773273 relative L2 0.0015549215022474527\n",
      "training 0.0010074524907395244 relative L2 0.0015549097442999482\n",
      "training 0.0010074371239170432 relative L2 0.0015548974042758346\n",
      "training 0.0010074215242639184 relative L2 0.00155488564632833\n",
      "training 0.0010074058081954718 relative L2 0.00155487353913486\n",
      "training 0.0010073904413729906 relative L2 0.0015548617811873555\n",
      "training 0.0010073749581351876 relative L2 0.0015548496739938855\n",
      "training 0.0010073595913127065 relative L2 0.0015548381488770247\n",
      "training 0.0010073439916595817 relative L2 0.001554825808852911\n",
      "training 0.0010073286248371005 relative L2 0.0015548146329820156\n",
      "training 0.0010073132580146194 relative L2 0.0015548021765425801\n",
      "training 0.0010072977747768164 relative L2 0.0015547905350103974\n",
      "training 0.0010072820587083697 relative L2 0.001554778078570962\n",
      "training 0.001007266459055245 relative L2 0.0015547660877928138\n",
      "training 0.0010072510922327638 relative L2 0.0015547542134299874\n",
      "training 0.0010072357254102826 relative L2 0.001554742455482483\n",
      "training 0.0010072202421724796 relative L2 0.0015547304647043347\n",
      "training 0.001007204526104033 relative L2 0.0015547185903415084\n",
      "training 0.0010071895085275173 relative L2 0.0015547065995633602\n",
      "training 0.0010071739088743925 relative L2 0.0015546947252005339\n",
      "training 0.0010071583092212677 relative L2 0.0015546826180070639\n",
      "training 0.0010071427095681429 relative L2 0.0015546709764748812\n",
      "training 0.00100712722633034 relative L2 0.0015546588692814112\n",
      "training 0.0010071119759231806 relative L2 0.0015546471113339067\n",
      "training 0.0010070964926853776 relative L2 0.0015546344220638275\n",
      "training 0.0010070811258628964 relative L2 0.0015546235954388976\n",
      "training 0.0010070657590404153 relative L2 0.0015546109061688185\n",
      "training 0.0010070501593872905 relative L2 0.0015545996138826013\n",
      "training 0.0010070345597341657 relative L2 0.0015545872738584876\n",
      "training 0.0010070191929116845 relative L2 0.0015545757487416267\n",
      "training 0.0010070038260892034 relative L2 0.0015545637579634786\n",
      "training 0.001006988575682044 relative L2 0.0015545517671853304\n",
      "training 0.001006973092444241 relative L2 0.001554539892822504\n",
      "training 0.0010069574927911162 relative L2 0.001554527785629034\n",
      "training 0.001006942242383957 relative L2 0.001554515678435564\n",
      "training 0.0010069268755614758 relative L2 0.001554503571242094\n",
      "training 0.001006911275908351 relative L2 0.0015544918132945895\n",
      "training 0.0010068956762552261 relative L2 0.0015544794732704759\n",
      "training 0.0010068801930174232 relative L2 0.0015544682973995805\n",
      "training 0.0010068647097796202 relative L2 0.0015544552588835359\n",
      "training 0.0010068491101264954 relative L2 0.0015544440830126405\n",
      "training 0.0010068336268886924 relative L2 0.0015544315101578832\n",
      "training 0.0010068182600662112 relative L2 0.001554420217871666\n",
      "training 0.0010068027768284082 relative L2 0.0015544075286015868\n",
      "training 0.0010067871771752834 relative L2 0.0015543961199000478\n",
      "training 0.0010067718103528023 relative L2 0.0015543830813840032\n",
      "training 0.0010067563271149993 relative L2 0.0015543722547590733\n",
      "training 0.0010067409602925181 relative L2 0.0015543592162430286\n",
      "training 0.0010067254770547152 relative L2 0.0015543480403721333\n",
      "training 0.0010067097609862685 relative L2 0.001554335467517376\n",
      "training 0.0010066945105791092 relative L2 0.0015543235931545496\n",
      "training 0.0010066787945106626 relative L2 0.0015543111367151141\n",
      "training 0.0010066631948575377 relative L2 0.0015542993787676096\n",
      "training 0.001006647595204413 relative L2 0.001554287038743496\n",
      "training 0.00100663211196661 relative L2 0.0015542752807959914\n",
      "training 0.0010066165123134851 relative L2 0.0015542631736025214\n",
      "training 0.0010066009126603603 relative L2 0.0015542511828243732\n",
      "training 0.001006585662253201 relative L2 0.0015542395412921906\n",
      "training 0.0010065702954307199 relative L2 0.001554227201268077\n",
      "training 0.0010065548121929169 relative L2 0.0015542152104899287\n",
      "training 0.001006539212539792 relative L2 0.0015542031032964587\n",
      "training 0.0010065240785479546 relative L2 0.0015541915781795979\n",
      "training 0.0010065087117254734 relative L2 0.0015541794709861279\n",
      "training 0.0010064933449029922 relative L2 0.0015541680622845888\n",
      "training 0.0010064777452498674 relative L2 0.0015541550237685442\n",
      "training 0.0010064622620120645 relative L2 0.0015541441971436143\n",
      "training 0.0010064467787742615 relative L2 0.0015541308093816042\n",
      "training 0.0010064315283671021 relative L2 0.0015541199827566743\n",
      "training 0.0010064160451292992 relative L2 0.001554106711409986\n",
      "training 0.0010064004454761744 relative L2 0.0015540964668616652\n",
      "training 0.0010063849622383714 relative L2 0.0015540822641924024\n",
      "training 0.001006369711831212 relative L2 0.0015540734166279435\n",
      "training 0.0010063544614240527 relative L2 0.0015540579333901405\n",
      "training 0.0010063395602628589 relative L2 0.0015540521126240492\n",
      "training 0.0010063251247629523 relative L2 0.0015540341846644878\n",
      "training 0.0010063126683235168 relative L2 0.0015540433814749122\n",
      "training 0.001006307196803391 relative L2 0.0015540329040959477\n",
      "training 0.0010063255904242396 relative L2 0.00155416049528867\n",
      "training 0.001006435020826757 relative L2 0.0015544138150289655\n",
      "training 0.0010068821720778942 relative L2 0.0015558605082333088\n",
      "training 0.0010085756657645106 relative L2 0.0015603357460349798\n",
      "training 0.0010149067966267467 relative L2 0.0015786760486662388\n",
      "training 0.0010385389905422926 relative L2 0.0016415787395089865\n",
      "training 0.0011260004248470068 relative L2 0.0018612071871757507\n",
      "training 0.001449580304324627 relative L2 0.0024819751270115376\n",
      "training 0.002598721766844392 relative L2 0.003881635842844844\n",
      "training 0.006369493436068296 relative L2 0.0058538350276649\n",
      "training 0.014528592117130756 relative L2 0.006972706876695156\n",
      "training 0.020604930818080902 relative L2 0.003977854270488024\n",
      "training 0.006700967438519001 relative L2 0.002345130080357194\n",
      "training 0.0023180986754596233 relative L2 0.0054099904373288155\n",
      "training 0.01239338330924511 relative L2 0.0035857229959219694\n",
      "training 0.005441884510219097 relative L2 0.002267545787617564\n",
      "training 0.0021660171914845705 relative L2 0.004587813280522823\n",
      "training 0.008906399831175804 relative L2 0.002375285141170025\n",
      "training 0.0023784127552062273 relative L2 0.0029356139712035656\n",
      "training 0.003641990479081869 relative L2 0.003797830082476139\n",
      "training 0.006096282973885536 relative L2 0.0015597115270793438\n",
      "training 0.0010138298384845257 relative L2 0.0034585101529955864\n",
      "training 0.0050613489001989365 relative L2 0.0026471333112567663\n",
      "training 0.002951111411675811 relative L2 0.002135797869414091\n",
      "training 0.0019144717371091247 relative L2 0.00320949568413198\n",
      "training 0.00435642572119832 relative L2 0.0016183520201593637\n",
      "training 0.0010919246124103665 relative L2 0.0027546731289476156\n",
      "training 0.0031971195712685585 relative L2 0.0022418033331632614\n",
      "training 0.0021163742057979107 relative L2 0.0019272593781352043\n",
      "training 0.0015591683331876993 relative L2 0.002592169912531972\n",
      "training 0.0028287197928875685 relative L2 0.001567793544381857\n",
      "training 0.0010242852149531245 relative L2 0.002412557601928711\n",
      "training 0.002453876193612814 relative L2 0.001884404569864273\n",
      "training 0.0014861257513985038 relative L2 0.0019237975357100368\n",
      "training 0.0015496620908379555 relative L2 0.0021722486708313227\n",
      "training 0.0019858519081026316 relative L2 0.001581952441483736\n",
      "training 0.0010434327414259315 relative L2 0.002142110373824835\n",
      "training 0.001925736665725708 relative L2 0.0016533557791262865\n",
      "training 0.0011419749353080988 relative L2 0.0018843610305339098\n",
      "training 0.0014895854983478785 relative L2 0.0018652236321941018\n",
      "training 0.0014557505492120981 relative L2 0.0016293005319312215\n",
      "training 0.001106895157136023 relative L2 0.001932445913553238\n",
      "training 0.0015675912145525217 relative L2 0.0015765862772241235\n",
      "training 0.0010356428101658821 relative L2 0.0018207625253126025\n",
      "training 0.001386420102789998 relative L2 0.001682385802268982\n",
      "training 0.0011832452146336436 relative L2 0.0016487512039020658\n",
      "training 0.0011355576571077108 relative L2 0.0017675587441772223\n",
      "training 0.0013056338066235185 relative L2 0.0015647405525669456\n",
      "training 0.0010200930992141366 relative L2 0.0017428376013413072\n",
      "training 0.0012713723117485642 relative L2 0.0015995833091437817\n",
      "training 0.0010664545698091388 relative L2 0.0016475813463330269\n",
      "training 0.00113230652641505 relative L2 0.0016621254617348313\n",
      "training 0.0011544645531103015 relative L2 0.0015704398974776268\n",
      "training 0.0010280907154083252 relative L2 0.0016739963321015239\n",
      "training 0.0011694298591464758 relative L2 0.001566956751048565\n",
      "training 0.0010234287474304438 relative L2 0.0016274937661364675\n",
      "training 0.0011059922398999333 relative L2 0.0016051915008574724\n",
      "training 0.0010740766301751137 relative L2 0.0015749114099889994\n",
      "training 0.0010334686376154423 relative L2 0.0016241868725046515\n",
      "training 0.0011014435440301895 relative L2 0.001558648538775742\n",
      "training 0.0010121404193341732 relative L2 0.0016064889496192336\n",
      "training 0.0010758383432403207 relative L2 0.0015761478571221232\n",
      "training 0.0010358849540352821 relative L2 0.0015726123237982392\n",
      "training 0.0010311318328604102 relative L2 0.0015937857097014785\n",
      "training 0.0010586690623313189 relative L2 0.0015567763475701213\n",
      "training 0.001009717583656311 relative L2 0.0015877604018896818\n",
      "training 0.0010516183683648705 relative L2 0.0015640968922525644\n",
      "training 0.0010191897163167596 relative L2 0.0015691157896071672\n",
      "training 0.0010257881367579103 relative L2 0.0015752597246319056\n",
      "training 0.0010347446659579873 relative L2 0.0015562426997348666\n",
      "training 0.001009128405712545 relative L2 0.0015754305059090257\n",
      "training 0.0010341369779780507 relative L2 0.0015582626219838858\n",
      "training 0.0010118933860212564 relative L2 0.001564278150908649\n",
      "training 0.001019995310343802 relative L2 0.0015659347409382463\n",
      "training 0.0010215896181762218 relative L2 0.0015559726161882281\n",
      "training 0.0010086148977279663 relative L2 0.0015665279934182763\n",
      "training 0.0010230320040136576 relative L2 0.0015561421168968081\n",
      "training 0.0010088239796459675 relative L2 0.0015611095586791635\n",
      "training 0.0010152622126042843 relative L2 0.001560220611281693\n",
      "training 0.0010145653504878283 relative L2 0.0015553138218820095\n",
      "training 0.0010079297935590148 relative L2 0.0015617657918483019\n",
      "training 0.0010161183308809996 relative L2 0.001554931397549808\n",
      "training 0.0010074126766994596 relative L2 0.0015581691404804587\n",
      "training 0.0010118167847394943 relative L2 0.001557660405524075\n",
      "training 0.001010773004963994 relative L2 0.0015549254603683949\n",
      "training 0.001007263781502843 relative L2 0.0015582606429234147\n",
      "training 0.0010119507787749171 relative L2 0.0015544862253591418\n",
      "training 0.0010067133698612452 relative L2 0.0015566501533612609\n",
      "training 0.001009468687698245 relative L2 0.0015558203449472785\n",
      "training 0.0010086633265018463 relative L2 0.0015543814515694976\n",
      "training 0.00100669264793396 relative L2 0.0015565886860713363\n",
      "training 0.0010093916207551956 relative L2 0.0015540944878011942\n",
      "training 0.001006295788101852 relative L2 0.0015552736585959792\n",
      "training 0.0010079367784783244 relative L2 0.0015550703974440694\n",
      "training 0.0010074471356347203 relative L2 0.0015541223110631108\n",
      "training 0.0010062576038762927 relative L2 0.001555201830342412\n",
      "training 0.0010078524937853217 relative L2 0.0015539273153990507\n",
      "training 0.0010060218628495932 relative L2 0.0015546715585514903\n",
      "training 0.0010069453855976462 relative L2 0.0015543478075414896\n",
      "training 0.0010066934628412127 relative L2 0.0015538112493231893\n",
      "training 0.0010059397900477052 relative L2 0.001554629416204989\n",
      "training 0.0010068941628560424 relative L2 0.0015537211438640952\n",
      "training 0.0010058187181130052 relative L2 0.0015540553722530603\n",
      "training 0.0010063046356663108 relative L2 0.0015540861058980227\n",
      "training 0.0010062125511467457 relative L2 0.001553668174892664\n",
      "training 0.0010057067265734076 relative L2 0.0015540358144789934\n",
      "training 0.0010062895016744733 relative L2 0.0015536309219896793\n",
      "training 0.0010056595783680677 relative L2 0.0015538210282102227\n",
      "training 0.0010058853076770902 relative L2 0.0015537416329607368\n",
      "training 0.001005886821076274 relative L2 0.0015534990234300494\n",
      "training 0.0010055287275463343 relative L2 0.0015538133447989821\n",
      "training 0.0010058751795440912 relative L2 0.0015534835401922464\n",
      "training 0.0010055210441350937 relative L2 0.0015535313868895173\n",
      "training 0.0010055992752313614 relative L2 0.0015536178834736347\n",
      "training 0.0010056354803964496 relative L2 0.0015534055419266224\n",
      "training 0.001005392987281084 relative L2 0.0015535166021436453\n",
      "training 0.0010055901948362589 relative L2 0.0015534175327047706\n",
      "training 0.001005397061817348 relative L2 0.0015534231206402183\n",
      "training 0.001005401136353612 relative L2 0.0015534129925072193\n",
      "training 0.0010054496815428138 relative L2 0.0015533091500401497\n",
      "training 0.0010052798315882683 relative L2 0.0015534134581685066\n",
      "training 0.0010053854202851653 relative L2 0.0015532977413386106\n",
      "training 0.0010052856523543596 relative L2 0.0015532731777057052\n",
      "training 0.0010052514262497425 relative L2 0.0015533381374552846\n",
      "training 0.0010052933357656002 relative L2 0.0015532280085608363\n",
      "training 0.0010051806457340717 relative L2 0.0015532451216131449\n",
      "training 0.0010052239522337914 relative L2 0.001553240930661559\n",
      "training 0.0010051785502582788 relative L2 0.0015531987883150578\n",
      "training 0.0010051323333755136 relative L2 0.0015531970420852304\n",
      "training 0.001005162368528545 relative L2 0.001553162350319326\n",
      "training 0.0010050891432911158 relative L2 0.0015531741082668304\n",
      "training 0.0010050958953797817 relative L2 0.0015531359240412712\n",
      "training 0.001005077618174255 relative L2 0.0015531097305938601\n",
      "training 0.0010050322161987424 relative L2 0.0015531346434727311\n",
      "training 0.0010050454875454307 relative L2 0.0015530823729932308\n",
      "training 0.0010050006676465273 relative L2 0.0015530716627836227\n",
      "training 0.0010049919364973903 relative L2 0.0015530827222391963\n",
      "training 0.0010049825068563223 relative L2 0.001553042558953166\n",
      "training 0.0010049428092315793 relative L2 0.0015530333621427417\n",
      "training 0.0010049456031993032 relative L2 0.0015530252130702138\n",
      "training 0.0010049152188003063 relative L2 0.0015530104283243418\n",
      "training 0.0010048969415947795 relative L2 0.0015529901720583439\n",
      "training 0.001004889258183539 relative L2 0.00155297527089715\n",
      "training 0.0010048581752926111 relative L2 0.0015529756201431155\n",
      "training 0.0010048511903733015 relative L2 0.0015529472148045897\n",
      "training 0.0010048308176919818 relative L2 0.0015529337106272578\n",
      "training 0.001004810445010662 relative L2 0.0015529351076111197\n",
      "training 0.0010048008989542723 relative L2 0.0015529083320870996\n",
      "training 0.0010047766845673323 relative L2 0.001552895875647664\n",
      "training 0.0010047656251117587 relative L2 0.0015528929652646184\n",
      "training 0.001004749326966703 relative L2 0.0015528727089986205\n",
      "training 0.0010047288378700614 relative L2 0.0015528579242527485\n",
      "training 0.0010047182440757751 relative L2 0.0015528497751802206\n",
      "training 0.001004698104225099 relative L2 0.0015528389485552907\n",
      "training 0.001004683319479227 relative L2 0.0015528200892731547\n",
      "training 0.0010046686511486769 relative L2 0.0015528089134022593\n",
      "training 0.0010046502575278282 relative L2 0.0015528026269748807\n",
      "training 0.0010046374518424273 relative L2 0.0015527831856161356\n",
      "training 0.0010046202223747969 relative L2 0.0015527707291767001\n",
      "training 0.0010046048555523157 relative L2 0.0015527658397331834\n",
      "training 0.001004591234959662 relative L2 0.0015527477953583002\n",
      "training 0.0010045735398307443 relative L2 0.0015527349896728992\n",
      "training 0.001004560268484056 relative L2 0.0015527281211689115\n",
      "training 0.001004544785246253 relative L2 0.0015527128707617521\n",
      "training 0.0010045283706858754 relative L2 0.001552698784507811\n",
      "training 0.0010045150993391871 relative L2 0.0015526897041127086\n",
      "training 0.0010044986847788095 relative L2 0.0015526782954111695\n",
      "training 0.0010044840164482594 relative L2 0.0015526632778346539\n",
      "training 0.0010044695809483528 relative L2 0.0015526528004556894\n",
      "training 0.0010044538648799062 relative L2 0.0015526433708146214\n",
      "training 0.001004439895041287 relative L2 0.0015526285860687494\n",
      "training 0.0010044247610494494 relative L2 0.001552616828121245\n",
      "training 0.001004410209134221 relative L2 0.0015526082133874297\n",
      "training 0.0010043962392956018 relative L2 0.0015525940107181668\n",
      "training 0.0010043808724731207 relative L2 0.0015525814378634095\n",
      "training 0.001004366553388536 relative L2 0.001552573055960238\n",
      "training 0.0010043522343039513 relative L2 0.0015525600174441934\n",
      "training 0.0010043373331427574 relative L2 0.001552546862512827\n",
      "training 0.0010043233633041382 relative L2 0.001552537432871759\n",
      "training 0.001004308694973588 relative L2 0.0015525262570008636\n",
      "training 0.001004294492304325 relative L2 0.001552513218484819\n",
      "training 0.0010042805224657059 relative L2 0.0015525027411058545\n",
      "training 0.0010042659705504775 relative L2 0.00155249226372689\n",
      "training 0.00100425211712718 relative L2 0.0015524793416261673\n",
      "training 0.0010042376816272736 relative L2 0.00155246804933995\n",
      "training 0.0010042235953733325 relative L2 0.0015524583868682384\n",
      "training 0.001004209741950035 relative L2 0.001552445930428803\n",
      "training 0.0010041955392807722 relative L2 0.001552434405311942\n",
      "training 0.0010041814530268312 relative L2 0.0015524247428402305\n",
      "training 0.001004167483188212 relative L2 0.0015524128684774041\n",
      "training 0.0010041535133495927 relative L2 0.001552400877699256\n",
      "training 0.0010041396599262953 relative L2 0.0015523910988122225\n",
      "training 0.001004125690087676 relative L2 0.001552379922941327\n",
      "training 0.0010041117202490568 relative L2 0.001552367815747857\n",
      "training 0.0010040977504104376 relative L2 0.0015523576876148582\n",
      "training 0.0010040837805718184 relative L2 0.0015523466281592846\n",
      "training 0.001004069927148521 relative L2 0.0015523351030424237\n",
      "training 0.0010040561901405454 relative L2 0.0015523246256634593\n",
      "training 0.001004042336717248 relative L2 0.001552314031869173\n",
      "training 0.0010040287161245942 relative L2 0.0015523022739216685\n",
      "training 0.0010040149791166186 relative L2 0.001552291796542704\n",
      "training 0.0010040014749392867 relative L2 0.0015522815519943833\n",
      "training 0.001003987854346633 relative L2 0.0015522699104622006\n",
      "training 0.0010039741173386574 relative L2 0.001552258967421949\n",
      "training 0.0010039603803306818 relative L2 0.00155224883928895\n",
      "training 0.0010039466433227062 relative L2 0.0015522371977567673\n",
      "training 0.0010039329063147306 relative L2 0.0015522263711318374\n",
      "training 0.0010039194021373987 relative L2 0.0015522162429988384\n",
      "training 0.0010039056651294231 relative L2 0.0015522051835432649\n",
      "training 0.0010038921609520912 relative L2 0.001552194356918335\n",
      "training 0.001003878889605403 relative L2 0.001552184228785336\n",
      "training 0.001003865385428071 relative L2 0.0015521734021604061\n",
      "training 0.001003851881250739 relative L2 0.0015521625755354762\n",
      "training 0.0010038387263193727 relative L2 0.0015521524474024773\n",
      "training 0.001003824989311397 relative L2 0.0015521415043622255\n",
      "training 0.0010038114851340652 relative L2 0.0015521305613219738\n",
      "training 0.0010037978645414114 relative L2 0.0015521198511123657\n",
      "training 0.0010037845931947231 relative L2 0.0015521099558100104\n",
      "training 0.001003771205432713 relative L2 0.0015520988963544369\n",
      "training 0.0010037579340860248 relative L2 0.0015520885353907943\n",
      "training 0.0010037447791546583 relative L2 0.0015520780580118299\n",
      "training 0.0010037313913926482 relative L2 0.0015520669985562563\n",
      "training 0.0010037177707999945 relative L2 0.0015520565211772919\n",
      "training 0.0010037043830379844 relative L2 0.001552046276628971\n",
      "training 0.001003691111691296 relative L2 0.001552035566419363\n",
      "training 0.0010036778403446078 relative L2 0.0015520252054557204\n",
      "training 0.0010036646854132414 relative L2 0.0015520149609073997\n",
      "training 0.0010036517633125186 relative L2 0.001552004599943757\n",
      "training 0.0010036386083811522 relative L2 0.0015519940061494708\n",
      "training 0.0010036253370344639 relative L2 0.0015519836451858282\n",
      "training 0.0010036121821030974 relative L2 0.0015519729349762201\n",
      "training 0.001003599027171731 relative L2 0.0015519626904278994\n",
      "training 0.0010035859886556864 relative L2 0.0015519526787102222\n",
      "training 0.0010035730665549636 relative L2 0.0015519424341619015\n",
      "training 0.0010035597952082753 relative L2 0.001551931956782937\n",
      "training 0.0010035469895228744 relative L2 0.0015519214794039726\n",
      "training 0.0010035339510068297 relative L2 0.0015519113512709737\n",
      "training 0.0010035209124907851 relative L2 0.0015519008738920093\n",
      "training 0.0010035076411440969 relative L2 0.0015518905129283667\n",
      "training 0.001003494719043374 relative L2 0.0015518807340413332\n",
      "training 0.001003481913357973 relative L2 0.0015518699074164033\n",
      "training 0.0010034687584266067 relative L2 0.0015518601285293698\n",
      "training 0.0010034558363258839 relative L2 0.001551849883981049\n",
      "training 0.001003442914225161 relative L2 0.0015518397558480501\n",
      "training 0.0010034298757091165 relative L2 0.0015518291620537639\n",
      "training 0.0010034171864390373 relative L2 0.0015518192667514086\n",
      "training 0.0010034043807536364 relative L2 0.0015518091386184096\n",
      "training 0.0010033913422375917 relative L2 0.001551798777654767\n",
      "training 0.001003378420136869 relative L2 0.001551788649521768\n",
      "training 0.0010033658472821116 relative L2 0.0015517787542194128\n",
      "training 0.0010033530415967107 relative L2 0.0015517688589170575\n",
      "training 0.0010033402359113097 relative L2 0.001551758497953415\n",
      "training 0.0010033274302259088 relative L2 0.001551748369820416\n",
      "training 0.0010033146245405078 relative L2 0.0015517385909333825\n",
      "training 0.0010033020516857505 relative L2 0.0015517283463850617\n",
      "training 0.0010032891295850277 relative L2 0.0015517185674980283\n",
      "training 0.0010032763238996267 relative L2 0.0015517082065343857\n",
      "training 0.001003263401798904 relative L2 0.0015516980784013867\n",
      "training 0.0010032507125288248 relative L2 0.0015516881830990314\n",
      "training 0.0010032380232587457 relative L2 0.0015516778221353889\n",
      "training 0.0010032251011580229 relative L2 0.0015516680432483554\n",
      "training 0.0010032124118879437 relative L2 0.0015516579151153564\n",
      "training 0.0010031997226178646 relative L2 0.001551647437736392\n",
      "training 0.0010031868005171418 relative L2 0.0015516376588493586\n",
      "training 0.001003173878416419 relative L2 0.0015516274143010378\n",
      "training 0.00100316118914634 relative L2 0.0015516174025833607\n",
      "training 0.0010031484998762608 relative L2 0.0015516072744503617\n",
      "training 0.0010031358106061816 relative L2 0.0015515971463173628\n",
      "training 0.0010031231213361025 relative L2 0.0015515873674303293\n",
      "training 0.0010031104320660233 relative L2 0.0015515771228820086\n",
      "training 0.0010030976263806224 relative L2 0.0015515671111643314\n",
      "training 0.0010030848206952214 relative L2 0.0015515569830313325\n",
      "training 0.0010030721314251423 relative L2 0.0015515470877289772\n",
      "training 0.001003059558570385 relative L2 0.0015515369595959783\n",
      "training 0.0010030468693003058 relative L2 0.0015515265986323357\n",
      "training 0.0010030340636149049 relative L2 0.0015515167033299804\n",
      "training 0.0010030213743448257 relative L2 0.0015515066916123033\n",
      "training 0.0010030088014900684 relative L2 0.0015514965634793043\n",
      "training 0.0010029962286353111 relative L2 0.0015514870174229145\n",
      "training 0.0010029833065345883 relative L2 0.0015514767728745937\n",
      "training 0.001002970733679831 relative L2 0.0015514669939875603\n",
      "training 0.0010029581608250737 relative L2 0.0015514572151005268\n",
      "training 0.00100294582080096 relative L2 0.0015514468541368842\n",
      "training 0.0010029328987002373 relative L2 0.0015514367260038853\n",
      "training 0.00100292032584548 relative L2 0.0015514269471168518\n",
      "training 0.0010029078694060445 relative L2 0.0015514171682298183\n",
      "training 0.0010028950637206435 relative L2 0.0015514070400968194\n",
      "training 0.0010028823744505644 relative L2 0.0015513973776251078\n",
      "training 0.001002869918011129 relative L2 0.0015513872494921088\n",
      "training 0.0010028574615716934 relative L2 0.0015513772377744317\n",
      "training 0.0010028447723016143 relative L2 0.001551367691718042\n",
      "training 0.0010028323158621788 relative L2 0.0015513576800003648\n",
      "training 0.0010028196265920997 relative L2 0.0015513477846980095\n",
      "training 0.001002807286567986 relative L2 0.0015513376565650105\n",
      "training 0.0010027949465438724 relative L2 0.0015513281105086207\n",
      "training 0.0010027828393504024 relative L2 0.0015513180987909436\n",
      "training 0.0010027701500803232 relative L2 0.0015513079706579447\n",
      "training 0.001002757460810244 relative L2 0.0015512981917709112\n",
      "training 0.0010027450043708086 relative L2 0.001551288296468556\n",
      "training 0.0010027325479313731 relative L2 0.0015512786339968443\n",
      "training 0.0010027200914919376 relative L2 0.0015512685058638453\n",
      "training 0.0010027076350525022 relative L2 0.00155125861056149\n",
      "training 0.001002694945782423 relative L2 0.0015512487152591348\n",
      "training 0.0010026827221736312 relative L2 0.0015512389363721013\n",
      "training 0.001002670032903552 relative L2 0.0015512289246544242\n",
      "training 0.0010026576928794384 relative L2 0.0015512193785980344\n",
      "training 0.001002645120024681 relative L2 0.0015512093668803573\n",
      "training 0.0010026328964158893 relative L2 0.0015511997044086456\n",
      "training 0.0010026205563917756 relative L2 0.001551190041936934\n",
      "training 0.0010026083327829838 relative L2 0.001551179913803935\n",
      "training 0.0010025957599282265 relative L2 0.0015511701349169016\n",
      "training 0.0010025831870734692 relative L2 0.0015511603560298681\n",
      "training 0.0010025707306340337 relative L2 0.0015511504607275128\n",
      "training 0.0010025585070252419 relative L2 0.0015511406818404794\n",
      "training 0.0010025459341704845 relative L2 0.0015511311357840896\n",
      "training 0.0010025337105616927 relative L2 0.0015511210076510906\n",
      "training 0.0010025211377069354 relative L2 0.0015511112287640572\n",
      "training 0.0010025086812675 relative L2 0.0015511014498770237\n",
      "training 0.0010024963412433863 relative L2 0.0015510913217440248\n",
      "training 0.001002483768388629 relative L2 0.001551081775687635\n",
      "training 0.0010024713119491935 relative L2 0.0015510719968006015\n",
      "training 0.0010024589719250798 relative L2 0.001551062217913568\n",
      "training 0.0010024465154856443 relative L2 0.0015510525554418564\n",
      "training 0.0010024341754615307 relative L2 0.001551042660139501\n",
      "training 0.001002421835437417 relative L2 0.0015510332304984331\n",
      "training 0.0010024096118286252 relative L2 0.0015510231023654342\n",
      "training 0.001002397038973868 relative L2 0.0015510133234784007\n",
      "training 0.001002384815365076 relative L2 0.001551003777422011\n",
      "training 0.0010023725917562842 relative L2 0.0015509939985349774\n",
      "training 0.0010023602517321706 relative L2 0.0015509844524785876\n",
      "training 0.0010023476788774133 relative L2 0.0015509744407609105\n",
      "training 0.0010023354552686214 relative L2 0.0015509648947045207\n",
      "training 0.001002322998829186 relative L2 0.001550955348648131\n",
      "training 0.001002310891635716 relative L2 0.0015509453369304538\n",
      "training 0.0010022985516116023 relative L2 0.001550935790874064\n",
      "training 0.0010022860951721668 relative L2 0.0015509257791563869\n",
      "training 0.001002273871563375 relative L2 0.0015509161166846752\n",
      "training 0.001002261764369905 relative L2 0.0015509063377976418\n",
      "training 0.0010022494243457913 relative L2 0.0015508966753259301\n",
      "training 0.0010022370843216777 relative L2 0.0015508871292695403\n",
      "training 0.0010022246278822422 relative L2 0.0015508775832131505\n",
      "training 0.0010022124042734504 relative L2 0.0015508674550801516\n",
      "training 0.001002199947834015 relative L2 0.001550857676193118\n",
      "training 0.0010021876078099012 relative L2 0.0015508482465520501\n",
      "training 0.0010021751513704658 relative L2 0.0015508384676650167\n",
      "training 0.001002162927761674 relative L2 0.0015508289216086268\n",
      "training 0.0010021505877375603 relative L2 0.0015508191427215934\n",
      "training 0.0010021383641287684 relative L2 0.0015508097130805254\n",
      "training 0.0010021260241046548 relative L2 0.0015507995849475265\n",
      "training 0.001002113800495863 relative L2 0.001550789806060493\n",
      "training 0.0010021012276411057 relative L2 0.0015507800271734595\n",
      "training 0.001002088887616992 relative L2 0.0015507704811170697\n",
      "training 0.0010020765475928783 relative L2 0.0015507607022300363\n",
      "training 0.0010020642075687647 relative L2 0.0015507511561736465\n",
      "training 0.001002051867544651 relative L2 0.001550741377286613\n",
      "training 0.0010020395275205374 relative L2 0.0015507317148149014\n",
      "training 0.0010020273039117455 relative L2 0.0015507220523431897\n",
      "training 0.001002014963887632 relative L2 0.001550712389871478\n",
      "training 0.00100200274027884 relative L2 0.0015507026109844446\n",
      "training 0.0010019905166700482 relative L2 0.0015506930649280548\n",
      "training 0.0010019784094765782 relative L2 0.001550683518871665\n",
      "training 0.0010019663022831082 relative L2 0.0015506737399846315\n",
      "training 0.0010019539622589946 relative L2 0.001550663961097598\n",
      "training 0.001001941622234881 relative L2 0.0015506544150412083\n",
      "training 0.0010019292822107673 relative L2 0.0015506447525694966\n",
      "training 0.0010019169421866536 relative L2 0.0015506349736824632\n",
      "training 0.0010019043693318963 relative L2 0.0015506251947954297\n",
      "training 0.0010018921457231045 relative L2 0.00155061564873904\n",
      "training 0.0010018798056989908 relative L2 0.0015506058698520064\n",
      "training 0.0010018674656748772 relative L2 0.0015505963237956166\n",
      "training 0.0010018552420660853 relative L2 0.0015505865449085832\n",
      "training 0.0010018429020419717 relative L2 0.0015505769988521934\n",
      "training 0.001001830562017858 relative L2 0.00155056721996516\n",
      "training 0.0010018183384090662 relative L2 0.0015505575574934483\n",
      "training 0.0010018061148002744 relative L2 0.0015505478950217366\n",
      "training 0.0010017936583608389 relative L2 0.001550538232550025\n",
      "training 0.0010017815511673689 relative L2 0.001550528802908957\n",
      "training 0.0010017692111432552 relative L2 0.0015505191404372454\n",
      "training 0.0010017568711191416 relative L2 0.001550509361550212\n",
      "training 0.0010017447639256716 relative L2 0.001550499931909144\n",
      "training 0.0010017325403168797 relative L2 0.0015504899201914668\n",
      "training 0.001001720200292766 relative L2 0.0015504806069657207\n",
      "training 0.001001708093099296 relative L2 0.0015504711773246527\n",
      "training 0.001001696102321148 relative L2 0.001550461514852941\n",
      "training 0.001001683878712356 relative L2 0.0015504519687965512\n",
      "training 0.001001671771518886 relative L2 0.0015504424227401614\n",
      "training 0.0010016595479100943 relative L2 0.0015504327602684498\n",
      "training 0.0010016472078859806 relative L2 0.00155042321421206\n",
      "training 0.0010016349842771888 relative L2 0.0015504134353250265\n",
      "training 0.0010016226442530751 relative L2 0.0015504038892686367\n",
      "training 0.0010016104206442833 relative L2 0.001550394343212247\n",
      "training 0.0010015983134508133 relative L2 0.0015503845643252134\n",
      "training 0.0010015860898420215 relative L2 0.0015503751346841455\n",
      "training 0.0010015739826485515 relative L2 0.0015503655886277556\n",
      "training 0.001001561526209116 relative L2 0.0015503558097407222\n",
      "training 0.0010015491861850023 relative L2 0.0015503461472690105\n",
      "training 0.0010015371954068542 relative L2 0.0015503366012126207\n",
      "training 0.0010015249717980623 relative L2 0.001550327055156231\n",
      "training 0.0010015122825279832 relative L2 0.0015503173926845193\n",
      "training 0.0010015001753345132 relative L2 0.0015503076137974858\n",
      "training 0.0010014880681410432 relative L2 0.0015502979513257742\n",
      "training 0.0010014759609475732 relative L2 0.0015502885216847062\n",
      "training 0.0010014636209234595 relative L2 0.0015502787427976727\n",
      "training 0.0010014512808993459 relative L2 0.0015502689639106393\n",
      "training 0.0010014392901211977 relative L2 0.001550259767100215\n",
      "training 0.0010014271829277277 relative L2 0.0015502499882131815\n",
      "training 0.0010014147264882922 relative L2 0.001550240209326148\n",
      "training 0.0010014026192948222 relative L2 0.0015502310125157237\n",
      "training 0.0010013903956860304 relative L2 0.0015502212336286902\n",
      "training 0.001001377939246595 relative L2 0.0015502115711569786\n",
      "training 0.001001365715637803 relative L2 0.0015502020251005888\n",
      "training 0.001001353608444333 relative L2 0.0015501922462135553\n",
      "training 0.0010013413848355412 relative L2 0.0015501827001571655\n",
      "training 0.0010013292776420712 relative L2 0.0015501732705160975\n",
      "training 0.0010013168212026358 relative L2 0.001550163491629064\n",
      "training 0.0010013044811785221 relative L2 0.001550154061987996\n",
      "training 0.0010012923739850521 relative L2 0.00155014474876225\n",
      "training 0.001001280383206904 relative L2 0.0015501354355365038\n",
      "training 0.0010012686252593994 relative L2 0.0015501255402341485\n",
      "training 0.0010012558195739985 relative L2 0.0015501159941777587\n",
      "training 0.0010012435959652066 relative L2 0.001550106331706047\n",
      "training 0.001001231255941093 relative L2 0.0015500964364036918\n",
      "training 0.0010012192651629448 relative L2 0.0015500872395932674\n",
      "training 0.0010012072743847966 relative L2 0.0015500778099521995\n",
      "training 0.0010011954000219703 relative L2 0.0015500682638958097\n",
      "training 0.0010011831764131784 relative L2 0.001550058601424098\n",
      "training 0.0010011708363890648 relative L2 0.0015500490553677082\n",
      "training 0.0010011587291955948 relative L2 0.0015500395093113184\n",
      "training 0.0010011463891714811 relative L2 0.001550029730424285\n",
      "training 0.0010011341655626893 relative L2 0.001550020300783217\n",
      "training 0.0010011218255385756 relative L2 0.0015500105218961835\n",
      "training 0.0010011096019297838 relative L2 0.0015500010922551155\n",
      "training 0.0010010974947363138 relative L2 0.0015499911969527602\n",
      "training 0.0010010851547122002 relative L2 0.0015499814180657268\n",
      "training 0.0010010728146880865 relative L2 0.0015499722212553024\n",
      "training 0.0010010609403252602 relative L2 0.0015499625587835908\n",
      "training 0.001001048949547112 relative L2 0.0015499533619731665\n",
      "training 0.0010010370751842856 relative L2 0.0015499438159167767\n",
      "training 0.0010010250844061375 relative L2 0.001549934153445065\n",
      "training 0.0010010127443820238 relative L2 0.0015499246073886752\n",
      "training 0.0010010007536038756 relative L2 0.0015499150613322854\n",
      "training 0.0010009886464104056 relative L2 0.0015499057481065392\n",
      "training 0.0010009764228016138 relative L2 0.0015498960856348276\n",
      "training 0.0010009643156081438 relative L2 0.0015498863067477942\n",
      "training 0.0010009523248299956 relative L2 0.0015498768771067262\n",
      "training 0.0010009401012212038 relative L2 0.0015498673310503364\n",
      "training 0.0010009277611970901 relative L2 0.0015498579014092684\n",
      "training 0.0010009156540036201 relative L2 0.0015498482389375567\n",
      "training 0.0010009035468101501 relative L2 0.0015498389257118106\n",
      "training 0.001000891556032002 relative L2 0.0015498289139941335\n",
      "training 0.0010008788667619228 relative L2 0.0015498191351071\n",
      "training 0.0010008664103224874 relative L2 0.0015498099382966757\n",
      "training 0.0010008544195443392 relative L2 0.0015498003922402859\n",
      "training 0.0010008423123508692 relative L2 0.0015497907297685742\n",
      "training 0.001000830321572721 relative L2 0.001549781416542828\n",
      "training 0.001000818214379251 relative L2 0.00154977198690176\n",
      "training 0.0010008062236011028 relative L2 0.0015497625572606921\n",
      "training 0.0010007942328229547 relative L2 0.0015497527783736587\n",
      "training 0.0010007820092141628 relative L2 0.0015497432323172688\n",
      "training 0.001000769785605371 relative L2 0.0015497338026762009\n",
      "training 0.001000757678411901 relative L2 0.001549724256619811\n",
      "training 0.001000745571218431 relative L2 0.0015497144777327776\n",
      "training 0.0010007331147789955 relative L2 0.0015497052809223533\n",
      "training 0.0010007210075855255 relative L2 0.0015496955020353198\n",
      "training 0.0010007089003920555 relative L2 0.0015496860723942518\n",
      "training 0.0010006969096139073 relative L2 0.001549676526337862\n",
      "training 0.0010006844531744719 relative L2 0.001549667096696794\n",
      "training 0.0010006724623963237 relative L2 0.0015496574342250824\n",
      "training 0.0010006603552028537 relative L2 0.0015496481209993362\n",
      "training 0.0010006484808400273 relative L2 0.0015496383421123028\n",
      "training 0.001000636606477201 relative L2 0.0015496289124712348\n",
      "training 0.0010006242664530873 relative L2 0.0015496194828301668\n",
      "training 0.0010006120428442955 relative L2 0.0015496097039431334\n",
      "training 0.0010005999356508255 relative L2 0.0015496002743020654\n",
      "training 0.0010005878284573555 relative L2 0.0015495909610763192\n",
      "training 0.0010005757212638855 relative L2 0.0015495812986046076\n",
      "training 0.0010005637304857373 relative L2 0.0015495717525482178\n",
      "training 0.0010005513904616237 relative L2 0.0015495623229071498\n",
      "training 0.0010005391668528318 relative L2 0.00154955277685076\n",
      "training 0.0010005272924900055 relative L2 0.001549543347209692\n",
      "training 0.0010005151852965355 relative L2 0.0015495338011533022\n",
      "training 0.0010005033109337091 relative L2 0.0015495240222662687\n",
      "training 0.0010004907380789518 relative L2 0.0015495142433792353\n",
      "training 0.0010004782816395164 relative L2 0.0015495048137381673\n",
      "training 0.0010004661744460464 relative L2 0.0015494952676817775\n",
      "training 0.0010004541836678982 relative L2 0.0015494858380407095\n",
      "training 0.0010004420764744282 relative L2 0.0015494764083996415\n",
      "training 0.00100043008569628 relative L2 0.0015494672115892172\n",
      "training 0.0010004183277487755 relative L2 0.0015494577819481492\n",
      "training 0.0010004063369706273 relative L2 0.0015494480030611157\n",
      "training 0.0010003942297771573 relative L2 0.0015494388062506914\n",
      "training 0.0010003822389990091 relative L2 0.001549429027363658\n",
      "training 0.0010003700153902173 relative L2 0.0015494197141379118\n",
      "training 0.0010003579081967473 relative L2 0.0015494100516662002\n",
      "training 0.0010003456845879555 relative L2 0.0015494006220251322\n",
      "training 0.0010003336938098073 relative L2 0.001549391308799386\n",
      "training 0.0010003215866163373 relative L2 0.0015493819955736399\n",
      "training 0.0010003094794228673 relative L2 0.0015493721002712846\n",
      "training 0.0010002972558140755 relative L2 0.0015493626706302166\n",
      "training 0.0010002851486206055 relative L2 0.0015493531245738268\n",
      "training 0.0010002731578424573 relative L2 0.0015493439277634025\n",
      "training 0.0010002613998949528 relative L2 0.0015493344981223345\n",
      "training 0.0010002492927014828 relative L2 0.001549324719235301\n",
      "training 0.001000237069092691 relative L2 0.0015493155224248767\n",
      "training 0.0010002253111451864 relative L2 0.0015493057435378432\n",
      "training 0.001000212854705751 relative L2 0.0015492963138967752\n",
      "training 0.0010002009803429246 relative L2 0.0015492867678403854\n",
      "training 0.0010001887567341328 relative L2 0.0015492773381993175\n",
      "training 0.0010001766495406628 relative L2 0.0015492677921429276\n",
      "training 0.0010001646587625146 relative L2 0.0015492584789171815\n",
      "training 0.0010001524351537228 relative L2 0.001549248700030148\n",
      "training 0.001000140211544931 relative L2 0.0015492393868044019\n",
      "training 0.001000127987936139 relative L2 0.001549229957163334\n",
      "training 0.0010001161135733128 relative L2 0.0015492202946916223\n",
      "training 0.001000103773549199 relative L2 0.0015492106322199106\n",
      "training 0.0010000914335250854 relative L2 0.0015492008533328772\n",
      "training 0.0010000790935009718 relative L2 0.0015491911908611655\n",
      "training 0.0010000669863075018 relative L2 0.0015491818776354194\n",
      "training 0.0010000548791140318 relative L2 0.0015491722151637077\n",
      "training 0.0010000424226745963 relative L2 0.0015491623198613524\n",
      "training 0.0010000303154811263 relative L2 0.0015491526573896408\n",
      "training 0.0010000180918723345 relative L2 0.001549143111333251\n",
      "training 0.0010000057518482208 relative L2 0.0015491333324462175\n",
      "training 0.000999993528239429 relative L2 0.0015491239028051496\n",
      "training 0.0009999811882153153 relative L2 0.0015491143567487597\n",
      "training 0.0009999688481912017 relative L2 0.0015491044614464045\n",
      "training 0.0009999563917517662 relative L2 0.0015490947989746928\n",
      "training 0.0009999439353123307 relative L2 0.001549085252918303\n",
      "training 0.0009999318281188607 relative L2 0.0015490757068619132\n",
      "training 0.000999919604510069 relative L2 0.0015490659279748797\n",
      "training 0.0009999071480706334 relative L2 0.001549056265503168\n",
      "training 0.0009998949244618416 relative L2 0.0015490467194467783\n",
      "training 0.000999882468022406 relative L2 0.0015490369405597448\n",
      "training 0.0009998702444136143 relative L2 0.0015490275109186769\n",
      "training 0.000999858253635466 relative L2 0.0015490181976929307\n",
      "training 0.0009998459136113524 relative L2 0.0015490084188058972\n",
      "training 0.000999833457171917 relative L2 0.0015489987563341856\n",
      "training 0.000999821349978447 relative L2 0.001548989093862474\n",
      "training 0.0009998086607083678 relative L2 0.0015489794313907623\n",
      "training 0.0009997963206842542 relative L2 0.0015489700017496943\n",
      "training 0.0009997838642448187 relative L2 0.0015489602228626609\n",
      "training 0.0009997717570513487 relative L2 0.001548950676806271\n",
      "training 0.0009997591841965914 relative L2 0.0015489408979192376\n",
      "training 0.0009997469605877995 relative L2 0.0015489311190322042\n",
      "training 0.0009997342713177204 relative L2 0.0015489215729758143\n",
      "training 0.000999721814878285 relative L2 0.0015489117940887809\n",
      "training 0.0009997093584388494 relative L2 0.0015489020152017474\n",
      "training 0.000999696901999414 relative L2 0.001548892236314714\n",
      "training 0.0009996844455599785 relative L2 0.0015488824574276805\n",
      "training 0.000999671989120543 relative L2 0.0015488729113712907\n",
      "training 0.0009996596490964293 relative L2 0.0015488631324842572\n",
      "training 0.0009996473090723157 relative L2 0.0015488533535972238\n",
      "training 0.0009996347362175584 relative L2 0.001548843807540834\n",
      "training 0.000999622279778123 relative L2 0.0015488340286538005\n",
      "training 0.0009996099397540092 relative L2 0.0015488241333514452\n",
      "training 0.0009995971340686083 relative L2 0.0015488143544644117\n",
      "training 0.0009995844447985291 relative L2 0.0015488042263314128\n",
      "training 0.00099957175552845 relative L2 0.0015487940981984138\n",
      "training 0.0009995588334277272 relative L2 0.001548783970065415\n",
      "training 0.000999546144157648 relative L2 0.0015487743075937033\n",
      "training 0.0009995333384722471 relative L2 0.0015487641794607043\n",
      "training 0.000999520649202168 relative L2 0.0015487545169889927\n",
      "training 0.000999507843516767 relative L2 0.0015487443888559937\n",
      "training 0.0009994952706620097 relative L2 0.0015487347263842821\n",
      "training 0.0009994828142225742 relative L2 0.0015487249474972486\n",
      "training 0.000999470241367817 relative L2 0.0015487150521948934\n",
      "training 0.0009994573192670941 relative L2 0.0015487050404772162\n",
      "training 0.0009994445135816932 relative L2 0.0015486949123442173\n",
      "training 0.0009994315914809704 relative L2 0.0015486847842112184\n",
      "training 0.0009994187857955694 relative L2 0.001548674888908863\n",
      "training 0.000999405630864203 relative L2 0.0015486644115298986\n",
      "training 0.0009993923595175147 relative L2 0.001548654050566256\n",
      "training 0.0009993790881708264 relative L2 0.001548643922433257\n",
      "training 0.00099936593323946 relative L2 0.0015486334450542927\n",
      "training 0.000999352429062128 relative L2 0.001548623200505972\n",
      "training 0.0009993391577154398 relative L2 0.0015486128395423293\n",
      "training 0.0009993258863687515 relative L2 0.0015486023621633649\n",
      "training 0.0009993123821914196 relative L2 0.001548591535538435\n",
      "training 0.0009992985287681222 relative L2 0.0015485810581594706\n",
      "training 0.0009992849081754684 relative L2 0.0015485701151192188\n",
      "training 0.0009992711711674929 relative L2 0.0015485596377402544\n",
      "training 0.000999257666990161 relative L2 0.0015485488111153245\n",
      "training 0.0009992438135668635 relative L2 0.0015485385665670037\n",
      "training 0.0009992299601435661 relative L2 0.001548527623526752\n",
      "training 0.0009992163395509124 relative L2 0.0015485171461477876\n",
      "training 0.0009992026025429368 relative L2 0.0015485062031075358\n",
      "training 0.000999188283458352 relative L2 0.0015484949108213186\n",
      "training 0.000999174197204411 relative L2 0.0015484843170270324\n",
      "training 0.00099916011095047 relative L2 0.0015484732575714588\n",
      "training 0.0009991456754505634 relative L2 0.0015484621981158853\n",
      "training 0.0009991315891966224 relative L2 0.0015484511386603117\n",
      "training 0.0009991173865273595 relative L2 0.0015484399627894163\n",
      "training 0.0009991031838580966 relative L2 0.001548428786918521\n",
      "training 0.0009990885155275464 relative L2 0.0015484173782169819\n",
      "training 0.0009990740800276399 relative L2 0.0015484063187614083\n",
      "training 0.0009990592952817678 relative L2 0.0015483945608139038\n",
      "training 0.0009990449761971831 relative L2 0.0015483833849430084\n",
      "training 0.000999030307866633 relative L2 0.0015483717434108257\n",
      "training 0.0009990156395360827 relative L2 0.0015483599854633212\n",
      "training 0.000999000621959567 relative L2 0.0015483485767617822\n",
      "training 0.0009989857207983732 relative L2 0.0015483374008908868\n",
      "training 0.000998971052467823 relative L2 0.0015483256429433823\n",
      "training 0.0009989559184759855 relative L2 0.0015483141178265214\n",
      "training 0.0009989409008994699 relative L2 0.0015483023598790169\n",
      "training 0.0009989256504923105 relative L2 0.0015482906019315124\n",
      "training 0.0009989104000851512 relative L2 0.0015482788439840078\n",
      "training 0.00099889503326267 relative L2 0.0015482667367905378\n",
      "training 0.0009988796664401889 relative L2 0.0015482550952583551\n",
      "training 0.0009988645324483514 relative L2 0.0015482432208955288\n",
      "training 0.000998849282041192 relative L2 0.0015482312301173806\n",
      "training 0.0009988335659727454 relative L2 0.0015482191229239106\n",
      "training 0.0009988178499042988 relative L2 0.001548207364976406\n",
      "training 0.0009988023666664958 relative L2 0.001548195257782936\n",
      "training 0.0009987869998440146 relative L2 0.001548183267004788\n",
      "training 0.0009987717494368553 relative L2 0.0015481715090572834\n",
      "training 0.0009987561497837305 relative L2 0.0015481598675251007\n",
      "training 0.0009987408993765712 relative L2 0.0015481478767469525\n",
      "training 0.0009987254161387682 relative L2 0.0015481358859688044\n",
      "training 0.0009987099329009652 relative L2 0.0015481241280212998\n",
      "training 0.0009986941004171968 relative L2 0.0015481122536584735\n",
      "training 0.0009986789664253592 relative L2 0.0015481002628803253\n",
      "training 0.0009986634831875563 relative L2 0.0015480885049328208\n",
      "training 0.0009986479999497533 relative L2 0.0015480767469853163\n",
      "training 0.000998632749542594 relative L2 0.0015480652218684554\n",
      "training 0.0009986178483814 relative L2 0.0015480534639209509\n",
      "training 0.0009986025979742408 relative L2 0.00154804193880409\n",
      "training 0.0009985872311517596 relative L2 0.0015480299480259418\n",
      "training 0.0009985720971599221 relative L2 0.001548018422909081\n",
      "training 0.0009985568467527628 relative L2 0.001548007014207542\n",
      "training 0.0009985418291762471 relative L2 0.0015479953726753592\n",
      "training 0.000998527044430375 relative L2 0.0015479841968044639\n",
      "training 0.0009985121432691813 relative L2 0.001547973370179534\n",
      "training 0.0009984975913539529 relative L2 0.0015479620778933167\n",
      "training 0.0009984832722693682 relative L2 0.0015479507856070995\n",
      "training 0.0009984684875234962 relative L2 0.0015479394933208823\n",
      "training 0.000998453819192946 relative L2 0.0015479286666959524\n",
      "training 0.000998439732939005 relative L2 0.001547918189316988\n",
      "training 0.0009984258795157075 relative L2 0.0015479071298614144\n",
      "training 0.0009984115604311228 relative L2 0.0015478960704058409\n",
      "training 0.00099839735776186 relative L2 0.0015478854766115546\n",
      "training 0.0009983837371692061 relative L2 0.0015478753484785557\n",
      "training 0.0009983698837459087 relative L2 0.0015478648710995913\n",
      "training 0.000998356263153255 relative L2 0.0015478545101359487\n",
      "training 0.0009983428753912449 relative L2 0.0015478442655876279\n",
      "training 0.0009983296040445566 relative L2 0.001547834137454629\n",
      "training 0.000998316565528512 relative L2 0.0015478241257369518\n",
      "training 0.000998303177766502 relative L2 0.0015478139976039529\n",
      "training 0.0009982900228351355 relative L2 0.0015478041023015976\n",
      "training 0.0009982769843190908 relative L2 0.0015477943234145641\n",
      "training 0.0009982638293877244 relative L2 0.0015477841952815652\n",
      "training 0.0009982509072870016 relative L2 0.001547774183563888\n",
      "training 0.000998237868770957 relative L2 0.0015477642882615328\n",
      "training 0.000998225063085556 relative L2 0.001547754742205143\n",
      "training 0.000998212373815477 relative L2 0.0015477447304874659\n",
      "training 0.000998199568130076 relative L2 0.0015477349516004324\n",
      "training 0.0009981868788599968 relative L2 0.001547725172713399\n",
      "training 0.0009981744224205613 relative L2 0.0015477153938263655\n",
      "training 0.0009981619659811258 relative L2 0.0015477057313546538\n",
      "training 0.0009981495095416903 relative L2 0.0015476959524676204\n",
      "training 0.000998136936686933 relative L2 0.0015476862899959087\n",
      "training 0.0009981245966628194 relative L2 0.001547676743939519\n",
      "training 0.000998112140223384 relative L2 0.0015476671978831291\n",
      "training 0.0009980998001992702 relative L2 0.0015476577682420611\n",
      "training 0.0009980874601751566 relative L2 0.0015476479893550277\n",
      "training 0.000998075120151043 relative L2 0.0015476385597139597\n",
      "training 0.0009980628965422511 relative L2 0.0015476287808269262\n",
      "training 0.000998050207272172 relative L2 0.0015476192347705364\n",
      "training 0.0009980378672480583 relative L2 0.0015476098051294684\n",
      "training 0.000998025992885232 relative L2 0.0015476002590730786\n",
      "training 0.0009980137692764401 relative L2 0.001547590596601367\n",
      "training 0.0009980015456676483 relative L2 0.0015475812833756208\n",
      "training 0.0009979893220588565 relative L2 0.0015475719701498747\n",
      "training 0.0009979773312807083 relative L2 0.0015475626569241285\n",
      "training 0.0009979653405025601 relative L2 0.0015475532272830606\n",
      "training 0.0009979531168937683 relative L2 0.0015475436812266707\n",
      "training 0.0009979411261156201 relative L2 0.0015475343680009246\n",
      "training 0.0009979290189221501 relative L2 0.0015475251711905003\n",
      "training 0.000997917028144002 relative L2 0.0015475156251341105\n",
      "training 0.000997904920950532 relative L2 0.0015475061954930425\n",
      "training 0.0009978929301723838 relative L2 0.00154749711509794\n",
      "training 0.0009978808229789138 relative L2 0.0015474875690415502\n",
      "training 0.0009978687157854438 relative L2 0.0015474781394004822\n",
      "training 0.0009978563757613301 relative L2 0.0015474689425900578\n",
      "training 0.000997844384983182 relative L2 0.0015474591637030244\n",
      "training 0.0009978325106203556 relative L2 0.0015474497340619564\n",
      "training 0.000997820170596242 relative L2 0.0015474403044208884\n",
      "training 0.000997808063402772 relative L2 0.0015474313404411077\n",
      "training 0.0009977961890399456 relative L2 0.0015474219108000398\n",
      "training 0.0009977839654311538 relative L2 0.00154741236474365\n",
      "training 0.0009977722074836493 relative L2 0.001547402935102582\n",
      "training 0.0009977598674595356 relative L2 0.0015473937382921576\n",
      "training 0.0009977476438507438 relative L2 0.0015473843086510897\n",
      "training 0.0009977355366572738 relative L2 0.0015473751118406653\n",
      "training 0.0009977236622944474 relative L2 0.0015473657986149192\n",
      "training 0.000997711787931621 relative L2 0.0015473566018044949\n",
      "training 0.0009976999135687947 relative L2 0.0015473471721634269\n",
      "training 0.000997687689960003 relative L2 0.0015473378589376807\n",
      "training 0.0009976756991818547 relative L2 0.0015473284292966127\n",
      "training 0.0009976635919883847 relative L2 0.0015473189996555448\n",
      "training 0.0009976517176255584 relative L2 0.0015473098028451204\n",
      "training 0.0009976399596780539 relative L2 0.0015473003732040524\n",
      "training 0.0009976279688999057 relative L2 0.0015472911763936281\n",
      "training 0.0009976159781217575 relative L2 0.0015472817467525601\n",
      "training 0.0009976038709282875 relative L2 0.0015472723171114922\n",
      "training 0.0009975919965654612 relative L2 0.0015472628874704242\n",
      "training 0.0009975798893719912 relative L2 0.0015472533414140344\n",
      "training 0.0009975675493478775 relative L2 0.0015472437953576446\n",
      "training 0.0009975552093237638 relative L2 0.001547234714962542\n",
      "training 0.0009975433349609375 relative L2 0.0015472255181521177\n",
      "training 0.0009975316934287548 relative L2 0.0015472163213416934\n",
      "training 0.0009975200518965721 relative L2 0.0015472070081159472\n",
      "training 0.0009975082939490676 relative L2 0.0015471980441361666\n",
      "training 0.0009974961867555976 relative L2 0.0015471888473257422\n",
      "training 0.000997484428808093 relative L2 0.0015471791848540306\n",
      "training 0.000997472321614623 relative L2 0.0015471699880436063\n",
      "training 0.0009974600980058312 relative L2 0.0015471605584025383\n",
      "training 0.0009974483400583267 relative L2 0.0015471511287614703\n",
      "training 0.0009974364656955004 relative L2 0.001547141931951046\n",
      "training 0.0009974242420867085 relative L2 0.001547132502309978\n",
      "training 0.0009974123677238822 relative L2 0.0015471231890842319\n",
      "training 0.0009974001441150904 relative L2 0.0015471138758584857\n",
      "training 0.0009973881533369422 relative L2 0.0015471046790480614\n",
      "training 0.000997376162558794 relative L2 0.0015470952494069934\n",
      "training 0.0009973641717806458 relative L2 0.0015470857033506036\n",
      "training 0.0009973520645871758 relative L2 0.0015470765065401793\n",
      "training 0.0009973400738090277 relative L2 0.0015470674261450768\n",
      "training 0.000997328432276845 relative L2 0.0015470579965040088\n",
      "training 0.0009973164414986968 relative L2 0.0015470486832782626\n",
      "training 0.0009973045671358705 relative L2 0.0015470396028831601\n",
      "training 0.0009972925763577223 relative L2 0.0015470301732420921\n",
      "training 0.000997280585579574 relative L2 0.0015470210928469896\n",
      "training 0.000997268478386104 relative L2 0.0015470115467905998\n",
      "training 0.000997256371192634 relative L2 0.00154700200073421\n",
      "training 0.0009972444968298078 relative L2 0.001546992571093142\n",
      "training 0.000997232273221016 relative L2 0.001546983141452074\n",
      "training 0.000997220166027546 relative L2 0.0015469739446416497\n",
      "training 0.0009972084080800414 relative L2 0.0015469645150005817\n",
      "training 0.0009971961844712496 relative L2 0.001546954968944192\n",
      "training 0.0009971839608624578 relative L2 0.0015469451900571585\n",
      "training 0.000997171737253666 relative L2 0.0015469359932467341\n",
      "training 0.0009971597464755177 relative L2 0.001546926680020988\n",
      "training 0.0009971479885280132 relative L2 0.0015469177160412073\n",
      "training 0.0009971364634111524 relative L2 0.0015469086356461048\n",
      "training 0.000997124589048326 relative L2 0.0015468994388356805\n",
      "training 0.0009971127146854997 relative L2 0.0015468898927792907\n",
      "training 0.0009971008403226733 relative L2 0.0015468805795535445\n",
      "training 0.0009970886167138815 relative L2 0.0015468711499124765\n",
      "training 0.0009970766259357333 relative L2 0.0015468616038560867\n",
      "training 0.0009970646351575851 relative L2 0.0015468524070456624\n",
      "training 0.0009970527607947588 relative L2 0.00154684332665056\n",
      "training 0.0009970410028472543 relative L2 0.00154683378059417\n",
      "training 0.000997029012069106 relative L2 0.0015468247001990676\n",
      "training 0.0009970171377062798 relative L2 0.001546815736219287\n",
      "training 0.0009970052633434534 relative L2 0.0015468059573322535\n",
      "training 0.0009969932725653052 relative L2 0.0015467971097677946\n",
      "training 0.0009969813982024789 relative L2 0.0015467876801267266\n",
      "training 0.0009969692910090089 relative L2 0.001546778017655015\n",
      "training 0.0009969575330615044 relative L2 0.0015467690536752343\n",
      "training 0.0009969457751139998 relative L2 0.00154675985686481\n",
      "training 0.0009969340171664953 relative L2 0.0015467505436390638\n",
      "training 0.0009969220263883471 relative L2 0.0015467413468286395\n",
      "training 0.0009969102684408426 relative L2 0.0015467321500182152\n",
      "training 0.0009968982776626945 relative L2 0.0015467224875465035\n",
      "training 0.0009968862868845463 relative L2 0.0015467136399820447\n",
      "training 0.0009968745289370418 relative L2 0.0015467043267562985\n",
      "training 0.0009968626545742154 relative L2 0.0015466948971152306\n",
      "training 0.0009968506637960672 relative L2 0.001546685816720128\n",
      "training 0.0009968390222638845 relative L2 0.00154667638707906\n",
      "training 0.0009968271479010582 relative L2 0.001546667073853314\n",
      "training 0.0009968150407075882 relative L2 0.0015466577606275678\n",
      "training 0.0009968027006834745 relative L2 0.0015466479817405343\n",
      "training 0.00099679094273597 relative L2 0.0015466384356841445\n",
      "training 0.0009967790683731437 relative L2 0.0015466298209503293\n",
      "training 0.000996767426840961 relative L2 0.0015466203913092613\n",
      "training 0.000996755319647491 relative L2 0.0015466107288375497\n",
      "training 0.0009967435616999865 relative L2 0.0015466018812730908\n",
      "training 0.0009967316873371601 relative L2 0.0015465924516320229\n",
      "training 0.0009967200458049774 relative L2 0.001546582905575633\n",
      "training 0.000996708171442151 relative L2 0.0015465738251805305\n",
      "training 0.000996696180664003 relative L2 0.0015465646283701062\n",
      "training 0.0009966841898858547 relative L2 0.0015465550823137164\n",
      "training 0.000996672548353672 relative L2 0.001546546001918614\n",
      "training 0.0009966605575755239 relative L2 0.0015465368051081896\n",
      "training 0.0009966486832126975 relative L2 0.0015465276082977653\n",
      "training 0.000996636925265193 relative L2 0.001546518411487341\n",
      "training 0.0009966251673176885 relative L2 0.001546508981846273\n",
      "training 0.0009966129437088966 relative L2 0.0015464993193745613\n",
      "training 0.0009966008365154266 relative L2 0.001546490122564137\n",
      "training 0.0009965888457372785 relative L2 0.0015464809257537127\n",
      "training 0.000996577087789774 relative L2 0.0015464718453586102\n",
      "training 0.000996565562672913 relative L2 0.0015464626485481858\n",
      "training 0.0009965538047254086 relative L2 0.0015464535681530833\n",
      "training 0.0009965415811166167 relative L2 0.001546444371342659\n",
      "training 0.000996529939584434 relative L2 0.0015464348252862692\n",
      "training 0.0009965177159756422 relative L2 0.0015464251628145576\n",
      "training 0.0009965058416128159 relative L2 0.0015464158495888114\n",
      "training 0.0009964940836653113 relative L2 0.001546406769193709\n",
      "training 0.000996482209302485 relative L2 0.0015463975723832846\n",
      "training 0.0009964703349396586 relative L2 0.0015463881427422166\n",
      "training 0.0009964584605768323 relative L2 0.001546379062347114\n",
      "training 0.0009964467026293278 relative L2 0.001546369749121368\n",
      "training 0.0009964348282665014 relative L2 0.0015463604358956218\n",
      "training 0.0009964231867343187 relative L2 0.0015463512390851974\n",
      "training 0.000996410963125527 relative L2 0.0015463420422747731\n",
      "training 0.0009963992051780224 relative L2 0.0015463328454643488\n",
      "training 0.0009963870979845524 relative L2 0.0015463234158232808\n",
      "training 0.0009963755728676915 relative L2 0.0015463142190128565\n",
      "training 0.0009963636985048652 relative L2 0.0015463049057871103\n",
      "training 0.000996351707726717 relative L2 0.0015462955925613642\n",
      "training 0.0009963398333638906 relative L2 0.001546286279335618\n",
      "training 0.0009963278425857425 relative L2 0.0015462773153558373\n",
      "training 0.0009963159682229161 relative L2 0.0015462676528841257\n",
      "training 0.000996303977444768 relative L2 0.0015462584560737014\n",
      "training 0.0009962922194972634 relative L2 0.0015462493756785989\n",
      "training 0.000996280345134437 relative L2 0.0015462401788681746\n",
      "training 0.0009962684707716107 relative L2 0.001546231098473072\n",
      "training 0.000996256829239428 relative L2 0.0015462219016626477\n",
      "training 0.0009962449548766017 relative L2 0.001546212355606258\n",
      "training 0.0009962329640984535 relative L2 0.0015462032752111554\n",
      "training 0.0009962210897356272 relative L2 0.0015461944276466966\n",
      "training 0.0009962095646187663 relative L2 0.0015461852308362722\n",
      "training 0.0009961980395019054 relative L2 0.0015461763832718134\n",
      "training 0.000996186281554401 relative L2 0.001546167186461389\n",
      "training 0.00099617475643754 relative L2 0.001546157873235643\n",
      "training 0.0009961629984900355 relative L2 0.0015461486764252186\n",
      "training 0.0009961511241272092 relative L2 0.0015461394796147943\n",
      "training 0.0009961392497643828 relative L2 0.0015461298171430826\n",
      "training 0.0009961274918168783 relative L2 0.0015461206203326583\n",
      "training 0.0009961153846234083 relative L2 0.0015461116563528776\n",
      "training 0.0009961037430912256 relative L2 0.0015461022267118096\n",
      "training 0.000996092101559043 relative L2 0.0015460930299013853\n",
      "training 0.000996079994365573 relative L2 0.0015460837166756392\n",
      "training 0.0009960682364180684 relative L2 0.0015460747526958585\n",
      "training 0.0009960565948858857 relative L2 0.0015460654394701123\n",
      "training 0.0009960444876924157 relative L2 0.0015460561262443662\n",
      "training 0.0009960322640836239 relative L2 0.00154604681301862\n",
      "training 0.000996020738966763 relative L2 0.0015460378490388393\n",
      "training 0.0009960090974345803 relative L2 0.0015460284193977714\n",
      "training 0.0009959971066564322 relative L2 0.0015460189897567034\n",
      "training 0.0009959855815395713 relative L2 0.0015460101421922445\n",
      "training 0.000995973707176745 relative L2 0.001546001061797142\n",
      "training 0.0009959619492292404 relative L2 0.0015459917485713959\n",
      "training 0.0009959499584510922 relative L2 0.0015459826681762934\n",
      "training 0.0009959384333342314 relative L2 0.0015459732385352254\n",
      "training 0.0009959264425560832 relative L2 0.0015459641581401229\n",
      "training 0.0009959149174392223 relative L2 0.0015459551941603422\n",
      "training 0.0009959032759070396 relative L2 0.0015459459973499179\n",
      "training 0.0009958915179595351 relative L2 0.0015459366841241717\n",
      "training 0.0009958796435967088 relative L2 0.0015459274873137474\n",
      "training 0.000995868002064526 relative L2 0.0015459179412573576\n",
      "training 0.000995856011286378 relative L2 0.0015459087444469333\n",
      "training 0.0009958441369235516 relative L2 0.0015458996640518308\n",
      "training 0.0009958322625607252 relative L2 0.0015458904672414064\n",
      "training 0.0009958206210285425 relative L2 0.0015458811540156603\n",
      "training 0.000995808863081038 relative L2 0.0015458721900358796\n",
      "training 0.0009957971051335335 relative L2 0.0015458627603948116\n",
      "training 0.0009957852307707071 relative L2 0.0015458535635843873\n",
      "training 0.0009957734728232026 relative L2 0.0015458447160199285\n",
      "training 0.00099576183129102 relative L2 0.0015458354027941823\n",
      "training 0.0009957500733435154 relative L2 0.001545826205983758\n",
      "training 0.0009957384318113327 relative L2 0.0015458171255886555\n",
      "training 0.00099572679027915 relative L2 0.0015458079287782311\n",
      "training 0.0009957150323316455 relative L2 0.0015457984991371632\n",
      "training 0.000995703274384141 relative L2 0.0015457895351573825\n",
      "training 0.0009956915164366364 relative L2 0.00154578045476228\n",
      "training 0.0009956798749044538 relative L2 0.0015457712579518557\n",
      "training 0.0009956680005416274 relative L2 0.0015457618283107877\n",
      "training 0.0009956560097634792 relative L2 0.0015457526315003633\n",
      "training 0.0009956443682312965 relative L2 0.0015457437839359045\n",
      "training 0.0009956327266991138 relative L2 0.0015457344707101583\n",
      "training 0.000995621201582253 relative L2 0.0015457251574844122\n",
      "training 0.0009956094436347485 relative L2 0.0015457160770893097\n",
      "training 0.0009955978021025658 relative L2 0.0015457068802788854\n",
      "training 0.000995586276985705 relative L2 0.0015456977998837829\n",
      "training 0.0009955744026228786 relative L2 0.0015456887194886804\n",
      "training 0.0009955625282600522 relative L2 0.001545679522678256\n",
      "training 0.0009955507703125477 relative L2 0.0015456704422831535\n",
      "training 0.0009955388959497213 relative L2 0.0015456612454727292\n",
      "training 0.0009955274872481823 relative L2 0.0015456520486623049\n",
      "training 0.0009955158457159996 relative L2 0.0015456433175131679\n",
      "training 0.0009955043205991387 relative L2 0.001545633771456778\n",
      "training 0.0009954925626516342 relative L2 0.0015456246910616755\n",
      "training 0.0009954806882888079 relative L2 0.0015456157270818949\n",
      "training 0.0009954689303413033 relative L2 0.0015456065302714705\n",
      "training 0.0009954571723937988 relative L2 0.0015455972170457244\n",
      "training 0.0009954454144462943 relative L2 0.0015455880202353\n",
      "training 0.0009954337729141116 relative L2 0.0015455791726708412\n",
      "training 0.0009954224806278944 relative L2 0.0015455703251063824\n",
      "training 0.0009954110719263554 relative L2 0.0015455612447112799\n",
      "training 0.0009953995468094945 relative L2 0.0015455520479008555\n",
      "training 0.00099538778886199 relative L2 0.0015455426182597876\n",
      "training 0.00099537568166852 relative L2 0.0015455334214493632\n",
      "training 0.0009953643893823028 relative L2 0.0015455246903002262\n",
      "training 0.0009953529806807637 relative L2 0.0015455157263204455\n",
      "training 0.000995341339148581 relative L2 0.0015455064130946994\n",
      "training 0.0009953298140317202 relative L2 0.0015454977983608842\n",
      "training 0.0009953181724995375 relative L2 0.0015454886015504599\n",
      "training 0.000995306414552033 relative L2 0.0015454792883247137\n",
      "training 0.0009952947730198503 relative L2 0.0015454704407602549\n",
      "training 0.0009952832479029894 relative L2 0.0015454614767804742\n",
      "training 0.0009952716063708067 relative L2 0.0015454525128006935\n",
      "training 0.0009952600812539458 relative L2 0.0015454436652362347\n",
      "training 0.000995248556137085 relative L2 0.0015454344684258103\n",
      "training 0.0009952367981895804 relative L2 0.001545425271615386\n",
      "training 0.0009952251566573977 relative L2 0.0015454161912202835\n",
      "training 0.0009952133987098932 relative L2 0.0015454069944098592\n",
      "training 0.0009952018735930324 relative L2 0.0015453981468454003\n",
      "training 0.0009951902320608497 relative L2 0.0015453886007890105\n",
      "training 0.0009951784741133451 relative L2 0.001545379520393908\n",
      "training 0.000995167065411806 relative L2 0.0015453704399988055\n",
      "training 0.0009951553074643016 relative L2 0.0015453612431883812\n",
      "training 0.000995143665932119 relative L2 0.0015453521627932787\n",
      "training 0.000995132140815258 relative L2 0.0015453429659828544\n",
      "training 0.0009951204992830753 relative L2 0.00154533376917243\n",
      "training 0.0009951087413355708 relative L2 0.0015453246887773275\n",
      "training 0.0009950968669727445 relative L2 0.0015453154919669032\n",
      "training 0.00099508510902524 relative L2 0.0015453066444024444\n",
      "training 0.000995073583908379 relative L2 0.0015452969819307327\n",
      "training 0.0009950620587915182 relative L2 0.0015452881343662739\n",
      "training 0.0009950503008440137 relative L2 0.0015452789375558496\n",
      "training 0.0009950385428965092 relative L2 0.0015452700899913907\n",
      "training 0.0009950271341949701 relative L2 0.0015452610095962882\n",
      "training 0.0009950152598321438 relative L2 0.0015452515799552202\n",
      "training 0.0009950035018846393 relative L2 0.001545242383144796\n",
      "training 0.0009949917439371347 relative L2 0.0015452337684109807\n",
      "training 0.0009949803352355957 relative L2 0.0015452246880158782\n",
      "training 0.0009949689265340567 relative L2 0.0015452156076207757\n",
      "training 0.000994957285001874 relative L2 0.001545206643640995\n",
      "training 0.0009949456434696913 relative L2 0.0015451975632458925\n",
      "training 0.0009949341183528304 relative L2 0.00154518848285079\n",
      "training 0.0009949227096512914 relative L2 0.0015451792860403657\n",
      "training 0.0009949107188731432 relative L2 0.0015451700892299414\n",
      "training 0.0009948991937562823 relative L2 0.001545160892419517\n",
      "training 0.0009948875522240996 relative L2 0.0015451518120244145\n",
      "training 0.0009948757942765951 relative L2 0.0015451429644599557\n",
      "training 0.0009948642691597342 relative L2 0.001545133301988244\n",
      "training 0.000994852394796908 relative L2 0.0015451248036697507\n",
      "training 0.000994840869680047 relative L2 0.0015451152576133609\n",
      "training 0.0009948291117325425 relative L2 0.001545106410048902\n",
      "training 0.000994817353785038 relative L2 0.0015450973296537995\n",
      "training 0.0009948057122528553 relative L2 0.0015450880164280534\n",
      "training 0.0009947941871359944 relative L2 0.0015450796345248818\n",
      "training 0.000994783011265099 relative L2 0.001545070088468492\n",
      "training 0.0009947710204869509 relative L2 0.0015450607752427459\n",
      "training 0.00099475949537009 relative L2 0.0015450522769242525\n",
      "training 0.0009947478538379073 relative L2 0.0015450427308678627\n",
      "training 0.000994735979475081 relative L2 0.0015450336504727602\n",
      "training 0.0009947242215275764 relative L2 0.0015450248029083014\n",
      "training 0.0009947129292413592 relative L2 0.001545015606097877\n",
      "training 0.000994701636955142 relative L2 0.0015450071077793837\n",
      "training 0.0009946901118382812 relative L2 0.0015449980273842812\n",
      "training 0.0009946785867214203 relative L2 0.0015449888305738568\n",
      "training 0.0009946670616045594 relative L2 0.0015449797501787543\n",
      "training 0.000994655187241733 relative L2 0.0015449704369530082\n",
      "training 0.0009946436621248722 relative L2 0.0015449615893885493\n",
      "training 0.0009946319041773677 relative L2 0.0015449525089934468\n",
      "training 0.0009946201462298632 relative L2 0.0015449433121830225\n",
      "training 0.000994608853943646 relative L2 0.0015449344646185637\n",
      "training 0.0009945972124114633 relative L2 0.0015449253842234612\n",
      "training 0.0009945856872946024 relative L2 0.0015449163038283587\n",
      "training 0.0009945739293470979 relative L2 0.0015449071070179343\n",
      "training 0.0009945621713995934 relative L2 0.00154489791020751\n",
      "training 0.0009945505298674107 relative L2 0.001544888480566442\n",
      "training 0.0009945386555045843 relative L2 0.0015448799822479486\n",
      "training 0.0009945272468030453 relative L2 0.0015448711346834898\n",
      "training 0.000994515954516828 relative L2 0.0015448624035343528\n",
      "training 0.0009945046622306108 relative L2 0.0015448532067239285\n",
      "training 0.00099449313711375 relative L2 0.001544844126328826\n",
      "training 0.0009944816119968891 relative L2 0.0015448349295184016\n",
      "training 0.0009944700868800282 relative L2 0.0015448258491232991\n",
      "training 0.0009944584453478456 relative L2 0.0015448170015588403\n",
      "training 0.0009944469202309847 relative L2 0.001544807804748416\n",
      "training 0.0009944350458681583 relative L2 0.0015447987243533134\n",
      "training 0.0009944237535819411 relative L2 0.0015447895275428891\n",
      "training 0.0009944121120497584 relative L2 0.0015447806799784303\n",
      "training 0.0009944007033482194 relative L2 0.0015447715995833278\n",
      "training 0.0009943890618160367 relative L2 0.0015447624027729034\n",
      "training 0.000994377420283854 relative L2 0.0015447532059624791\n",
      "training 0.0009943656623363495 relative L2 0.0015447437763214111\n",
      "training 0.000994353904388845 relative L2 0.0015447349287569523\n",
      "training 0.000994342495687306 relative L2 0.0015447260811924934\n",
      "training 0.000994330970570445 relative L2 0.001544717000797391\n",
      "training 0.0009943194454535842 relative L2 0.001544708153232932\n",
      "training 0.0009943080367520452 relative L2 0.0015446989564225078\n",
      "training 0.0009942962788045406 relative L2 0.001544690108858049\n",
      "training 0.0009942847536876798 relative L2 0.0015446807956323028\n",
      "training 0.0009942733449861407 relative L2 0.0015446715988218784\n",
      "training 0.0009942614706233144 relative L2 0.001544662402011454\n",
      "training 0.0009942498290911317 relative L2 0.0015446539036929607\n",
      "training 0.0009942387696355581 relative L2 0.0015446447068825364\n",
      "training 0.0009942270116880536 relative L2 0.0015446358593180776\n",
      "training 0.0009942154865711927 relative L2 0.0015446264296770096\n",
      "training 0.0009942037286236882 relative L2 0.0015446175821125507\n",
      "training 0.0009941925527527928 relative L2 0.0015446085017174482\n",
      "training 0.000994181027635932 relative L2 0.001544599304907024\n",
      "training 0.0009941690368577838 relative L2 0.0015445902245119214\n",
      "training 0.000994157511740923 relative L2 0.0015445813769474626\n",
      "training 0.0009941461030393839 relative L2 0.0015445725293830037\n",
      "training 0.0009941348107531667 relative L2 0.0015445633325725794\n",
      "training 0.0009941234020516276 relative L2 0.0015445540193468332\n",
      "training 0.000994111760519445 relative L2 0.001544545404613018\n",
      "training 0.000994100351817906 relative L2 0.00154453597497195\n",
      "training 0.0009940887102857232 relative L2 0.0015445268945768476\n",
      "training 0.0009940769523382187 relative L2 0.0015445180470123887\n",
      "training 0.0009940654272213578 relative L2 0.001544509083032608\n",
      "training 0.000994053902104497 relative L2 0.0015444997698068619\n",
      "training 0.0009940426098182797 relative L2 0.0015444911550730467\n",
      "training 0.0009940310847014189 relative L2 0.0015444820746779442\n",
      "training 0.0009940193267539144 relative L2 0.0015444729942828417\n",
      "training 0.0009940078016370535 relative L2 0.0015444637974724174\n",
      "training 0.000993996043689549 relative L2 0.0015444549499079585\n",
      "training 0.0009939844021573663 relative L2 0.0015444457530975342\n",
      "training 0.0009939728770405054 relative L2 0.0015444370219483972\n",
      "training 0.0009939613519236445 relative L2 0.0015444281743839383\n",
      "training 0.000993950292468071 relative L2 0.0015444193268194795\n",
      "training 0.0009939387673512101 relative L2 0.0015444104792550206\n",
      "training 0.0009939277078956366 relative L2 0.0015444016316905618\n",
      "training 0.0009939165320247412 relative L2 0.001544392784126103\n",
      "training 0.0009939048904925585 relative L2 0.001544383354485035\n",
      "training 0.0009938932489603758 relative L2 0.0015443748561665416\n",
      "training 0.0009938819566741586 relative L2 0.001544365775771439\n",
      "training 0.0009938706643879414 relative L2 0.0015443568117916584\n",
      "training 0.0009938591392710805 relative L2 0.001544347731396556\n",
      "training 0.0009938477305695415 relative L2 0.0015443390002474189\n",
      "training 0.0009938364382833242 relative L2 0.0015443298034369946\n",
      "training 0.0009938249131664634 relative L2 0.0015443209558725357\n",
      "training 0.0009938132716342807 relative L2 0.0015443118754774332\n",
      "training 0.0009938017465174198 relative L2 0.0015443029114976525\n",
      "training 0.0009937904542312026 relative L2 0.0015442939475178719\n",
      "training 0.0009937791619449854 relative L2 0.0015442853327840567\n",
      "training 0.0009937681024894118 relative L2 0.0015442768344655633\n",
      "training 0.0009937570430338383 relative L2 0.0015442674048244953\n",
      "training 0.0009937455179169774 relative L2 0.0015442587900906801\n",
      "training 0.0009937342256307602 relative L2 0.0015442498261108994\n",
      "training 0.0009937228169292212 relative L2 0.0015442409785464406\n",
      "training 0.0009937114082276821 relative L2 0.00154423201456666\n",
      "training 0.000993699999526143 relative L2 0.001544223283417523\n",
      "training 0.0009936888236552477 relative L2 0.0015442140866070986\n",
      "training 0.0009936772985383868 relative L2 0.0015442053554579616\n",
      "training 0.0009936658898368478 relative L2 0.0015441965078935027\n",
      "training 0.0009936544811353087 relative L2 0.0015441873110830784\n",
      "training 0.0009936431888490915 relative L2 0.0015441784635186195\n",
      "training 0.0009936316637322307 relative L2 0.0015441692667081952\n",
      "training 0.000993620022200048 relative L2 0.0015441603027284145\n",
      "training 0.000993608497083187 relative L2 0.0015441513387486339\n",
      "training 0.000993597088381648 relative L2 0.0015441422583535314\n",
      "training 0.0009935857960954309 relative L2 0.0015441335272043943\n",
      "training 0.0009935743873938918 relative L2 0.00154412433039397\n",
      "training 0.000993562862277031 relative L2 0.0015441154828295112\n",
      "training 0.0009935515699908137 relative L2 0.0015441064024344087\n",
      "training 0.0009935400448739529 relative L2 0.0015440975548699498\n",
      "training 0.000993528519757092 relative L2 0.0015440885908901691\n",
      "training 0.000993517111055553 relative L2 0.0015440796269103885\n",
      "training 0.000993505702354014 relative L2 0.0015440707793459296\n",
      "training 0.0009934946428984404 relative L2 0.001544061815366149\n",
      "training 0.0009934831177815795 relative L2 0.00154405296780169\n",
      "training 0.0009934717090800405 relative L2 0.0015440440038219094\n",
      "training 0.000993460533209145 relative L2 0.001544034807011485\n",
      "training 0.0009934490080922842 relative L2 0.0015440259594470263\n",
      "training 0.0009934371337294579 relative L2 0.001544016762636602\n",
      "training 0.0009934257250279188 relative L2 0.001544008031487465\n",
      "training 0.0009934143163263798 relative L2 0.0015439988346770406\n",
      "training 0.000993402791209519 relative L2 0.0015439901035279036\n",
      "training 0.0009933913825079799 relative L2 0.0015439811395481229\n",
      "training 0.0009933799738064408 relative L2 0.001543972291983664\n",
      "training 0.00099336844868958 relative L2 0.0015439632115885615\n",
      "training 0.000993357039988041 relative L2 0.0015439537819474936\n",
      "training 0.00099334551487118 relative L2 0.0015439449343830347\n",
      "training 0.0009933339897543192 relative L2 0.0015439363196492195\n",
      "training 0.000993322697468102 relative L2 0.001543927239254117\n",
      "training 0.000993311288766563 relative L2 0.0015439180424436927\n",
      "training 0.0009933002293109894 relative L2 0.0015439093112945557\n",
      "training 0.0009932888206094503 relative L2 0.0015439002308994532\n",
      "training 0.0009932774119079113 relative L2 0.0015438912669196725\n",
      "training 0.0009932660032063723 relative L2 0.0015438825357705355\n",
      "training 0.000993254710920155 relative L2 0.0015438736882060766\n",
      "training 0.0009932434186339378 relative L2 0.0015438648406416178\n",
      "training 0.0009932321263477206 relative L2 0.001543855993077159\n",
      "training 0.000993221066892147 relative L2 0.0015438471455127\n",
      "training 0.000993209658190608 relative L2 0.001543838414363563\n",
      "training 0.0009931984823197126 relative L2 0.0015438293339684606\n",
      "training 0.0009931871900334954 relative L2 0.0015438207192346454\n",
      "training 0.0009931757813319564 relative L2 0.001543811522424221\n",
      "training 0.0009931642562150955 relative L2 0.0015438024420291185\n",
      "training 0.0009931529639288783 relative L2 0.001543793361634016\n",
      "training 0.0009931415552273393 relative L2 0.0015437847469002008\n",
      "training 0.0009931303793564439 relative L2 0.001543775899335742\n",
      "training 0.000993118854239583 relative L2 0.0015437668189406395\n",
      "training 0.0009931075619533658 relative L2 0.001543757738545537\n",
      "training 0.0009930958040058613 relative L2 0.001543748308904469\n",
      "training 0.0009930843953043222 relative L2 0.0015437393449246883\n",
      "training 0.0009930728701874614 relative L2 0.001543730846606195\n",
      "training 0.0009930615779012442 relative L2 0.001543722115457058\n",
      "training 0.0009930506348609924 relative L2 0.0015437130350619555\n",
      "training 0.0009930393425747752 relative L2 0.0015437044203281403\n",
      "training 0.000993028050288558 relative L2 0.0015436955727636814\n",
      "training 0.0009930167580023408 relative L2 0.001543686492368579\n",
      "training 0.0009930053493008018 relative L2 0.0015436774119734764\n",
      "training 0.0009929938241839409 relative L2 0.0015436687972396612\n",
      "training 0.0009929825318977237 relative L2 0.0015436596004292369\n",
      "training 0.0009929711231961846 relative L2 0.0015436505200341344\n",
      "training 0.0009929598309099674 relative L2 0.0015436417888849974\n",
      "training 0.0009929485386237502 relative L2 0.0015436329413205385\n",
      "training 0.0009929371299222112 relative L2 0.0015436240937560797\n",
      "training 0.000992925837635994 relative L2 0.0015436150133609772\n",
      "training 0.000992914428934455 relative L2 0.0015436060493811965\n",
      "training 0.0009929027874022722 relative L2 0.0015435968525707722\n",
      "training 0.000992891495116055 relative L2 0.0015435878885909915\n",
      "training 0.0009928799699991941 relative L2 0.0015435792738571763\n",
      "training 0.0009928689105436206 relative L2 0.0015435704262927175\n",
      "training 0.0009928576182574034 relative L2 0.001543561345897615\n",
      "training 0.0009928463259711862 relative L2 0.001543552614748478\n",
      "training 0.0009928351501002908 relative L2 0.001543543767184019\n",
      "training 0.0009928237413987517 relative L2 0.0015435349196195602\n",
      "training 0.0009928126819431782 relative L2 0.0015435260720551014\n",
      "training 0.0009928010404109955 relative L2 0.0015435174573212862\n",
      "training 0.0009927900973707438 relative L2 0.0015435084933415055\n",
      "training 0.0009927788050845265 relative L2 0.0015434996457770467\n",
      "training 0.0009927673963829875 relative L2 0.001543490681797266\n",
      "training 0.0009927559876814485 relative L2 0.001543481950648129\n",
      "training 0.000992744811810553 relative L2 0.0015434728702530265\n",
      "training 0.0009927336359396577 relative L2 0.0015434642555192113\n",
      "training 0.0009927221108227968 relative L2 0.0015434548258781433\n",
      "training 0.0009927107021212578 relative L2 0.0015434459783136845\n",
      "training 0.0009926994098350406 relative L2 0.0015434372471645474\n",
      "training 0.0009926882339641452 relative L2 0.0015434283996000886\n",
      "training 0.0009926770580932498 relative L2 0.0015434195520356297\n",
      "training 0.0009926656493917108 relative L2 0.001543410704471171\n",
      "training 0.0009926544735208154 relative L2 0.0015434019733220339\n",
      "training 0.0009926430648192763 relative L2 0.0015433927765116096\n",
      "training 0.0009926316561177373 relative L2 0.0015433841617777944\n",
      "training 0.0009926205966621637 relative L2 0.0015433751977980137\n",
      "training 0.0009926091879606247 relative L2 0.0015433665830641985\n",
      "training 0.0009925977792590857 relative L2 0.001543357502669096\n",
      "training 0.0009925866033881903 relative L2 0.0015433486551046371\n",
      "training 0.0009925755439326167 relative L2 0.0015433395747095346\n",
      "training 0.000992563902400434 relative L2 0.0015433309599757195\n",
      "training 0.0009925527265295386 relative L2 0.001543321879580617\n",
      "training 0.0009925412014126778 relative L2 0.0015433127991855145\n",
      "training 0.0009925297927111387 relative L2 0.0015433045336976647\n",
      "training 0.0009925187332555652 relative L2 0.0015432951040565968\n",
      "training 0.0009925072081387043 relative L2 0.0015432864893227816\n",
      "training 0.000992495915852487 relative L2 0.0015432779910042882\n",
      "training 0.0009924847399815917 relative L2 0.0015432689106091857\n",
      "training 0.0009924736805260181 relative L2 0.0015432605287060142\n",
      "training 0.0009924626210704446 relative L2 0.001543251215480268\n",
      "training 0.0009924513287842274 relative L2 0.0015432427171617746\n",
      "training 0.0009924398036673665 relative L2 0.0015432338695973158\n",
      "training 0.0009924286277964711 relative L2 0.0015432247892022133\n",
      "training 0.0009924175683408976 relative L2 0.0015432164072990417\n",
      "training 0.0009924063924700022 relative L2 0.0015432070940732956\n",
      "training 0.0009923952165991068 relative L2 0.001543198712170124\n",
      "training 0.0009923841571435332 relative L2 0.0015431895153596997\n",
      "training 0.000992372864857316 relative L2 0.0015431807842105627\n",
      "training 0.0009923616889864206 relative L2 0.0015431721694767475\n",
      "training 0.000992350629530847 relative L2 0.0015431629726663232\n",
      "training 0.0009923391044139862 relative L2 0.0015431544743478298\n",
      "training 0.000992327812127769 relative L2 0.0015431453939527273\n",
      "training 0.0009923167526721954 relative L2 0.0015431370120495558\n",
      "training 0.0009923055768013 relative L2 0.0015431283973157406\n",
      "training 0.000992294866591692 relative L2 0.001543119316920638\n",
      "training 0.0009922835743054748 relative L2 0.0015431107021868229\n",
      "training 0.0009922722820192575 relative L2 0.0015431016217917204\n",
      "training 0.0009922609897330403 relative L2 0.0015430928906425834\n",
      "training 0.0009922496974468231 relative L2 0.0015430839266628027\n",
      "training 0.000992238405160606 relative L2 0.0015430754283443093\n",
      "training 0.0009922271128743887 relative L2 0.0015430663479492068\n",
      "training 0.0009922158205881715 relative L2 0.0015430572675541043\n",
      "training 0.000992204761132598 relative L2 0.0015430486528202891\n",
      "training 0.0009921934688463807 relative L2 0.0015430395724251866\n",
      "training 0.0009921819437295198 relative L2 0.0015430301427841187\n",
      "training 0.000992170418612659 relative L2 0.0015430212952196598\n",
      "training 0.0009921591263264418 relative L2 0.0015430120984092355\n",
      "training 0.000992147601209581 relative L2 0.0015430036000907421\n",
      "training 0.0009921365417540073 relative L2 0.0015429944032803178\n",
      "training 0.0009921250166371465 relative L2 0.0015429859049618244\n",
      "training 0.0009921137243509293 relative L2 0.0015429771738126874\n",
      "training 0.0009921026648953557 relative L2 0.001542968093417585\n",
      "training 0.0009920913726091385 relative L2 0.001542959245853126\n",
      "training 0.0009920798474922776 relative L2 0.0015429502818733454\n",
      "training 0.000992068788036704 relative L2 0.0015429415507242084\n",
      "training 0.0009920576121658087 relative L2 0.0015429327031597495\n",
      "training 0.0009920465527102351 relative L2 0.0015429242048412561\n",
      "training 0.000992035144008696 relative L2 0.0015429150080308318\n",
      "training 0.0009920238517224789 relative L2 0.0015429062768816948\n",
      "training 0.0009920127922669053 relative L2 0.0015428976621478796\n",
      "training 0.000992001499980688 relative L2 0.001542888581752777\n",
      "training 0.0009919899748638272 relative L2 0.00154287985060364\n",
      "training 0.00099197868257761 relative L2 0.0015428708866238594\n",
      "training 0.0009919673902913928 relative L2 0.0015428621554747224\n",
      "training 0.000991956447251141 relative L2 0.001542853657156229\n",
      "training 0.0009919452713802457 relative L2 0.0015428444603458047\n",
      "training 0.0009919339790940285 relative L2 0.0015428359620273113\n",
      "training 0.0009919226868078113 relative L2 0.0015428265323862433\n",
      "training 0.000991911394521594 relative L2 0.0015428182668983936\n",
      "training 0.0009919001022353768 relative L2 0.001542809302918613\n",
      "training 0.0009918888099491596 relative L2 0.0015428006881847978\n",
      "training 0.000991877750493586 relative L2 0.0015427923062816262\n",
      "training 0.0009918672731146216 relative L2 0.001542783109471202\n",
      "training 0.0009918558644130826 relative L2 0.001542774960398674\n",
      "training 0.000991844804957509 relative L2 0.0015427657635882497\n",
      "training 0.0009918335126712918 relative L2 0.0015427572652697563\n",
      "training 0.0009918222203850746 relative L2 0.0015427478356286883\n",
      "training 0.0009918108116835356 relative L2 0.0015427389880642295\n",
      "training 0.0009917994029819965 relative L2 0.0015427308389917016\n",
      "training 0.000991788343526423 relative L2 0.0015427216421812773\n",
      "training 0.0009917772840708494 relative L2 0.0015427129110321403\n",
      "training 0.0009917658753693104 relative L2 0.001542703714221716\n",
      "training 0.0009917544666677713 relative L2 0.0015426950994879007\n",
      "training 0.0009917431743815541 relative L2 0.00154268613550812\n",
      "training 0.000991731882095337 relative L2 0.0015426771715283394\n",
      "training 0.0009917208226397634 relative L2 0.001542668673209846\n",
      "training 0.0009917092975229025 relative L2 0.0015426594763994217\n",
      "training 0.0009916980052366853 relative L2 0.0015426508616656065\n",
      "training 0.0009916868293657899 relative L2 0.0015426420141011477\n",
      "training 0.0009916756534948945 relative L2 0.0015426333993673325\n",
      "training 0.0009916647104546428 relative L2 0.0015426245518028736\n",
      "training 0.0009916534181684256 relative L2 0.0015426152385771275\n",
      "training 0.0009916420094668865 relative L2 0.001542606740258634\n",
      "training 0.0009916307171806693 relative L2 0.001542598125524819\n",
      "training 0.0009916196577250957 relative L2 0.0015425896272063255\n",
      "training 0.000991608714684844 relative L2 0.0015425808960571885\n",
      "training 0.0009915977716445923 relative L2 0.0015425720484927297\n",
      "training 0.000991586479358375 relative L2 0.0015425634337589145\n",
      "training 0.0009915756527334452 relative L2 0.001542554935440421\n",
      "training 0.0009915644768625498 relative L2 0.0015425458550453186\n",
      "training 0.0009915531845763326 relative L2 0.001542536774650216\n",
      "training 0.0009915420087054372 relative L2 0.001542528159916401\n",
      "training 0.0009915306000038981 relative L2 0.0015425194287672639\n",
      "training 0.0009915194241330028 relative L2 0.001542510581202805\n",
      "training 0.0009915081318467855 relative L2 0.001542501850053668\n",
      "training 0.0009914968395605683 relative L2 0.0015424932353198528\n",
      "training 0.0009914858965203166 relative L2 0.0015424845041707158\n",
      "training 0.0009914746042340994 relative L2 0.0015424758894369006\n",
      "training 0.000991463428363204 relative L2 0.0015424664597958326\n",
      "training 0.0009914522524923086 relative L2 0.0015424579614773393\n",
      "training 0.0009914409602060914 relative L2 0.0015424491139128804\n",
      "training 0.0009914299007505178 relative L2 0.0015424409648403525\n",
      "training 0.0009914193069562316 relative L2 0.00154243188444525\n",
      "training 0.0009914080146700144 relative L2 0.0015424233861267567\n",
      "training 0.0009913969552144408 relative L2 0.0015424143057316542\n",
      "training 0.0009913857793435454 relative L2 0.0015424058074131608\n",
      "training 0.0009913748363032937 relative L2 0.0015423971926793456\n",
      "training 0.0009913635440170765 relative L2 0.0015423883451148868\n",
      "training 0.000991352368146181 relative L2 0.0015423799632117152\n",
      "training 0.0009913414251059294 relative L2 0.0015423711156472564\n",
      "training 0.0009913305984809995 relative L2 0.0015423629665747285\n",
      "training 0.0009913196554407477 relative L2 0.0015423537697643042\n",
      "training 0.0009913085959851742 relative L2 0.0015423450386151671\n",
      "training 0.0009912974201142788 relative L2 0.0015423358418047428\n",
      "training 0.0009912860114127398 relative L2 0.001542326994240284\n",
      "training 0.0009912749519571662 relative L2 0.0015423183795064688\n",
      "training 0.0009912638925015926 relative L2 0.001542309415526688\n",
      "training 0.000991252833046019 relative L2 0.0015423009172081947\n",
      "training 0.0009912418900057673 relative L2 0.0015422920696437359\n",
      "training 0.0009912308305501938 relative L2 0.0015422835713252425\n",
      "training 0.0009912197710946202 relative L2 0.0015422749565914273\n",
      "training 0.000991208478808403 relative L2 0.0015422661090269685\n",
      "training 0.0009911971865221858 relative L2 0.0015422573778778315\n",
      "training 0.000991186243481934 relative L2 0.001542248297482729\n",
      "training 0.0009911749511957169 relative L2 0.0015422396827489138\n",
      "training 0.0009911637753248215 relative L2 0.0015422309515997767\n",
      "training 0.000991152599453926 relative L2 0.0015422223368659616\n",
      "training 0.0009911414235830307 relative L2 0.0015422136057168245\n",
      "training 0.000991130480542779 relative L2 0.0015422048745676875\n",
      "training 0.0009911195375025272 relative L2 0.0015421962598338723\n",
      "training 0.0009911087108775973 relative L2 0.0015421871794387698\n",
      "training 0.0009910974185913801 relative L2 0.0015421786811202765\n",
      "training 0.0009910862427204847 relative L2 0.001542169600725174\n",
      "training 0.000991075299680233 relative L2 0.0015421611024066806\n",
      "training 0.0009910641238093376 relative L2 0.0015421526040881872\n",
      "training 0.0009910531807690859 relative L2 0.001542143989354372\n",
      "training 0.000991042354144156 relative L2 0.001542135258205235\n",
      "training 0.000991031643934548 relative L2 0.0015421264106407762\n",
      "training 0.0009910205844789743 relative L2 0.001542117795906961\n",
      "training 0.0009910090593621135 relative L2 0.0015421092975884676\n",
      "training 0.0009909982327371836 relative L2 0.0015421011485159397\n",
      "training 0.000990987871773541 relative L2 0.0015420920681208372\n",
      "training 0.0009909768123179674 relative L2 0.0015420835698023438\n",
      "training 0.0009909657528623939 relative L2 0.0015420748386532068\n",
      "training 0.0009909546934068203 relative L2 0.0015420662239193916\n",
      "training 0.000990943517535925 relative L2 0.0015420571435242891\n",
      "training 0.0009909321088343859 relative L2 0.0015420479467138648\n",
      "training 0.000990920583717525 relative L2 0.001542039099149406\n",
      "training 0.0009909095242619514 relative L2 0.0015420306008309126\n",
      "training 0.0009908984648063779 relative L2 0.0015420217532664537\n",
      "training 0.0009908874053508043 relative L2 0.0015420132549479604\n",
      "training 0.0009908764623105526 relative L2 0.0015420039417222142\n",
      "training 0.0009908650536090136 relative L2 0.0015419954434037209\n",
      "training 0.0009908541105687618 relative L2 0.001541986595839262\n",
      "training 0.00099084316752851 relative L2 0.0015419779811054468\n",
      "training 0.0009908319916576147 relative L2 0.001541969133540988\n",
      "training 0.0009908208157867193 relative L2 0.001541960402391851\n",
      "training 0.0009908101055771112 relative L2 0.0015419519040733576\n",
      "training 0.000990798813290894 relative L2 0.0015419430565088987\n",
      "training 0.0009907877538353205 relative L2 0.0015419346746057272\n",
      "training 0.0009907768107950687 relative L2 0.001541926059871912\n",
      "training 0.000990765867754817 relative L2 0.0015419175615534186\n",
      "training 0.0009907546918839216 relative L2 0.0015419084811583161\n",
      "training 0.0009907435160130262 relative L2 0.0015418999828398228\n",
      "training 0.0009907323401421309 relative L2 0.0015418909024447203\n",
      "training 0.0009907212806865573 relative L2 0.0015418828697875142\n",
      "training 0.000990710686892271 relative L2 0.0015418739058077335\n",
      "training 0.0009906996274366975 relative L2 0.0015418654074892402\n",
      "training 0.000990688567981124 relative L2 0.001541856792755425\n",
      "training 0.0009906776249408722 relative L2 0.0015418479451909661\n",
      "training 0.0009906664490699768 relative L2 0.0015418395632877946\n",
      "training 0.0009906553896144032 relative L2 0.0015418303664773703\n",
      "training 0.0009906446794047952 relative L2 0.001541822450235486\n",
      "training 0.0009906336199492216 relative L2 0.0015418131370097399\n",
      "training 0.0009906227933242917 relative L2 0.0015418048715218902\n",
      "training 0.0009906116174533963 relative L2 0.0015417960239574313\n",
      "training 0.000990600441582501 relative L2 0.001541787525638938\n",
      "training 0.0009905894985422492 relative L2 0.0015417784452438354\n",
      "training 0.0009905785555019975 relative L2 0.0015417694812640548\n",
      "training 0.0009905670303851366 relative L2 0.001541760517284274\n",
      "training 0.0009905558545142412 relative L2 0.001541751902550459\n",
      "training 0.0009905446786433458 relative L2 0.0015417434042319655\n",
      "training 0.000990533735603094 relative L2 0.0015417345566675067\n",
      "training 0.0009905227925628424 relative L2 0.0015417258255183697\n",
      "training 0.0009905117331072688 relative L2 0.0015417169779539108\n",
      "training 0.0009905006736516953 relative L2 0.0015417084796354175\n",
      "training 0.000990489381365478 relative L2 0.0015416996320709586\n",
      "training 0.0009904782054945827 relative L2 0.0015416909009218216\n",
      "training 0.000990467146039009 relative L2 0.0015416820533573627\n",
      "training 0.0009904560865834355 relative L2 0.001541673787869513\n",
      "training 0.0009904452599585056 relative L2 0.0015416649403050542\n",
      "training 0.0009904339676722884 relative L2 0.0015416559763252735\n",
      "training 0.0009904229082167149 relative L2 0.0015416474780067801\n",
      "training 0.000990412081591785 relative L2 0.0015416386304423213\n",
      "training 0.0009904010221362114 relative L2 0.001541630132123828\n",
      "training 0.0009903899626806378 relative L2 0.0015416215173900127\n",
      "training 0.0009903789032250643 relative L2 0.001541612669825554\n",
      "training 0.0009903679601848125 relative L2 0.0015416041715070605\n",
      "training 0.000990356900729239 relative L2 0.0015415952075272799\n",
      "training 0.0009903457248583436 relative L2 0.0015415865927934647\n",
      "training 0.0009903345489874482 relative L2 0.0015415778616443276\n",
      "training 0.0009903237223625183 relative L2 0.0015415693633258343\n",
      "training 0.000990312546491623 relative L2 0.0015415605157613754\n",
      "training 0.0009903014870360494 relative L2 0.0015415519010275602\n",
      "training 0.0009902906604111195 relative L2 0.0015415431698784232\n",
      "training 0.000990279600955546 relative L2 0.0015415349043905735\n",
      "training 0.000990268774330616 relative L2 0.0015415259404107928\n",
      "training 0.0009902575984597206 relative L2 0.0015415174420922995\n",
      "training 0.000990246539004147 relative L2 0.0015415084781125188\n",
      "training 0.0009902353631332517 relative L2 0.0015414999797940254\n",
      "training 0.0009902241872623563 relative L2 0.0015414912486448884\n",
      "training 0.0009902132442221045 relative L2 0.0015414826339110732\n",
      "training 0.000990202184766531 relative L2 0.0015414741355925798\n",
      "training 0.000990191358141601 relative L2 0.0015414656372740865\n",
      "training 0.000990180647931993 relative L2 0.001541457255370915\n",
      "training 0.0009901697048917413 relative L2 0.001541448407806456\n",
      "training 0.0009901589946821332 relative L2 0.0015414402587339282\n",
      "training 0.0009901479352265596 relative L2 0.0015414310619235039\n",
      "training 0.000990136875770986 relative L2 0.0015414226800203323\n",
      "training 0.0009901258163154125 relative L2 0.0015414137160405517\n",
      "training 0.0009901148732751608 relative L2 0.0015414053341373801\n",
      "training 0.000990103930234909 relative L2 0.001541396719403565\n",
      "training 0.0009900928707793355 relative L2 0.001541387871839106\n",
      "training 0.000990081811323762 relative L2 0.0015413794899359345\n",
      "training 0.000990070984698832 relative L2 0.0015413702931255102\n",
      "training 0.0009900600416585803 relative L2 0.0015413622604683042\n",
      "training 0.000990048865787685 relative L2 0.0015413530636578798\n",
      "training 0.0009900379227474332 relative L2 0.0015413445653393865\n",
      "training 0.0009900268632918596 relative L2 0.0015413359506055713\n",
      "training 0.000990015803836286 relative L2 0.0015413275687023997\n",
      "training 0.0009900052100419998 relative L2 0.0015413189539685845\n",
      "training 0.00098999438341707 relative L2 0.0015413102228194475\n",
      "training 0.0009899830911308527 relative L2 0.0015413019573315978\n",
      "training 0.0009899723809212446 relative L2 0.0015412932261824608\n",
      "training 0.0009899615542963147 relative L2 0.001541284378618002\n",
      "training 0.0009899504948407412 relative L2 0.0015412758802995086\n",
      "training 0.0009899394353851676 relative L2 0.001541267498396337\n",
      "training 0.0009899288415908813 relative L2 0.0015412590000778437\n",
      "training 0.0009899181313812733 relative L2 0.0015412505017593503\n",
      "training 0.0009899071883410215 relative L2 0.001541241887025535\n",
      "training 0.0009898960124701262 relative L2 0.0015412335051223636\n",
      "training 0.0009898851858451962 relative L2 0.0015412248903885484\n",
      "training 0.0009898741263896227 relative L2 0.001541216392070055\n",
      "training 0.0009898628341034055 relative L2 0.0015412073116749525\n",
      "training 0.000989851774647832 relative L2 0.0015411990461871028\n",
      "training 0.0009898411808535457 relative L2 0.0015411900822073221\n",
      "training 0.0009898303542286158 relative L2 0.0015411821659654379\n",
      "training 0.0009898195276036859 relative L2 0.001541173318400979\n",
      "training 0.0009898085845634341 relative L2 0.0015411644708365202\n",
      "training 0.0009897976415231824 relative L2 0.0015411557396873832\n",
      "training 0.0009897865820676088 relative L2 0.0015411474741995335\n",
      "training 0.0009897758718580008 relative L2 0.00154113897588104\n",
      "training 0.0009897648124024272 relative L2 0.0015411305939778686\n",
      "training 0.0009897539857774973 relative L2 0.0015411217464134097\n",
      "training 0.0009897431591525674 relative L2 0.0015411132480949163\n",
      "training 0.0009897322161123157 relative L2 0.0015411044005304575\n",
      "training 0.0009897213894873857 relative L2 0.0015410961350426078\n",
      "training 0.0009897103300318122 relative L2 0.0015410870546475053\n",
      "training 0.000989699037745595 relative L2 0.0015410783234983683\n",
      "training 0.0009896880947053432 relative L2 0.001541069708764553\n",
      "training 0.0009896770352497697 relative L2 0.0015410608612000942\n",
      "training 0.000989666092209518 relative L2 0.0015410527121275663\n",
      "training 0.0009896550327539444 relative L2 0.0015410438645631075\n",
      "training 0.0009896442061290145 relative L2 0.001541035482659936\n",
      "training 0.0009896333795040846 relative L2 0.0015410268679261208\n",
      "training 0.0009896227857097983 relative L2 0.0015410183696076274\n",
      "training 0.0009896118426695466 relative L2 0.0015410094056278467\n",
      "training 0.0009896004339680076 relative L2 0.001541001140139997\n",
      "training 0.0009895896073430777 relative L2 0.00154099240899086\n",
      "training 0.000989579246379435 relative L2 0.0015409841435030103\n",
      "training 0.0009895681869238615 relative L2 0.001540975645184517\n",
      "training 0.0009895573602989316 relative L2 0.0015409671468660235\n",
      "training 0.0009895460680127144 relative L2 0.0015409586485475302\n",
      "training 0.0009895352413877845 relative L2 0.0015409499173983932\n",
      "training 0.0009895242983475327 relative L2 0.0015409414190798998\n",
      "training 0.0009895137045532465 relative L2 0.00154093315359205\n",
      "training 0.0009895029943436384 relative L2 0.0015409247716888785\n",
      "training 0.0009894921677187085 relative L2 0.0015409165062010288\n",
      "training 0.0009894814575091004 relative L2 0.0015409080078825355\n",
      "training 0.0009894703980535269 relative L2 0.0015408991603180766\n",
      "training 0.0009894594550132751 relative L2 0.001540890778414905\n",
      "training 0.000989448744803667 relative L2 0.0015408822800964117\n",
      "training 0.0009894381510093808 relative L2 0.0015408741310238838\n",
      "training 0.000989427324384451 relative L2 0.0015408651670441031\n",
      "training 0.0009894163813441992 relative L2 0.0015408570179715753\n",
      "training 0.0009894059039652348 relative L2 0.0015408482868224382\n",
      "training 0.000989394960924983 relative L2 0.0015408400213345885\n",
      "training 0.0009893839014694095 relative L2 0.0015408311737701297\n",
      "training 0.0009893730748444796 relative L2 0.0015408231411129236\n",
      "training 0.000989362015388906 relative L2 0.0015408142935484648\n",
      "training 0.0009893514215946198 relative L2 0.0015408057952299714\n",
      "training 0.0009893402457237244 relative L2 0.0015407969476655126\n",
      "training 0.000989329069852829 relative L2 0.0015407886821776628\n",
      "training 0.000989318359643221 relative L2 0.0015407799510285258\n",
      "training 0.0009893076494336128 relative L2 0.0015407720347866416\n",
      "training 0.000989296822808683 relative L2 0.0015407627215608954\n",
      "training 0.0009892858797684312 relative L2 0.0015407544560730457\n",
      "training 0.0009892749367281795 relative L2 0.0015407457249239087\n",
      "training 0.000989263877272606 relative L2 0.001540737459436059\n",
      "training 0.0009892531670629978 relative L2 0.0015407291939482093\n",
      "training 0.000989242224022746 relative L2 0.0015407204627990723\n",
      "training 0.0009892316302284598 relative L2 0.001540712546557188\n",
      "training 0.0009892210364341736 relative L2 0.0015407034661620855\n",
      "training 0.0009892103262245655 relative L2 0.0015406953170895576\n",
      "training 0.0009891993831843138 relative L2 0.0015406862366944551\n",
      "training 0.0009891885565593839 relative L2 0.001540678320452571\n",
      "training 0.0009891773806884885 relative L2 0.001540668890811503\n",
      "training 0.000989166204817593 relative L2 0.001540660741738975\n",
      "training 0.0009891551453620195 relative L2 0.0015406517777591944\n",
      "training 0.0009891442023217678 relative L2 0.001540643279440701\n",
      "training 0.0009891331428661942 relative L2 0.0015406347811222076\n",
      "training 0.0009891223162412643 relative L2 0.0015406260499730706\n",
      "training 0.0009891114896163344 relative L2 0.001540617784485221\n",
      "training 0.0009891004301607609 relative L2 0.0015406088205054402\n",
      "training 0.0009890894871205091 relative L2 0.0015406006714329123\n",
      "training 0.0009890783112496138 relative L2 0.001540591474622488\n",
      "training 0.0009890674846246839 relative L2 0.0015405835583806038\n",
      "training 0.000989056657999754 relative L2 0.001540574710816145\n",
      "training 0.0009890457149595022 relative L2 0.0015405663289129734\n",
      "training 0.0009890348883345723 relative L2 0.0015405573649331927\n",
      "training 0.000989023712463677 relative L2 0.0015405489830300212\n",
      "training 0.0009890127694234252 relative L2 0.001540540368296206\n",
      "training 0.000989002175629139 relative L2 0.0015405319863930345\n",
      "training 0.000988991349004209 relative L2 0.0015405231388285756\n",
      "training 0.0009889801731333137 relative L2 0.001540514873340726\n",
      "training 0.000988969230093062 relative L2 0.0015405059093609452\n",
      "training 0.0009889580542221665 relative L2 0.00154049729462713\n",
      "training 0.0009889468783512712 relative L2 0.0015404889127239585\n",
      "training 0.0009889359353110194 relative L2 0.0015404802979901433\n",
      "training 0.0009889252251014113 relative L2 0.0015404721489176154\n",
      "training 0.0009889142820611596 relative L2 0.001540462952107191\n",
      "training 0.000988903222605586 relative L2 0.0015404551522806287\n",
      "training 0.0009888921631500125 relative L2 0.0015404457226395607\n",
      "training 0.0009888812201097608 relative L2 0.0015404385048896074\n",
      "training 0.0009888706263154745 relative L2 0.0015404283767566085\n",
      "training 0.0009888599161058664 relative L2 0.0015404222067445517\n",
      "training 0.000988849438726902 relative L2 0.0015404104487970471\n",
      "training 0.0009888401255011559 relative L2 0.0015404215082526207\n",
      "training 0.0009888396598398685 relative L2 0.0015404440928250551\n",
      "training 0.0009889090433716774 relative L2 0.0015409981133416295\n",
      "training 0.0009895239491015673 relative L2 0.0015445022145286202\n",
      "training 0.0009944163030013442 relative L2 0.00157420733012259\n",
      "training 0.0010328699136152864 relative L2 0.0017834476893767715\n",
      "training 0.0013332845410332084 relative L2 0.0029395294841378927\n",
      "training 0.0036442542914301157 relative L2 0.0067227850668132305\n",
      "training 0.019166935235261917 relative L2 0.012904028408229351\n",
      "training 0.07066015154123306 relative L2 0.009244874119758606\n",
      "training 0.03625475615262985 relative L2 0.003422429086640477\n",
      "training 0.004956752993166447 relative L2 0.010109780356287956\n",
      "training 0.04335755854845047 relative L2 0.0019288202747702599\n",
      "training 0.0015623342478647828 relative L2 0.008832982741296291\n",
      "training 0.03309459239244461 relative L2 0.004316761624068022\n",
      "training 0.007882792502641678 relative L2 0.006940304767340422\n",
      "training 0.020415276288986206 relative L2 0.004468454048037529\n",
      "training 0.008459365926682949 relative L2 0.0056454772129654884\n",
      "training 0.013511423021554947 relative L2 0.004602106753736734\n",
      "training 0.008961138315498829 relative L2 0.004981308709830046\n",
      "training 0.010502536781132221 relative L2 0.004046066664159298\n",
      "training 0.006931230425834656 relative L2 0.004470425192266703\n",
      "training 0.008464955724775791 relative L2 0.003670117584988475\n",
      "training 0.005688964854925871 relative L2 0.0042777396738529205\n",
      "training 0.007737015374004841 relative L2 0.0030089234933257103\n",
      "training 0.0038239904679358006 relative L2 0.004084858577698469\n",
      "training 0.007064257748425007 relative L2 0.002609055722132325\n",
      "training 0.002862689783796668 relative L2 0.003933911677449942\n",
      "training 0.006538368295878172 relative L2 0.002097860909998417\n",
      "training 0.0018473705276846886 relative L2 0.0037719998508691788\n",
      "training 0.006020412314683199 relative L2 0.001868529710918665\n",
      "training 0.0014577781548723578 relative L2 0.003539269557222724\n",
      "training 0.005287792067974806 relative L2 0.0017031331080943346\n",
      "training 0.0012089998926967382 relative L2 0.0033115956466645002\n",
      "training 0.004636106546968222 relative L2 0.0016954044112935662\n",
      "training 0.001198262325488031 relative L2 0.003023123135790229\n",
      "training 0.003852369962260127 relative L2 0.0017766213277354836\n",
      "training 0.0013168706791475415 relative L2 0.0027517550624907017\n",
      "training 0.003195285564288497 relative L2 0.0018397963140159845\n",
      "training 0.0014166584005579352 relative L2 0.0024810219183564186\n",
      "training 0.002588185016065836 relative L2 0.0019315327517688274\n",
      "training 0.0015607901150360703 relative L2 0.002234739949926734\n",
      "training 0.002100977348163724 relative L2 0.0019634466152638197\n",
      "training 0.0016172111500054598 relative L2 0.0020366590470075607\n",
      "training 0.0017379747005179524 relative L2 0.0019910852424800396\n",
      "training 0.0016603099647909403 relative L2 0.0018660483183339238\n",
      "training 0.0014591250801458955 relative L2 0.001978531014174223\n",
      "training 0.0016431146068498492 relative L2 0.0017542780842632055\n",
      "training 0.0012849338818341494 relative L2 0.001956135267391801\n",
      "training 0.0016022393247112632 relative L2 0.0016657205997034907\n",
      "training 0.0011586258187890053 relative L2 0.001916932757012546\n",
      "training 0.0015415995148941875 relative L2 0.001617869478650391\n",
      "training 0.0010905576637014747 relative L2 0.0018734566401690245\n",
      "training 0.0014682385371997952 relative L2 0.0015841511776670814\n",
      "training 0.0010458414908498526 relative L2 0.0018267323030158877\n",
      "training 0.0013983538374304771 relative L2 0.0015691498992964625\n",
      "training 0.001025233417749405 relative L2 0.0017827023984864354\n",
      "training 0.0013277337420731783 relative L2 0.0015608570538461208\n",
      "training 0.0010145441628992558 relative L2 0.0017406399128958583\n",
      "training 0.0012679480714723468 relative L2 0.0015580074395984411\n",
      "training 0.0010108682326972485 relative L2 0.0017052572220563889\n",
      "training 0.0012134663993492723 relative L2 0.0015579578466713428\n",
      "training 0.001010628417134285 relative L2 0.0016727223992347717\n",
      "training 0.0011694564018398523 relative L2 0.0015578779857605696\n",
      "training 0.001011032029055059 relative L2 0.0016476705204695463\n",
      "training 0.0011319111799821258 relative L2 0.0015591216506436467\n",
      "training 0.0010122162057086825 relative L2 0.0016246616141870618\n",
      "training 0.0011021196842193604 relative L2 0.001558781135827303\n",
      "training 0.0010124981636181474 relative L2 0.00160831434186548\n",
      "training 0.0010779239237308502 relative L2 0.0015594081487506628\n",
      "training 0.0010127259884029627 relative L2 0.0015930415829643607\n",
      "training 0.001058913883753121 relative L2 0.001558325020596385\n",
      "training 0.0010121132945641875 relative L2 0.0015829814365133643\n",
      "training 0.0010439814068377018 relative L2 0.001558226882480085\n",
      "training 0.0010113280732184649 relative L2 0.0015732829924672842\n",
      "training 0.0010323505848646164 relative L2 0.001556623145006597\n",
      "training 0.0010100218933075666 relative L2 0.0015673079760745168\n",
      "training 0.001023336430080235 relative L2 0.0015560827450826764\n",
      "training 0.0010086551774293184 relative L2 0.0015612804563716054\n",
      "training 0.0010163718834519386 relative L2 0.001554342801682651\n",
      "training 0.0010071053402498364 relative L2 0.0015578832244500518\n",
      "training 0.0010110673028975725 relative L2 0.0015536460559815168\n",
      "training 0.0010055667953565717 relative L2 0.001554154441691935\n",
      "training 0.0010069238487631083 relative L2 0.001551992492750287\n",
      "training 0.0010040454799309373 relative L2 0.001552220550365746\n",
      "training 0.0010037529282271862 relative L2 0.0015513419639319181\n",
      "training 0.0010026234667748213 relative L2 0.0015499199507758021\n",
      "training 0.0010013114660978317 relative L2 0.0015498618595302105\n",
      "training 0.0010012452257797122 relative L2 0.001548834959976375\n",
      "training 0.000999404932372272 relative L2 0.001549281645566225\n",
      "training 0.0009999815374612808 relative L2 0.001547352410852909\n",
      "training 0.0009978991001844406 relative L2 0.001548043335787952\n",
      "training 0.0009988390374928713 relative L2 0.0015467420453205705\n",
      "training 0.0009967295918613672 relative L2 0.0015476136468350887\n",
      "training 0.0009978426387533545 relative L2 0.0015457794070243835\n",
      "training 0.0009958025766536593 relative L2 0.00154661457054317\n",
      "training 0.0009969414677470922 relative L2 0.001545433420687914\n",
      "training 0.000995064852759242 relative L2 0.0015462995506823063\n",
      "training 0.0009961627656593919 relative L2 0.0015447858022525907\n",
      "training 0.0009944760240614414 relative L2 0.0015455203829333186\n",
      "training 0.0009954858105629683 relative L2 0.0015445982571691275\n",
      "training 0.0009940130403265357 relative L2 0.0015453038504347205\n",
      "training 0.0009948997758328915 relative L2 0.001544143888168037\n",
      "training 0.0009936215355992317 relative L2 0.001544695463962853\n",
      "training 0.0009943897603079677 relative L2 0.0015440253773704171\n",
      "training 0.000993299181573093 relative L2 0.0015445512253791094\n",
      "training 0.0009939523879438639 relative L2 0.0015436927787959576\n",
      "training 0.0009930258383974433 relative L2 0.001544068451039493\n",
      "training 0.0009935577400028706 relative L2 0.0015436060493811965\n",
      "training 0.000992780551314354 relative L2 0.001543964259326458\n",
      "training 0.0009932194370776415 relative L2 0.0015433436492457986\n",
      "training 0.0009925669291988015 relative L2 0.001543586258776486\n",
      "training 0.000992919784039259 relative L2 0.001543274149298668\n",
      "training 0.0009923723991960287 relative L2 0.0015435116365551949\n",
      "training 0.0009926577331498265 relative L2 0.0015430623898282647\n",
      "training 0.0009921998716890812 relative L2 0.0015432091895490885\n",
      "training 0.0009924216428771615 relative L2 0.0015430068597197533\n",
      "training 0.0009920437587425113 relative L2 0.0015431503998115659\n",
      "training 0.0009922116296365857 relative L2 0.001542834099382162\n",
      "training 0.0009919042931869626 relative L2 0.0015429062768816948\n",
      "training 0.0009920205920934677 relative L2 0.0015427862526848912\n",
      "training 0.0009917712304741144 relative L2 0.0015428607584908605\n",
      "training 0.0009918533032760024 relative L2 0.0015426367754116654\n",
      "training 0.0009916472481563687 relative L2 0.0015426635509356856\n",
      "training 0.0009916980052366853 relative L2 0.0015425900928676128\n",
      "training 0.0009915262926369905 relative L2 0.0015426174504682422\n",
      "training 0.0009915510891005397 relative L2 0.0015424595912918448\n",
      "training 0.0009914145339280367 relative L2 0.0015424557495862246\n",
      "training 0.0009914195397868752 relative L2 0.0015424180310219526\n",
      "training 0.0009913077810779214 relative L2 0.001542415819130838\n",
      "training 0.0009912983514368534 relative L2 0.0015423009172081947\n",
      "training 0.000991204404272139 relative L2 0.0015422821743413806\n",
      "training 0.0009911855449900031 relative L2 0.0015422600554302335\n",
      "training 0.000991106266155839 relative L2 0.001542240148410201\n",
      "training 0.0009910768130794168 relative L2 0.0015421542339026928\n",
      "training 0.0009910096414387226 relative L2 0.0015421264106407762\n",
      "training 0.00099097634665668 relative L2 0.0015421129064634442\n",
      "training 0.0009909176733344793 relative L2 0.001542084151878953\n",
      "training 0.0009908790234476328 relative L2 0.0015420172130689025\n",
      "training 0.0009908287320286036 relative L2 0.001541985897347331\n",
      "training 0.0009907882194966078 relative L2 0.0015419756527990103\n",
      "training 0.0009907421190291643 relative L2 0.0015419414266943932\n",
      "training 0.0009906989289447665 relative L2 0.0015418869443237782\n",
      "training 0.0009906578343361616 relative L2 0.0015418538823723793\n",
      "training 0.0009906139457598329 relative L2 0.0015418441034853458\n",
      "training 0.0009905754122883081 relative L2 0.0015418068505823612\n",
      "training 0.0009905300103127956 relative L2 0.001541760517284274\n",
      "training 0.0009904946200549603 relative L2 0.0015417271060869098\n",
      "training 0.0009904486360028386 relative L2 0.0015417162794619799\n",
      "training 0.000990414060652256 relative L2 0.0015416790265589952\n",
      "training 0.0009903700556606054 relative L2 0.0015416373498737812\n",
      "training 0.0009903365280479193 relative L2 0.001541605801321566\n",
      "training 0.0009902921738103032 relative L2 0.001541593112051487\n",
      "training 0.000990259344689548 relative L2 0.0015415563248097897\n",
      "training 0.0009902168530970812 relative L2 0.0015415193047374487\n",
      "training 0.0009901842568069696 relative L2 0.0015414896188303828\n",
      "training 0.000990143045783043 relative L2 0.0015414752997457981\n",
      "training 0.0009901114972308278 relative L2 0.0015414408408105373\n",
      "training 0.0009900707518681884 relative L2 0.0015414071967825294\n",
      "training 0.0009900395525619388 relative L2 0.0015413798391819\n",
      "training 0.0009900014847517014 relative L2 0.0015413661021739244\n",
      "training 0.0009899724973365664 relative L2 0.001541332807391882\n",
      "training 0.000989933731034398 relative L2 0.001541301142424345\n",
      "training 0.000989902880974114 relative L2 0.0015412752982228994\n",
      "training 0.000989865860901773 relative L2 0.0015412579523399472\n",
      "training 0.0009898347780108452 relative L2 0.001541226520203054\n",
      "training 0.0009897982235997915 relative L2 0.001541196252219379\n",
      "training 0.0009897666750475764 relative L2 0.001541172037832439\n",
      "training 0.0009897316340357065 relative L2 0.0015411527128890157\n",
      "training 0.000989700318314135 relative L2 0.0015411226777359843\n",
      "training 0.000989665393717587 relative L2 0.0015410949708893895\n",
      "training 0.0009896354749798775 relative L2 0.0015410714549943805\n",
      "training 0.0009896011324599385 relative L2 0.0015410520136356354\n",
      "training 0.0009895702823996544 relative L2 0.0015410237247124314\n",
      "training 0.0009895372204482555 relative L2 0.0015409975312650204\n",
      "training 0.0009895077673718333 relative L2 0.0015409745974466205\n",
      "training 0.0009894750546664 relative L2 0.0015409548068419099\n",
      "training 0.0009894456015899777 relative L2 0.0015409279149025679\n",
      "training 0.0009894135873764753 relative L2 0.001540902885608375\n",
      "training 0.000989384949207306 relative L2 0.0015408809995278716\n",
      "training 0.0009893536334857345 relative L2 0.001540860626846552\n",
      "training 0.000989324413239956 relative L2 0.0015408347826451063\n",
      "training 0.0009892937960103154 relative L2 0.0015408109175041318\n",
      "training 0.000989264459349215 relative L2 0.0015407900791615248\n",
      "training 0.0009892343077808619 relative L2 0.0015407692408189178\n",
      "training 0.000989206018857658 relative L2 0.0015407440951094031\n",
      "training 0.0009891761001199484 relative L2 0.0015407224418595433\n",
      "training 0.000989148742519319 relative L2 0.0015407028840854764\n",
      "training 0.000989120570011437 relative L2 0.0015406833263114095\n",
      "training 0.0009890937944874167 relative L2 0.001540660043247044\n",
      "training 0.0009890659712255 relative L2 0.0015406387392431498\n",
      "training 0.000989038497209549 relative L2 0.0015406201127916574\n",
      "training 0.0009890118381008506 relative L2 0.0015405999729409814\n",
      "training 0.0009889850625768304 relative L2 0.0015405777376145124\n",
      "training 0.0009889581706374884 relative L2 0.0015405586455017328\n",
      "training 0.000988932908512652 relative L2 0.0015405399026349187\n",
      "training 0.0009889067150652409 relative L2 0.0015405198791995645\n",
      "training 0.000988880987279117 relative L2 0.0015404989244416356\n",
      "training 0.0009888546774163842 relative L2 0.0015404792502522469\n",
      "training 0.000988828600384295 relative L2 0.0015404613222926855\n",
      "training 0.000988803687505424 relative L2 0.001540441415272653\n",
      "training 0.0009887785417959094 relative L2 0.0015404215082526207\n",
      "training 0.0009887536289170384 relative L2 0.0015404034638777375\n",
      "training 0.0009887289488688111 relative L2 0.001540385652333498\n",
      "training 0.0009887046180665493 relative L2 0.0015403672587126493\n",
      "training 0.0009886809857562184 relative L2 0.0015403483994305134\n",
      "training 0.0009886573534458876 relative L2 0.0015403312863782048\n",
      "training 0.0009886338375508785 relative L2 0.00154031440615654\n",
      "training 0.0009886105544865131 relative L2 0.0015402967110276222\n",
      "training 0.0009885879699140787 relative L2 0.001540278666652739\n",
      "training 0.0009885651525110006 relative L2 0.0015402621356770396\n",
      "training 0.0009885429171845317 relative L2 0.0015402460703626275\n",
      "training 0.0009885206818580627 relative L2 0.0015402290737256408\n",
      "training 0.000988499028608203 relative L2 0.0015402124263346195\n",
      "training 0.000988477491773665 relative L2 0.0015401964774355292\n",
      "training 0.0009884560713544488 relative L2 0.0015401809941977262\n",
      "training 0.0009884352330118418 relative L2 0.0015401648124679923\n",
      "training 0.000988414278253913 relative L2 0.0015401489799842238\n",
      "training 0.0009883935563266277 relative L2 0.0015401337295770645\n",
      "training 0.0009883736493065953 relative L2 0.0015401187120005488\n",
      "training 0.0009883535094559193 relative L2 0.0015401036944240332\n",
      "training 0.0009883339516818523 relative L2 0.0015400885604321957\n",
      "training 0.0009883143939077854 relative L2 0.001540074241347611\n",
      "training 0.0009882950689643621 relative L2 0.0015400595730170608\n",
      "training 0.0009882758604362607 relative L2 0.0015400453703477979\n",
      "training 0.0009882571175694466 relative L2 0.0015400308184325695\n",
      "training 0.0009882381418719888 relative L2 0.001540017663501203\n",
      "training 0.000988220446743071 relative L2 0.0015400039264932275\n",
      "training 0.000988202285952866 relative L2 0.0015399906551465392\n",
      "training 0.0009881850564852357 relative L2 0.0015399771509692073\n",
      "training 0.000988167361356318 relative L2 0.0015399637632071972\n",
      "training 0.0009881495498120785 relative L2 0.001539950375445187\n",
      "training 0.000988132320344448 relative L2 0.0015399373369291425\n",
      "training 0.0009881153237074614 relative L2 0.0015399245312437415\n",
      "training 0.0009880983270704746 relative L2 0.001539912074804306\n",
      "training 0.000988081912510097 relative L2 0.0015398995019495487\n",
      "training 0.000988065730780363 relative L2 0.0015398869290947914\n",
      "training 0.0009880495490506291 relative L2 0.0015398745890706778\n",
      "training 0.0009880338329821825 relative L2 0.0015398628311231732\n",
      "training 0.0009880182333290577 relative L2 0.001539850840345025\n",
      "training 0.0009880028665065765 relative L2 0.0015398395480588078\n",
      "training 0.0009879880817607045 relative L2 0.0015398279065266252\n",
      "training 0.000987972947768867 relative L2 0.0015398164978250861\n",
      "training 0.0009879580466076732 relative L2 0.0015398047398775816\n",
      "training 0.000987942679785192 relative L2 0.0015397935640066862\n",
      "training 0.0009879282442852855 relative L2 0.0015397828537970781\n",
      "training 0.0009879140416160226 relative L2 0.0015397720271721482\n",
      "training 0.0009878999553620815 relative L2 0.0015397615497931838\n",
      "training 0.0009878861019387841 relative L2 0.0015397508395835757\n",
      "training 0.0009878721321001649 relative L2 0.0015397403622046113\n",
      "training 0.0009878582786768675 relative L2 0.0015397302340716124\n",
      "training 0.0009878450073301792 relative L2 0.0015397201059386134\n",
      "training 0.0009878320852294564 relative L2 0.0015397099778056145\n",
      "training 0.0009878191631287336 relative L2 0.0015396998496726155\n",
      "training 0.000987806124612689 relative L2 0.0015396897215396166\n",
      "training 0.000987793318927288 relative L2 0.0015396805247291923\n",
      "training 0.000987781211733818 relative L2 0.0015396710950881243\n",
      "training 0.000987768406048417 relative L2 0.001539661199785769\n",
      "training 0.0009877560660243034 relative L2 0.0015396523522213101\n",
      "training 0.0009877447737380862 relative L2 0.0015396426897495985\n",
      "training 0.0009877326665446162 relative L2 0.0015396337257698178\n",
      "training 0.0009877210250124335 relative L2 0.001539624878205359\n",
      "training 0.0009877096163108945 relative L2 0.001539615448564291\n",
      "training 0.00098769785836339 relative L2 0.0015396062517538667\n",
      "training 0.0009876862168312073 relative L2 0.0015395975206047297\n",
      "training 0.0009876750409603119 relative L2 0.0015395887894555926\n",
      "training 0.0009876636322587729 relative L2 0.001539579825475812\n",
      "training 0.0009876525728031993 relative L2 0.001539571094326675\n",
      "training 0.000987641396932304 relative L2 0.0015395624795928597\n",
      "training 0.0009876302210614085 relative L2 0.0015395539812743664\n",
      "training 0.0009876192780211568 relative L2 0.0015395452501252294\n",
      "training 0.0009876085678115487 relative L2 0.0015395373338833451\n",
      "training 0.0009875980904325843 relative L2 0.0015395288355648518\n",
      "training 0.0009875876130536199 relative L2 0.0015395204536616802\n",
      "training 0.000987577368505299 relative L2 0.001539512537419796\n",
      "training 0.0009875672403723001 relative L2 0.001539504388347268\n",
      "training 0.0009875571122393012 relative L2 0.0015394965885207057\n",
      "training 0.0009875469841063023 relative L2 0.0015394886722788215\n",
      "training 0.0009875373216345906 relative L2 0.0015394809888675809\n",
      "training 0.0009875271935015917 relative L2 0.0015394734218716621\n",
      "training 0.0009875176474452019 relative L2 0.0015394658548757434\n",
      "training 0.000987508217804134 relative L2 0.0015394579386338592\n",
      "training 0.0009874983225017786 relative L2 0.0015394504880532622\n",
      "training 0.0009874888928607106 relative L2 0.0015394430374726653\n",
      "training 0.0009874793468043208 relative L2 0.001539435237646103\n",
      "training 0.000987469800747931 relative L2 0.0015394279034808278\n",
      "training 0.0009874604875221848 relative L2 0.0015394204529002309\n",
      "training 0.0009874511742964387 relative L2 0.0015394132351502776\n",
      "training 0.0009874418610706925 relative L2 0.0015394057845696807\n",
      "training 0.0009874326642602682 relative L2 0.001539398799650371\n",
      "training 0.0009874237002804875 relative L2 0.0015393916983157396\n",
      "training 0.0009874148527160287 relative L2 0.0015393844805657864\n",
      "training 0.0009874060051515698 relative L2 0.001539377262815833\n",
      "training 0.0009873969247564673 relative L2 0.0015393701614812016\n",
      "training 0.0009873879607766867 relative L2 0.001539363176561892\n",
      "training 0.000987378996796906 relative L2 0.001539355842396617\n",
      "training 0.0009873701492324471 relative L2 0.0015393489738926291\n",
      "training 0.0009873614180833101 relative L2 0.0015393424546346068\n",
      "training 0.000987352686934173 relative L2 0.0015393358189612627\n",
      "training 0.0009873447706922889 relative L2 0.0015393287176266313\n",
      "training 0.0009873361559584737 relative L2 0.001539322198368609\n",
      "training 0.0009873275412246585 relative L2 0.0015393153298646212\n",
      "training 0.000987319159321487 relative L2 0.0015393084613606334\n",
      "training 0.0009873108938336372 relative L2 0.0015393018256872892\n",
      "training 0.0009873023955151439 relative L2 0.0015392955392599106\n",
      "training 0.0009872947121039033 relative L2 0.0015392890200018883\n",
      "training 0.00098728621378541 relative L2 0.0015392822679132223\n",
      "training 0.000987278064712882 relative L2 0.0015392756322398782\n",
      "training 0.0009872696828097105 relative L2 0.0015392692293971777\n",
      "training 0.0009872617665678263 relative L2 0.0015392627101391554\n",
      "training 0.0009872537339106202 relative L2 0.0015392558416351676\n",
      "training 0.0009872454684227705 relative L2 0.001539249555207789\n",
      "training 0.0009872373193502426 relative L2 0.0015392433851957321\n",
      "training 0.0009872294031083584 relative L2 0.001539236749522388\n",
      "training 0.0009872212540358305 relative L2 0.0015392303466796875\n",
      "training 0.0009872133377939463 relative L2 0.0015392238274216652\n",
      "training 0.0009872053051367402 relative L2 0.001539217890240252\n",
      "training 0.0009871975053101778 relative L2 0.0015392113709822297\n",
      "training 0.0009871898218989372 relative L2 0.001539205084554851\n",
      "training 0.0009871816728264093 relative L2 0.0015391989145427942\n",
      "training 0.000987173756584525 relative L2 0.001539192395284772\n",
      "training 0.0009871660731732845 relative L2 0.0015391864581033587\n",
      "training 0.0009871582733467221 relative L2 0.0015391800552606583\n",
      "training 0.000987150240689516 relative L2 0.001539173536002636\n",
      "training 0.0009871424408629537 relative L2 0.001539167482405901\n",
      "training 0.000987134873867035 relative L2 0.001539161428809166\n",
      "training 0.0009871273068711162 relative L2 0.0015391551423817873\n",
      "training 0.0009871196234598756 relative L2 0.0015391492052003741\n",
      "training 0.0009871120564639568 relative L2 0.0015391430351883173\n",
      "training 0.00098710460588336 relative L2 0.001539137214422226\n",
      "training 0.0009870969224721193 relative L2 0.0015391310444101691\n",
      "training 0.0009870893554762006 relative L2 0.0015391249908134341\n",
      "training 0.0009870817884802818 relative L2 0.0015391188208013773\n",
      "training 0.0009870739886537194 relative L2 0.0015391124179586768\n",
      "training 0.0009870664216578007 relative L2 0.0015391064807772636\n",
      "training 0.0009870586218312383 relative L2 0.0015391005435958505\n",
      "training 0.000987051404081285 relative L2 0.0015390946064144373\n",
      "training 0.000987043953500688 relative L2 0.001539088785648346\n",
      "training 0.0009870363865047693 relative L2 0.0015390829648822546\n",
      "training 0.0009870292851701379 relative L2 0.0015390770277008414\n",
      "training 0.0009870219510048628 relative L2 0.0015390710905194283\n",
      "training 0.0009870145004242659 relative L2 0.001539065269753337\n",
      "training 0.000987007049843669 relative L2 0.0015390594489872456\n",
      "training 0.0009869997156783938 relative L2 0.0015390536282211542\n",
      "training 0.000986992265097797 relative L2 0.001539047691039741\n",
      "training 0.0009869850473478436 relative L2 0.0015390417538583279\n",
      "training 0.0009869777131825686 relative L2 0.001539035583846271\n",
      "training 0.0009869701461866498 relative L2 0.0015390297630801797\n",
      "training 0.0009869631612673402 relative L2 0.0015390240587294102\n",
      "training 0.0009869555942714214 relative L2 0.0015390180051326752\n",
      "training 0.0009869482601061463 relative L2 0.001539012067951262\n",
      "training 0.0009869409259408712 relative L2 0.0015390061307698488\n",
      "training 0.000986933708190918 relative L2 0.001539000659249723\n",
      "training 0.0009869266068562865 relative L2 0.0015389947220683098\n",
      "training 0.0009869193891063333 relative L2 0.001538989134132862\n",
      "training 0.00098691217135638 relative L2 0.001538983196951449\n",
      "training 0.0009869048371911049 relative L2 0.0015389773761853576\n",
      "training 0.000986897386610508 relative L2 0.0015389714390039444\n",
      "training 0.0009868898196145892 relative L2 0.0015389653854072094\n",
      "training 0.0009868824854493141 relative L2 0.0015389594482257962\n",
      "training 0.0009868752676993608 relative L2 0.0015389538602903485\n",
      "training 0.0009868682827800512 relative L2 0.0015389480395242572\n",
      "training 0.000986861065030098 relative L2 0.0015389422187581658\n",
      "training 0.0009868537308648229 relative L2 0.0015389365144073963\n",
      "training 0.0009868467459455132 relative L2 0.001538930693641305\n",
      "training 0.0009868394117802382 relative L2 0.0015389248728752136\n",
      "training 0.000986832077614963 relative L2 0.001538919284939766\n",
      "training 0.0009868249762803316 relative L2 0.0015389134641736746\n",
      "training 0.0009868181077763438 relative L2 0.0015389078762382269\n",
      "training 0.0009868110064417124 relative L2 0.0015389022883027792\n",
      "training 0.000986803905107081 relative L2 0.0015388964675366879\n",
      "training 0.0009867968037724495 relative L2 0.0015388907631859183\n",
      "training 0.000986789702437818 relative L2 0.0015388847095891833\n",
      "training 0.000986782368272543 relative L2 0.0015388790052384138\n",
      "training 0.0009867752669379115 relative L2 0.0015388731844723225\n",
      "training 0.0009867680491879582 relative L2 0.0015388677129521966\n",
      "training 0.0009867611806839705 relative L2 0.001538862008601427\n",
      "training 0.000986754079349339 relative L2 0.0015388564206659794\n",
      "training 0.0009867472108453512 relative L2 0.0015388509491458535\n",
      "training 0.0009867402259260416 relative L2 0.0015388451283797622\n",
      "training 0.0009867331245914102 relative L2 0.0015388395404443145\n",
      "training 0.0009867260232567787 relative L2 0.0015388339525088668\n",
      "training 0.000986719154752791 relative L2 0.0015388281317427754\n",
      "training 0.0009867120534181595 relative L2 0.0015388227766379714\n",
      "training 0.000986704952083528 relative L2 0.00153881695587188\n",
      "training 0.0009866978507488966 relative L2 0.0015388113679364324\n",
      "training 0.0009866907494142652 relative L2 0.0015388053143396974\n",
      "training 0.0009866831824183464 relative L2 0.0015387996099889278\n",
      "training 0.0009866761974990368 relative L2 0.0015387937892228365\n",
      "training 0.0009866689797490835 relative L2 0.001538788783363998\n",
      "training 0.0009866622276604176 relative L2 0.0015387830790132284\n",
      "training 0.000986655242741108 relative L2 0.0015387771418318152\n",
      "training 0.0009866479085758328 relative L2 0.0015387716703116894\n",
      "training 0.0009866409236565232 relative L2 0.0015387659659609199\n",
      "training 0.0009866339387372136 relative L2 0.001538760494440794\n",
      "training 0.000986626953817904 relative L2 0.0015387549065053463\n",
      "training 0.0009866198524832726 relative L2 0.0015387489693239331\n",
      "training 0.0009866127511486411 relative L2 0.0015387431485578418\n",
      "training 0.0009866057662293315 relative L2 0.0015387379098683596\n",
      "training 0.0009865987813100219 relative L2 0.0015387323219329119\n",
      "training 0.0009865915635600686 relative L2 0.0015387265011668205\n",
      "training 0.0009865846950560808 relative L2 0.0015387209132313728\n",
      "training 0.0009865777101367712 relative L2 0.0015387153252959251\n",
      "training 0.0009865709580481052 relative L2 0.0015387097373604774\n",
      "training 0.0009865638567134738 relative L2 0.001538704033009708\n",
      "training 0.000986556988209486 relative L2 0.0015386986779049039\n",
      "training 0.0009865497704595327 relative L2 0.0015386928571388125\n",
      "training 0.0009865426691249013 relative L2 0.0015386872692033648\n",
      "training 0.0009865358006209135 relative L2 0.001538681797683239\n",
      "training 0.0009865291649475694 relative L2 0.0015386762097477913\n",
      "training 0.0009865218307822943 relative L2 0.0015386703889817\n",
      "training 0.0009865148458629847 relative L2 0.0015386648010462523\n",
      "training 0.000986507860943675 relative L2 0.0015386590966954827\n",
      "training 0.0009865008760243654 relative L2 0.0015386536251753569\n",
      "training 0.0009864940075203776 relative L2 0.001538648153655231\n",
      "training 0.0009864866733551025 relative L2 0.0015386424493044615\n",
      "training 0.000986479688435793 relative L2 0.0015386369777843356\n",
      "training 0.0009864730527624488 relative L2 0.001538631389848888\n",
      "training 0.0009864660678431392 relative L2 0.001538625918328762\n",
      "training 0.0009864591993391514 relative L2 0.0015386202139779925\n",
      "training 0.00098645209800452 relative L2 0.0015386149752885103\n",
      "training 0.0009864452295005322 relative L2 0.001538609154522419\n",
      "training 0.0009864380117505789 relative L2 0.0015386033337563276\n",
      "training 0.0009864309104159474 relative L2 0.00153859774582088\n",
      "training 0.0009864239254966378 relative L2 0.0015385921578854322\n",
      "training 0.0009864169405773282 relative L2 0.001538586337119341\n",
      "training 0.0009864098392426968 relative L2 0.0015385810984298587\n",
      "training 0.0009864032035693526 relative L2 0.001538575510494411\n",
      "training 0.0009863963350653648 relative L2 0.0015385699225589633\n",
      "training 0.0009863892337307334 relative L2 0.0015385638689622283\n",
      "training 0.000986382132396102 relative L2 0.0015385585138574243\n",
      "training 0.0009863751474767923 relative L2 0.0015385529259219766\n",
      "training 0.000986368046142161 relative L2 0.0015385474544018507\n",
      "training 0.0009863614104688168 relative L2 0.0015385419828817248\n",
      "training 0.0009863546583801508 relative L2 0.0015385363949462771\n",
      "training 0.0009863476734608412 relative L2 0.0015385308070108294\n",
      "training 0.0009863409213721752 relative L2 0.0015385255683213472\n",
      "training 0.0009863339364528656 relative L2 0.0015385199803858995\n",
      "training 0.0009863270679488778 relative L2 0.0015385140432044864\n",
      "training 0.000986319500952959 relative L2 0.0015385085716843605\n",
      "training 0.000986312865279615 relative L2 0.0015385033329948783\n",
      "training 0.0009863059967756271 relative L2 0.0015384977450594306\n",
      "training 0.0009862990118563175 relative L2 0.0015384922735393047\n",
      "training 0.0009862921433523297 relative L2 0.001538486685603857\n",
      "training 0.0009862853912636638 relative L2 0.0015384812140837312\n",
      "training 0.0009862786391749978 relative L2 0.001538475975394249\n",
      "training 0.0009862716542556882 relative L2 0.0015384702710434794\n",
      "training 0.0009862649021670222 relative L2 0.0015384650323539972\n",
      "training 0.0009862580336630344 relative L2 0.0015384593280032277\n",
      "training 0.0009862510487437248 relative L2 0.00153845374006778\n",
      "training 0.0009862440638244152 relative L2 0.001538448384962976\n",
      "training 0.0009862371953204274 relative L2 0.0015384427970275283\n",
      "training 0.000986230093985796 relative L2 0.0015384370926767588\n",
      "training 0.0009862232254818082 relative L2 0.0015384318539872766\n",
      "training 0.000986216589808464 relative L2 0.001538426149636507\n",
      "training 0.0009862094884738326 relative L2 0.0015384203288704157\n",
      "training 0.0009862023871392012 relative L2 0.0015384148573502898\n",
      "training 0.000986195751465857 relative L2 0.0015384095022454858\n",
      "training 0.0009861887665465474 relative L2 0.001538403914310038\n",
      "training 0.0009861818980425596 relative L2 0.0015383984427899122\n",
      "training 0.0009861747967079282 relative L2 0.0015383928548544645\n",
      "training 0.0009861679282039404 relative L2 0.0015383872669190168\n",
      "training 0.0009861610596999526 relative L2 0.0015383815625682473\n",
      "training 0.0009861539583653212 relative L2 0.0015383762074634433\n",
      "training 0.0009861472062766552 relative L2 0.001538370968773961\n",
      "training 0.0009861402213573456 relative L2 0.0015383653808385134\n",
      "training 0.0009861334692686796 relative L2 0.0015383599093183875\n",
      "training 0.0009861266007646918 relative L2 0.0015383544377982616\n",
      "training 0.0009861198486760259 relative L2 0.0015383490826934576\n",
      "training 0.000986112980172038 relative L2 0.0015383436111733317\n",
      "training 0.0009861062280833721 relative L2 0.001538338023237884\n",
      "training 0.0009860990103334188 relative L2 0.0015383330173790455\n",
      "training 0.0009860927239060402 relative L2 0.0015383275458589196\n",
      "training 0.0009860859718173742 relative L2 0.001538321957923472\n",
      "training 0.0009860791033133864 relative L2 0.0015383163699880242\n",
      "training 0.0009860721183940768 relative L2 0.0015383108984678984\n",
      "training 0.0009860654827207327 relative L2 0.0015383056597784162\n",
      "training 0.000986058497801423 relative L2 0.0015383001882582903\n",
      "training 0.0009860516292974353 relative L2 0.0015382946003228426\n",
      "training 0.0009860447607934475 relative L2 0.0015382893616333604\n",
      "training 0.0009860382415354252 relative L2 0.0015382837736979127\n",
      "training 0.0009860313730314374 relative L2 0.0015382783021777868\n",
      "training 0.0009860245045274496 relative L2 0.0015382730634883046\n",
      "training 0.0009860177524387836 relative L2 0.0015382677083835006\n",
      "training 0.000986010767519474 relative L2 0.0015382618876174092\n",
      "training 0.0009860037826001644 relative L2 0.0015382562996819615\n",
      "training 0.000985996681265533 relative L2 0.0015382507117465138\n",
      "training 0.0009859896963462234 relative L2 0.0015382448909804225\n",
      "training 0.00098598247859627 relative L2 0.0015382394194602966\n",
      "training 0.0009859754936769605 relative L2 0.001538233831524849\n",
      "training 0.0009859688580036163 relative L2 0.0015382288256660104\n",
      "training 0.0009859619894996285 relative L2 0.0015382232377305627\n",
      "training 0.000985955004580319 relative L2 0.0015382177662104368\n",
      "training 0.0009859481360763311 relative L2 0.0015382121782749891\n",
      "training 0.0009859412675723433 relative L2 0.0015382067067548633\n",
      "training 0.000985934166237712 relative L2 0.0015382011188194156\n",
      "training 0.0009859271813184023 relative L2 0.0015381955308839679\n",
      "training 0.0009859204292297363 relative L2 0.0015381902921944857\n",
      "training 0.0009859137935563922 relative L2 0.0015381848206743598\n",
      "training 0.0009859069250524044 relative L2 0.0015381791163235903\n",
      "training 0.000985899823717773 relative L2 0.0015381736448034644\n",
      "training 0.000985893071629107 relative L2 0.0015381681732833385\n",
      "training 0.0009858860867097974 relative L2 0.001538162468932569\n",
      "training 0.000985878985375166 relative L2 0.0015381569974124432\n",
      "training 0.0009858723497018218 relative L2 0.0015381518751382828\n",
      "training 0.000985865481197834 relative L2 0.0015381465200334787\n",
      "training 0.0009858586126938462 relative L2 0.0015381410485133529\n",
      "training 0.0009858518606051803 relative L2 0.0015381356934085488\n",
      "training 0.0009858451085165143 relative L2 0.0015381304547190666\n",
      "training 0.000985838589258492 relative L2 0.0015381249831989408\n",
      "training 0.000985831837169826 relative L2 0.001538119395263493\n",
      "training 0.0009858247358351946 relative L2 0.0015381142729893327\n",
      "training 0.000985818332992494 relative L2 0.001538108685053885\n",
      "training 0.0009858112316578627 relative L2 0.001538103329949081\n",
      "training 0.0009858043631538749 relative L2 0.0015380982076749206\n",
      "training 0.0009857978438958526 relative L2 0.0015380927361547947\n",
      "training 0.0009857909753918648 relative L2 0.001538087148219347\n",
      "training 0.0009857843397185206 relative L2 0.0015380819095298648\n",
      "training 0.0009857775876298547 relative L2 0.001538076321594417\n",
      "training 0.0009857703698799014 relative L2 0.0015380708500742912\n",
      "training 0.00098576326854527 relative L2 0.0015380652621388435\n",
      "training 0.000985756516456604 relative L2 0.0015380599070340395\n",
      "training 0.0009857495315372944 relative L2 0.0015380544355139136\n",
      "training 0.0009857426630333066 relative L2 0.001538048847578466\n",
      "training 0.000985735678113997 relative L2 0.0015380430268123746\n",
      "training 0.0009857285767793655 relative L2 0.0015380376717075706\n",
      "training 0.000985721591860056 relative L2 0.0015380322001874447\n",
      "training 0.0009857147233560681 relative L2 0.0015380269614979625\n",
      "training 0.0009857079712674022 relative L2 0.0015380213735625148\n",
      "training 0.0009857011027634144 relative L2 0.001538015902042389\n",
      "training 0.0009856944670900702 relative L2 0.0015380105469375849\n",
      "training 0.0009856877150014043 relative L2 0.0015380053082481027\n",
      "training 0.0009856809629127383 relative L2 0.0015379998367279768\n",
      "training 0.0009856739779934287 relative L2 0.001537994248792529\n",
      "training 0.000985667109489441 relative L2 0.001537989010103047\n",
      "training 0.0009856602409854531 relative L2 0.0015379837714135647\n",
      "training 0.0009856534888967872 relative L2 0.0015379782998934388\n",
      "training 0.000985646853223443 relative L2 0.0015379730612039566\n",
      "training 0.0009856403339654207 relative L2 0.0015379678225144744\n",
      "training 0.000985633465461433 relative L2 0.0015379622345790267\n",
      "training 0.0009856264805421233 relative L2 0.0015379567630589008\n",
      "training 0.0009856197284534574 relative L2 0.0015379514079540968\n",
      "training 0.0009856129763647914 relative L2 0.0015379457036033273\n",
      "training 0.0009856061078608036 relative L2 0.0015379403484985232\n",
      "training 0.0009855992393568158 relative L2 0.001537935109809041\n",
      "training 0.0009855921380221844 relative L2 0.0015379292890429497\n",
      "training 0.0009855852695181966 relative L2 0.0015379238175228238\n",
      "training 0.0009855784010142088 relative L2 0.0015379182295873761\n",
      "training 0.0009855714160948992 relative L2 0.001537912990897894\n",
      "training 0.0009855644311755896 relative L2 0.0015379076357930899\n",
      "training 0.0009855575626716018 relative L2 0.001537902164272964\n",
      "training 0.000985550694167614 relative L2 0.0015378965763375163\n",
      "training 0.000985543942078948 relative L2 0.001537891337648034\n",
      "training 0.000985537189990282 relative L2 0.0015378858661279082\n",
      "training 0.0009855303214862943 relative L2 0.0015378802781924605\n",
      "training 0.0009855233365669847 relative L2 0.0015378750395029783\n",
      "training 0.0009855165844783187 relative L2 0.001537869218736887\n",
      "training 0.0009855094831436872 relative L2 0.0015378636308014393\n",
      "training 0.0009855026146396995 relative L2 0.0015378580428659916\n",
      "training 0.0009854956297203898 relative L2 0.0015378525713458657\n",
      "training 0.0009854885283857584 relative L2 0.0015378472162410617\n",
      "training 0.0009854818927124143 relative L2 0.0015378419775515795\n",
      "training 0.0009854750242084265 relative L2 0.0015378363896161318\n",
      "training 0.0009854681557044387 relative L2 0.0015378312673419714\n",
      "training 0.0009854614036157727 relative L2 0.0015378257958218455\n",
      "training 0.0009854550007730722 relative L2 0.0015378204407170415\n",
      "training 0.0009854480158537626 relative L2 0.0015378148527815938\n",
      "training 0.000985441030934453 relative L2 0.0015378096140921116\n",
      "training 0.000985434278845787 relative L2 0.0015378043754026294\n",
      "training 0.0009854277595877647 relative L2 0.0015377989038825035\n",
      "training 0.0009854207746684551 relative L2 0.0015377936651930213\n",
      "training 0.0009854139061644673 relative L2 0.001537788426503539\n",
      "training 0.0009854072704911232 relative L2 0.0015377831878140569\n",
      "training 0.0009854004019871354 relative L2 0.001537777716293931\n",
      "training 0.0009853936498984694 relative L2 0.0015377721283584833\n",
      "training 0.0009853867813944817 relative L2 0.0015377666568383574\n",
      "training 0.0009853799128904939 relative L2 0.0015377614181488752\n",
      "training 0.000985373160801828 relative L2 0.0015377560630440712\n",
      "training 0.0009853665251284838 relative L2 0.0015377504751086235\n",
      "training 0.0009853594237938523 relative L2 0.0015377450035884976\n",
      "training 0.0009853526717051864 relative L2 0.0015377395320683718\n",
      "training 0.0009853456867858768 relative L2 0.0015377341769635677\n",
      "training 0.0009853390511125326 relative L2 0.0015377287054434419\n",
      "training 0.0009853319497779012 relative L2 0.0015377231175079942\n",
      "training 0.0009853249648585916 relative L2 0.0015377176459878683\n",
      "training 0.0009853182127699256 relative L2 0.001537712407298386\n",
      "training 0.0009853113442659378 relative L2 0.0015377071686089039\n",
      "training 0.0009853045921772718 relative L2 0.0015377019299194217\n",
      "training 0.0009852978400886059 relative L2 0.0015376965748146176\n",
      "training 0.0009852912044152617 relative L2 0.0015376911032944918\n",
      "training 0.000985284335911274 relative L2 0.001537685631774366\n",
      "training 0.000985277583822608 relative L2 0.0015376803930848837\n",
      "training 0.000985270831733942 relative L2 0.0015376750379800797\n",
      "training 0.0009852639632299542 relative L2 0.0015376695664599538\n",
      "training 0.0009852570947259665 relative L2 0.0015376643277704716\n",
      "training 0.0009852501098066568 relative L2 0.0015376589726656675\n",
      "training 0.0009852434741333127 relative L2 0.0015376535011455417\n",
      "training 0.0009852367220446467 relative L2 0.0015376483788713813\n",
      "training 0.0009852300863713026 relative L2 0.0015376427909359336\n",
      "training 0.0009852228686213493 relative L2 0.0015376375522464514\n",
      "training 0.0009852162329480052 relative L2 0.0015376323135569692\n",
      "training 0.0009852094808593392 relative L2 0.001537627074867487\n",
      "training 0.0009852027287706733 relative L2 0.001537621603347361\n",
      "training 0.0009851960930973291 relative L2 0.0015376163646578789\n",
      "training 0.0009851892245933414 relative L2 0.0015376111259683967\n",
      "training 0.0009851825889199972 relative L2 0.0015376057708635926\n",
      "training 0.0009851758368313313 relative L2 0.0015376002993434668\n",
      "training 0.0009851692011579871 relative L2 0.0015375952934846282\n",
      "training 0.000985162565484643 relative L2 0.0015375898219645023\n",
      "training 0.0009851559298112988 relative L2 0.0015375848161056638\n",
      "training 0.0009851494105532765 relative L2 0.001537579344585538\n",
      "training 0.0009851425420492887 relative L2 0.001537573873065412\n",
      "training 0.000985135673545301 relative L2 0.001537568517960608\n",
      "training 0.0009851288050413132 relative L2 0.0015375630464404821\n",
      "training 0.0009851217037066817 relative L2 0.001537557691335678\n",
      "training 0.000985114835202694 relative L2 0.001537552336230874\n",
      "training 0.0009851081995293498 relative L2 0.0015375470975413918\n",
      "training 0.0009851014474406838 relative L2 0.0015375417424365878\n",
      "training 0.0009850948117673397 relative L2 0.0015375365037471056\n",
      "training 0.000985087943263352 relative L2 0.0015375312650576234\n",
      "training 0.0009850810747593641 relative L2 0.0015375257935374975\n",
      "training 0.0009850743226706982 relative L2 0.001537520787678659\n",
      "training 0.0009850674541667104 relative L2 0.0015375150833278894\n",
      "training 0.0009850605856627226 relative L2 0.0015375094953924417\n",
      "training 0.0009850538335740566 relative L2 0.0015375042567029595\n",
      "training 0.0009850469650700688 relative L2 0.001537499250844121\n",
      "training 0.0009850405622273684 relative L2 0.0015374940121546388\n",
      "training 0.0009850335773080587 relative L2 0.0015374887734651566\n",
      "training 0.0009850268252193928 relative L2 0.0015374833019450307\n",
      "training 0.0009850198403000832 relative L2 0.0015374778304249048\n",
      "training 0.000985013204626739 relative L2 0.0015374722424894571\n",
      "training 0.0009850059868767858 relative L2 0.0015374672366306186\n",
      "training 0.0009849993512034416 relative L2 0.0015374621143564582\n",
      "training 0.0009849925991147757 relative L2 0.0015374567592516541\n",
      "training 0.0009849860798567533 relative L2 0.0015374512877315283\n",
      "training 0.0009849793277680874 relative L2 0.001537446049042046\n",
      "training 0.0009849725756794214 relative L2 0.0015374405775219202\n",
      "training 0.0009849658235907555 relative L2 0.0015374352224171162\n",
      "training 0.0009849590715020895 relative L2 0.001537429983727634\n",
      "training 0.0009849523194134235 relative L2 0.0015374247450381517\n",
      "training 0.0009849454509094357 relative L2 0.0015374192735180259\n",
      "training 0.000984938582405448 relative L2 0.0015374138019979\n",
      "training 0.0009849319467321038 relative L2 0.0015374093782156706\n",
      "training 0.0009849262423813343 relative L2 0.0015374042559415102\n",
      "training 0.0009849198395386338 relative L2 0.0015373986680060625\n",
      "training 0.000984912971034646 relative L2 0.0015373935457319021\n",
      "training 0.0009849061025306582 relative L2 0.0015373879577964544\n",
      "training 0.0009848992340266705 relative L2 0.0015373829519376159\n",
      "training 0.0009848922491073608 relative L2 0.00153737748041749\n",
      "training 0.0009848854970186949 relative L2 0.001537372125312686\n",
      "training 0.000984878628514707 relative L2 0.0015373668866232038\n",
      "training 0.000984871992841363 relative L2 0.0015373611822724342\n",
      "training 0.0009848650079220533 relative L2 0.001537355943582952\n",
      "training 0.0009848581394180655 relative L2 0.001537350588478148\n",
      "training 0.000984851154498756 relative L2 0.0015373451169580221\n",
      "training 0.0009848445188254118 relative L2 0.0015373396454378963\n",
      "training 0.0009848378831520677 relative L2 0.001537334406748414\n",
      "training 0.000984830898232758 relative L2 0.0015373291680589318\n",
      "training 0.0009848240297287703 relative L2 0.0015373235801234841\n",
      "training 0.0009848171612247825 relative L2 0.00153731822501868\n",
      "training 0.0009848105255514383 relative L2 0.0015373127534985542\n",
      "training 0.0009848036570474505 relative L2 0.0015373072819784284\n",
      "training 0.0009847967885434628 relative L2 0.0015373020432889462\n",
      "training 0.0009847901528701186 relative L2 0.001537296804599464\n",
      "training 0.000984783167950809 relative L2 0.0015372917987406254\n",
      "training 0.0009847765322774649 relative L2 0.0015372863272204995\n",
      "training 0.000984769663773477 relative L2 0.0015372807392850518\n",
      "training 0.000984763028100133 relative L2 0.0015372757334262133\n",
      "training 0.0009847559267655015 relative L2 0.0015372702619060874\n",
      "training 0.0009847494075074792 relative L2 0.0015372650232166052\n",
      "training 0.0009847425390034914 relative L2 0.0015372594352811575\n",
      "training 0.0009847359033301473 relative L2 0.0015372543130069971\n",
      "training 0.0009847290348261595 relative L2 0.001537249074317515\n",
      "training 0.0009847225155681372 relative L2 0.0015372440684586763\n",
      "training 0.0009847161127254367 relative L2 0.0015372388297691941\n",
      "training 0.0009847094770520926 relative L2 0.001537233591079712\n",
      "training 0.0009847023757174611 relative L2 0.0015372283523902297\n",
      "training 0.0009846955072134733 relative L2 0.0015372226480394602\n",
      "training 0.0009846888715401292 relative L2 0.001537217409349978\n",
      "training 0.0009846818866208196 relative L2 0.001537212054245174\n",
      "training 0.0009846750181168318 relative L2 0.001537206582725048\n",
      "training 0.0009846683824434876 relative L2 0.0015372013440355659\n",
      "training 0.0009846612811088562 relative L2 0.0015371961053460836\n",
      "training 0.0009846549946814775 relative L2 0.0015371909830719233\n",
      "training 0.0009846481261774898 relative L2 0.001537185744382441\n",
      "training 0.0009846413740888238 relative L2 0.001537180389277637\n",
      "training 0.0009846348548308015 relative L2 0.0015371749177575111\n",
      "training 0.0009846276370808482 relative L2 0.001537169679068029\n",
      "training 0.000984621001407504 relative L2 0.0015371644403785467\n",
      "training 0.0009846142493188381 relative L2 0.0015371592016890645\n",
      "training 0.000984607613645494 relative L2 0.001537154195830226\n",
      "training 0.000984600861556828 relative L2 0.0015371487243101\n",
      "training 0.000984594109468162 relative L2 0.0015371436020359397\n",
      "training 0.000984587357379496 relative L2 0.001537138014100492\n",
      "training 0.000984580721706152 relative L2 0.0015371327754110098\n",
      "training 0.0009845737367868423 relative L2 0.0015371277695521712\n",
      "training 0.0009845669846981764 relative L2 0.001537122530862689\n",
      "training 0.000984560465440154 relative L2 0.0015371169429272413\n",
      "training 0.0009845539461821318 relative L2 0.0015371121698990464\n",
      "training 0.0009845471940934658 relative L2 0.0015371065819635987\n",
      "training 0.000984540325589478 relative L2 0.0015371014596894383\n",
      "training 0.0009845336899161339 relative L2 0.0015370961045846343\n",
      "training 0.0009845270542427897 relative L2 0.001537090865895152\n",
      "training 0.000984520185738802 relative L2 0.0015370857436209917\n",
      "training 0.0009845133172348142 relative L2 0.0015370805049315095\n",
      "training 0.0009845067979767919 relative L2 0.0015370751498267055\n",
      "training 0.000984500045888126 relative L2 0.0015370696783065796\n",
      "training 0.0009844929445534945 relative L2 0.0015370644396170974\n",
      "training 0.0009844863088801503 relative L2 0.0015370589680969715\n",
      "training 0.0009844793239608407 relative L2 0.0015370537294074893\n",
      "training 0.0009844728047028184 relative L2 0.001537048490718007\n",
      "training 0.0009844660526141524 relative L2 0.0015370432520285249\n",
      "training 0.0009844590676948428 relative L2 0.0015370378969237208\n",
      "training 0.000984452199190855 relative L2 0.001537032425403595\n",
      "training 0.0009844455635175109 relative L2 0.0015370271867141128\n",
      "training 0.000984438811428845 relative L2 0.0015370219480246305\n",
      "training 0.000984432059340179 relative L2 0.0015370168257504702\n",
      "training 0.0009844254236668348 relative L2 0.0015370114706456661\n",
      "training 0.0009844186715781689 relative L2 0.0015370058827102184\n",
      "training 0.0009844116866588593 relative L2 0.0015370009932667017\n",
      "training 0.000984405167400837 relative L2 0.0015369955217465758\n",
      "training 0.0009843982988968492 relative L2 0.00153699005022645\n",
      "training 0.0009843914303928614 relative L2 0.0015369850443676114\n",
      "training 0.0009843850275501609 relative L2 0.0015369798056781292\n",
      "training 0.000984378159046173 relative L2 0.0015369742177426815\n",
      "training 0.0009843711741268635 relative L2 0.0015369689790531993\n",
      "training 0.0009843643056228757 relative L2 0.001536963740363717\n",
      "training 0.0009843570878729224 relative L2 0.0015369585016742349\n",
      "training 0.000984350685030222 relative L2 0.0015369532629847527\n",
      "training 0.0009843435836955905 relative L2 0.0015369477914646268\n",
      "training 0.0009843369480222464 relative L2 0.0015369425527751446\n",
      "training 0.0009843301959335804 relative L2 0.0015369374305009842\n",
      "training 0.0009843235602602363 relative L2 0.0015369320753961802\n",
      "training 0.0009843168081715703 relative L2 0.0015369271859526634\n",
      "training 0.0009843104053288698 relative L2 0.0015369217144325376\n",
      "training 0.0009843034204095602 relative L2 0.0015369164757430553\n",
      "training 0.000984296901151538 relative L2 0.0015369115862995386\n",
      "training 0.000984290149062872 relative L2 0.0015369062311947346\n",
      "training 0.000984283396974206 relative L2 0.0015369007596746087\n",
      "training 0.0009842765284702182 relative L2 0.0015368952881544828\n",
      "training 0.0009842696599662304 relative L2 0.0015368897002190351\n",
      "training 0.000984262558631599 relative L2 0.0015368843451142311\n",
      "training 0.0009842555737122893 relative L2 0.0015368788735941052\n",
      "training 0.0009842487052083015 relative L2 0.0015368734020739794\n",
      "training 0.00098424160387367 relative L2 0.0015368680469691753\n",
      "training 0.0009842347353696823 relative L2 0.0015368628082796931\n",
      "training 0.00098422821611166 relative L2 0.001536857569590211\n",
      "training 0.0009842213476076722 relative L2 0.001536852098070085\n",
      "training 0.0009842144791036844 relative L2 0.0015368468593806028\n",
      "training 0.0009842076105996966 relative L2 0.0015368416206911206\n",
      "training 0.0009842009749263525 relative L2 0.0015368363820016384\n",
      "training 0.0009841941064223647 relative L2 0.0015368309104815125\n",
      "training 0.000984187121503055 relative L2 0.0015368253225460649\n",
      "training 0.0009841801365837455 relative L2 0.0015368203166872263\n",
      "training 0.000984173733741045 relative L2 0.001536815194413066\n",
      "training 0.0009841668652370572 relative L2 0.0015368099557235837\n",
      "training 0.0009841599967330694 relative L2 0.001536804367788136\n",
      "training 0.0009841532446444035 relative L2 0.0015367994783446193\n",
      "training 0.0009841466089710593 relative L2 0.0015367940068244934\n",
      "training 0.0009841397404670715 relative L2 0.0015367886517196894\n",
      "training 0.0009841329883784056 relative L2 0.0015367831801995635\n",
      "training 0.0009841261198744178 relative L2 0.0015367779415100813\n",
      "training 0.0009841193677857518 relative L2 0.0015367725864052773\n",
      "training 0.0009841123828664422 relative L2 0.0015367674641311169\n",
      "training 0.0009841056307777762 relative L2 0.001536761992610991\n",
      "training 0.0009840987622737885 relative L2 0.001536756637506187\n",
      "training 0.0009840921266004443 relative L2 0.0015367513988167048\n",
      "training 0.0009840851416811347 relative L2 0.0015367466257885098\n",
      "training 0.0009840786224231124 relative L2 0.0015367412706837058\n",
      "training 0.0009840719867497683 relative L2 0.00153673579916358\n",
      "training 0.0009840651182457805 relative L2 0.0015367309097200632\n",
      "training 0.00098405871540308 relative L2 0.0015367259038612247\n",
      "training 0.000984051963314414 relative L2 0.0015367204323410988\n",
      "training 0.0009840450948104262 relative L2 0.0015367154264822602\n",
      "training 0.000984038575552404 relative L2 0.001536710187792778\n",
      "training 0.000984031823463738 relative L2 0.0015367047162726521\n",
      "training 0.0009840246057137847 relative L2 0.0015366992447525263\n",
      "training 0.0009840178536251187 relative L2 0.001536694006063044\n",
      "training 0.000984010985121131 relative L2 0.0015366887673735619\n",
      "training 0.0009840044658631086 relative L2 0.0015366835286840796\n",
      "training 0.000983997480943799 relative L2 0.0015366781735792756\n",
      "training 0.000983990146778524 relative L2 0.001536672469228506\n",
      "training 0.0009839832782745361 relative L2 0.0015366668812930584\n",
      "training 0.0009839764097705483 relative L2 0.0015366618754342198\n",
      "training 0.000983969890512526 relative L2 0.0015366567531600595\n",
      "training 0.00098396313842386 relative L2 0.0015366515144705772\n",
      "training 0.000983956502750516 relative L2 0.001536646275781095\n",
      "training 0.0009839498670771718 relative L2 0.001536640920676291\n",
      "training 0.0009839428821578622 relative L2 0.0015366356819868088\n",
      "training 0.0009839357808232307 relative L2 0.0015366303268820047\n",
      "training 0.0009839293779805303 relative L2 0.0015366249717772007\n",
      "training 0.0009839225094765425 relative L2 0.0015366198495030403\n",
      "training 0.0009839158738031983 relative L2 0.0015366149600595236\n",
      "training 0.0009839092381298542 relative L2 0.0015366096049547195\n",
      "training 0.00098390260245651 relative L2 0.0015366043662652373\n",
      "training 0.0009838957339525223 relative L2 0.0015365991275757551\n",
      "training 0.0009838890982791781 relative L2 0.001536593888886273\n",
      "training 0.0009838822297751904 relative L2 0.0015365887666121125\n",
      "training 0.0009838758269324899 relative L2 0.0015365834115073085\n",
      "training 0.0009838691912591457 relative L2 0.0015365781728178263\n",
      "training 0.0009838624391704798 relative L2 0.0015365728177130222\n",
      "training 0.0009838559199124575 relative L2 0.0015365674626082182\n",
      "training 0.0009838489349931479 relative L2 0.001536562223918736\n",
      "training 0.0009838422993198037 relative L2 0.0015365569852292538\n",
      "training 0.000983835430815816 relative L2 0.0015365517465397716\n",
      "training 0.0009838284458965063 relative L2 0.0015365465078502893\n",
      "training 0.0009838216938078403 relative L2 0.0015365410363301635\n",
      "training 0.0009838148253038526 relative L2 0.0015365355648100376\n",
      "training 0.000983807840384543 relative L2 0.001536530558951199\n",
      "training 0.0009838012047111988 relative L2 0.0015365253202617168\n",
      "training 0.000983794336207211 relative L2 0.0015365200815722346\n",
      "training 0.000983787584118545 relative L2 0.0015365148428827524\n",
      "training 0.0009837810648605227 relative L2 0.001536509720608592\n",
      "training 0.0009837739635258913 relative L2 0.0015365041326731443\n",
      "training 0.0009837670950219035 relative L2 0.0015364988939836621\n",
      "training 0.0009837602265179157 relative L2 0.0015364934224635363\n",
      "training 0.0009837534744292498 relative L2 0.001536488183774054\n",
      "training 0.000983746605925262 relative L2 0.0015364831779152155\n",
      "training 0.000983739853836596 relative L2 0.0015364777063950896\n",
      "training 0.0009837329853326082 relative L2 0.0015364724677056074\n",
      "training 0.0009837260004132986 relative L2 0.0015364672290161252\n",
      "training 0.0009837192483246326 relative L2 0.0015364618739113212\n",
      "training 0.0009837124962359667 relative L2 0.0015364567516371608\n",
      "training 0.000983705511316657 relative L2 0.0015364515129476786\n",
      "training 0.000983698759227991 relative L2 0.0015364460414275527\n",
      "training 0.0009836918907240033 relative L2 0.0015364406863227487\n",
      "training 0.0009836850222200155 relative L2 0.0015364355640485883\n",
      "training 0.0009836781537160277 relative L2 0.0015364302089437842\n",
      "training 0.0009836710523813963 relative L2 0.0015364247374236584\n",
      "training 0.0009836643002927303 relative L2 0.0015364197315648198\n",
      "training 0.0009836575482040644 relative L2 0.001536414260044694\n",
      "training 0.0009836506797000766 relative L2 0.001536408788524568\n",
      "training 0.0009836438111960888 relative L2 0.0015364035498350859\n",
      "training 0.0009836370591074228 relative L2 0.0015363983111456037\n",
      "training 0.0009836300741881132 relative L2 0.0015363926067948341\n",
      "training 0.0009836229728534818 relative L2 0.0015363874845206738\n",
      "training 0.000983616104349494 relative L2 0.0015363822458311915\n",
      "training 0.0009836091194301844 relative L2 0.0015363767743110657\n",
      "training 0.0009836023673415184 relative L2 0.0015363715356215835\n",
      "training 0.0009835954988375306 relative L2 0.0015363659476861358\n",
      "training 0.0009835886303335428 relative L2 0.0015363605925813317\n",
      "training 0.0009835814125835896 relative L2 0.0015363554703071713\n",
      "training 0.0009835747769102454 relative L2 0.0015363499987870455\n",
      "training 0.0009835679084062576 relative L2 0.0015363447600975633\n",
      "training 0.0009835610399022698 relative L2 0.001536339521408081\n",
      "training 0.0009835544042289257 relative L2 0.0015363339334726334\n",
      "training 0.0009835473028942943 relative L2 0.0015363283455371857\n",
      "training 0.0009835402015596628 relative L2 0.0015363231068477035\n",
      "training 0.0009835334494709969 relative L2 0.0015363178681582212\n",
      "training 0.0009835264645516872 relative L2 0.0015363127458840609\n",
      "training 0.000983519828878343 relative L2 0.0015363070415332913\n",
      "training 0.0009835128439590335 relative L2 0.0015363018028438091\n",
      "training 0.0009835060918703675 relative L2 0.0015362963313236833\n",
      "training 0.000983498990535736 relative L2 0.0015362913254648447\n",
      "training 0.0009834921220317483 relative L2 0.0015362856211140752\n",
      "training 0.0009834851371124387 relative L2 0.0015362806152552366\n",
      "training 0.0009834782686084509 relative L2 0.0015362751437351108\n",
      "training 0.0009834716329351068 relative L2 0.0015362699050456285\n",
      "training 0.000983464764431119 relative L2 0.0015362645499408245\n",
      "training 0.0009834578959271312 relative L2 0.0015362594276666641\n",
      "training 0.0009834510274231434 relative L2 0.001536254188977182\n",
      "training 0.0009834443917497993 relative L2 0.0015362486010417342\n",
      "training 0.0009834375232458115 relative L2 0.0015362431295216084\n",
      "training 0.0009834307711571455 relative L2 0.0015362381236627698\n",
      "training 0.0009834239026531577 relative L2 0.0015362328849732876\n",
      "training 0.0009834171505644917 relative L2 0.0015362276462838054\n",
      "training 0.0009834105148911476 relative L2 0.0015362221747636795\n",
      "training 0.0009834034135565162 relative L2 0.0015362168196588755\n",
      "training 0.0009833964286372066 relative L2 0.0015362114645540714\n",
      "training 0.000983389443717897 relative L2 0.0015362058766186237\n",
      "training 0.0009833825752139091 relative L2 0.001536200288683176\n",
      "training 0.0009833753574639559 relative L2 0.001536194933578372\n",
      "training 0.0009833683725446463 relative L2 0.001536189578473568\n",
      "training 0.0009833615040406585 relative L2 0.0015361843397840858\n",
      "training 0.0009833546355366707 relative L2 0.001536178751848638\n",
      "training 0.0009833475342020392 relative L2 0.001536173396743834\n",
      "training 0.0009833405492827296 relative L2 0.0015361681580543518\n",
      "training 0.0009833337971940637 relative L2 0.0015361629193648696\n",
      "training 0.000983326812274754 relative L2 0.0015361572150141\n",
      "training 0.0009833198273554444 relative L2 0.001536151859909296\n",
      "training 0.0009833130752667785 relative L2 0.0015361467376351357\n",
      "training 0.0009833060903474689 relative L2 0.001536141731776297\n",
      "training 0.0009832996875047684 relative L2 0.0015361362602561712\n",
      "training 0.0009832923533394933 relative L2 0.0015361309051513672\n",
      "training 0.000983285834081471 relative L2 0.0015361257828772068\n",
      "training 0.0009832789655774832 relative L2 0.001536120311357081\n",
      "training 0.0009832720970734954 relative L2 0.0015361146070063114\n",
      "training 0.0009832647629082203 relative L2 0.0015361093683168292\n",
      "training 0.0009832580108195543 relative L2 0.0015361038967967033\n",
      "training 0.0009832512587308884 relative L2 0.0015360988909378648\n",
      "training 0.0009832445066422224 relative L2 0.001536093419417739\n",
      "training 0.000983237405307591 relative L2 0.001536087947897613\n",
      "training 0.0009832304203882813 relative L2 0.0015360829420387745\n",
      "training 0.0009832234354689717 relative L2 0.0015360773541033268\n",
      "training 0.000983216566964984 relative L2 0.0015360721154138446\n",
      "training 0.0009832094656303525 relative L2 0.0015360666438937187\n",
      "training 0.0009832022478803992 relative L2 0.0015360611723735929\n",
      "training 0.0009831953793764114 relative L2 0.0015360558172687888\n",
      "training 0.0009831883944571018 relative L2 0.0015360505785793066\n",
      "training 0.0009831819916144013 relative L2 0.0015360456891357899\n",
      "training 0.0009831751231104136 relative L2 0.001536040217615664\n",
      "training 0.0009831682546064258 relative L2 0.00153603486251086\n",
      "training 0.0009831616189330816 relative L2 0.001536029390990734\n",
      "training 0.0009831548668444157 relative L2 0.0015360243851318955\n",
      "training 0.0009831477655097842 relative L2 0.0015360189136117697\n",
      "training 0.0009831407805904746 relative L2 0.0015360134420916438\n",
      "training 0.000983133795671165 relative L2 0.0015360080869868398\n",
      "training 0.0009831271599978209 relative L2 0.0015360028482973576\n",
      "training 0.0009831200586631894 relative L2 0.0015359976096078753\n",
      "training 0.0009831134229898453 relative L2 0.0015359919052571058\n",
      "training 0.0009831063216552138 relative L2 0.0015359865501523018\n",
      "training 0.0009830992203205824 relative L2 0.0015359813114628196\n",
      "training 0.0009830923518165946 relative L2 0.0015359760727733374\n",
      "training 0.0009830854833126068 relative L2 0.0015359704848378897\n",
      "training 0.000983078614808619 relative L2 0.0015359650133177638\n",
      "training 0.0009830715134739876 relative L2 0.0015359598910436034\n",
      "training 0.0009830646449699998 relative L2 0.0015359545359387994\n",
      "training 0.0009830576600506902 relative L2 0.001535948715172708\n",
      "training 0.0009830506751313806 relative L2 0.0015359434764832258\n",
      "training 0.0009830434573814273 relative L2 0.0015359382377937436\n",
      "training 0.0009830365888774395 relative L2 0.001535933231934905\n",
      "training 0.000983030186034739 relative L2 0.0015359275275841355\n",
      "training 0.0009830233175307512 relative L2 0.0015359222888946533\n",
      "training 0.0009830164490267634 relative L2 0.001535917050205171\n",
      "training 0.0009830094641074538 relative L2 0.001535911695100367\n",
      "training 0.000983002595603466 relative L2 0.0015359062235802412\n",
      "training 0.0009829956106841564 relative L2 0.0015359007520601153\n",
      "training 0.0009829883929342031 relative L2 0.0015358955133706331\n",
      "training 0.0009829814080148935 relative L2 0.001535890158265829\n",
      "training 0.0009829748887568712 relative L2 0.0015358846867457032\n",
      "training 0.0009829680202528834 relative L2 0.001535879448056221\n",
      "training 0.0009829611517488956 relative L2 0.0015358742093667388\n",
      "training 0.0009829538175836205 relative L2 0.0015358683886006474\n",
      "training 0.000982946832664311 relative L2 0.0015358630334958434\n",
      "training 0.0009829398477450013 relative L2 0.0015358577948063612\n",
      "training 0.0009829329792410135 relative L2 0.001535852556116879\n",
      "training 0.000982925877906382 relative L2 0.0015358470845967531\n",
      "training 0.000982919242233038 relative L2 0.001535841845907271\n",
      "training 0.0009829123737290502 relative L2 0.001535836374387145\n",
      "training 0.0009829055052250624 relative L2 0.0015358311356976628\n",
      "training 0.000982898403890431 relative L2 0.0015358255477622151\n",
      "training 0.0009828915353864431 relative L2 0.001535820192657411\n",
      "training 0.0009828845504671335 relative L2 0.0015358144883066416\n",
      "training 0.0009828772163018584 relative L2 0.001535808783955872\n",
      "training 0.0009828699985519052 relative L2 0.0015358033124357462\n",
      "training 0.0009828627808019519 relative L2 0.001535798073746264\n",
      "training 0.000982856028713286 relative L2 0.0015357928350567818\n",
      "training 0.0009828490437939763 relative L2 0.0015357875963672996\n",
      "training 0.0009828421752899885 relative L2 0.0015357821248471737\n",
      "training 0.000982835190370679 relative L2 0.0015357767697423697\n",
      "training 0.0009828282054513693 relative L2 0.001535771181806922\n",
      "training 0.000982820987701416 relative L2 0.0015357659431174397\n",
      "training 0.0009828140027821064 relative L2 0.0015357604715973139\n",
      "training 0.0009828071342781186 relative L2 0.0015357548836618662\n",
      "training 0.0009828002657741308 relative L2 0.001535749644972384\n",
      "training 0.0009827932808548212 relative L2 0.0015357441734522581\n",
      "training 0.0009827861795201898 relative L2 0.001535738818347454\n",
      "training 0.000982779311016202 relative L2 0.0015357331139966846\n",
      "training 0.0009827720932662487 relative L2 0.0015357277588918805\n",
      "training 0.0009827649919316173 relative L2 0.0015357225202023983\n",
      "training 0.0009827583562582731 relative L2 0.0015357170486822724\n",
      "training 0.0009827512549236417 relative L2 0.0015357118099927902\n",
      "training 0.000982744386419654 relative L2 0.0015357059892266989\n",
      "training 0.0009827370522543788 relative L2 0.0015357006341218948\n",
      "training 0.0009827299509197474 relative L2 0.001535695162601769\n",
      "training 0.0009827230824157596 relative L2 0.0015356899239122868\n",
      "training 0.0009827163303270936 relative L2 0.001535684335976839\n",
      "training 0.000982709345407784 relative L2 0.0015356787480413914\n",
      "training 0.0009827020112425089 relative L2 0.0015356735093519092\n",
      "training 0.0009826954919844866 relative L2 0.0015356680378317833\n",
      "training 0.0009826886234804988 relative L2 0.0015356630319729447\n",
      "training 0.0009826815221458673 relative L2 0.0015356575604528189\n",
      "training 0.000982674420811236 relative L2 0.0015356519725173712\n",
      "training 0.0009826675523072481 relative L2 0.001535646733827889\n",
      "training 0.0009826606838032603 relative L2 0.0015356411458924413\n",
      "training 0.0009826535824686289 relative L2 0.001535635907202959\n",
      "training 0.000982646713964641 relative L2 0.001535630552098155\n",
      "training 0.0009826399618759751 relative L2 0.0015356248477473855\n",
      "training 0.0009826329769566655 relative L2 0.0015356193762272596\n",
      "training 0.0009826257592067122 relative L2 0.0015356140211224556\n",
      "training 0.0009826188907027245 relative L2 0.0015356087824329734\n",
      "training 0.000982611789368093 relative L2 0.0015356030780822039\n",
      "training 0.0009826048044487834 relative L2 0.0015355973737314343\n",
      "training 0.0009825974702835083 relative L2 0.001535592251457274\n",
      "training 0.0009825906017795205 relative L2 0.0015355870127677917\n",
      "training 0.000982583616860211 relative L2 0.001535581424832344\n",
      "training 0.000982576748356223 relative L2 0.0015355759533122182\n",
      "training 0.0009825697634369135 relative L2 0.001535570714622736\n",
      "training 0.0009825627785176039 relative L2 0.0015355654759332538\n",
      "training 0.000982556026428938 relative L2 0.0015355604700744152\n",
      "training 0.000982549274340272 relative L2 0.0015355546493083239\n",
      "training 0.0009825420565903187 relative L2 0.001535549177788198\n",
      "training 0.0009825351880863309 relative L2 0.0015355441719293594\n",
      "training 0.0009825280867516994 relative L2 0.0015355385839939117\n",
      "training 0.0009825212182477117 relative L2 0.001535532996058464\n",
      "training 0.0009825141169130802 relative L2 0.0015355277573689818\n",
      "training 0.0009825070155784488 relative L2 0.0015355220530182123\n",
      "training 0.0009824997978284955 relative L2 0.0015355166979134083\n",
      "training 0.0009824924636632204 relative L2 0.001535510877147317\n",
      "training 0.0009824852459132671 relative L2 0.0015355052892118692\n",
      "training 0.0009824780281633139 relative L2 0.0015354998176917434\n",
      "training 0.0009824709268286824 relative L2 0.0015354945790022612\n",
      "training 0.0009824640583246946 relative L2 0.0015354889910668135\n",
      "training 0.0009824569569900632 relative L2 0.0015354837523773313\n",
      "training 0.0009824499720707536 relative L2 0.0015354781644418836\n",
      "training 0.0009824428707361221 relative L2 0.0015354726929217577\n",
      "training 0.0009824358858168125 relative L2 0.0015354673378169537\n",
      "training 0.0009824286680668592 relative L2 0.0015354615170508623\n",
      "training 0.000982421450316906 relative L2 0.00153545627836138\n",
      "training 0.0009824148146435618 relative L2 0.0015354511560872197\n",
      "training 0.000982407946139574 relative L2 0.0015354458009824157\n",
      "training 0.0009824008448049426 relative L2 0.0015354397473856807\n",
      "training 0.0009823935106396675 relative L2 0.0015354345086961985\n",
      "training 0.000982386525720358 relative L2 0.0015354289207607508\n",
      "training 0.0009823793079704046 relative L2 0.0015354235656559467\n",
      "training 0.0009823722066357732 relative L2 0.0015354184433817863\n",
      "training 0.0009823656873777509 relative L2 0.0015354128554463387\n",
      "training 0.0009823584696277976 relative L2 0.0015354073839262128\n",
      "training 0.0009823517175391316 relative L2 0.0015354020288214087\n",
      "training 0.0009823446162045002 relative L2 0.0015353969065472484\n",
      "training 0.0009823377477005124 relative L2 0.0015353912021964788\n",
      "training 0.0009823307627812028 relative L2 0.0015353859635069966\n",
      "training 0.000982323894277215 relative L2 0.0015353804919868708\n",
      "training 0.0009823166765272617 relative L2 0.001535375020466745\n",
      "training 0.0009823095751926303 relative L2 0.0015353694325312972\n",
      "training 0.0009823024738579988 relative L2 0.0015353638445958495\n",
      "training 0.0009822954889386892 relative L2 0.0015353591879829764\n",
      "training 0.000982288853265345 relative L2 0.001535353367216885\n",
      "training 0.00098228151910007 relative L2 0.0015353477792814374\n",
      "training 0.0009822745341807604 relative L2 0.0015353423077613115\n",
      "training 0.0009822676656767726 relative L2 0.0015353367198258638\n",
      "training 0.0009822604479268193 relative L2 0.0015353308990597725\n",
      "training 0.000982253230176866 relative L2 0.0015353255439549685\n",
      "training 0.0009822462452575564 relative L2 0.0015353200724348426\n",
      "training 0.0009822387946769595 relative L2 0.0015353146009147167\n",
      "training 0.000982231693342328 relative L2 0.0015353088965639472\n",
      "training 0.0009822247084230185 relative L2 0.0015353030757978559\n",
      "training 0.0009822173742577434 relative L2 0.00153529760427773\n",
      "training 0.0009822100400924683 relative L2 0.0015352920163422823\n",
      "training 0.0009822029387578368 relative L2 0.0015352866612374783\n",
      "training 0.0009821958374232054 relative L2 0.001535281422547996\n",
      "training 0.0009821889689192176 relative L2 0.0015352761838585138\n",
      "training 0.0009821821004152298 relative L2 0.001535270712338388\n",
      "training 0.0009821747662499547 relative L2 0.0015352654736489058\n",
      "training 0.0009821681305766106 relative L2 0.0015352602349594235\n",
      "training 0.0009821610292419791 relative L2 0.0015352544141933322\n",
      "training 0.0009821538114920259 relative L2 0.0015352488262578845\n",
      "training 0.0009821464773267508 relative L2 0.0015352432383224368\n",
      "training 0.0009821393759921193 relative L2 0.0015352381160482764\n",
      "training 0.0009821325074881315 relative L2 0.0015352327609434724\n",
      "training 0.0009821256389841437 relative L2 0.0015352272894233465\n",
      "training 0.0009821190033107996 relative L2 0.0015352218179032207\n",
      "training 0.0009821119019761682 relative L2 0.0015352164627984166\n",
      "training 0.0009821049170568585 relative L2 0.0015352112241089344\n",
      "training 0.0009820981649681926 relative L2 0.0015352052869275212\n",
      "training 0.0009820908308029175 relative L2 0.0015351998154073954\n",
      "training 0.0009820836130529642 relative L2 0.0015351943438872695\n",
      "training 0.000982076395303011 relative L2 0.0015351887559518218\n",
      "training 0.0009820691775530577 relative L2 0.0015351834008470178\n",
      "training 0.0009820623090490699 relative L2 0.001535177929326892\n",
      "training 0.000982055440545082 relative L2 0.0015351726906374097\n",
      "training 0.0009820484556257725 relative L2 0.0015351668698713183\n",
      "training 0.0009820412378758192 relative L2 0.0015351612819358706\n",
      "training 0.0009820341365411878 relative L2 0.0015351560432463884\n",
      "training 0.0009820266859605908 relative L2 0.0015351505717262626\n",
      "training 0.0009820194682106376 relative L2 0.0015351452166214585\n",
      "training 0.000982012483291328 relative L2 0.0015351396286860108\n",
      "training 0.0009820052655413747 relative L2 0.0015351339243352413\n",
      "training 0.0009819979313760996 relative L2 0.0015351285692304373\n",
      "training 0.00098199094645679 relative L2 0.001535122748464346\n",
      "training 0.0009819837287068367 relative L2 0.0015351169276982546\n",
      "training 0.0009819762781262398 relative L2 0.001535111339762807\n",
      "training 0.0009819691767916083 relative L2 0.0015351057518273592\n",
      "training 0.0009819614933803678 relative L2 0.0015351002803072333\n",
      "training 0.0009819545084610581 relative L2 0.0015350946923717856\n",
      "training 0.000981947174295783 relative L2 0.001535089686512947\n",
      "training 0.0009819403057917953 relative L2 0.0015350842149928212\n",
      "training 0.0009819333208724856 relative L2 0.001535078277811408\n",
      "training 0.0009819261031225324 relative L2 0.0015350726898759604\n",
      "training 0.000981919001787901 relative L2 0.0015350671019405127\n",
      "training 0.0009819117840379477 relative L2 0.0015350616304203868\n",
      "training 0.0009819044498726726 relative L2 0.0015350566245615482\n",
      "training 0.0009818978141993284 relative L2 0.0015350509202107787\n",
      "training 0.000981890712864697 relative L2 0.001535045332275331\n",
      "training 0.0009818833786994219 relative L2 0.001535039977170527\n",
      "training 0.0009818763937801123 relative L2 0.0015350341564044356\n",
      "training 0.0009818689431995153 relative L2 0.001535028568468988\n",
      "training 0.0009818614926189184 relative L2 0.0015350229805335402\n",
      "training 0.0009818542748689651 relative L2 0.001535017741844058\n",
      "training 0.0009818474063649774 relative L2 0.0015350120374932885\n",
      "training 0.000981840305030346 relative L2 0.001535006333142519\n",
      "training 0.0009818327380344272 relative L2 0.0015350008616223931\n",
      "training 0.0009818256366997957 relative L2 0.0015349953901022673\n",
      "training 0.0009818185353651643 relative L2 0.0015349900349974632\n",
      "training 0.0009818115504458547 relative L2 0.0015349844470620155\n",
      "training 0.0009818046819418669 relative L2 0.0015349783934652805\n",
      "training 0.0009817968821153045 relative L2 0.0015349728055298328\n",
      "training 0.0009817895479500294 relative L2 0.0015349675668403506\n",
      "training 0.0009817826794460416 relative L2 0.0015349617460742593\n",
      "training 0.0009817755781114101 relative L2 0.0015349563909694552\n",
      "training 0.000981768243946135 relative L2 0.0015349508030340075\n",
      "training 0.0009817610261961818 relative L2 0.0015349449822679162\n",
      "training 0.0009817536920309067 relative L2 0.0015349395107477903\n",
      "training 0.0009817463578656316 relative L2 0.0015349341556429863\n",
      "training 0.0009817391401156783 relative L2 0.001534928334876895\n",
      "training 0.0009817321551963687 relative L2 0.0015349233290180564\n",
      "training 0.0009817256359383464 relative L2 0.0015349176246672869\n",
      "training 0.0009817183017730713 relative L2 0.0015349122695624828\n",
      "training 0.0009817113168537617 relative L2 0.0015349066816270351\n",
      "training 0.000981704331934452 relative L2 0.0015349012101069093\n",
      "training 0.0009816968813538551 relative L2 0.001534895389340818\n",
      "training 0.00098168954718858 relative L2 0.0015348901506513357\n",
      "training 0.0009816825622692704 relative L2 0.001534884562715888\n",
      "training 0.000981675460934639 relative L2 0.0015348790911957622\n",
      "training 0.0009816683596000075 relative L2 0.0015348737360909581\n",
      "training 0.000981661374680698 relative L2 0.0015348681481555104\n",
      "training 0.0009816542733460665 relative L2 0.0015348626766353846\n",
      "training 0.000981647172011435 relative L2 0.0015348573215305805\n",
      "training 0.0009816403035074472 relative L2 0.0015348515007644892\n",
      "training 0.0009816328529268503 relative L2 0.0015348459128290415\n",
      "training 0.0009816252859309316 relative L2 0.001534840208478272\n",
      "training 0.0009816181845963001 relative L2 0.001534834853373468\n",
      "training 0.0009816113160923123 relative L2 0.0015348290326073766\n",
      "training 0.0009816039819270372 relative L2 0.0015348234446719289\n",
      "training 0.0009815969970077276 relative L2 0.0015348182059824467\n",
      "training 0.0009815898956730962 relative L2 0.0015348123852163553\n",
      "training 0.0009815824450924993 relative L2 0.0015348067972809076\n",
      "training 0.000981575227342546 relative L2 0.00153480120934546\n",
      "training 0.0009815676603466272 relative L2 0.0015347953885793686\n",
      "training 0.0009815603261813521 relative L2 0.0015347900334745646\n",
      "training 0.0009815534576773643 relative L2 0.001534784329123795\n",
      "training 0.000981546356342733 relative L2 0.0015347787411883473\n",
      "training 0.0009815391385927796 relative L2 0.0015347731532528996\n",
      "training 0.0009815318044275045 relative L2 0.001534767565317452\n",
      "training 0.0009815245866775513 relative L2 0.0015347616281360388\n",
      "training 0.0009815171360969543 relative L2 0.0015347558073699474\n",
      "training 0.0009815095691010356 relative L2 0.0015347505686804652\n",
      "training 0.000981502584181726 relative L2 0.0015347450971603394\n",
      "training 0.0009814954828470945 relative L2 0.0015347393928095698\n",
      "training 0.0009814882650971413 relative L2 0.001534733921289444\n",
      "training 0.0009814812801778316 relative L2 0.0015347286825999618\n",
      "training 0.000981474295258522 relative L2 0.001534723094664514\n",
      "training 0.0009814671939238906 relative L2 0.0015347175067290664\n",
      "training 0.0009814597433432937 relative L2 0.0015347120352089405\n",
      "training 0.000981452758423984 relative L2 0.0015347064472734928\n",
      "training 0.000981445424258709 relative L2 0.001534700975753367\n",
      "training 0.0009814385557547212 relative L2 0.0015346953878179193\n",
      "training 0.000981431338004768 relative L2 0.0015346897998824716\n",
      "training 0.0009814240038394928 relative L2 0.0015346838627010584\n",
      "training 0.0009814165532588959 relative L2 0.001534678041934967\n",
      "training 0.0009814093355089426 relative L2 0.0015346725704148412\n",
      "training 0.0009814020013436675 relative L2 0.0015346675645560026\n",
      "training 0.0009813952492550015 relative L2 0.0015346616273745894\n",
      "training 0.0009813877986744046 relative L2 0.0015346561558544636\n",
      "training 0.0009813806973397732 relative L2 0.0015346509171649814\n",
      "training 0.000981374061666429 relative L2 0.0015346453292295337\n",
      "training 0.0009813668439164758 relative L2 0.0015346400905400515\n",
      "training 0.0009813598589971662 relative L2 0.0015346346190199256\n",
      "training 0.0009813528740778565 relative L2 0.0015346287982538342\n",
      "training 0.0009813455399125814 relative L2 0.0015346230939030647\n",
      "training 0.0009813379729166627 relative L2 0.001534617505967617\n",
      "training 0.0009813306387513876 relative L2 0.0015346118016168475\n",
      "training 0.0009813234210014343 relative L2 0.001534606097266078\n",
      "training 0.000981316203251481 relative L2 0.0015346006257459521\n",
      "training 0.000981308869086206 relative L2 0.0015345948049798608\n",
      "training 0.0009813013020902872 relative L2 0.001534589217044413\n",
      "training 0.0009812939679250121 relative L2 0.001534583279863\n",
      "training 0.000981286633759737 relative L2 0.0015345776919275522\n",
      "training 0.0009812794160097837 relative L2 0.0015345721039921045\n",
      "training 0.0009812720818445086 relative L2 0.0015345669817179441\n",
      "training 0.0009812652133405209 relative L2 0.001534561044536531\n",
      "training 0.000981257646344602 relative L2 0.0015345559222623706\n",
      "training 0.0009812507778406143 relative L2 0.001534550217911601\n",
      "training 0.0009812434436753392 relative L2 0.0015345445135608315\n",
      "training 0.0009812363423407078 relative L2 0.0015345389256253839\n",
      "training 0.0009812291245907545 relative L2 0.0015345335705205798\n",
      "training 0.0009812217904254794 relative L2 0.001534528099000454\n",
      "training 0.0009812149219214916 relative L2 0.0015345225110650063\n",
      "training 0.0009812075877562165 relative L2 0.001534516573883593\n",
      "training 0.000981199904344976 relative L2 0.0015345113351941109\n",
      "training 0.0009811928030103445 relative L2 0.0015345052815973759\n",
      "training 0.0009811855852603912 relative L2 0.0015344999264925718\n",
      "training 0.0009811781346797943 relative L2 0.0015344941057264805\n",
      "training 0.0009811706840991974 relative L2 0.0015344881685450673\n",
      "training 0.0009811632335186005 relative L2 0.0015344825806096196\n",
      "training 0.0009811560157686472 relative L2 0.0015344771090894938\n",
      "training 0.000981148681603372 relative L2 0.0015344716375693679\n",
      "training 0.000981141347438097 relative L2 0.0015344660496339202\n",
      "training 0.0009811341296881437 relative L2 0.0015344606945291162\n",
      "training 0.0009811270283535123 relative L2 0.0015344548737630248\n",
      "training 0.0009811196941882372 relative L2 0.0015344490529969335\n",
      "training 0.000981112360022962 relative L2 0.0015344432322308421\n",
      "training 0.0009811049094423652 relative L2 0.0015344375278800726\n",
      "training 0.00098109757527709 relative L2 0.0015344320563599467\n",
      "training 0.000981090241111815 relative L2 0.0015344267012551427\n",
      "training 0.0009810831397771835 relative L2 0.0015344206476584077\n",
      "training 0.0009810758056119084 relative L2 0.0015344149433076382\n",
      "training 0.0009810682386159897 relative L2 0.0015344092389568686\n",
      "training 0.0009810610208660364 relative L2 0.001534403651021421\n",
      "training 0.0009810535702854395 relative L2 0.0015343980630859733\n",
      "training 0.0009810462361201644 relative L2 0.001534392242319882\n",
      "training 0.0009810386691242456 relative L2 0.0015343868872150779\n",
      "training 0.0009810314513742924 relative L2 0.0015343810664489865\n",
      "training 0.0009810241172090173 relative L2 0.0015343752456828952\n",
      "training 0.0009810170158743858 relative L2 0.001534370006993413\n",
      "training 0.0009810099145397544 relative L2 0.0015343640698119998\n",
      "training 0.0009810025803744793 relative L2 0.0015343581326305866\n",
      "training 0.000980995362624526 relative L2 0.001534352544695139\n",
      "training 0.0009809877956286073 relative L2 0.0015343468403443694\n",
      "training 0.0009809804614633322 relative L2 0.0015343414852395654\n",
      "training 0.0009809733601287007 relative L2 0.001534335664473474\n",
      "training 0.0009809661423787475 relative L2 0.0015343300765380263\n",
      "training 0.0009809586917981505 relative L2 0.0015343246050179005\n",
      "training 0.0009809514740481973 relative L2 0.0015343185514211655\n",
      "training 0.0009809439070522785 relative L2 0.0015343127306550741\n",
      "training 0.0009809363400563598 relative L2 0.0015343071427196264\n",
      "training 0.000980928773060441 relative L2 0.001534301438368857\n",
      "training 0.0009809217881411314 relative L2 0.0015342964325100183\n",
      "training 0.0009809149196371436 relative L2 0.0015342902624979615\n",
      "training 0.000980907236225903 relative L2 0.0015342849073931575\n",
      "training 0.0009809000184759498 relative L2 0.0015342788537964225\n",
      "training 0.0009808928007259965 relative L2 0.0015342734986916184\n",
      "training 0.0009808855829760432 relative L2 0.0015342674450948834\n",
      "training 0.0009808780159801245 relative L2 0.0015342613914981484\n",
      "training 0.000980870332568884 relative L2 0.0015342559199780226\n",
      "training 0.0009808631148189306 relative L2 0.0015342503320425749\n",
      "training 0.0009808560134842992 relative L2 0.0015342450933530927\n",
      "training 0.000980848795734346 relative L2 0.001534239505417645\n",
      "training 0.0009808414615690708 relative L2 0.0015342336846515536\n",
      "training 0.0009808341274037957 relative L2 0.001534228096716106\n",
      "training 0.0009808267932385206 relative L2 0.0015342225087806582\n",
      "training 0.0009808195754885674 relative L2 0.001534216688014567\n",
      "training 0.000980812474153936 relative L2 0.0015342108672484756\n",
      "training 0.000980805023573339 relative L2 0.0015342055121436715\n",
      "training 0.0009807981550693512 relative L2 0.0015342000406235456\n",
      "training 0.0009807907044887543 relative L2 0.0015341945691034198\n",
      "training 0.0009807837195694447 relative L2 0.001534188981167972\n",
      "training 0.0009807766182348132 relative L2 0.0015341833932325244\n",
      "training 0.0009807692840695381 relative L2 0.0015341772232204676\n",
      "training 0.0009807614842429757 relative L2 0.0015341714024543762\n",
      "training 0.0009807540336623788 relative L2 0.0015341653488576412\n",
      "training 0.0009807462338358164 relative L2 0.0015341598773375154\n",
      "training 0.0009807388996705413 relative L2 0.001534154056571424\n",
      "training 0.0009807313326746225 relative L2 0.0015341484686359763\n",
      "training 0.0009807241149246693 relative L2 0.0015341428807005286\n",
      "training 0.0009807167807593942 relative L2 0.0015341367106884718\n",
      "training 0.0009807092137634754 relative L2 0.0015341308899223804\n",
      "training 0.0009807015303522348 relative L2 0.001534125185571611\n",
      "training 0.000980694079771638 relative L2 0.0015341193648055196\n",
      "training 0.000980686629191041 relative L2 0.0015341141261160374\n",
      "training 0.0009806796442717314 relative L2 0.001534108305349946\n",
      "training 0.0009806723101064563 relative L2 0.0015341030666604638\n",
      "training 0.000980665092356503 relative L2 0.001534096896648407\n",
      "training 0.000980657641775906 relative L2 0.0015340911922976375\n",
      "training 0.0009806498419493437 relative L2 0.001534084789454937\n",
      "training 0.0009806420421227813 relative L2 0.0015340788522735238\n",
      "training 0.0009806343587115407 relative L2 0.0015340732643380761\n",
      "training 0.0009806269081309438 relative L2 0.001534068025648594\n",
      "training 0.0009806196903809905 relative L2 0.001534062554128468\n",
      "training 0.0009806128218770027 relative L2 0.001534057199023664\n",
      "training 0.000980605836957693 relative L2 0.0015340514946728945\n",
      "training 0.0009805988520383835 relative L2 0.001534045790322125\n",
      "training 0.0009805914014577866 relative L2 0.00153403973672539\n",
      "training 0.0009805834852159023 relative L2 0.001534033683128655\n",
      "training 0.0009805753361433744 relative L2 0.00153402762953192\n",
      "training 0.0009805678855627775 relative L2 0.0015340220415964723\n",
      "training 0.000980560784228146 relative L2 0.0015340163372457027\n",
      "training 0.0009805532172322273 relative L2 0.0015340108657255769\n",
      "training 0.000980545999482274 relative L2 0.0015340052777901292\n",
      "training 0.0009805388981476426 relative L2 0.0015339994570240378\n",
      "training 0.0009805314475670457 relative L2 0.0015339935198426247\n",
      "training 0.0009805237641558051 relative L2 0.0015339875826612115\n",
      "training 0.0009805161971598864 relative L2 0.0015339814126491547\n",
      "training 0.0009805085137486458 relative L2 0.0015339759411290288\n",
      "training 0.0009805011795833707 relative L2 0.0015339701203629375\n",
      "training 0.0009804937290027738 relative L2 0.0015339645324274898\n",
      "training 0.0009804863948374987 relative L2 0.0015339592937380075\n",
      "training 0.0009804792935028672 relative L2 0.0015339533565565944\n",
      "training 0.0009804718429222703 relative L2 0.0015339474193751812\n",
      "training 0.0009804642759263515 relative L2 0.0015339417150244117\n",
      "training 0.0009804567089304328 relative L2 0.0015339356614276767\n",
      "training 0.000980449141934514 relative L2 0.0015339301899075508\n",
      "training 0.0009804419241845608 relative L2 0.001533924718387425\n",
      "training 0.0009804349392652512 relative L2 0.0015339191304519773\n",
      "training 0.0009804278379306197 relative L2 0.0015339135425165296\n",
      "training 0.00098042085301131 relative L2 0.0015339079545810819\n",
      "training 0.0009804137516766787 relative L2 0.0015339020173996687\n",
      "training 0.00098040618468076 relative L2 0.0015338961966335773\n",
      "training 0.0009803981520235538 relative L2 0.0015338902594521642\n",
      "training 0.000980390585027635 relative L2 0.0015338844386860728\n",
      "training 0.0009803830180317163 relative L2 0.0015338785015046597\n",
      "training 0.0009803756838664412 relative L2 0.001533872913569212\n",
      "training 0.0009803681168705225 relative L2 0.0015338667435571551\n",
      "training 0.000980360433459282 relative L2 0.0015338606899604201\n",
      "training 0.0009803526336327195 relative L2 0.0015338546363636851\n",
      "training 0.0009803450666368008 relative L2 0.0015338489320129156\n",
      "training 0.0009803376160562038 relative L2 0.001533843344077468\n",
      "training 0.0009803302818909287 relative L2 0.0015338377561420202\n",
      "training 0.0009803229477256536 relative L2 0.0015338315861299634\n",
      "training 0.0009803154971450567 relative L2 0.0015338262310251594\n",
      "training 0.0009803080465644598 relative L2 0.001533820410259068\n",
      "training 0.0009803005959838629 relative L2 0.0015338149387389421\n",
      "training 0.000980293145403266 relative L2 0.0015338088851422071\n",
      "training 0.0009802855784073472 relative L2 0.0015338031807914376\n",
      "training 0.0009802780114114285 relative L2 0.001533797476440668\n",
      "training 0.0009802707936614752 relative L2 0.0015337916556745768\n",
      "training 0.0009802633430808783 relative L2 0.0015337858349084854\n",
      "training 0.0009802556596696377 relative L2 0.001533780130557716\n",
      "training 0.0009802483255043626 relative L2 0.0015337744262069464\n",
      "training 0.0009802409913390875 relative L2 0.0015337687218561769\n",
      "training 0.0009802335407584906 relative L2 0.0015337629010900855\n",
      "training 0.0009802262065932155 relative L2 0.0015337570803239942\n",
      "training 0.0009802186395972967 relative L2 0.0015337514923885465\n",
      "training 0.0009802111890166998 relative L2 0.0015337453223764896\n",
      "training 0.0009802035056054592 relative L2 0.001533739734441042\n",
      "training 0.0009801961714401841 relative L2 0.0015337339136749506\n",
      "training 0.000980188837274909 relative L2 0.001533728325739503\n",
      "training 0.000980181386694312 relative L2 0.0015337227378040552\n",
      "training 0.000980174052529037 relative L2 0.0015337164513766766\n",
      "training 0.0009801664855331182 relative L2 0.0015337106306105852\n",
      "training 0.0009801583364605904 relative L2 0.0015337044605985284\n",
      "training 0.0009801506530493498 relative L2 0.0015336985234171152\n",
      "training 0.0009801429696381092 relative L2 0.0015336929354816675\n",
      "training 0.0009801355190575123 relative L2 0.0015336869983002543\n",
      "training 0.0009801279520615935 relative L2 0.0015336809447035193\n",
      "training 0.0009801203850656748 relative L2 0.0015336752403527498\n",
      "training 0.0009801129344850779 relative L2 0.0015336694195866585\n",
      "training 0.0009801052510738373 relative L2 0.0015336635988205671\n",
      "training 0.0009800978004932404 relative L2 0.0015336574288085103\n",
      "training 0.000980090000666678 relative L2 0.0015336518408730626\n",
      "training 0.000980082550086081 relative L2 0.0015336460201069713\n",
      "training 0.000980075099505484 relative L2 0.0015336404321715236\n",
      "training 0.0009800675325095654 relative L2 0.0015336342621594667\n",
      "training 0.0009800601983442903 relative L2 0.001533628674224019\n",
      "training 0.0009800526313483715 relative L2 0.0015336225042119622\n",
      "training 0.000980044947937131 relative L2 0.0015336167998611927\n",
      "training 0.0009800372645258904 relative L2 0.0015336107462644577\n",
      "training 0.000980029464699328 relative L2 0.0015336046926677227\n",
      "training 0.0009800218977034092 relative L2 0.0015335988719016314\n",
      "training 0.0009800140978768468 relative L2 0.0015335927018895745\n",
      "training 0.000980006530880928 relative L2 0.0015335868811234832\n",
      "training 0.0009799989638850093 relative L2 0.0015335811767727137\n",
      "training 0.0009799912804737687 relative L2 0.0015335750067606568\n",
      "training 0.0009799833642318845 relative L2 0.0015335694188252091\n",
      "training 0.0009799761464819312 relative L2 0.0015335639473050833\n",
      "training 0.0009799688123166561 relative L2 0.0015335585922002792\n",
      "training 0.0009799619438126683 relative L2 0.0015335528878495097\n",
      "training 0.0009799543768167496 relative L2 0.0015335469506680965\n",
      "training 0.0009799469262361526 relative L2 0.0015335410134866834\n",
      "training 0.0009799391264095902 relative L2 0.0015335350763052702\n",
      "training 0.0009799315594136715 relative L2 0.0015335286734625697\n",
      "training 0.0009799236431717873 relative L2 0.0015335227362811565\n",
      "training 0.000979915726929903 relative L2 0.0015335165662690997\n",
      "training 0.0009799079271033406 relative L2 0.0015335103962570429\n",
      "training 0.0009799003601074219 relative L2 0.0015335046919062734\n",
      "training 0.0009798926766961813 relative L2 0.0015334986383095384\n",
      "training 0.0009798851097002625 relative L2 0.0015334929339587688\n",
      "training 0.000979877426289022 relative L2 0.0015334868803620338\n",
      "training 0.0009798696264624596 relative L2 0.001533480710349977\n",
      "training 0.000979861943051219 relative L2 0.001533475355245173\n",
      "training 0.0009798549581319094 relative L2 0.0015334697673097253\n",
      "training 0.0009798473911359906 relative L2 0.0015334635972976685\n",
      "training 0.0009798399405553937 relative L2 0.0015334576601162553\n",
      "training 0.0009798324899747968 relative L2 0.0015334521885961294\n",
      "training 0.0009798246901482344 relative L2 0.0015334462514147162\n",
      "training 0.0009798173559829593 relative L2 0.0015334400814026594\n",
      "training 0.0009798099054023623 relative L2 0.0015334344934672117\n",
      "training 0.0009798021055758 relative L2 0.0015334286727011204\n",
      "training 0.000979794654995203 relative L2 0.001533422851935029\n",
      "training 0.000979787320829928 relative L2 0.001533416798338294\n",
      "training 0.0009797796374186873 relative L2 0.001533410744741559\n",
      "training 0.0009797719540074468 relative L2 0.0015334050403907895\n",
      "training 0.000979764387011528 relative L2 0.0015333989867940545\n",
      "training 0.0009797567036002874 relative L2 0.0015333936316892505\n",
      "training 0.000979749602265656 relative L2 0.0015333875780925155\n",
      "training 0.0009797416860237718 relative L2 0.001533381873741746\n",
      "training 0.0009797342354431748 relative L2 0.001533375820145011\n",
      "training 0.000979726668447256 relative L2 0.0015333702322095633\n",
      "training 0.000979719334281981 relative L2 0.0015333645278587937\n",
      "training 0.0009797117672860622 relative L2 0.001533358357846737\n",
      "training 0.0009797039674594998 relative L2 0.00153335218783468\n",
      "training 0.0009796962840482593 relative L2 0.001533346250653267\n",
      "training 0.000979688367806375 relative L2 0.00153334008064121\n",
      "training 0.0009796805679798126 relative L2 0.0015333339106291533\n",
      "training 0.0009796726517379284 relative L2 0.00153332797344774\n",
      "training 0.000979664851911366 relative L2 0.0015333221526816487\n",
      "training 0.0009796572849154472 relative L2 0.0015333163319155574\n",
      "training 0.0009796497179195285 relative L2 0.0015333107439801097\n",
      "training 0.0009796425001695752 relative L2 0.0015333049232140183\n",
      "training 0.0009796348167583346 relative L2 0.0015332988696172833\n",
      "training 0.0009796273661777377 relative L2 0.001533293048851192\n",
      "training 0.0009796195663511753 relative L2 0.0015332868788391352\n",
      "training 0.000979611766524613 relative L2 0.0015332810580730438\n",
      "training 0.0009796039666980505 relative L2 0.0015332751208916306\n",
      "training 0.0009795963997021317 relative L2 0.0015332693001255393\n",
      "training 0.0009795889491215348 relative L2 0.0015332635957747698\n",
      "training 0.000979581382125616 relative L2 0.0015332577750086784\n",
      "training 0.0009795739315450191 relative L2 0.0015332521870732307\n",
      "training 0.0009795664809644222 relative L2 0.001533246017061174\n",
      "training 0.000979558564722538 relative L2 0.0015332400798797607\n",
      "training 0.0009795508813112974 relative L2 0.0015332340262830257\n",
      "training 0.0009795433143153787 relative L2 0.0015332279726862907\n",
      "training 0.00097953574731946 relative L2 0.0015332219190895557\n",
      "training 0.0009795279474928975 relative L2 0.0015332159819081426\n",
      "training 0.0009795204969123006 relative L2 0.0015332100447267294\n",
      "training 0.0009795126970857382 relative L2 0.0015332038747146726\n",
      "training 0.0009795046644285321 relative L2 0.0015331977047026157\n",
      "training 0.0009794968646019697 relative L2 0.001533192116767168\n",
      "training 0.000979489297606051 relative L2 0.0015331861795857549\n",
      "training 0.0009794816141948104 relative L2 0.0015331803588196635\n",
      "training 0.0009794741636142135 relative L2 0.0015331740723922849\n",
      "training 0.00097946566529572 relative L2 0.001533167902380228\n",
      "training 0.0009794579818844795 relative L2 0.0015331620816141367\n",
      "training 0.0009794505313038826 relative L2 0.0015331568429246545\n",
      "training 0.0009794434299692512 relative L2 0.0015331506729125977\n",
      "training 0.0009794356301426888 relative L2 0.001533144386485219\n",
      "training 0.0009794277139008045 relative L2 0.0015331382164731622\n",
      "training 0.0009794196812435985 relative L2 0.0015331326285377145\n",
      "training 0.0009794122306630015 relative L2 0.0015331264585256577\n",
      "training 0.0009794044308364391 relative L2 0.0015331202885136008\n",
      "training 0.0009793968638405204 relative L2 0.0015331143513321877\n",
      "training 0.0009793889475986362 relative L2 0.0015331084141507745\n",
      "training 0.000979381031356752 relative L2 0.0015331022441387177\n",
      "training 0.0009793733479455113 relative L2 0.0015330963069573045\n",
      "training 0.000979365548118949 relative L2 0.001533090602606535\n",
      "training 0.0009793577482923865 relative L2 0.0015330846654251218\n",
      "training 0.0009793502977117896 relative L2 0.001533078495413065\n",
      "training 0.0009793424978852272 relative L2 0.0015330725582316518\n",
      "training 0.0009793350473046303 relative L2 0.0015330666210502386\n",
      "training 0.0009793272474780679 relative L2 0.001533061033114791\n",
      "training 0.0009793195640668273 relative L2 0.0015330546302720904\n",
      "training 0.0009793119970709085 relative L2 0.0015330490423366427\n",
      "training 0.0009793044300749898 relative L2 0.0015330431051552296\n",
      "training 0.0009792969794943929 relative L2 0.0015330371679738164\n",
      "training 0.0009792891796678305 relative L2 0.0015330309979617596\n",
      "training 0.000979281379841268 relative L2 0.0015330248279497027\n",
      "training 0.0009792736964300275 relative L2 0.0015330190071836114\n",
      "training 0.000979266013018787 relative L2 0.0015330128371715546\n",
      "training 0.0009792579803615808 relative L2 0.001533006434328854\n",
      "training 0.0009792500641196966 relative L2 0.0015330000314861536\n",
      "training 0.000979241798631847 relative L2 0.001532994443550706\n",
      "training 0.0009792346972972155 relative L2 0.001532988273538649\n",
      "training 0.000979226897470653 relative L2 0.0015329824527725577\n",
      "training 0.0009792189812287688 relative L2 0.0015329767484217882\n",
      "training 0.000979211530648172 relative L2 0.0015329709276556969\n",
      "training 0.0009792039636522532 relative L2 0.0015329649904742837\n",
      "training 0.0009791961638256907 relative L2 0.0015329585876315832\n",
      "training 0.0009791882475838065 relative L2 0.0015329523012042046\n",
      "training 0.0009791799820959568 relative L2 0.0015329463640227914\n",
      "training 0.0009791721822693944 relative L2 0.0015329401940107346\n",
      "training 0.0009791644988581538 relative L2 0.0015329343732446432\n",
      "training 0.0009791571646928787 relative L2 0.00153292843606323\n",
      "training 0.0009791492484509945 relative L2 0.0015329219168052077\n",
      "training 0.000979141448624432 relative L2 0.0015329160960391164\n",
      "training 0.0009791337652131915 relative L2 0.0015329099260270596\n",
      "training 0.0009791258489713073 relative L2 0.0015329038724303246\n",
      "training 0.0009791180491447449 relative L2 0.001532897469587624\n",
      "training 0.000979109900072217 relative L2 0.001532891532406211\n",
      "training 0.00097910244949162 relative L2 0.0015328855952247977\n",
      "training 0.000979094416834414 relative L2 0.0015328797744587064\n",
      "training 0.000979087082669139 relative L2 0.0015328738372772932\n",
      "training 0.0009790795156732202 relative L2 0.0015328676672652364\n",
      "training 0.000979071483016014 relative L2 0.0015328614972531796\n",
      "training 0.0009790636831894517 relative L2 0.0015328553272411227\n",
      "training 0.0009790558833628893 relative L2 0.0015328493900597095\n",
      "training 0.0009790480835363269 relative L2 0.0015328434528782964\n",
      "training 0.0009790402837097645 relative L2 0.001532837050035596\n",
      "training 0.0009790321346372366 relative L2 0.001532830996438861\n",
      "training 0.000979024451225996 relative L2 0.001532824826426804\n",
      "training 0.00097901641856879 relative L2 0.001532818772830069\n",
      "training 0.0009790087351575494 relative L2 0.0015328124864026904\n",
      "training 0.0009790005860850215 relative L2 0.0015328065492212772\n",
      "training 0.0009789930190891027 relative L2 0.0015328009612858295\n",
      "training 0.0009789852192625403 relative L2 0.0015327949076890945\n",
      "training 0.0009789775358512998 relative L2 0.001532788504846394\n",
      "training 0.0009789695031940937 relative L2 0.0015327826840803027\n",
      "training 0.0009789618197828531 relative L2 0.0015327767468988895\n",
      "training 0.000978953903540969 relative L2 0.0015327706933021545\n",
      "training 0.000978946452960372 relative L2 0.0015327641740441322\n",
      "training 0.0009789385367184877 relative L2 0.0015327581204473972\n",
      "training 0.0009789303876459599 relative L2 0.0015327522996813059\n",
      "training 0.0009789225878193974 relative L2 0.0015327458968386054\n",
      "training 0.0009789149044081569 relative L2 0.0015327401924878359\n",
      "training 0.0009789071045815945 relative L2 0.0015327341388911009\n",
      "training 0.0009788995375856757 relative L2 0.0015327282017096877\n",
      "training 0.0009788916213437915 relative L2 0.0015327216824516654\n",
      "training 0.0009788835886865854 relative L2 0.0015327157452702522\n",
      "training 0.000978875788860023 relative L2 0.0015327099245041609\n",
      "training 0.000978868338279426 relative L2 0.0015327039873227477\n",
      "training 0.0009788606548681855 relative L2 0.0015326980501413345\n",
      "training 0.000978852971456945 relative L2 0.0015326919965445995\n",
      "training 0.0009788452880457044 relative L2 0.0015326860593631864\n",
      "training 0.0009788371389731765 relative L2 0.0015326797729358077\n",
      "training 0.000978829339146614 relative L2 0.0015326736029237509\n",
      "training 0.0009788214229047298 relative L2 0.001532667432911694\n",
      "training 0.0009788131574168801 relative L2 0.0015326612628996372\n",
      "training 0.0009788053575903177 relative L2 0.001532655325718224\n",
      "training 0.0009787974413484335 relative L2 0.0015326490392908454\n",
      "training 0.000978789757937193 relative L2 0.0015326429856941104\n",
      "training 0.0009787818416953087 relative L2 0.0015326370485126972\n",
      "training 0.0009787738090381026 relative L2 0.0015326308785006404\n",
      "training 0.0009787663584575057 relative L2 0.0015326248249039054\n",
      "training 0.0009787585586309433 relative L2 0.0015326187713071704\n",
      "training 0.0009787504095584154 relative L2 0.0015326126012951136\n",
      "training 0.0009787423769012094 relative L2 0.0015326065476983786\n",
      "training 0.0009787344606593251 relative L2 0.0015326004941016436\n",
      "training 0.00097872712649405 relative L2 0.0015325945569202304\n",
      "training 0.0009787192102521658 relative L2 0.0015325883869081736\n",
      "training 0.0009787112940102816 relative L2 0.001532582100480795\n",
      "training 0.0009787032613530755 relative L2 0.001532575930468738\n",
      "training 0.0009786952286958694 relative L2 0.0015325696440413594\n",
      "training 0.0009786873124539852 relative L2 0.001532563241198659\n",
      "training 0.0009786790469661355 relative L2 0.001532557187601924\n",
      "training 0.0009786711307242513 relative L2 0.001532551134005189\n",
      "training 0.0009786630980670452 relative L2 0.0015325449639931321\n",
      "training 0.0009786554146558046 relative L2 0.001532539026811719\n",
      "training 0.0009786476148292422 relative L2 0.0015325330896303058\n",
      "training 0.0009786398150026798 relative L2 0.0015325271524488926\n",
      "training 0.0009786320151761174 relative L2 0.0015325207496061921\n",
      "training 0.0009786239825189114 relative L2 0.0015325144631788135\n",
      "training 0.0009786158334463835 relative L2 0.0015325082931667566\n",
      "training 0.000978608033619821 relative L2 0.0015325023559853435\n",
      "training 0.0009786003502085805 relative L2 0.001532495953142643\n",
      "training 0.0009785923175513744 relative L2 0.001532489899545908\n",
      "training 0.0009785841684788465 relative L2 0.0015324837295338511\n",
      "training 0.0009785761358216405 relative L2 0.0015324775595217943\n",
      "training 0.000978568335995078 relative L2 0.0015324713895097375\n",
      "training 0.0009785604197531939 relative L2 0.0015324652194976807\n",
      "training 0.0009785523870959878 relative L2 0.0015324587002396584\n",
      "training 0.0009785445872694254 relative L2 0.0015324527630582452\n",
      "training 0.000978536787442863 relative L2 0.0015324465930461884\n",
      "training 0.0009785289876163006 relative L2 0.0015324404230341315\n",
      "training 0.0009785210713744164 relative L2 0.0015324341366067529\n",
      "training 0.0009785130387172103 relative L2 0.001532427966594696\n",
      "training 0.0009785047732293606 relative L2 0.0015324217965826392\n",
      "training 0.0009784967405721545 relative L2 0.0015324157429859042\n",
      "training 0.0009784889407455921 relative L2 0.0015324096893891692\n",
      "training 0.0009784807916730642 relative L2 0.0015324036357924342\n",
      "training 0.0009784729918465018 relative L2 0.0015323978150263429\n",
      "training 0.000978465541265905 relative L2 0.0015323918778449297\n",
      "training 0.0009784576250240207 relative L2 0.0015323858242481947\n",
      "training 0.0009784501744434237 relative L2 0.001532380236312747\n",
      "training 0.0009784428402781487 relative L2 0.0015323736006394029\n",
      "training 0.0009784346912056208 relative L2 0.001532367430627346\n",
      "training 0.0009784266585484147 relative L2 0.0015323612606152892\n",
      "training 0.0009784186258912086 relative L2 0.0015323549741879106\n",
      "training 0.0009784107096493244 relative L2 0.0015323488041758537\n",
      "training 0.0009784026769921184 relative L2 0.0015323424013331532\n",
      "training 0.0009783946443349123 relative L2 0.0015323357656598091\n",
      "training 0.0009783863788470626 relative L2 0.0015323294792324305\n",
      "training 0.0009783783461898565 relative L2 0.0015323234256356955\n",
      "training 0.0009783703135326505 relative L2 0.0015323171392083168\n",
      "training 0.0009783623972907662 relative L2 0.0015323108527809381\n",
      "training 0.0009783543646335602 relative L2 0.0015323044499382377\n",
      "training 0.0009783465648069978 relative L2 0.0015322985127568245\n",
      "training 0.0009783384157344699 relative L2 0.0015322919934988022\n",
      "training 0.000978330266661942 relative L2 0.0015322857070714235\n",
      "training 0.0009783221175894141 relative L2 0.001532280002720654\n",
      "training 0.0009783145505934954 relative L2 0.001532273949123919\n",
      "training 0.0009783065179362893 relative L2 0.0015322677791118622\n",
      "training 0.000978298718109727 relative L2 0.0015322613762691617\n",
      "training 0.0009782910346984863 relative L2 0.0015322554390877485\n",
      "training 0.000978283118456602 relative L2 0.0015322489198297262\n",
      "training 0.0009782748529687524 relative L2 0.0015322427498176694\n",
      "training 0.0009782669367268682 relative L2 0.0015322366962209344\n",
      "training 0.0009782591369003057 relative L2 0.0015322305262088776\n",
      "training 0.0009782509878277779 relative L2 0.001532224123366177\n",
      "training 0.0009782430715858936 relative L2 0.001532218069769442\n",
      "training 0.0009782351553440094 relative L2 0.0015322116669267416\n",
      "training 0.000978226657025516 relative L2 0.001532205380499363\n",
      "training 0.00097821862436831 relative L2 0.0015321995597332716\n",
      "training 0.0009782110573723912 relative L2 0.0015321933897212148\n",
      "training 0.0009782033739611506 relative L2 0.0015321876853704453\n",
      "training 0.0009781953413039446 relative L2 0.0015321816317737103\n",
      "training 0.0009781874250620604 relative L2 0.0015321749961003661\n",
      "training 0.0009781790431588888 relative L2 0.0015321688260883093\n",
      "training 0.000978170894086361 relative L2 0.001532162306830287\n",
      "training 0.0009781626285985112 relative L2 0.0015321560204029083\n",
      "training 0.0009781544795259833 relative L2 0.0015321496175602078\n",
      "training 0.0009781463304534554 relative L2 0.0015321435639634728\n",
      "training 0.0009781382977962494 relative L2 0.0015321370447054505\n",
      "training 0.0009781301487237215 relative L2 0.00153213064186275\n",
      "training 0.0009781225817278028 relative L2 0.0015321247046813369\n",
      "training 0.000978114316239953 relative L2 0.00153211853466928\n",
      "training 0.0009781063999980688 relative L2 0.0015321122482419014\n",
      "training 0.0009780983673408628 relative L2 0.0015321063110604882\n",
      "training 0.0009780904510989785 relative L2 0.0015320995589718223\n",
      "training 0.0009780824184417725 relative L2 0.001532093621790409\n",
      "training 0.0009780742693692446 relative L2 0.0015320872189477086\n",
      "training 0.0009780662367120385 relative L2 0.0015320811653509736\n",
      "training 0.0009780584368854761 relative L2 0.0015320749953389168\n",
      "training 0.0009780501713976264 relative L2 0.0015320684760808945\n",
      "training 0.0009780417894944549 relative L2 0.001532062073238194\n",
      "training 0.0009780337568372488 relative L2 0.001532056019641459\n",
      "training 0.000978025607764721 relative L2 0.001532049267552793\n",
      "training 0.000978017458692193 relative L2 0.0015320429811254144\n",
      "training 0.000978009426034987 relative L2 0.001532037160359323\n",
      "training 0.000978001393377781 relative L2 0.0015320312231779099\n",
      "training 0.0009779937099665403 relative L2 0.001532025053165853\n",
      "training 0.000977985793724656 relative L2 0.0015320188831537962\n",
      "training 0.0009779776446521282 relative L2 0.0015320125967264175\n",
      "training 0.0009779693791642785 relative L2 0.0015320064267143607\n",
      "training 0.000977961579337716 relative L2 0.0015320002567023039\n",
      "training 0.0009779534302651882 relative L2 0.0015319943195208907\n",
      "training 0.000977945514023304 relative L2 0.0015319878002628684\n",
      "training 0.0009779372485354543 relative L2 0.0015319810481742024\n",
      "training 0.0009779288666322827 relative L2 0.001531974645331502\n",
      "training 0.0009779207175597548 relative L2 0.0015319682424888015\n",
      "training 0.0009779124520719051 relative L2 0.0015319619560614228\n",
      "training 0.0009779040701687336 relative L2 0.0015319554368034005\n",
      "training 0.000977895688265562 relative L2 0.0015319492667913437\n",
      "training 0.000977887655608356 relative L2 0.0015319433296099305\n",
      "training 0.0009778797393664718 relative L2 0.0015319365775212646\n",
      "training 0.000977871473878622 relative L2 0.001531930291093886\n",
      "training 0.000977863441221416 relative L2 0.001531924121081829\n",
      "training 0.0009778551757335663 relative L2 0.0015319179510697722\n",
      "training 0.0009778473759070039 relative L2 0.00153191143181175\n",
      "training 0.000977839226834476 relative L2 0.001531905378215015\n",
      "training 0.0009778314270079136 relative L2 0.001531899208202958\n",
      "training 0.0009778232779353857 relative L2 0.0015318926889449358\n",
      "training 0.000977815012447536 relative L2 0.0015318867517635226\n",
      "training 0.0009778070962056518 relative L2 0.0015318803489208221\n",
      "training 0.0009777991799637675 relative L2 0.001531873713247478\n",
      "training 0.0009777904488146305 relative L2 0.0015318675432354212\n",
      "training 0.00097778276540339 relative L2 0.001531861606054008\n",
      "training 0.0009777747327461839 relative L2 0.001531854853965342\n",
      "training 0.0009777664672583342 relative L2 0.0015318485675379634\n",
      "training 0.000977758434601128 relative L2 0.0015318423975259066\n",
      "training 0.000977750401943922 relative L2 0.0015318364603444934\n",
      "training 0.0009777424857020378 relative L2 0.001531829941086471\n",
      "training 0.0009777344530448318 relative L2 0.0015318234218284488\n",
      "training 0.0009777258383110166 relative L2 0.0015318169025704265\n",
      "training 0.0009777172235772014 relative L2 0.0015318103833124042\n",
      "training 0.0009777089580893517 relative L2 0.0015318040968850255\n",
      "training 0.0009777009254321456 relative L2 0.0015317979268729687\n",
      "training 0.0009776926599442959 relative L2 0.0015317915240302682\n",
      "training 0.000977684510871768 relative L2 0.001531784888356924\n",
      "training 0.000977676478214562 relative L2 0.0015317787183448672\n",
      "training 0.0009776685619726777 relative L2 0.001531772082671523\n",
      "training 0.0009776600636541843 relative L2 0.0015317656798288226\n",
      "training 0.0009776516817510128 relative L2 0.001531759393401444\n",
      "training 0.0009776432998478413 relative L2 0.0015317534562200308\n",
      "training 0.0009776352671906352 relative L2 0.0015317469369620085\n",
      "training 0.0009776271181181073 relative L2 0.0015317401848733425\n",
      "training 0.0009776187362149358 relative L2 0.0015317338984459639\n",
      "training 0.0009776107035577297 relative L2 0.001531727728433907\n",
      "training 0.0009776025544852018 relative L2 0.0015317212091758847\n",
      "training 0.000977594405412674 relative L2 0.001531714922748506\n",
      "training 0.0009775859070941806 relative L2 0.0015317085199058056\n",
      "training 0.0009775778744369745 relative L2 0.0015317023498937488\n",
      "training 0.0009775698417797685 relative L2 0.0015316962962970138\n",
      "training 0.0009775618091225624 relative L2 0.0015316898934543133\n",
      "training 0.0009775538928806782 relative L2 0.0015316836070269346\n",
      "training 0.0009775458602234721 relative L2 0.0015316774370148778\n",
      "training 0.0009775379439815879 relative L2 0.0015316711505874991\n",
      "training 0.0009775296784937382 relative L2 0.0015316647477447987\n",
      "training 0.0009775214130058885 relative L2 0.00153165846131742\n",
      "training 0.0009775132639333606 relative L2 0.0015316519420593977\n",
      "training 0.0009775051148608327 relative L2 0.0015316453063860536\n",
      "training 0.0009774967329576612 relative L2 0.001531638903543353\n",
      "training 0.0009774883510544896 relative L2 0.0015316323842853308\n",
      "training 0.0009774805512279272 relative L2 0.0015316260978579521\n",
      "training 0.0009774721693247557 relative L2 0.0015316195785999298\n",
      "training 0.0009774640202522278 relative L2 0.0015316132921725512\n",
      "training 0.0009774556383490562 relative L2 0.0015316071221604943\n",
      "training 0.0009774473728612065 relative L2 0.0015316007193177938\n",
      "training 0.000977438990958035 relative L2 0.0015315946657210588\n",
      "training 0.0009774314239621162 relative L2 0.0015315882628783584\n",
      "training 0.0009774232748895884 relative L2 0.001531582442112267\n",
      "training 0.0009774151258170605 relative L2 0.0015315758064389229\n",
      "training 0.000977406743913889 relative L2 0.0015315692871809006\n",
      "training 0.0009773984784260392 relative L2 0.001531563000753522\n",
      "training 0.0009773902129381895 relative L2 0.0015315565979108214\n",
      "training 0.0009773821802809834 relative L2 0.0015315500786527991\n",
      "training 0.0009773741476237774 relative L2 0.0015315440250560641\n",
      "training 0.0009773654164746404 relative L2 0.0015315372729673982\n",
      "training 0.0009773570345714688 relative L2 0.0015315301716327667\n",
      "training 0.0009773484198376536 relative L2 0.0015315241180360317\n",
      "training 0.0009773403871804476 relative L2 0.0015315177151933312\n",
      "training 0.000977332005277276 relative L2 0.001531511195935309\n",
      "training 0.0009773237397894263 relative L2 0.0015315042110159993\n",
      "training 0.0009773148922249675 relative L2 0.0015314980410039425\n",
      "training 0.0009773067431524396 relative L2 0.0015314912889152765\n",
      "training 0.000977298361249268 relative L2 0.0015314852353185415\n",
      "training 0.0009772902121767402 relative L2 0.0015314787160605192\n",
      "training 0.0009772818302735686 relative L2 0.0015314723132178187\n",
      "training 0.000977273564785719 relative L2 0.0015314657939597964\n",
      "training 0.0009772652992978692 relative L2 0.0015314592747017741\n",
      "training 0.0009772568009793758 relative L2 0.0015314532211050391\n",
      "training 0.0009772490011528134 relative L2 0.0015314468182623386\n",
      "training 0.0009772408520802855 relative L2 0.0015314402990043163\n",
      "training 0.0009772322373464704 relative L2 0.0015314336633309722\n",
      "training 0.0009772239718586206 relative L2 0.0015314271440729499\n",
      "training 0.000977215706370771 relative L2 0.001531420974060893\n",
      "training 0.0009772074408829212 relative L2 0.001531414338387549\n",
      "training 0.0009771990589797497 relative L2 0.001531407586298883\n",
      "training 0.0009771904442459345 relative L2 0.0015314014162868261\n",
      "training 0.0009771824115887284 relative L2 0.0015313951298594475\n",
      "training 0.0009771742625162005 relative L2 0.0015313886106014252\n",
      "training 0.000977165880613029 relative L2 0.0015313823241740465\n",
      "training 0.0009771577315405011 relative L2 0.0015313755720853806\n",
      "training 0.0009771495824679732 relative L2 0.001531369285658002\n",
      "training 0.0009771413169801235 relative L2 0.0015313628828153014\n",
      "training 0.0009771328186616302 relative L2 0.0015313562471419573\n",
      "training 0.000977124203927815 relative L2 0.001531349727883935\n",
      "training 0.000977116054855287 relative L2 0.0015313437907025218\n",
      "training 0.000977108022198081 relative L2 0.0015313372714444995\n",
      "training 0.0009770997567102313 relative L2 0.0015313305193558335\n",
      "training 0.0009770911419764161 relative L2 0.0015313240000978112\n",
      "training 0.0009770826436579227 relative L2 0.0015313179465010762\n",
      "training 0.0009770747274160385 relative L2 0.001531311427243054\n",
      "training 0.0009770664619281888 relative L2 0.001531304675154388\n",
      "training 0.0009770577307790518 relative L2 0.0015312980394810438\n",
      "training 0.0009770493488758802 relative L2 0.0015312916366383433\n",
      "training 0.0009770413162186742 relative L2 0.0015312850009649992\n",
      "training 0.0009770329343155026 relative L2 0.0015312788309529424\n",
      "training 0.0009770250180736184 relative L2 0.0015312719624489546\n",
      "training 0.0009770164033398032 relative L2 0.001531265676021576\n",
      "training 0.0009770079050213099 relative L2 0.0015312591567635536\n",
      "training 0.0009769996395334601 relative L2 0.0015312526375055313\n",
      "training 0.0009769912576302886 relative L2 0.0015312457690015435\n",
      "training 0.000976982875727117 relative L2 0.0015312392497435212\n",
      "training 0.00097697414457798 relative L2 0.0015312323812395334\n",
      "training 0.000976965413428843 relative L2 0.0015312258619815111\n",
      "training 0.0009769572643563151 relative L2 0.0015312196919694543\n",
      "training 0.0009769488824531436 relative L2 0.0015312134055420756\n",
      "training 0.0009769408497959375 relative L2 0.0015312068862840533\n",
      "training 0.000976932467892766 relative L2 0.0015312001341953874\n",
      "training 0.0009769239695742726 relative L2 0.0015311938477680087\n",
      "training 0.0009769154712557793 relative L2 0.00153118756134063\n",
      "training 0.0009769072057679296 relative L2 0.001531180809251964\n",
      "training 0.0009768985910341144 relative L2 0.0015311742899939418\n",
      "training 0.0009768902091309428 relative L2 0.0015311676543205976\n",
      "training 0.0009768817108124495 relative L2 0.0015311607858166099\n",
      "training 0.000976873212493956 relative L2 0.001531154033727944\n",
      "training 0.000976864481344819 relative L2 0.0015311475144699216\n",
      "training 0.0009768560994416475 relative L2 0.001531141228042543\n",
      "training 0.0009768479503691196 relative L2 0.0015311347087845206\n",
      "training 0.0009768399177119136 relative L2 0.001531128422357142\n",
      "training 0.0009768316522240639 relative L2 0.0015311220195144415\n",
      "training 0.000976823503151536 relative L2 0.0015311150345951319\n",
      "training 0.0009768148884177208 relative L2 0.0015311086317524314\n",
      "training 0.000976806622929871 relative L2 0.001531102112494409\n",
      "training 0.0009767982410266995 relative L2 0.0015310958260670304\n",
      "training 0.0009767900919541717 relative L2 0.0015310893068090081\n",
      "training 0.0009767817100510001 relative L2 0.0015310827875509858\n",
      "training 0.000976773095317185 relative L2 0.0015310761518776417\n",
      "training 0.0009767645969986916 relative L2 0.0015310693997889757\n",
      "training 0.0009767563315108418 relative L2 0.0015310628805309534\n",
      "training 0.0009767477167770267 relative L2 0.001531056361272931\n",
      "training 0.0009767395677044988 relative L2 0.0015310494927689433\n",
      "training 0.0009767308365553617 relative L2 0.0015310433227568865\n",
      "training 0.000976722571067512 relative L2 0.0015310368034988642\n",
      "training 0.0009767143055796623 relative L2 0.0015310302842408419\n",
      "training 0.0009767059236764908 relative L2 0.001531023415736854\n",
      "training 0.0009766973089426756 relative L2 0.00153101678006351\n",
      "training 0.0009766888106241822 relative L2 0.001531010027974844\n",
      "training 0.0009766803123056889 relative L2 0.0015310037415474653\n",
      "training 0.000976672163233161 relative L2 0.0015309971058741212\n",
      "training 0.0009766636649146676 relative L2 0.0015309903537854552\n",
      "training 0.0009766548173502088 relative L2 0.0015309834852814674\n",
      "training 0.0009766460862010717 relative L2 0.0015309775481000543\n",
      "training 0.0009766380535438657 relative L2 0.001530971028842032\n",
      "training 0.0009766294388100505 relative L2 0.0015309641603380442\n",
      "training 0.0009766209404915571 relative L2 0.0015309576410800219\n",
      "training 0.0009766125585883856 relative L2 0.0015309511218219995\n",
      "training 0.0009766039438545704 relative L2 0.0015309442533180118\n",
      "training 0.0009765953291207552 relative L2 0.0015309376176446676\n",
      "training 0.00097658671438694 relative L2 0.0015309310983866453\n",
      "training 0.0009765782160684466 relative L2 0.0015309243462979794\n",
      "training 0.000976569834165275 relative L2 0.001530917827039957\n",
      "training 0.0009765615104697645 relative L2 0.0015309113077819347\n",
      "training 0.0009765527793206275 relative L2 0.001530904439277947\n",
      "training 0.0009765442227944732 relative L2 0.0015308979200199246\n",
      "training 0.0009765360155142844 relative L2 0.0015308912843465805\n",
      "training 0.000976527517195791 relative L2 0.001530884183011949\n",
      "training 0.0009765186114236712 relative L2 0.001530877547338605\n",
      "training 0.0009765101131051779 relative L2 0.001530871377326548\n",
      "training 0.0009765019058249891 relative L2 0.0015308650908991694\n",
      "training 0.0009764935239218175 relative L2 0.0015308583388105035\n",
      "training 0.0009764850256033242 relative L2 0.001530851237475872\n",
      "training 0.0009764762362465262 relative L2 0.0015308448346331716\n",
      "training 0.0009764680289663374 relative L2 0.0015308380825445056\n",
      "training 0.0009764595306478441 relative L2 0.0015308315632864833\n",
      "training 0.0009764509159140289 relative L2 0.0015308249276131392\n",
      "training 0.0009764422429725528 relative L2 0.0015308180591091514\n",
      "training 0.0009764337446540594 relative L2 0.0015308113070204854\n",
      "training 0.0009764248970896006 relative L2 0.0015308044385164976\n",
      "training 0.0009764162823557854 relative L2 0.0015307978028431535\n",
      "training 0.0009764076676219702 relative L2 0.0015307912835851312\n",
      "training 0.000976399052888155 relative L2 0.0015307848807424307\n",
      "training 0.0009763909038156271 relative L2 0.0015307782450690866\n",
      "training 0.0009763820562511683 relative L2 0.0015307713765650988\n",
      "training 0.0009763733251020312 relative L2 0.0015307648573070765\n",
      "training 0.000976364710368216 relative L2 0.0015307586872950196\n",
      "training 0.0009763568523339927 relative L2 0.001530752400867641\n",
      "training 0.0009763482376001775 relative L2 0.001530745648778975\n",
      "training 0.0009763399139046669 relative L2 0.0015307390131056309\n",
      "training 0.0009763314737938344 relative L2 0.001530732144601643\n",
      "training 0.0009763227426446974 relative L2 0.0015307252760976553\n",
      "training 0.0009763140697032213 relative L2 0.0015307188732549548\n",
      "training 0.0009763058042153716 relative L2 0.0015307122375816107\n",
      "training 0.0009762975969351828 relative L2 0.0015307053690776229\n",
      "training 0.0009762889822013676 relative L2 0.0015306988498196006\n",
      "training 0.0009762801928445697 relative L2 0.0015306917484849691\n",
      "training 0.0009762714034877717 relative L2 0.0015306848799809813\n",
      "training 0.0009762627887539566 relative L2 0.0015306782443076372\n",
      "training 0.0009762539993971586 relative L2 0.0015306713758036494\n",
      "training 0.0009762452682480216 relative L2 0.0015306645072996616\n",
      "training 0.0009762370027601719 relative L2 0.0015306581044569612\n",
      "training 0.0009762287372723222 relative L2 0.0015306515851989388\n",
      "training 0.0009762198897078633 relative L2 0.001530644716694951\n",
      "training 0.0009762115078046918 relative L2 0.001530638081021607\n",
      "training 0.0009762026602402329 relative L2 0.0015306315617635846\n",
      "training 0.0009761942783370614 relative L2 0.0015306246932595968\n",
      "training 0.0009761854307726026 relative L2 0.0015306184068322182\n",
      "training 0.0009761772817000747 relative L2 0.0015306116547435522\n",
      "training 0.000976169016212225 relative L2 0.0015306047862395644\n",
      "training 0.0009761604014784098 relative L2 0.0015305979177355766\n",
      "training 0.0009761513792909682 relative L2 0.0015305910492315888\n",
      "training 0.0009761425317265093 relative L2 0.0015305839478969574\n",
      "training 0.0009761338587850332 relative L2 0.0015305773122236133\n",
      "training 0.0009761250694282353 relative L2 0.0015305704437196255\n",
      "training 0.000976116512902081 relative L2 0.0015305636916309595\n",
      "training 0.000976107781752944 relative L2 0.0015305570559576154\n",
      "training 0.0009760992252267897 relative L2 0.0015305503038689494\n",
      "training 0.0009760906104929745 relative L2 0.001530543784610927\n",
      "training 0.0009760822867974639 relative L2 0.0015305374981835485\n",
      "training 0.0009760740213096142 relative L2 0.0015305306296795607\n",
      "training 0.000976065406575799 relative L2 0.0015305237611755729\n",
      "training 0.0009760567336343229 relative L2 0.0015305171255022287\n",
      "training 0.000976047886069864 relative L2 0.0015305097913369536\n",
      "training 0.0009760390967130661 relative L2 0.0015305031556636095\n",
      "training 0.0009760303073562682 relative L2 0.0015304962871596217\n",
      "training 0.0009760216344147921 relative L2 0.0015304897679015994\n",
      "training 0.000976012903265655 relative L2 0.0015304825501516461\n",
      "training 0.0009760038228705525 relative L2 0.0015304756816476583\n",
      "training 0.0009759950917214155 relative L2 0.001530469162389636\n",
      "training 0.0009759867680259049 relative L2 0.0015304626431316137\n",
      "training 0.0009759782114997506 relative L2 0.001530455774627626\n",
      "training 0.0009759693639352918 relative L2 0.0015304486732929945\n",
      "training 0.0009759605163708329 relative L2 0.0015304420376196504\n",
      "training 0.0009759518434293568 relative L2 0.0015304351691156626\n",
      "training 0.0009759434033185244 relative L2 0.0015304283006116748\n",
      "training 0.0009759347303770483 relative L2 0.001530421432107687\n",
      "training 0.0009759260574355721 relative L2 0.001530414680019021\n",
      "training 0.0009759176173247397 relative L2 0.0015304076950997114\n",
      "training 0.0009759088861756027 relative L2 0.0015304009430110455\n",
      "training 0.0009759000968188047 relative L2 0.0015303943073377013\n",
      "training 0.0009758913074620068 relative L2 0.0015303874388337135\n",
      "training 0.000975882459897548 relative L2 0.001530380337499082\n",
      "training 0.0009758736123330891 relative L2 0.0015303738182410598\n",
      "training 0.0009758654050529003 relative L2 0.0015303671825677156\n",
      "training 0.000975856848526746 relative L2 0.0015303606633096933\n",
      "training 0.0009758482920005918 relative L2 0.00153035344555974\n",
      "training 0.0009758390369825065 relative L2 0.0015303463442251086\n",
      "training 0.0009758301312103868 relative L2 0.0015303392428904772\n",
      "training 0.0009758214582689106 relative L2 0.0015303327236324549\n",
      "training 0.00097581249428913 relative L2 0.0015303255058825016\n",
      "training 0.0009758035303093493 relative L2 0.0015303188702091575\n",
      "training 0.0009757952648214996 relative L2 0.0015303120017051697\n",
      "training 0.000975786242634058 relative L2 0.0015303047839552164\n",
      "training 0.0009757773950695992 relative L2 0.001530297682620585\n",
      "training 0.0009757684310898185 relative L2 0.0015302910469472408\n",
      "training 0.0009757594671100378 relative L2 0.0015302842948585749\n",
      "training 0.0009757509105838835 relative L2 0.001530277426354587\n",
      "training 0.0009757421794347465 relative L2 0.001530270790681243\n",
      "training 0.0009757335647009313 relative L2 0.0015302634565159678\n",
      "training 0.0009757245425134897 relative L2 0.0015302564715966582\n",
      "training 0.0009757154621183872 relative L2 0.0015302497195079923\n",
      "training 0.0009757066145539284 relative L2 0.0015302428510040045\n",
      "training 0.0009756980580277741 relative L2 0.0015302359825000167\n",
      "training 0.0009756894432939589 relative L2 0.0015302294632419944\n",
      "training 0.0009756808867678046 relative L2 0.0015302225947380066\n",
      "training 0.0009756724466569722 relative L2 0.0015302159590646625\n",
      "training 0.0009756637155078351 relative L2 0.0015302092069759965\n",
      "training 0.000975655042566359 relative L2 0.0015302018728107214\n",
      "training 0.0009756459621712565 relative L2 0.00153019477147609\n",
      "training 0.0009756373474374413 relative L2 0.0015301882522180676\n",
      "training 0.000975628150627017 relative L2 0.001530181267298758\n",
      "training 0.0009756193030625582 relative L2 0.0015301747480407357\n",
      "training 0.0009756110375747085 relative L2 0.0015301676467061043\n",
      "training 0.0009756022482179105 relative L2 0.0015301605453714728\n",
      "training 0.000975593167822808 relative L2 0.001530153676867485\n",
      "training 0.0009755842620506883 relative L2 0.0015301466919481754\n",
      "training 0.0009755753562785685 relative L2 0.0015301399398595095\n",
      "training 0.0009755668579600751 relative L2 0.0015301329549401999\n",
      "training 0.0009755578357726336 relative L2 0.0015301258536055684\n",
      "training 0.0009755486971698701 relative L2 0.0015301189851015806\n",
      "training 0.0009755398496054113 relative L2 0.0015301123494282365\n",
      "training 0.0009755311184562743 relative L2 0.0015301054809242487\n",
      "training 0.0009755222708918154 relative L2 0.001530098612420261\n",
      "training 0.0009755134815350175 relative L2 0.0015300915110856295\n",
      "training 0.0009755044593475759 relative L2 0.0015300846425816417\n",
      "training 0.0009754957864060998 relative L2 0.0015300775412470102\n",
      "training 0.0009754868224263191 relative L2 0.0015300705563277006\n",
      "training 0.0009754778584465384 relative L2 0.0015300633385777473\n",
      "training 0.000975468719843775 relative L2 0.0015300564700737596\n",
      "training 0.0009754601633176208 relative L2 0.0015300497179850936\n",
      "training 0.0009754514321684837 relative L2 0.001530042733065784\n",
      "training 0.0009754427592270076 relative L2 0.0015300353989005089\n",
      "training 0.0009754336206242442 relative L2 0.0015300287632271647\n",
      "training 0.000975425005890429 relative L2 0.0015300215454772115\n",
      "training 0.0009754159254953265 relative L2 0.0015300143277272582\n",
      "training 0.0009754069033078849 relative L2 0.0015300074592232704\n",
      "training 0.0009753979393281043 relative L2 0.001530000357888639\n",
      "training 0.0009753890917636454 relative L2 0.0015299932565540075\n",
      "training 0.0009753802441991866 relative L2 0.001529986853711307\n",
      "training 0.0009753717458806932 relative L2 0.0015299798687919974\n",
      "training 0.0009753631893545389 relative L2 0.0015299730002880096\n",
      "training 0.0009753545164130628 relative L2 0.0015299663646146655\n",
      "training 0.0009753458434715867 relative L2 0.0015299591468647122\n",
      "training 0.000975336879491806 relative L2 0.0015299522783607244\n",
      "training 0.0009753279155120254 relative L2 0.001529945177026093\n",
      "training 0.0009753191843628883 relative L2 0.0015299383085221052\n",
      "training 0.0009753101621754467 relative L2 0.001529931090772152\n",
      "training 0.0009753014310263097 relative L2 0.0015299239894375205\n",
      "training 0.0009752925834618509 relative L2 0.0015299171209335327\n",
      "training 0.000975283794105053 relative L2 0.001529910252429545\n",
      "training 0.0009752746555022895 relative L2 0.0015299033839255571\n",
      "training 0.0009752658079378307 relative L2 0.001529896049760282\n",
      "training 0.0009752566111274064 relative L2 0.001529889297671616\n",
      "training 0.0009752479963935912 relative L2 0.0015298823127523065\n",
      "training 0.0009752390324138105 relative L2 0.0015298748621717095\n",
      "training 0.0009752297773957253 relative L2 0.0015298679936677217\n",
      "training 0.0009752209298312664 relative L2 0.001529861125163734\n",
      "training 0.0009752120822668076 relative L2 0.0015298539074137807\n",
      "training 0.0009752028272487223 relative L2 0.001529847038909793\n",
      "training 0.0009751939214766026 relative L2 0.001529839588329196\n",
      "training 0.000975184899289161 relative L2 0.0015298327198252082\n",
      "training 0.0009751762263476849 relative L2 0.0015298256184905767\n",
      "training 0.0009751668549142778 relative L2 0.0015298186335712671\n",
      "training 0.0009751584148034453 relative L2 0.0015298114158213139\n",
      "training 0.000975149217993021 relative L2 0.0015298043144866824\n",
      "training 0.0009751401375979185 relative L2 0.0015297968639060855\n",
      "training 0.0009751311736181378 relative L2 0.0015297898789867759\n",
      "training 0.0009751219186000526 relative L2 0.0015297826612368226\n",
      "training 0.0009751130710355937 relative L2 0.0015297757927328348\n",
      "training 0.0009751043398864567 relative L2 0.0015297688078135252\n",
      "training 0.0009750953176990151 relative L2 0.0015297618228942156\n",
      "training 0.0009750868193805218 relative L2 0.0015297547215595841\n",
      "training 0.0009750776225700974 relative L2 0.0015297477366402745\n",
      "training 0.0009750686585903168 relative L2 0.0015297408681362867\n",
      "training 0.0009750600438565016 relative L2 0.0015297337668016553\n",
      "training 0.0009750506142154336 relative L2 0.0015297267818823457\n",
      "training 0.0009750415338203311 relative L2 0.0015297194477170706\n",
      "training 0.0009750323370099068 relative L2 0.0015297128120437264\n",
      "training 0.0009750237804837525 relative L2 0.0015297059435397387\n",
      "training 0.0009750151075422764 relative L2 0.0015296988422051072\n",
      "training 0.0009750060853548348 relative L2 0.0015296918572857976\n",
      "training 0.000974997237790376 relative L2 0.0015296844067052007\n",
      "training 0.0009749880991876125 relative L2 0.001529676839709282\n",
      "training 0.0009749781456775963 relative L2 0.001529669389128685\n",
      "training 0.0009749690652824938 relative L2 0.0015296627534553409\n",
      "training 0.000974960217718035 relative L2 0.001529656001366675\n",
      "training 0.0009749517776072025 relative L2 0.0015296489000320435\n",
      "training 0.0009749429882504046 relative L2 0.0015296416822820902\n",
      "training 0.0009749339078553021 relative L2 0.001529634464532137\n",
      "training 0.0009749247692525387 relative L2 0.0015296272467821836\n",
      "training 0.0009749154560267925 relative L2 0.0015296201454475522\n",
      "training 0.0009749062592163682 relative L2 0.001529612927697599\n",
      "training 0.0009748971788212657 relative L2 0.0015296059427782893\n",
      "training 0.000974888214841485 relative L2 0.0015295983757823706\n",
      "training 0.0009748791344463825 relative L2 0.0015295912744477391\n",
      "training 0.0009748699958436191 relative L2 0.0015295841731131077\n",
      "training 0.000974860682617873 relative L2 0.001529577188193798\n",
      "training 0.0009748520678840578 relative L2 0.0015295703196898103\n",
      "training 0.0009748431039042771 relative L2 0.0015295628691092134\n",
      "training 0.0009748336742632091 relative L2 0.00152955565135926\n",
      "training 0.0009748240699991584 relative L2 0.0015295484336093068\n",
      "training 0.0009748149896040559 relative L2 0.0015295409830287099\n",
      "training 0.0009748060256242752 relative L2 0.0015295338816940784\n",
      "training 0.0009747969452291727 relative L2 0.0015295263146981597\n",
      "training 0.0009747873991727829 relative L2 0.00152951932977885\n",
      "training 0.0009747782023623586 relative L2 0.0015295124612748623\n",
      "training 0.0009747693547978997 relative L2 0.0015295047778636217\n",
      "training 0.0009747601579874754 relative L2 0.001529497909359634\n",
      "training 0.0009747510775923729 relative L2 0.0015294899931177497\n",
      "training 0.0009747418225742877 relative L2 0.0015294827753677964\n",
      "training 0.0009747323347255588 relative L2 0.001529475674033165\n",
      "training 0.0009747233125381172 relative L2 0.0015294684562832117\n",
      "training 0.0009747143485583365 relative L2 0.0015294618206098676\n",
      "training 0.0009747055009938776 relative L2 0.0015294544864445925\n",
      "training 0.0009746964788064361 relative L2 0.0015294476179406047\n",
      "training 0.0009746872237883508 relative L2 0.0015294404001906514\n",
      "training 0.0009746780269779265 relative L2 0.0015294334152713418\n",
      "training 0.0009746690629981458 relative L2 0.0015294257318601012\n",
      "training 0.0009746596915647388 relative L2 0.0015294187469407916\n",
      "training 0.0009746505529619753 relative L2 0.0015294109471142292\n",
      "training 0.0009746410651132464 relative L2 0.001529403729364276\n",
      "training 0.0009746316354721785 relative L2 0.0015293961623683572\n",
      "training 0.0009746221476234496 relative L2 0.0015293884789571166\n",
      "training 0.0009746127761900425 relative L2 0.001529381494037807\n",
      "training 0.0009746035793796182 relative L2 0.0015293743927031755\n",
      "training 0.0009745946736074984 relative L2 0.0015293670585379004\n",
      "training 0.0009745857096277177 relative L2 0.0015293596079573035\n",
      "training 0.0009745761635713279 relative L2 0.0015293523902073503\n",
      "training 0.0009745667921379209 relative L2 0.0015293449396267533\n",
      "training 0.0009745577699504793 relative L2 0.0015293380711227655\n",
      "training 0.0009745487477630377 relative L2 0.0015293306205421686\n",
      "training 0.0009745396091602743 relative L2 0.0015293237520381808\n",
      "training 0.0009745304705575109 relative L2 0.0015293164178729057\n",
      "training 0.0009745213319547474 relative L2 0.0015293094329535961\n",
      "training 0.0009745123679749668 relative L2 0.001529302098788321\n",
      "training 0.0009745031711645424 relative L2 0.0015292945317924023\n",
      "training 0.0009744938579387963 relative L2 0.0015292871976271272\n",
      "training 0.0009744848357513547 relative L2 0.001529279863461852\n",
      "training 0.0009744754061102867 relative L2 0.0015292727621272206\n",
      "training 0.0009744663839228451 relative L2 0.0015292653115466237\n",
      "training 0.000974457070697099 relative L2 0.0015292579773813486\n",
      "training 0.0009744479320943356 relative L2 0.0015292507596313953\n",
      "training 0.0009744384442456067 relative L2 0.001529242959804833\n",
      "training 0.0009744288981892169 relative L2 0.0015292358584702015\n",
      "training 0.0009744195849634707 relative L2 0.001529228058643639\n",
      "training 0.0009744103881530464 relative L2 0.0015292208408936858\n",
      "training 0.0009744010749273002 relative L2 0.001529213273897767\n",
      "training 0.0009743915870785713 relative L2 0.0015292057069018483\n",
      "training 0.0009743818663991988 relative L2 0.0015291986055672169\n",
      "training 0.0009743725531734526 relative L2 0.00152919115498662\n",
      "training 0.000974363530986011 relative L2 0.0015291841700673103\n",
      "training 0.0009743544505909085 relative L2 0.0015291767194867134\n",
      "training 0.0009743451955728233 relative L2 0.0015291691524907947\n",
      "training 0.000974335940554738 relative L2 0.001529161585494876\n",
      "training 0.0009743263944983482 relative L2 0.001529154134914279\n",
      "training 0.0009743168484419584 relative L2 0.0015291469171643257\n",
      "training 0.000974307709839195 relative L2 0.0015291394665837288\n",
      "training 0.0009742986294440925 relative L2 0.001529132598079741\n",
      "training 0.00097428954904899 relative L2 0.0015291247982531786\n",
      "training 0.0009742802358232439 relative L2 0.0015291175805032253\n",
      "training 0.000974270747974515 relative L2 0.0015291105955839157\n",
      "training 0.0009742619004100561 relative L2 0.0015291029121726751\n",
      "training 0.0009742528782226145 relative L2 0.0015290953451767564\n",
      "training 0.000974243157543242 relative L2 0.0015290877781808376\n",
      "training 0.0009742335532791913 relative L2 0.0015290809096768498\n",
      "training 0.0009742243564687669 relative L2 0.001529073342680931\n",
      "training 0.0009742152760736644 relative L2 0.0015290662413462996\n",
      "training 0.0009742060792632401 relative L2 0.0015290590235963464\n",
      "training 0.0009741964749991894 relative L2 0.001529051805846393\n",
      "training 0.0009741877438500524 relative L2 0.0015290447045117617\n",
      "training 0.0009741784306243062 relative L2 0.001529037137515843\n",
      "training 0.0009741693502292037 relative L2 0.001529030967503786\n",
      "training 0.0009741609101183712 relative L2 0.0015290234005078673\n",
      "training 0.0009741517133079469 relative L2 0.0015290158335119486\n",
      "training 0.0009741419926285744 relative L2 0.0015290087321773171\n",
      "training 0.0009741325047798455 relative L2 0.0015290011651813984\n",
      "training 0.0009741233661770821 relative L2 0.001528994645923376\n",
      "training 0.000974114635027945 relative L2 0.0015289877774193883\n",
      "training 0.0009741061949171126 relative L2 0.001528980559669435\n",
      "training 0.0009740966488607228 relative L2 0.0015289734583348036\n",
      "training 0.0009740874520502985 relative L2 0.0015289662405848503\n",
      "training 0.0009740781970322132 relative L2 0.0015289591392502189\n",
      "training 0.0009740693494677544 relative L2 0.0015289519215002656\n",
      "training 0.000974060210864991 relative L2 0.0015289447037503123\n",
      "training 0.0009740508976392448 relative L2 0.001528937486000359\n",
      "training 0.0009740414097905159 relative L2 0.0015289299190044403\n",
      "training 0.0009740320383571088 relative L2 0.0015289228176698089\n",
      "training 0.0009740227833390236 relative L2 0.0015289157163351774\n",
      "training 0.0009740134701132774 relative L2 0.0015289087314158678\n",
      "training 0.000974004331510514 relative L2 0.0015289015136659145\n",
      "training 0.0009739952511154115 relative L2 0.0015288940630853176\n",
      "training 0.0009739857050590217 relative L2 0.0015288866125047207\n",
      "training 0.0009739763918332756 relative L2 0.001528879744000733\n",
      "training 0.0009739669039845467 relative L2 0.0015288719441741705\n",
      "training 0.0009739575907588005 relative L2 0.0015288646100088954\n",
      "training 0.0009739483357407153 relative L2 0.001528858090750873\n",
      "training 0.0009739394881762564 relative L2 0.0015288506401702762\n",
      "training 0.000973930349573493 relative L2 0.0015288441209122539\n",
      "training 0.0009739215020090342 relative L2 0.0015288371359929442\n",
      "training 0.0009739124216139317 relative L2 0.0015288294525817037\n",
      "training 0.0009739029337652028 relative L2 0.0015288228169083595\n",
      "training 0.0009738939697854221 relative L2 0.0015288153663277626\n",
      "training 0.0009738843073137105 relative L2 0.001528808381408453\n",
      "training 0.0009738751105032861 relative L2 0.0015288015129044652\n",
      "training 0.0009738658554852009 relative L2 0.00152879417873919\n",
      "training 0.0009738568915054202 relative L2 0.0015287869609892368\n",
      "training 0.0009738473454490304 relative L2 0.0015287799760699272\n",
      "training 0.0009738381486386061 relative L2 0.0015287725254893303\n",
      "training 0.0009738290100358427 relative L2 0.0015287656569853425\n",
      "training 0.0009738198714330792 relative L2 0.0015287582064047456\n",
      "training 0.0009738109074532986 relative L2 0.001528751221485436\n",
      "training 0.0009738015360198915 relative L2 0.001528743770904839\n",
      "training 0.0009737919899635017 relative L2 0.0015287365531548858\n",
      "training 0.0009737826185300946 relative L2 0.001528729684650898\n",
      "training 0.0009737734799273312 relative L2 0.0015287224669009447\n",
      "training 0.0009737645741552114 relative L2 0.0015287153655663133\n",
      "training 0.000973755493760109 relative L2 0.0015287083806470037\n",
      "training 0.0009737462387420237 relative L2 0.0015287010464817286\n",
      "training 0.0009737368673086166 relative L2 0.0015286941779777408\n",
      "training 0.0009737274958752096 relative L2 0.0015286864945665002\n",
      "training 0.0009737179498188198 relative L2 0.0015286793932318687\n",
      "training 0.0009737087530083954 relative L2 0.0015286721754819155\n",
      "training 0.0009736993233673275 relative L2 0.0015286646084859967\n",
      "training 0.0009736900101415813 relative L2 0.0015286575071513653\n",
      "training 0.0009736806969158351 relative L2 0.0015286506386473775\n",
      "training 0.0009736716747283936 relative L2 0.0015286437701433897\n",
      "training 0.0009736629435792565 relative L2 0.0015286365523934364\n",
      "training 0.000973653222899884 relative L2 0.0015286288689821959\n",
      "training 0.0009736435604281723 relative L2 0.0015286216512322426\n",
      "training 0.0009736343054100871 relative L2 0.0015286147827282548\n",
      "training 0.000973624992184341 relative L2 0.0015286074485629797\n",
      "training 0.0009736157953739166 relative L2 0.0015286001143977046\n",
      "training 0.000973606132902205 relative L2 0.0015285927802324295\n",
      "training 0.0009735969360917807 relative L2 0.0015285853296518326\n",
      "training 0.0009735872154124081 relative L2 0.001528578344732523\n",
      "training 0.0009735780768096447 relative L2 0.0015285710105672479\n",
      "training 0.0009735684143379331 relative L2 0.0015285637928172946\n",
      "training 0.0009735592175275087 relative L2 0.0015285565750673413\n",
      "training 0.0009735499043017626 relative L2 0.001528549357317388\n",
      "training 0.0009735407656989992 relative L2 0.0015285422559827566\n",
      "training 0.0009735312196426094 relative L2 0.0015285349218174815\n",
      "training 0.0009735220810398459 relative L2 0.0015285281697288156\n",
      "training 0.0009735130006447434 relative L2 0.0015285209519788623\n",
      "training 0.0009735036292113364 relative L2 0.0015285133849829435\n",
      "training 0.0009734939085319638 relative L2 0.0015285066328942776\n",
      "training 0.0009734849445521832 relative L2 0.0015284992987290025\n",
      "training 0.0009734758641570807 relative L2 0.0015284925466403365\n",
      "training 0.0009734669001773 relative L2 0.001528485561721027\n",
      "training 0.0009734576451592147 relative L2 0.0015284782275557518\n",
      "training 0.0009734481573104858 relative L2 0.001528470660559833\n",
      "training 0.0009734383784234524 relative L2 0.0015284630935639143\n",
      "training 0.0009734289487823844 relative L2 0.0015284559922292829\n",
      "training 0.0009734196355566382 relative L2 0.0015284490073099732\n",
      "training 0.0009734103223308921 relative L2 0.0015284415567293763\n",
      "training 0.000973400950897485 relative L2 0.001528434338979423\n",
      "training 0.0009733916376717389 relative L2 0.0015284265391528606\n",
      "training 0.00097338214982301 relative L2 0.0015284193214029074\n",
      "training 0.0009733727783896029 relative L2 0.001528412802144885\n",
      "training 0.000973363930825144 relative L2 0.0015284048859030008\n",
      "training 0.0009733541519381106 relative L2 0.0015283983666449785\n",
      "training 0.0009733447805047035 relative L2 0.0015283909160643816\n",
      "training 0.0009733359911479056 relative L2 0.0015283840475603938\n",
      "training 0.0009733266779221594 relative L2 0.0015283774118870497\n",
      "training 0.0009733180049806833 relative L2 0.001528369728475809\n",
      "training 0.0009733084007166326 relative L2 0.0015283632092177868\n",
      "training 0.0009732990874908864 relative L2 0.001528355060145259\n",
      "training 0.0009732894832268357 relative L2 0.0015283487737178802\n",
      "training 0.0009732800535857677 relative L2 0.001528340857475996\n",
      "training 0.0009732707985676825 relative L2 0.0015283344546332955\n",
      "training 0.0009732616017572582 relative L2 0.001528326771222055\n",
      "training 0.0009732517646625638 relative L2 0.0015283194370567799\n",
      "training 0.0009732426260598004 relative L2 0.001528312568552792\n",
      "training 0.0009732331382110715 relative L2 0.0015283050015568733\n",
      "training 0.0009732238249853253 relative L2 0.0015282980166375637\n",
      "training 0.0009732145699672401 relative L2 0.0015282905660569668\n",
      "training 0.0009732050821185112 relative L2 0.0015282831154763699\n",
      "training 0.0009731957106851041 relative L2 0.0015282761305570602\n",
      "training 0.0009731859900057316 relative L2 0.0015282686799764633\n",
      "training 0.0009731767931953073 relative L2 0.0015282615786418319\n",
      "training 0.0009731674217619002 relative L2 0.0015282543608918786\n",
      "training 0.0009731581667438149 relative L2 0.0015282471431419253\n",
      "training 0.0009731489117257297 relative L2 0.0015282396925613284\n",
      "training 0.0009731391910463572 relative L2 0.001528231892734766\n",
      "training 0.0009731295285746455 relative L2 0.0015282246749848127\n",
      "training 0.000973119807895273 relative L2 0.0015282173408195376\n",
      "training 0.0009731106110848486 relative L2 0.0015282104723155499\n",
      "training 0.0009731011814437807 relative L2 0.0015282026724889874\n",
      "training 0.0009730916935950518 relative L2 0.0015281954547390342\n",
      "training 0.0009730823221616447 relative L2 0.001528188236989081\n",
      "training 0.0009730731835588813 relative L2 0.0015281811356544495\n",
      "training 0.000973063928540796 relative L2 0.001528173335827887\n",
      "training 0.0009730541496537626 relative L2 0.0015281658852472901\n",
      "training 0.0009730442543514073 relative L2 0.001528159249573946\n",
      "training 0.0009730354067869484 relative L2 0.0015281521482393146\n",
      "training 0.0009730261517688632 relative L2 0.001528145163320005\n",
      "training 0.0009730170713737607 relative L2 0.001528137712739408\n",
      "training 0.0009730076417326927 relative L2 0.0015281304949894547\n",
      "training 0.0009729983285069466 relative L2 0.0015281226951628923\n",
      "training 0.0009729887242428958 relative L2 0.0015281158266589046\n",
      "training 0.0009729789453558624 relative L2 0.0015281080268323421\n",
      "training 0.0009729695157147944 relative L2 0.0015281011583283544\n",
      "training 0.0009729604935273528 relative L2 0.0015280935913324356\n",
      "training 0.0009729507146403193 relative L2 0.0015280863735824823\n",
      "training 0.0009729413432069123 relative L2 0.0015280789230018854\n",
      "training 0.0009729316225275397 relative L2 0.0015280717052519321\n",
      "training 0.0009729224257171154 relative L2 0.001528065069578588\n",
      "training 0.0009729136363603175 relative L2 0.0015280573861673474\n",
      "training 0.0009729037410579622 relative L2 0.001528049586340785\n",
      "training 0.0009728939039632678 relative L2 0.0015280429506674409\n",
      "training 0.0009728846489451826 relative L2 0.0015280351508408785\n",
      "training 0.0009728752775117755 relative L2 0.0015280285151675344\n",
      "training 0.0009728657896630466 relative L2 0.001528020715340972\n",
      "training 0.0009728563600219786 relative L2 0.0015280138468369842\n",
      "training 0.0009728471050038934 relative L2 0.0015280060470104218\n",
      "training 0.0009728372097015381 relative L2 0.0015279982471838593\n",
      "training 0.0009728274308145046 relative L2 0.001527991029433906\n",
      "training 0.000972817768342793 relative L2 0.0015279840445145965\n",
      "training 0.0009728084551170468 relative L2 0.0015279763611033559\n",
      "training 0.0009727987926453352 relative L2 0.0015279691433534026\n",
      "training 0.00097278953762725 relative L2 0.0015279613435268402\n",
      "training 0.0009727797587402165 relative L2 0.0015279540093615651\n",
      "training 0.0009727702126838267 relative L2 0.0015279469080269337\n",
      "training 0.0009727608994580805 relative L2 0.0015279389917850494\n",
      "training 0.0009727508877404034 relative L2 0.0015279321232810616\n",
      "training 0.0009727415745146573 relative L2 0.001527924439869821\n",
      "training 0.0009727322030812502 relative L2 0.0015279174549505115\n",
      "training 0.0009727227734401822 relative L2 0.0015279100043699145\n",
      "training 0.0009727132855914533 relative L2 0.0015279027866199613\n",
      "training 0.0009727035067044199 relative L2 0.001527895568870008\n",
      "training 0.0009726941934786737 relative L2 0.0015278877690434456\n",
      "training 0.0009726842981763184 relative L2 0.0015278805512934923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "n_epochs = 10000\n",
    "net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "loss_hist6 = []\n",
    "loss_hist7 = []\n",
    "logging.info(f'{net}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "min_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = timer()\n",
    "for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "    prediction = net(X_train_tensor)\n",
    "    loss = lossFunction(prediction, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist6.append(loss.item())\n",
    "    \n",
    "    # Calculate the relative L2 loss\n",
    "    prediction2 = net(X_test_tensor)\n",
    "    relative_L2 = torch.sqrt(torch.mean((prediction2 - y_test_tensor)**2)) / torch.sqrt(torch.mean(y_test_tensor**2))\n",
    "    loss_hist7.append(relative_L2.item())\n",
    "    \n",
    "    if relative_L2.item() < min_loss:\n",
    "        min_loss = relative_L2.item()\n",
    "        final_model = net.state_dict()\n",
    "    pass\n",
    "\n",
    "# torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# # Save the training loss history as a CSV file\n",
    "# loss_df = pd.DataFrame(loss_hist7)\n",
    "# loss_df.to_csv('default/nn_loss.csv', index=False)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print('training', loss_hist6[i], 'relative L2', loss_hist7[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
