{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(data, os.path.join(path, filename))\n",
    "    pass\n",
    "\n",
    "def save_loss(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    data.to_csv(os.path.join(path, filename), index=False)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling\n",
    "Here in our case, the system is European Call Option PDE and the physical information about the system consists of Boundary Value conditions, final Value conditions and the PDE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "loss_weights = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AWPINN with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "n_epochs = 5000\n",
    "lr_rate_list = [lr/10000, lr/100, lr/10, lr, lr*10, lr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.88it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.84it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.81it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.84it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.32it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 60.10it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.39it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 58.93it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:29<00:00, 55.88it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:26<00:00, 57.93it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.87it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 60.99it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 59.42it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.40it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 58.92it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:28<00:00, 56.31it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:36<00:00, 51.83it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:35<00:00, 52.09it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:35<00:00, 52.13it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:35<00:00, 52.45it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:38<00:00, 50.56it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:38<00:00, 50.82it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:38<00:00, 50.65it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:38<00:00, 50.66it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:27<00:00, 56.88it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:27<00:00, 57.18it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:27<00:00, 56.93it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 58.91it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:26<00:00, 58.11it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.63it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.91it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 59.10it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.37it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.64it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 60.06it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.60it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.62it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.78it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:20<00:00, 61.77it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 58.84it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.64it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:27<00:00, 57.42it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:25<00:00, 58.43it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:27<00:00, 57.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:29<00:00, 56.05it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 59.46it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.63it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.76it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.01it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:23<00:00, 59.76it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:24<00:00, 59.02it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:47<00:00, 46.33it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:48<00:00, 46.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:57<00:00, 42.54it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:54<00:00, 43.59it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:53<00:00, 44.07it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:52<00:00, 44.58it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:57<00:00, 42.48it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:53<00:00, 43.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awpinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # Save the model's state dictionary\n",
    "        save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 5000/5000 [02:45<00:00, 30.12it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:53<00:00, 28.76it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:48<00:00, 29.63it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:47<00:00, 29.87it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:48<00:00, 29.60it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:47<00:00, 29.87it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:28<00:00, 33.75it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:34<00:00, 32.41it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:40<00:00, 31.19it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.34it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:40<00:00, 31.24it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:41<00:00, 31.05it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:41<00:00, 30.95it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.35it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.26it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:43<00:00, 30.62it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:45<00:00, 30.25it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:48<00:00, 29.63it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:51<00:00, 29.11it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:46<00:00, 30.04it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:11<00:00, 37.94it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:08<00:00, 38.91it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:08<00:00, 38.89it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:26<00:00, 34.21it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:44<00:00, 30.40it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:46<00:00, 29.99it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:44<00:00, 30.43it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:48<00:00, 29.71it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:46<00:00, 29.97it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:47<00:00, 29.87it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:46<00:00, 29.96it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:47<00:00, 29.79it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:44<00:00, 30.45it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:47<00:00, 29.86it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:45<00:00, 30.25it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:41<00:00, 30.98it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:06<00:00, 39.41it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:26<00:00, 34.03it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.26it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:45<00:00, 30.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.29it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:42<00:00, 30.69it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [03:02<00:00, 27.47it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [03:02<00:00, 27.33it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:59<00:00, 27.78it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:39<00:00, 31.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:43<00:00, 30.49it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:43<00:00, 30.60it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:41<00:00, 31.05it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:45<00:00, 30.26it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:42<00:00, 30.86it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:14<00:00, 37.13it/s]\n",
      "[Training procedure]:  25%|##4       | 1242/5000 [00:31<01:35, 39.41it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awipinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # Save the model's state dictionary\n",
    "        save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_weights/pinn\"\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    save_model(f'{path}/model', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPINN with Different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_weights/ipinn\"\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    save_model(f'{path}/model', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "# model1.to(device)\n",
    "# model1.load_state_dict(torch.load('default/pinn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.load_state_dict(torch.load('default/awpinn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.load_state_dict(torch.load('default/nn.pth'))\n",
    "# model1.eval()\n",
    "# prediction = model1(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "# model2 = networks.ImprovedNeuralNetwork(sizes, 'relu', 0, 0.1, 10.0)\n",
    "# model2.to(device)\n",
    "# model2.load_state_dict(torch.load('default/ipinn.pth'))\n",
    "# model2.eval()\n",
    "# prediction = model2(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.load_state_dict(torch.load('default/awipinn.pth'))\n",
    "# model2.eval()\n",
    "# prediction = model2(X_test_tensor)\n",
    "# print(lossFunction(prediction, y_test_tensor).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:34<00:00, 64.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.28it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = f\"test_final_performance/pinn\"\n",
    "    pinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=0, loss_weights=loss_weights, adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # Save the model's state dictionary\n",
    "    save_model(f'{path}/model', f\"{i}.pth\", min_model)\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [03:54<00:00, 42.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:44<00:00, 44.45it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.99it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:36<00:00, 46.17it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.76it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:36<00:00, 46.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = f\"test_final_performance/ipinn\"\n",
    "    ipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate =0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    \n",
    "    # Save the model's state dictionary\n",
    "    save_model(f'{path}/model', f\"{i}.pth\", min_model)\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:31<00:00, 66.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:29<00:00, 66.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.68it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:30<00:00, 66.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:29<00:00, 67.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.77it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:29<00:00, 66.98it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:31<00:00, 66.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:30<00:00, 66.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:30<00:00, 66.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = f\"test_final_performance/awpinn\"\n",
    "    awpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    save_model(f'{path}/model', f\"{i}.pth\", min_model)\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [03:42<00:00, 44.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:43<00:00, 44.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:35<00:00, 46.34it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:38<00:00, 45.72it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 46.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:37<00:00, 45.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:35<00:00, 46.35it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:39<00:00, 45.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:36<00:00, 46.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = f\"test_final_performance/awipinn\"\n",
    "    awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=loss_weights, adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # Save the model's state dictionary\n",
    "    save_model(f'{path}/model', f\"{i}.pth\", min_model)\n",
    "\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "    # Append the loss histories to the respective lists\n",
    "    all_mse_loss_hist.append(mse_loss_hist)\n",
    "    all_pde_loss_hist.append(pde_loss_hist)\n",
    "    all_bc_loss_hist.append(bc_loss_hist)\n",
    "    all_data_loss_hist.append(data_loss_hist)\n",
    "    pass\n",
    "\n",
    "# Calculate the average losses among all 10 training sessions for each component\n",
    "average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "# Save the average losses as CSV files\n",
    "average_loss_df = pd.DataFrame({\n",
    "    'Average_MSE_Loss': average_mse_loss,\n",
    "    'Average_PDE_Loss': average_pde_loss,\n",
    "    'Average_BC_Loss': average_bc_loss,\n",
    "    'Average_Data_Loss': average_data_loss\n",
    "})\n",
    "save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "loss_hist6 = []\n",
    "logging.info(f'{net}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "min_train_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = timer()\n",
    "for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "    prediction = net(X_train_tensor)\n",
    "    loss = lossFunction(prediction, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist6.append(loss.item())\n",
    "    \n",
    "    if loss.item() < min_train_loss:\n",
    "        min_train_loss = loss.item()\n",
    "        final_model = net.state_dict()\n",
    "    pass\n",
    "\n",
    "torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# Save the training loss history as a CSV file\n",
    "loss_df = pd.DataFrame(loss_hist6)\n",
    "loss_df.to_csv('default/nn_loss.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
