{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    \n",
    "def save_model(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(data, os.path.join(path, filename))\n",
    "    pass\n",
    "\n",
    "def save_loss(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    data.to_csv(os.path.join(path, filename), index=False)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "# model parameters\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "\n",
    "# training parameters\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AWPINN with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "n_epochs = 5000\n",
    "lr_rate_list = [lr/10000, lr/100, lr/10, lr, lr*10, lr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awpinn/v2/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awipinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN and IPINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/pinn\"\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/ipinn\"\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibpinn and awipinn with different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "# awpinn_weights = [[1, 1, 1], [5, 1, 1], [1, 1, 2], [1, 2, 1], [1, 5, 1]]\n",
    "awpinn_weights = [[1, 5, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.45it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.03it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.35it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:28<00:00, 56.75it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:39<00:00, 50.10it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:36<00:00, 51.82it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.53it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:26<00:00, 58.14it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:22<00:00, 60.27it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:21<00:00, 61.19it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:01<00:00, 41.26it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:00<00:00, 41.51it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [01:59<00:00, 41.74it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:04<00:00, 40.26it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:07<00:00, 39.33it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:08<00:00, 38.83it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:05<00:00, 39.99it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:18<00:00, 36.03it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:22<00:00, 34.99it/s]\n",
      "[Training procedure]: 100%|##########| 5000/5000 [02:01<00:00, 41.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_and_save(awpinn_weights, net_type):\n",
    "    for weights in awpinn_weights:\n",
    "        w1, w2, w3 = weights\n",
    "        \n",
    "        mse_loss_histories, pde_loss_histories, bc_loss_histories, data_loss_histories, relative_L2_loss_histories = [], [], [], [], []\n",
    "        \n",
    "        path = f\"test_loss_weights/{net_type}/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "        # Run 10 iterations for each weight configuration\n",
    "        for j in range(10):\n",
    "            adaptive_rate=0 if net_type=='pinn' else 0.1\n",
    "            awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "                K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], \n",
    "                RNG_key=123, device=device, net=net_type, sizes=sizes, activation=activation, \n",
    "                learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0,\n",
    "                loss_weights=weights, adaptive_weight=True, \n",
    "                X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, \n",
    "                y_test_tensor=y_test_tensor)\n",
    "\n",
    "            # Save the training loss histories for all components\n",
    "            loss_df = pd.DataFrame({\n",
    "                'MSE_Loss': mse_loss_hist,\n",
    "                'PDE_Loss': pde_loss_hist,\n",
    "                'BC_Loss': bc_loss_hist,\n",
    "                'Data_Loss': data_loss_hist,\n",
    "                'Relative_L2_Loss': relative_L2_loss_hist\n",
    "            })\n",
    "            save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "            # Collect histories\n",
    "            mse_loss_histories.append(mse_loss_hist)\n",
    "            pde_loss_histories.append(pde_loss_hist)\n",
    "            bc_loss_histories.append(bc_loss_hist)\n",
    "            data_loss_histories.append(data_loss_hist)\n",
    "            relative_L2_loss_histories.append(relative_L2_loss_hist)\n",
    "\n",
    "        # Compute average losses and save\n",
    "        avg_losses = {\n",
    "            'Average_MSE_Loss': pd.DataFrame(mse_loss_histories).mean(axis=0),\n",
    "            'Average_PDE_Loss': pd.DataFrame(pde_loss_histories).mean(axis=0),\n",
    "            'Average_BC_Loss': pd.DataFrame(bc_loss_histories).mean(axis=0),\n",
    "            'Average_Data_Loss': pd.DataFrame(data_loss_histories).mean(axis=0),\n",
    "            'Average_Relative_L2_Loss': pd.DataFrame(relative_L2_loss_histories).mean(axis=0)\n",
    "        }\n",
    "        save_loss(path, 'average.csv', pd.DataFrame(avg_losses))\n",
    "\n",
    "# Train and save results for both neural network types\n",
    "train_and_save(awpinn_weights, 'pinn')\n",
    "train_and_save(awpinn_weights, 'ipinn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "# ipinn_loss_weight = [[1, 1, 1], [1, 1.5, 1.5], [1, 2, 2] [1.5, 1, 1], [2, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1.5-1-1', [1.5,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1.5-1-1', [1.5,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[2, 25, 25, 25, 25, 25, 25, 25, 25, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "    \n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[2, 50, 50, 50, 50, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "n_epochs = 10000\n",
    "net = networks.FeedforwardNeuralNetwork(2, 100, 1, 10)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "loss_hist6 = []\n",
    "loss_hist7 = []\n",
    "logging.info(f'{net}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "min_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = timer()\n",
    "for j in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "    prediction = net(X_train_tensor)\n",
    "    loss = lossFunction(prediction, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist6.append(loss.item())\n",
    "    \n",
    "    # Calculate the relative L2 loss\n",
    "    prediction2 = net(X_test_tensor)\n",
    "    relative_L2 = torch.sqrt(torch.mean((prediction2 - y_test_tensor)**2)) / torch.sqrt(torch.mean(y_test_tensor**2))\n",
    "    loss_hist7.append(relative_L2.item())\n",
    "    \n",
    "    if relative_L2.item() < min_loss:\n",
    "        min_loss = relative_L2.item()\n",
    "        final_model = net.state_dict()\n",
    "    pass\n",
    "\n",
    "# torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# # Save the training loss history as a CSV file\n",
    "# loss_df = pd.DataFrame(loss_hist7)\n",
    "# loss_df.to_csv('default/nn_loss.csv', index=False)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print('training', loss_hist6[i], 'relative L2', loss_hist7[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
