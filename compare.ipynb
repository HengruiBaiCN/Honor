{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as tgrad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import errno\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    \n",
    "def save_model(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(data, os.path.join(path, filename))\n",
    "    pass\n",
    "\n",
    "def save_loss(path, filename, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    data.to_csv(os.path.join(path, filename), index=False)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "K = 10\n",
    "r = 0.035\n",
    "sigma = 0.2\n",
    "T = 1\n",
    "S_range = [0, int(5*K)]\n",
    "t_range = [0, T]\n",
    "gs = lambda x: np.fmax(x-K, 0)\n",
    "M = 100\n",
    "N = 5000\n",
    "\n",
    "# model parameters\n",
    "lossFunction = nn.MSELoss()\n",
    "sizes=[2, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "lr = 3e-5\n",
    "activation = 'relu'\n",
    "\n",
    "# training parameters\n",
    "samples = {\"pde\": 5000, \"bc\":500, \"fc\":500}\n",
    "\n",
    "# sample data generated by finite difference method\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = utils.fdm_data(S_range[-1], T, M, N, \"500000sample.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AWPINN with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss histories\n",
    "n_epochs = 5000\n",
    "lr_rate_list = [lr/10000, lr/100, lr/10, lr, lr*10, lr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awpinn/v2/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_rate_list)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    weight_lr = lr_rate_list[i]\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_learning_rate/awipinn/{weight_lr}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=weight_lr, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=[], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(path, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN and IPINN with Different Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "weights = [[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/pinn\"\n",
    "    wpinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='pinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=None, adaptive_rate=None, adaptive_rate_scaler=None, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the loss histories for all components\n",
    "all_mse_loss_hist = []\n",
    "all_pde_loss_hist = []\n",
    "all_bc_loss_hist = []\n",
    "all_data_loss_hist = []\n",
    "\n",
    "# train models with different weights\n",
    "for i in range(len(weights)):\n",
    "    w1 = weights[i][0]\n",
    "    w2 = weights[i][1]\n",
    "    w3 = weights[i][2]\n",
    "    path = f\"test_loss_weights/ipinn\"\n",
    "    wipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weights_hist = utils.network_training(\n",
    "        K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "        device=device, net='ipinn', sizes=sizes, activation='relu', learning_rate=lr, aw_learning_rate=lr, n_epochs=n_epochs, \n",
    "        lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=weights[i], adaptive_weight=None, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "        )\n",
    "    # save_model(f'{path}/model2', f\"{w1}-{w2}-{w3}.pth\", min_model)  # Save the model's state dictionary\n",
    "    # Save the training loss histories for all components as CSV files\n",
    "    loss_df = pd.DataFrame({\n",
    "        'MSE_Loss': mse_loss_hist,\n",
    "        'PDE_Loss': pde_loss_hist,\n",
    "        'BC_Loss': bc_loss_hist,\n",
    "        'Data_Loss': data_loss_hist\n",
    "    })\n",
    "    save_loss(f'{path}/loss10', f'{w1}-{w2}-{w3}.csv', loss_df)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibpinn and awipinn with different loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "awpinn_weights = [[1.5, 1, 1], [5, 1, 1], [1, 1, 2], [1, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awpinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='pinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=None, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(awpinn_weights)):\n",
    "    # Initialize lists to store the loss histories for all components\n",
    "    w1 = awpinn_weights[i][0]\n",
    "    w2 = awpinn_weights[i][1]\n",
    "    w3 = awpinn_weights[i][2]\n",
    "    \n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    \n",
    "    path = f\"test_loss_weights/awipinn/{w1}-{w2}-{w3}/\"\n",
    "\n",
    "    # run 10 times for each learning rate\n",
    "    for j in range(10):\n",
    "        awipinn, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net='ipinn', sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=0.001, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=0.1, adaptive_rate_scaler=10.0, loss_weights=awpinn_weights[i], adaptive_weight=True, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "            )\n",
    "        # # Save the model's state dictionary\n",
    "        # save_model(f'{path}/model', f\"{j}.pth\", min_model)\n",
    "        \n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{j}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        pass\n",
    "\n",
    "    # Calculate the average losses among all 10 training sessions for each learning rate\n",
    "    average_mse_loss = pd.DataFrame(all_mse_loss_hist).mean(axis=0)\n",
    "    average_pde_loss = pd.DataFrame(all_pde_loss_hist).mean(axis=0)\n",
    "    average_bc_loss = pd.DataFrame(all_bc_loss_hist).mean(axis=0)\n",
    "    average_data_loss = pd.DataFrame(all_data_loss_hist).mean(axis=0)\n",
    "\n",
    "    # Save the average losses as CSV files\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': average_mse_loss,\n",
    "        'Average_PDE_Loss': average_pde_loss,\n",
    "        'Average_BC_Loss': average_bc_loss,\n",
    "        'Average_Data_Loss': average_data_loss\n",
    "    })\n",
    "    save_loss(f'{path}', f'{w1}-{w2}-{w3}.csv', loss_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs IPINN vs AWPINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "# ipinn_loss_weight = [[1, 1, 1], [1, 1.5, 1.5], [1, 2, 2] [1.5, 1, 1], [2, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [03:04<00:00, 54.34it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:26<00:00, 48.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:25<00:00, 48.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:22<00:00, 49.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:23<00:00, 49.25it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:29<00:00, 47.77it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:20<00:00, 49.95it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:25<00:00, 48.66it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:26<00:00, 48.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:22<00:00, 49.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:29<00:00, 37.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:55<00:00, 33.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:53<00:00, 34.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:14<00:00, 39.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:57<00:00, 42.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:50<00:00, 34.43it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:37<00:00, 36.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:34<00:00, 36.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:25<00:00, 37.68it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:45<00:00, 35.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:24<00:00, 48.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:18<00:00, 50.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:16<00:00, 50.92it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:17<00:00, 50.67it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:13<00:00, 51.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:07<00:00, 53.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:06<00:00, 53.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:28<00:00, 47.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:33<00:00, 46.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:17<00:00, 38.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:10<00:00, 27.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:10<00:00, 26.98it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:11<00:00, 26.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:58<00:00, 27.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:13<00:00, 31.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:11<00:00, 32.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:02<00:00, 27.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:08<00:00, 27.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:15<00:00, 26.66it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [06:18<00:00, 26.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [04:05<00:00, 40.71it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:17<00:00, 50.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:18<00:00, 50.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:19<00:00, 50.19it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:19<00:00, 50.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:18<00:00, 50.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:21<00:00, 49.68it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:22<00:00, 49.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:20<00:00, 49.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:55<00:00, 56.86it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:47<00:00, 34.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:59<00:00, 33.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:00<00:00, 33.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:03<00:00, 32.98it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:58<00:00, 33.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:06<00:00, 32.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:21<00:00, 38.17it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:11<00:00, 39.71it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:11<00:00, 39.84it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:12<00:00, 39.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:55<00:00, 56.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:43<00:00, 61.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:42<00:00, 61.37it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:47<00:00, 59.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 57.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:53<00:00, 57.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:52<00:00, 58.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:51<00:00, 58.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:10<00:00, 39.85it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:08<00:00, 32.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [05:08<00:00, 32.43it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:47<00:00, 34.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:54<00:00, 33.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:21<00:00, 38.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:18<00:00, 38.76it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:11<00:00, 39.71it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:09<00:00, 40.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [04:09<00:00, 40.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/1.5-1-1', [1.5,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/1.5-1-1', [1.5,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/1.5-1-1', [1.5,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [02:46<00:00, 60.21it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:42<00:00, 61.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:33<00:00, 65.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:33<00:00, 65.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:32<00:00, 65.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:34<00:00, 64.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:33<00:00, 65.25it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:47<00:00, 43.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:47<00:00, 43.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:46<00:00, 44.14it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:46<00:00, 44.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:48<00:00, 43.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:47<00:00, 44.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:46<00:00, 44.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:46<00:00, 44.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:47<00:00, 43.91it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:47<00:00, 43.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.67it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 62.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 62.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:37<00:00, 63.44it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 62.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 63.12it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:38<00:00, 62.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:52<00:00, 43.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.81it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.85it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.82it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.84it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:51<00:00, 43.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:54<00:00, 42.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [03:53<00:00, 42.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_network('pinn', 'test_final_performance/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_final_performance/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_final_performance/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_final_performance/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 119.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 120.72it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.94it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 120.76it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.46it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.66it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:07<00:00, 78.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.53it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.25it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.20it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.45it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.22it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.11it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.35it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.48it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.38it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.98it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.49it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.93it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 121.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.55it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.32it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 122.35it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:21<00:00, 121.99it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.48it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.37it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.48it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.64it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.50it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.48it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.72it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.89it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.72it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.40it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.46it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.43it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 113.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.47it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:10<00:00, 76.83it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.18it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 76.96it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.07it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:09<00:00, 77.04it/s]\n"
     ]
    }
   ],
   "source": [
    "sizes=[2, 25, 25, 25, 25, 25, 25, 25, 25, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "    \n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/25neurons/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/25neurons/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/25neurons/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.24it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.45it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.58it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 117.60it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.42it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.97it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 120.17it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:57<00:00, 84.77it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:00<00:00, 83.09it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:57<00:00, 84.80it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.41it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.51it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:58<00:00, 84.72it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:00<00:00, 82.99it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:58<00:00, 84.54it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.50it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:00<00:00, 82.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.37it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.76it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.37it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.63it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.50it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.98it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.11it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.71it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.62it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.61it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:08<00:00, 77.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.65it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.46it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.31it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.70it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 81.04it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.81it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.69it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.59it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.39it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:24<00:00, 118.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 120.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:22<00:00, 121.16it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:23<00:00, 119.06it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:25<00:00, 117.37it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:57<00:00, 85.10it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 84.03it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:00<00:00, 83.27it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:57<00:00, 85.36it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.87it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 83.73it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:59<00:00, 84.02it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:00<00:00, 83.08it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.15it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 111.75it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 112.56it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.57it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 112.11it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:29<00:00, 112.13it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:27<00:00, 114.68it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:28<00:00, 113.59it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [01:30<00:00, 110.90it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:02<00:00, 81.30it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.28it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:05<00:00, 79.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.85it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.05it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 81.01it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.26it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:04<00:00, 80.00it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:03<00:00, 80.78it/s]\n",
      "[Training procedure]: 100%|##########| 10000/10000 [02:06<00:00, 79.12it/s]\n"
     ]
    }
   ],
   "source": [
    "sizes=[2, 50, 50, 50, 50, 1]\n",
    "\n",
    "def train_network(net_type, path_prefix, loss_weights, adaptive_rate=None, aw_learning_rate=0.001, adaptive_weight=None):\n",
    "    all_mse_loss_hist = []\n",
    "    all_pde_loss_hist = []\n",
    "    all_bc_loss_hist = []\n",
    "    all_data_loss_hist = []\n",
    "    all_relative_error_hist = []\n",
    "\n",
    "    for i in range(10):\n",
    "        path = f\"{path_prefix}/{i}\"\n",
    "        model, min_model, mse_loss_hist, pde_loss_hist, bc_loss_hist, data_loss_hist, weight_hist, relative_L2_loss_hist = utils.network_training(\n",
    "            K, r, sigma, T, S_range[-1], S_range, t_range, gs, samples['bc'], samples['fc'], samples['pde'], RNG_key=123,\n",
    "            device=device, net=net_type, sizes=sizes, activation=activation, learning_rate=lr, aw_learning_rate=aw_learning_rate, n_epochs=n_epochs, lossFunction=lossFunction, dropout_rate=0, adaptive_rate=adaptive_rate, adaptive_rate_scaler=10.0 if adaptive_rate else 0, loss_weights=loss_weights, adaptive_weight=adaptive_weight, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor, X_test_tensor=X_test_tensor, y_test_tensor=y_test_tensor\n",
    "        )\n",
    "\n",
    "        # Save the training loss histories for all components as CSV files\n",
    "        loss_df = pd.DataFrame({\n",
    "            'MSE_Loss': mse_loss_hist,\n",
    "            'PDE_Loss': pde_loss_hist,\n",
    "            'BC_Loss': bc_loss_hist,\n",
    "            'Data_Loss': data_loss_hist,\n",
    "            'Relative_L2_Loss': relative_L2_loss_hist\n",
    "        })\n",
    "        save_loss(f'{path}/loss', f'{i}.csv', loss_df)\n",
    "\n",
    "        # Append the loss histories to the respective lists\n",
    "        all_mse_loss_hist.append(mse_loss_hist)\n",
    "        all_pde_loss_hist.append(pde_loss_hist)\n",
    "        all_bc_loss_hist.append(bc_loss_hist)\n",
    "        all_data_loss_hist.append(data_loss_hist)\n",
    "        all_relative_error_hist.append(relative_L2_loss_hist)\n",
    "\n",
    "    # Calculate and save average losses\n",
    "    average_loss_df = pd.DataFrame({\n",
    "        'Average_MSE_Loss': pd.DataFrame(all_mse_loss_hist).mean(axis=0),\n",
    "        'Average_PDE_Loss': pd.DataFrame(all_pde_loss_hist).mean(axis=0),\n",
    "        'Average_BC_Loss': pd.DataFrame(all_bc_loss_hist).mean(axis=0),\n",
    "        'Average_Data_Loss': pd.DataFrame(all_data_loss_hist).mean(axis=0),\n",
    "        'Average_Relative_L2_Loss': pd.DataFrame(all_relative_error_hist).mean(axis=0)\n",
    "    })\n",
    "    save_loss(path_prefix, 'average_loss.csv', average_loss_df)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/1-1-1', [1,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/1-1-1', [1,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/1-1-1', [1,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)\n",
    "\n",
    "train_network('pinn', 'test_diff_nn/4_layers/pinn/2-1-1', [2,1,1])\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/ipinn/2-1-1', [2,1,1], adaptive_rate=0.1)\n",
    "train_network('pinn', 'test_diff_nn/4_layers/awpinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_weight=True)\n",
    "train_network('ipinn', 'test_diff_nn/4_layers/awipinn/2-1-1', [2,1,1], aw_learning_rate=0.001, adaptive_rate=0.1, adaptive_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training procedure]: 100%|##########| 10000/10000 [01:48<00:00, 92.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 426.87786865234375 relative L2 1.0031088590621948\n",
      "training 426.85150146484375 relative L2 1.0030776262283325\n",
      "training 426.824951171875 relative L2 1.0030461549758911\n",
      "training 426.7981872558594 relative L2 1.0030150413513184\n",
      "training 426.77166748046875 relative L2 1.0029845237731934\n",
      "training 426.7457580566406 relative L2 1.0029544830322266\n",
      "training 426.7201232910156 relative L2 1.0029246807098389\n",
      "training 426.69482421875 relative L2 1.002895474433899\n",
      "training 426.669921875 relative L2 1.0028666257858276\n",
      "training 426.6452941894531 relative L2 1.0028380155563354\n",
      "training 426.62091064453125 relative L2 1.0028095245361328\n",
      "training 426.5968017578125 relative L2 1.0027815103530884\n",
      "training 426.57293701171875 relative L2 1.0027538537979126\n",
      "training 426.5494689941406 relative L2 1.0027273893356323\n",
      "training 426.52691650390625 relative L2 1.002702236175537\n",
      "training 426.5055847167969 relative L2 1.0026774406433105\n",
      "training 426.4844970703125 relative L2 1.0026525259017944\n",
      "training 426.4633483886719 relative L2 1.0026276111602783\n",
      "training 426.442138671875 relative L2 1.0026026964187622\n",
      "training 426.4208984375 relative L2 1.002577781677246\n",
      "training 426.39971923828125 relative L2 1.0025526285171509\n",
      "training 426.3783874511719 relative L2 1.0025277137756348\n",
      "training 426.35711669921875 relative L2 1.0025025606155396\n",
      "training 426.3357849121094 relative L2 1.0024774074554443\n",
      "training 426.31439208984375 relative L2 1.0024526119232178\n",
      "training 426.2933349609375 relative L2 1.002428650856018\n",
      "training 426.2730407714844 relative L2 1.0024052858352661\n",
      "training 426.25311279296875 relative L2 1.0023820400238037\n",
      "training 426.23333740234375 relative L2 1.0023589134216309\n",
      "training 426.213623046875 relative L2 1.002335786819458\n",
      "training 426.1940002441406 relative L2 1.0023126602172852\n",
      "training 426.1743469238281 relative L2 1.0022895336151123\n",
      "training 426.1547546386719 relative L2 1.002266526222229\n",
      "training 426.1351623535156 relative L2 1.0022435188293457\n",
      "training 426.1156005859375 relative L2 1.0022205114364624\n",
      "training 426.0959777832031 relative L2 1.002197504043579\n",
      "training 426.0765075683594 relative L2 1.0021748542785645\n",
      "training 426.0570983886719 relative L2 1.0021522045135498\n",
      "training 426.037841796875 relative L2 1.0021294355392456\n",
      "training 426.0185852050781 relative L2 1.0021069049835205\n",
      "training 425.9993896484375 relative L2 1.0020843744277954\n",
      "training 425.980224609375 relative L2 1.0020620822906494\n",
      "training 425.9612731933594 relative L2 1.002039909362793\n",
      "training 425.9423828125 relative L2 1.0020177364349365\n",
      "training 425.9235534667969 relative L2 1.00199556350708\n",
      "training 425.90472412109375 relative L2 1.0019731521606445\n",
      "training 425.8857116699219 relative L2 1.0019506216049194\n",
      "training 425.8665466308594 relative L2 1.0019277334213257\n",
      "training 425.8470153808594 relative L2 1.0019044876098633\n",
      "training 425.8273620605469 relative L2 1.0018811225891113\n",
      "training 425.8074951171875 relative L2 1.0018576383590698\n",
      "training 425.7875061035156 relative L2 1.0018339157104492\n",
      "training 425.7674255371094 relative L2 1.001810073852539\n",
      "training 425.7471923828125 relative L2 1.001786231994629\n",
      "training 425.7267761230469 relative L2 1.00176203250885\n",
      "training 425.706298828125 relative L2 1.0017375946044922\n",
      "training 425.685546875 relative L2 1.0017125606536865\n",
      "training 425.664306640625 relative L2 1.0016872882843018\n",
      "training 425.642822265625 relative L2 1.001661777496338\n",
      "training 425.62115478515625 relative L2 1.001636028289795\n",
      "training 425.5992736816406 relative L2 1.0016101598739624\n",
      "training 425.5772399902344 relative L2 1.0015840530395508\n",
      "training 425.55511474609375 relative L2 1.0015578269958496\n",
      "training 425.53289794921875 relative L2 1.0015316009521484\n",
      "training 425.51055908203125 relative L2 1.0015051364898682\n",
      "training 425.48797607421875 relative L2 1.0014783143997192\n",
      "training 425.4653015136719 relative L2 1.0014513731002808\n",
      "training 425.44244384765625 relative L2 1.0014243125915527\n",
      "training 425.41943359375 relative L2 1.0013971328735352\n",
      "training 425.39630126953125 relative L2 1.001369595527649\n",
      "training 425.3730163574219 relative L2 1.0013421773910522\n",
      "training 425.3496398925781 relative L2 1.001314401626587\n",
      "training 425.3261413574219 relative L2 1.0012866258621216\n",
      "training 425.3024597167969 relative L2 1.0012587308883667\n",
      "training 425.2787170410156 relative L2 1.0012305974960327\n",
      "training 425.2547912597656 relative L2 1.00120210647583\n",
      "training 425.230712890625 relative L2 1.001173734664917\n",
      "training 425.20654296875 relative L2 1.0011451244354248\n",
      "training 425.1822204589844 relative L2 1.001116156578064\n",
      "training 425.1577453613281 relative L2 1.0010871887207031\n",
      "training 425.13311767578125 relative L2 1.0010579824447632\n",
      "training 425.10821533203125 relative L2 1.001028299331665\n",
      "training 425.0830383300781 relative L2 1.0009982585906982\n",
      "training 425.0576171875 relative L2 1.000968098640442\n",
      "training 425.031982421875 relative L2 1.000937819480896\n",
      "training 425.0062255859375 relative L2 1.0009074211120605\n",
      "training 424.9804382324219 relative L2 1.000877022743225\n",
      "training 424.9546203613281 relative L2 1.0008469820022583\n",
      "training 424.92913818359375 relative L2 1.0008174180984497\n",
      "training 424.90399169921875 relative L2 1.0007877349853516\n",
      "training 424.8788757324219 relative L2 1.000758171081543\n",
      "training 424.85369873046875 relative L2 1.0007283687591553\n",
      "training 424.8284606933594 relative L2 1.000698447227478\n",
      "training 424.8030700683594 relative L2 1.0006682872772217\n",
      "training 424.7774963378906 relative L2 1.0006378889083862\n",
      "training 424.7516784667969 relative L2 1.0006073713302612\n",
      "training 424.7257385253906 relative L2 1.0005768537521362\n",
      "training 424.6998291015625 relative L2 1.0005462169647217\n",
      "training 424.6738586425781 relative L2 1.0005155801773071\n",
      "training 424.6479187011719 relative L2 1.0004849433898926\n",
      "training 424.6219177246094 relative L2 1.0004544258117676\n",
      "training 424.5959777832031 relative L2 1.0004241466522217\n",
      "training 424.5703125 relative L2 1.0003942251205444\n",
      "training 424.54486083984375 relative L2 1.0003644227981567\n",
      "training 424.51959228515625 relative L2 1.000334620475769\n",
      "training 424.494384765625 relative L2 1.0003050565719604\n",
      "training 424.46929931640625 relative L2 1.000275731086731\n",
      "training 424.44439697265625 relative L2 1.000246524810791\n",
      "training 424.4195556640625 relative L2 1.000217318534851\n",
      "training 424.394775390625 relative L2 1.0001881122589111\n",
      "training 424.37005615234375 relative L2 1.0001590251922607\n",
      "training 424.34539794921875 relative L2 1.0001299381256104\n",
      "training 424.3207092285156 relative L2 1.00010085105896\n",
      "training 424.29595947265625 relative L2 1.0000715255737305\n",
      "training 424.2711486816406 relative L2 1.000042200088501\n",
      "training 424.2463073730469 relative L2 1.000012755393982\n",
      "training 424.2213134765625 relative L2 0.9999833106994629\n",
      "training 424.1963195800781 relative L2 0.9999536275863647\n",
      "training 424.1711120605469 relative L2 0.9999238848686218\n",
      "training 424.1458435058594 relative L2 0.9998937845230103\n",
      "training 424.12042236328125 relative L2 0.9998638033866882\n",
      "training 424.0949401855469 relative L2 0.9998337626457214\n",
      "training 424.0694274902344 relative L2 0.9998036026954651\n",
      "training 424.0439147949219 relative L2 0.9997736215591431\n",
      "training 424.0184326171875 relative L2 0.9997435808181763\n",
      "training 423.9929504394531 relative L2 0.9997134804725647\n",
      "training 423.9674987792969 relative L2 0.9996833801269531\n",
      "training 423.9419860839844 relative L2 0.9996532201766968\n",
      "training 423.9163818359375 relative L2 0.9996229410171509\n",
      "training 423.8907165527344 relative L2 0.9995924830436707\n",
      "training 423.8648681640625 relative L2 0.9995618462562561\n",
      "training 423.8388671875 relative L2 0.9995309114456177\n",
      "training 423.8127136230469 relative L2 0.9994997978210449\n",
      "training 423.7863464355469 relative L2 0.9994685053825378\n",
      "training 423.7597961425781 relative L2 0.9994369745254517\n",
      "training 423.7331237792969 relative L2 0.9994054436683655\n",
      "training 423.7063903808594 relative L2 0.9993738532066345\n",
      "training 423.6795349121094 relative L2 0.9993422031402588\n",
      "training 423.6526184082031 relative L2 0.9993102550506592\n",
      "training 423.62567138671875 relative L2 0.9992783665657043\n",
      "training 423.5986328125 relative L2 0.99924635887146\n",
      "training 423.5715026855469 relative L2 0.999214231967926\n",
      "training 423.5442810058594 relative L2 0.9991819858551025\n",
      "training 423.51702880859375 relative L2 0.9991498589515686\n",
      "training 423.4896545410156 relative L2 0.9991174340248108\n",
      "training 423.4623107910156 relative L2 0.9990851283073425\n",
      "training 423.43487548828125 relative L2 0.9990526437759399\n",
      "training 423.4073486328125 relative L2 0.9990200400352478\n",
      "training 423.3797607421875 relative L2 0.9989873170852661\n",
      "training 423.3520202636719 relative L2 0.9989545345306396\n",
      "training 423.32421875 relative L2 0.9989215731620789\n",
      "training 423.2962951660156 relative L2 0.9988885521888733\n",
      "training 423.268310546875 relative L2 0.9988554120063782\n",
      "training 423.24029541015625 relative L2 0.9988222122192383\n",
      "training 423.2121887207031 relative L2 0.9987890124320984\n",
      "training 423.1839904785156 relative L2 0.9987555742263794\n",
      "training 423.1556701660156 relative L2 0.9987219572067261\n",
      "training 423.12725830078125 relative L2 0.9986883401870728\n",
      "training 423.0987243652344 relative L2 0.9986545443534851\n",
      "training 423.070068359375 relative L2 0.9986204504966736\n",
      "training 423.041259765625 relative L2 0.9985861778259277\n",
      "training 423.0122985839844 relative L2 0.9985518455505371\n",
      "training 422.9831848144531 relative L2 0.9985172152519226\n",
      "training 422.95391845703125 relative L2 0.9984824657440186\n",
      "training 422.9244384765625 relative L2 0.9984474778175354\n",
      "training 422.894775390625 relative L2 0.9984121918678284\n",
      "training 422.8648986816406 relative L2 0.9983766674995422\n",
      "training 422.8348388671875 relative L2 0.998340904712677\n",
      "training 422.8045959472656 relative L2 0.9983049035072327\n",
      "training 422.7740783691406 relative L2 0.9982686042785645\n",
      "training 422.7433776855469 relative L2 0.9982321262359619\n",
      "training 422.7124328613281 relative L2 0.998195230960846\n",
      "training 422.6812438964844 relative L2 0.9981580376625061\n",
      "training 422.6497497558594 relative L2 0.9981204271316528\n",
      "training 422.617919921875 relative L2 0.9980825781822205\n",
      "training 422.5858154296875 relative L2 0.9980443120002747\n",
      "training 422.5534362792969 relative L2 0.9980058073997498\n",
      "training 422.5207824707031 relative L2 0.9979668855667114\n",
      "training 422.4879150390625 relative L2 0.997927725315094\n",
      "training 422.4547424316406 relative L2 0.997888445854187\n",
      "training 422.4215087890625 relative L2 0.9978489279747009\n",
      "training 422.3880615234375 relative L2 0.9978092908859253\n",
      "training 422.35455322265625 relative L2 0.9977694749832153\n",
      "training 422.32080078125 relative L2 0.9977293610572815\n",
      "training 422.286865234375 relative L2 0.9976891279220581\n",
      "training 422.25274658203125 relative L2 0.9976484775543213\n",
      "training 422.2183837890625 relative L2 0.9976076483726501\n",
      "training 422.183837890625 relative L2 0.9975665211677551\n",
      "training 422.1490173339844 relative L2 0.9975250959396362\n",
      "training 422.1139831542969 relative L2 0.9974833726882935\n",
      "training 422.0786437988281 relative L2 0.997441291809082\n",
      "training 422.04302978515625 relative L2 0.9973987936973572\n",
      "training 422.0071105957031 relative L2 0.9973559379577637\n",
      "training 421.9708557128906 relative L2 0.9973127245903015\n",
      "training 421.93426513671875 relative L2 0.9972689747810364\n",
      "training 421.8973083496094 relative L2 0.9972249269485474\n",
      "training 421.8600158691406 relative L2 0.9971804022789001\n",
      "training 421.8222961425781 relative L2 0.9971352815628052\n",
      "training 421.78424072265625 relative L2 0.9970898032188416\n",
      "training 421.7457580566406 relative L2 0.9970439076423645\n",
      "training 421.7068176269531 relative L2 0.9969974160194397\n",
      "training 421.66754150390625 relative L2 0.9969503879547119\n",
      "training 421.6277770996094 relative L2 0.9969028830528259\n",
      "training 421.5876159667969 relative L2 0.9968547821044922\n",
      "training 421.54693603515625 relative L2 0.9968062043190002\n",
      "training 421.5058288574219 relative L2 0.9967570304870605\n",
      "training 421.46429443359375 relative L2 0.9967072606086731\n",
      "training 421.4222412109375 relative L2 0.9966568946838379\n",
      "training 421.3796691894531 relative L2 0.9966058731079102\n",
      "training 421.3365478515625 relative L2 0.9965543150901794\n",
      "training 421.2928771972656 relative L2 0.9965019822120667\n",
      "training 421.2486572265625 relative L2 0.9964491128921509\n",
      "training 421.20404052734375 relative L2 0.9963959455490112\n",
      "training 421.1590270996094 relative L2 0.9963424205780029\n",
      "training 421.1138000488281 relative L2 0.9962884783744812\n",
      "training 421.06817626953125 relative L2 0.996234118938446\n",
      "training 421.0223083496094 relative L2 0.996179461479187\n",
      "training 420.97613525390625 relative L2 0.9961243867874146\n",
      "training 420.92962646484375 relative L2 0.9960688352584839\n",
      "training 420.8826599121094 relative L2 0.9960126280784607\n",
      "training 420.8351745605469 relative L2 0.9959556460380554\n",
      "training 420.787109375 relative L2 0.9958979487419128\n",
      "training 420.7384033203125 relative L2 0.9958396553993225\n",
      "training 420.6891174316406 relative L2 0.9957808256149292\n",
      "training 420.639404296875 relative L2 0.9957214593887329\n",
      "training 420.58929443359375 relative L2 0.995661735534668\n",
      "training 420.5389404296875 relative L2 0.9956016540527344\n",
      "training 420.4881591796875 relative L2 0.9955409169197083\n",
      "training 420.4369201660156 relative L2 0.9954797029495239\n",
      "training 420.3852233886719 relative L2 0.9954179525375366\n",
      "training 420.3330993652344 relative L2 0.9953556060791016\n",
      "training 420.28045654296875 relative L2 0.9952925443649292\n",
      "training 420.2272033691406 relative L2 0.9952285885810852\n",
      "training 420.17327880859375 relative L2 0.9951636791229248\n",
      "training 420.11846923828125 relative L2 0.9950977563858032\n",
      "training 420.062744140625 relative L2 0.9950307011604309\n",
      "training 420.0061950683594 relative L2 0.994962751865387\n",
      "training 419.94879150390625 relative L2 0.9948937296867371\n",
      "training 419.890625 relative L2 0.9948239326477051\n",
      "training 419.8316345214844 relative L2 0.9947530031204224\n",
      "training 419.7718200683594 relative L2 0.9946810603141785\n",
      "training 419.711181640625 relative L2 0.9946082830429077\n",
      "training 419.6497497558594 relative L2 0.9945345520973206\n",
      "training 419.5875244140625 relative L2 0.9944598078727722\n",
      "training 419.5245056152344 relative L2 0.9943831562995911\n",
      "training 419.4598388671875 relative L2 0.9943040609359741\n",
      "training 419.3930358886719 relative L2 0.99422287940979\n",
      "training 419.3245544433594 relative L2 0.9941399693489075\n",
      "training 419.254638671875 relative L2 0.994055449962616\n",
      "training 419.183349609375 relative L2 0.9939694404602051\n",
      "training 419.11077880859375 relative L2 0.9938819408416748\n",
      "training 419.0369873046875 relative L2 0.9937930107116699\n",
      "training 418.9620361328125 relative L2 0.99370276927948\n",
      "training 418.8858947753906 relative L2 0.9936108589172363\n",
      "training 418.8084716796875 relative L2 0.9935175180435181\n",
      "training 418.7298278808594 relative L2 0.9934226274490356\n",
      "training 418.6498718261719 relative L2 0.9933263063430786\n",
      "training 418.5686340332031 relative L2 0.9932284355163574\n",
      "training 418.48614501953125 relative L2 0.9931292533874512\n",
      "training 418.4025573730469 relative L2 0.9930287003517151\n",
      "training 418.3179626464844 relative L2 0.9929273128509521\n",
      "training 418.23248291015625 relative L2 0.9928247332572937\n",
      "training 418.1461181640625 relative L2 0.9927210211753845\n",
      "training 418.05865478515625 relative L2 0.992615818977356\n",
      "training 417.97015380859375 relative L2 0.9925093054771423\n",
      "training 417.8804626464844 relative L2 0.9924015402793884\n",
      "training 417.7897644042969 relative L2 0.9922924637794495\n",
      "training 417.69793701171875 relative L2 0.9921822547912598\n",
      "training 417.6051940917969 relative L2 0.9920709729194641\n",
      "training 417.5114440917969 relative L2 0.9919582009315491\n",
      "training 417.4165344238281 relative L2 0.9918437600135803\n",
      "training 417.3202819824219 relative L2 0.9917277097702026\n",
      "training 417.2226257324219 relative L2 0.9916098117828369\n",
      "training 417.12353515625 relative L2 0.991490364074707\n",
      "training 417.0229797363281 relative L2 0.9913690686225891\n",
      "training 416.92095947265625 relative L2 0.9912458062171936\n",
      "training 416.8173828125 relative L2 0.9911206960678101\n",
      "training 416.71221923828125 relative L2 0.9909936785697937\n",
      "training 416.6054382324219 relative L2 0.9908647537231445\n",
      "training 416.4969482421875 relative L2 0.9907336831092834\n",
      "training 416.38677978515625 relative L2 0.9906003475189209\n",
      "training 416.2747497558594 relative L2 0.9904645681381226\n",
      "training 416.16070556640625 relative L2 0.9903262257575989\n",
      "training 416.04443359375 relative L2 0.9901852011680603\n",
      "training 415.92596435546875 relative L2 0.9900414943695068\n",
      "training 415.80523681640625 relative L2 0.9898949861526489\n",
      "training 415.6822204589844 relative L2 0.9897459149360657\n",
      "training 415.55694580078125 relative L2 0.9895939826965332\n",
      "training 415.4293518066406 relative L2 0.9894391298294067\n",
      "training 415.2994689941406 relative L2 0.9892815947532654\n",
      "training 415.16717529296875 relative L2 0.98912113904953\n",
      "training 415.0325622558594 relative L2 0.9889578223228455\n",
      "training 414.8954772949219 relative L2 0.9887914061546326\n",
      "training 414.75592041015625 relative L2 0.9886216521263123\n",
      "training 414.6134338378906 relative L2 0.9884480237960815\n",
      "training 414.4679870605469 relative L2 0.9882709980010986\n",
      "training 414.3194274902344 relative L2 0.9880902767181396\n",
      "training 414.16790771484375 relative L2 0.9879061579704285\n",
      "training 414.013671875 relative L2 0.9877188205718994\n",
      "training 413.8565368652344 relative L2 0.98752760887146\n",
      "training 413.6963806152344 relative L2 0.9873329997062683\n",
      "training 413.5333557128906 relative L2 0.9871348142623901\n",
      "training 413.3673400878906 relative L2 0.9869325757026672\n",
      "training 413.197998046875 relative L2 0.9867261052131653\n",
      "training 413.0251159667969 relative L2 0.98651522397995\n",
      "training 412.8487548828125 relative L2 0.9863002300262451\n",
      "training 412.6688232421875 relative L2 0.9860809445381165\n",
      "training 412.4853210449219 relative L2 0.9858571290969849\n",
      "training 412.2982177734375 relative L2 0.9856290817260742\n",
      "training 412.10748291015625 relative L2 0.9853965640068054\n",
      "training 411.9131164550781 relative L2 0.9851595759391785\n",
      "training 411.71502685546875 relative L2 0.9849181771278381\n",
      "training 411.51318359375 relative L2 0.984671950340271\n",
      "training 411.3076171875 relative L2 0.9844213128089905\n",
      "training 411.0982360839844 relative L2 0.9841658473014832\n",
      "training 410.8848571777344 relative L2 0.9839054346084595\n",
      "training 410.6675109863281 relative L2 0.9836401343345642\n",
      "training 410.44610595703125 relative L2 0.9833702445030212\n",
      "training 410.2208557128906 relative L2 0.9830957651138306\n",
      "training 409.9919128417969 relative L2 0.9828163981437683\n",
      "training 409.759033203125 relative L2 0.9825320839881897\n",
      "training 409.5220031738281 relative L2 0.9822426438331604\n",
      "training 409.28076171875 relative L2 0.9819480180740356\n",
      "training 409.0353088378906 relative L2 0.9816482663154602\n",
      "training 408.78564453125 relative L2 0.981343150138855\n",
      "training 408.5316162109375 relative L2 0.9810327291488647\n",
      "training 408.2733154296875 relative L2 0.9807173013687134\n",
      "training 408.0107116699219 relative L2 0.9803964495658875\n",
      "training 407.74383544921875 relative L2 0.9800702929496765\n",
      "training 407.4726257324219 relative L2 0.979738712310791\n",
      "training 407.197021484375 relative L2 0.9794013500213623\n",
      "training 406.9167175292969 relative L2 0.9790582060813904\n",
      "training 406.6315612792969 relative L2 0.978708803653717\n",
      "training 406.3415222167969 relative L2 0.9783535003662109\n",
      "training 406.0464782714844 relative L2 0.9779919385910034\n",
      "training 405.74639892578125 relative L2 0.9776239991188049\n",
      "training 405.4411926269531 relative L2 0.9772496223449707\n",
      "training 405.13079833984375 relative L2 0.9768688678741455\n",
      "training 404.815185546875 relative L2 0.9764816164970398\n",
      "training 404.49432373046875 relative L2 0.9760876893997192\n",
      "training 404.1680603027344 relative L2 0.9756870269775391\n",
      "training 403.83636474609375 relative L2 0.9752798080444336\n",
      "training 403.4993591308594 relative L2 0.9748682975769043\n",
      "training 403.1589050292969 relative L2 0.9744516015052795\n",
      "training 402.81439208984375 relative L2 0.9740280508995056\n",
      "training 402.4643249511719 relative L2 0.9735972881317139\n",
      "training 402.10845947265625 relative L2 0.9731593728065491\n",
      "training 401.746826171875 relative L2 0.9727140069007874\n",
      "training 401.3792724609375 relative L2 0.9722610712051392\n",
      "training 401.0055236816406 relative L2 0.9718002080917358\n",
      "training 400.6255798339844 relative L2 0.9713314771652222\n",
      "training 400.23919677734375 relative L2 0.9708545207977295\n",
      "training 399.8462219238281 relative L2 0.970369279384613\n",
      "training 399.44677734375 relative L2 0.9698758125305176\n",
      "training 399.04058837890625 relative L2 0.969373881816864\n",
      "training 398.6276550292969 relative L2 0.9688632488250732\n",
      "training 398.2079772949219 relative L2 0.9683440923690796\n",
      "training 397.7812805175781 relative L2 0.9678159952163696\n",
      "training 397.3475646972656 relative L2 0.9672788977622986\n",
      "training 396.9067077636719 relative L2 0.9667326807975769\n",
      "training 396.4586181640625 relative L2 0.9661771655082703\n",
      "training 396.00323486328125 relative L2 0.9656122922897339\n",
      "training 395.5404357910156 relative L2 0.9650381207466125\n",
      "training 395.070068359375 relative L2 0.9644539952278137\n",
      "training 394.5920715332031 relative L2 0.9638599753379822\n",
      "training 394.1062316894531 relative L2 0.9632559418678284\n",
      "training 393.6124267578125 relative L2 0.9626417756080627\n",
      "training 393.11065673828125 relative L2 0.9620171785354614\n",
      "training 392.6007995605469 relative L2 0.9613820314407349\n",
      "training 392.0826416015625 relative L2 0.9607362747192383\n",
      "training 391.5561218261719 relative L2 0.960079550743103\n",
      "training 391.0210266113281 relative L2 0.9594117999076843\n",
      "training 390.47735595703125 relative L2 0.9587329030036926\n",
      "training 389.9249572753906 relative L2 0.958042562007904\n",
      "training 389.3636779785156 relative L2 0.9573404788970947\n",
      "training 388.7933349609375 relative L2 0.956626832485199\n",
      "training 388.2138977050781 relative L2 0.9559011459350586\n",
      "training 387.62518310546875 relative L2 0.9551632404327393\n",
      "training 387.027099609375 relative L2 0.9544130563735962\n",
      "training 386.41943359375 relative L2 0.9536504149436951\n",
      "training 385.8021545410156 relative L2 0.9528748989105225\n",
      "training 385.1750183105469 relative L2 0.9520865082740784\n",
      "training 384.5379943847656 relative L2 0.9512851238250732\n",
      "training 383.8908996582031 relative L2 0.9504702091217041\n",
      "training 383.23358154296875 relative L2 0.9496419429779053\n",
      "training 382.56597900390625 relative L2 0.948799729347229\n",
      "training 381.8878173828125 relative L2 0.9479437470436096\n",
      "training 381.1990966796875 relative L2 0.9470734596252441\n",
      "training 380.4996032714844 relative L2 0.946188747882843\n",
      "training 379.78912353515625 relative L2 0.9452895522117615\n",
      "training 379.0675964355469 relative L2 0.9443749189376831\n",
      "training 378.3345642089844 relative L2 0.9434446096420288\n",
      "training 377.58953857421875 relative L2 0.9424977898597717\n",
      "training 376.8321533203125 relative L2 0.9415342807769775\n",
      "training 376.0621643066406 relative L2 0.9405536651611328\n",
      "training 375.2792663574219 relative L2 0.9395557045936584\n",
      "training 374.4833984375 relative L2 0.9385402202606201\n",
      "training 373.6743469238281 relative L2 0.9375067949295044\n",
      "training 372.8520202636719 relative L2 0.9364554286003113\n",
      "training 372.0162658691406 relative L2 0.9353858232498169\n",
      "training 371.1670227050781 relative L2 0.9342978596687317\n",
      "training 370.30419921875 relative L2 0.9331913590431213\n",
      "training 369.4276428222656 relative L2 0.9320659041404724\n",
      "training 368.5372009277344 relative L2 0.9309214949607849\n",
      "training 367.6329040527344 relative L2 0.9297579526901245\n",
      "training 366.71453857421875 relative L2 0.9285750389099121\n",
      "training 365.7820739746094 relative L2 0.9273723363876343\n",
      "training 364.8352966308594 relative L2 0.926149845123291\n",
      "training 363.8741455078125 relative L2 0.9249072074890137\n",
      "training 362.8984680175781 relative L2 0.9236442446708679\n",
      "training 361.90814208984375 relative L2 0.9223605394363403\n",
      "training 360.9030456542969 relative L2 0.9210560917854309\n",
      "training 359.8830261230469 relative L2 0.9197304844856262\n",
      "training 358.8479919433594 relative L2 0.9183835387229919\n",
      "training 357.7976989746094 relative L2 0.917015016078949\n",
      "training 356.73223876953125 relative L2 0.9156262278556824\n",
      "training 355.6527099609375 relative L2 0.9142171740531921\n",
      "training 354.5589904785156 relative L2 0.912787139415741\n",
      "training 353.4508361816406 relative L2 0.9113357663154602\n",
      "training 352.3277893066406 relative L2 0.9098623394966125\n",
      "training 351.1896057128906 relative L2 0.9083665013313293\n",
      "training 350.0359802246094 relative L2 0.9068478941917419\n",
      "training 348.86669921875 relative L2 0.9053060412406921\n",
      "training 347.6815185546875 relative L2 0.9037404656410217\n",
      "training 346.4801940917969 relative L2 0.9021509289741516\n",
      "training 345.2625427246094 relative L2 0.9005368947982788\n",
      "training 344.0284423828125 relative L2 0.8988983631134033\n",
      "training 342.7777404785156 relative L2 0.8972343802452087\n",
      "training 341.5099792480469 relative L2 0.8955442905426025\n",
      "training 340.2246398925781 relative L2 0.8938270211219788\n",
      "training 338.9212646484375 relative L2 0.892082691192627\n",
      "training 337.5998229980469 relative L2 0.8903107047080994\n",
      "training 336.2601623535156 relative L2 0.8885111212730408\n",
      "training 334.90228271484375 relative L2 0.8866835832595825\n",
      "training 333.5262145996094 relative L2 0.8848281502723694\n",
      "training 332.13189697265625 relative L2 0.8829442858695984\n",
      "training 330.71929931640625 relative L2 0.88103187084198\n",
      "training 329.28839111328125 relative L2 0.8790907859802246\n",
      "training 327.8391418457031 relative L2 0.8771207928657532\n",
      "training 326.37158203125 relative L2 0.8751217722892761\n",
      "training 324.88580322265625 relative L2 0.8730936646461487\n",
      "training 323.3818359375 relative L2 0.8710357546806335\n",
      "training 321.8593444824219 relative L2 0.8689486980438232\n",
      "training 320.31890869140625 relative L2 0.8668316006660461\n",
      "training 318.76019287109375 relative L2 0.8646833896636963\n",
      "training 317.1823425292969 relative L2 0.8625034689903259\n",
      "training 315.585205078125 relative L2 0.8602914214134216\n",
      "training 313.96881103515625 relative L2 0.8580472469329834\n",
      "training 312.3329772949219 relative L2 0.8557702302932739\n",
      "training 310.6777038574219 relative L2 0.8534602522850037\n",
      "training 309.0028991699219 relative L2 0.851116955280304\n",
      "training 307.30859375 relative L2 0.8487398624420166\n",
      "training 305.5946350097656 relative L2 0.8463287949562073\n",
      "training 303.8609924316406 relative L2 0.8438832759857178\n",
      "training 302.107666015625 relative L2 0.8414030075073242\n",
      "training 300.3346252441406 relative L2 0.8388877511024475\n",
      "training 298.5418395996094 relative L2 0.8363367319107056\n",
      "training 296.7291564941406 relative L2 0.8337501287460327\n",
      "training 294.8966979980469 relative L2 0.8311272263526917\n",
      "training 293.04443359375 relative L2 0.8284679055213928\n",
      "training 291.17230224609375 relative L2 0.8257716298103333\n",
      "training 289.2803955078125 relative L2 0.8230381011962891\n",
      "training 287.3685302734375 relative L2 0.8202670216560364\n",
      "training 285.43695068359375 relative L2 0.817457914352417\n",
      "training 283.4855041503906 relative L2 0.8146081566810608\n",
      "training 281.5125427246094 relative L2 0.8117163777351379\n",
      "training 279.517578125 relative L2 0.8087817430496216\n",
      "training 277.50042724609375 relative L2 0.8058044910430908\n",
      "training 275.46136474609375 relative L2 0.8027846813201904\n",
      "training 273.4007873535156 relative L2 0.79972243309021\n",
      "training 271.3192443847656 relative L2 0.796617865562439\n",
      "training 269.217041015625 relative L2 0.7934710383415222\n",
      "training 267.09454345703125 relative L2 0.7902817726135254\n",
      "training 264.9519958496094 relative L2 0.7870499491691589\n",
      "training 262.7896423339844 relative L2 0.7837753891944885\n",
      "training 260.6076965332031 relative L2 0.7804578542709351\n",
      "training 258.40643310546875 relative L2 0.7770971655845642\n",
      "training 256.18609619140625 relative L2 0.7736932039260864\n",
      "training 253.94686889648438 relative L2 0.7702456116676331\n",
      "training 251.68899536132812 relative L2 0.766754150390625\n",
      "training 249.4126739501953 relative L2 0.7632187008857727\n",
      "training 247.11819458007812 relative L2 0.7596390843391418\n",
      "training 244.80581665039062 relative L2 0.756014883518219\n",
      "training 242.4757537841797 relative L2 0.7523404359817505\n",
      "training 240.124755859375 relative L2 0.7486145496368408\n",
      "training 237.75253295898438 relative L2 0.7448400259017944\n",
      "training 235.361328125 relative L2 0.741017758846283\n",
      "training 232.95223999023438 relative L2 0.737148106098175\n",
      "training 230.52589416503906 relative L2 0.7332310676574707\n",
      "training 228.08279418945312 relative L2 0.7292667627334595\n",
      "training 225.6234130859375 relative L2 0.7252550721168518\n",
      "training 223.1482696533203 relative L2 0.7211961150169373\n",
      "training 220.65782165527344 relative L2 0.717089831829071\n",
      "training 218.1525115966797 relative L2 0.7129360437393188\n",
      "training 215.6328582763672 relative L2 0.708734929561615\n",
      "training 213.0993194580078 relative L2 0.7044863104820251\n",
      "training 210.5523681640625 relative L2 0.7001901865005493\n",
      "training 207.9925079345703 relative L2 0.6958465576171875\n",
      "training 205.42027282714844 relative L2 0.6914554834365845\n",
      "training 202.83619689941406 relative L2 0.6870169639587402\n",
      "training 200.24087524414062 relative L2 0.6825312972068787\n",
      "training 197.63491821289062 relative L2 0.6779983043670654\n",
      "training 195.018798828125 relative L2 0.6734181642532349\n",
      "training 192.39317321777344 relative L2 0.6687910556793213\n",
      "training 189.75868225097656 relative L2 0.6641169786453247\n",
      "training 187.11593627929688 relative L2 0.659396231174469\n",
      "training 184.46559143066406 relative L2 0.6546289920806885\n",
      "training 181.80833435058594 relative L2 0.649815559387207\n",
      "training 179.14488220214844 relative L2 0.644956111907959\n",
      "training 176.47586059570312 relative L2 0.640051007270813\n",
      "training 173.80209350585938 relative L2 0.6351004242897034\n",
      "training 171.12428283691406 relative L2 0.6301048994064331\n",
      "training 168.4431915283203 relative L2 0.6250647306442261\n",
      "training 165.75962829589844 relative L2 0.6199804544448853\n",
      "training 163.07435607910156 relative L2 0.6148524284362793\n",
      "training 160.38821411132812 relative L2 0.6096811294555664\n",
      "training 157.7020263671875 relative L2 0.6044672727584839\n",
      "training 155.01666259765625 relative L2 0.5992113351821899\n",
      "training 152.3329620361328 relative L2 0.5939139127731323\n",
      "training 149.6518096923828 relative L2 0.5885757803916931\n",
      "training 146.97415161132812 relative L2 0.5831976532936096\n",
      "training 144.3008270263672 relative L2 0.5777803659439087\n",
      "training 141.63287353515625 relative L2 0.5723244547843933\n",
      "training 138.97105407714844 relative L2 0.5668310523033142\n",
      "training 136.3164520263672 relative L2 0.5613011121749878\n",
      "training 133.6700439453125 relative L2 0.5557353496551514\n",
      "training 131.03271484375 relative L2 0.5501351356506348\n",
      "training 128.4055633544922 relative L2 0.5445016026496887\n",
      "training 125.78959655761719 relative L2 0.5388352274894714\n",
      "training 123.18557739257812 relative L2 0.5331385135650635\n",
      "training 120.59503936767578 relative L2 0.5274152755737305\n",
      "training 118.02017974853516 relative L2 0.5216639041900635\n",
      "training 115.46063995361328 relative L2 0.5158853530883789\n",
      "training 112.91726684570312 relative L2 0.5100810527801514\n",
      "training 110.39107513427734 relative L2 0.5042526125907898\n",
      "training 107.88314056396484 relative L2 0.4984015226364136\n",
      "training 105.39443969726562 relative L2 0.49252957105636597\n",
      "training 102.92607116699219 relative L2 0.4866385757923126\n",
      "training 100.47904968261719 relative L2 0.48073023557662964\n",
      "training 98.0544204711914 relative L2 0.4748065173625946\n",
      "training 95.65320587158203 relative L2 0.4688693881034851\n",
      "training 93.27642059326172 relative L2 0.4629209339618683\n",
      "training 90.92507934570312 relative L2 0.4569632411003113\n",
      "training 88.60018920898438 relative L2 0.4509985148906708\n",
      "training 86.30271911621094 relative L2 0.44502973556518555\n",
      "training 84.03387451171875 relative L2 0.4390576481819153\n",
      "training 81.79402160644531 relative L2 0.4330861568450928\n",
      "training 79.58466339111328 relative L2 0.42711737751960754\n",
      "training 77.4065170288086 relative L2 0.4211539030075073\n",
      "training 75.26046752929688 relative L2 0.4151982367038727\n",
      "training 73.1473617553711 relative L2 0.40925338864326477\n",
      "training 71.068115234375 relative L2 0.40332233905792236\n",
      "training 69.02352142333984 relative L2 0.3974074125289917\n",
      "training 67.01425170898438 relative L2 0.39151495695114136\n",
      "training 65.04208374023438 relative L2 0.3856401741504669\n",
      "training 63.10517501831055 relative L2 0.37979352474212646\n",
      "training 61.20658874511719 relative L2 0.3739759027957916\n",
      "training 59.34621810913086 relative L2 0.36819031834602356\n",
      "training 57.524574279785156 relative L2 0.36244234442710876\n",
      "training 55.742862701416016 relative L2 0.35673192143440247\n",
      "training 54.00054931640625 relative L2 0.35106635093688965\n",
      "training 52.299259185791016 relative L2 0.34544575214385986\n",
      "training 50.63837814331055 relative L2 0.3398759067058563\n",
      "training 49.018917083740234 relative L2 0.33435705304145813\n",
      "training 47.44023895263672 relative L2 0.3288954496383667\n",
      "training 45.903385162353516 relative L2 0.3234943747520447\n",
      "training 44.40842819213867 relative L2 0.3181638717651367\n",
      "training 42.95726776123047 relative L2 0.31290000677108765\n",
      "training 41.54789733886719 relative L2 0.30770593881607056\n",
      "training 40.18024826049805 relative L2 0.30259275436401367\n",
      "training 38.85624694824219 relative L2 0.2975590229034424\n",
      "training 37.574485778808594 relative L2 0.29260748624801636\n",
      "training 36.33460998535156 relative L2 0.2877429127693176\n",
      "training 35.13676834106445 relative L2 0.28296807408332825\n",
      "training 33.98053741455078 relative L2 0.27828752994537354\n",
      "training 32.865901947021484 relative L2 0.2737039029598236\n",
      "training 31.792346954345703 relative L2 0.26921984553337097\n",
      "training 30.759361267089844 relative L2 0.264838844537735\n",
      "training 29.766576766967773 relative L2 0.2605617940425873\n",
      "training 28.81304931640625 relative L2 0.2563931345939636\n",
      "training 27.89861488342285 relative L2 0.25235840678215027\n",
      "training 27.027585983276367 relative L2 0.24843110144138336\n",
      "training 26.1929988861084 relative L2 0.24459293484687805\n",
      "training 25.3900089263916 relative L2 0.2408951371908188\n",
      "training 24.628204345703125 relative L2 0.23732146620750427\n",
      "training 23.902978897094727 relative L2 0.23386962711811066\n",
      "training 23.212749481201172 relative L2 0.23053690791130066\n",
      "training 22.555925369262695 relative L2 0.22732585668563843\n",
      "training 21.931974411010742 relative L2 0.22423486411571503\n",
      "training 21.339630126953125 relative L2 0.22126296162605286\n",
      "training 20.777729034423828 relative L2 0.21841666102409363\n",
      "training 20.24656105041504 relative L2 0.21572883427143097\n",
      "training 19.751266479492188 relative L2 0.21312174201011658\n",
      "training 19.2767276763916 relative L2 0.21064306795597076\n",
      "training 18.830894470214844 relative L2 0.20830322802066803\n",
      "training 18.414798736572266 relative L2 0.206104576587677\n",
      "training 18.027997970581055 relative L2 0.2040315717458725\n",
      "training 17.667097091674805 relative L2 0.20204907655715942\n",
      "training 17.325349807739258 relative L2 0.20017118752002716\n",
      "training 17.004688262939453 relative L2 0.1984030157327652\n",
      "training 16.705493927001953 relative L2 0.19675123691558838\n",
      "training 16.42840003967285 relative L2 0.1952061802148819\n",
      "training 16.171293258666992 relative L2 0.19376324117183685\n",
      "training 15.932979583740234 relative L2 0.19242097437381744\n",
      "training 15.71286678314209 relative L2 0.1911664456129074\n",
      "training 15.508519172668457 relative L2 0.19000005722045898\n",
      "training 15.319717407226562 relative L2 0.18891268968582153\n",
      "training 15.144734382629395 relative L2 0.1879078447818756\n",
      "training 14.983912467956543 relative L2 0.18698006868362427\n",
      "training 14.83617115020752 relative L2 0.1861242651939392\n",
      "training 14.700531959533691 relative L2 0.18533387780189514\n",
      "training 14.57579231262207 relative L2 0.18460872769355774\n",
      "training 14.461812019348145 relative L2 0.1839480698108673\n",
      "training 14.35836124420166 relative L2 0.1833423227071762\n",
      "training 14.263824462890625 relative L2 0.1827881932258606\n",
      "training 14.177614212036133 relative L2 0.18227553367614746\n",
      "training 14.098076820373535 relative L2 0.18179412186145782\n",
      "training 14.023578643798828 relative L2 0.1813737154006958\n",
      "training 13.958673477172852 relative L2 0.18099743127822876\n",
      "training 13.900691032409668 relative L2 0.18065425753593445\n",
      "training 13.847904205322266 relative L2 0.1803433746099472\n",
      "training 13.800175666809082 relative L2 0.18005982041358948\n",
      "training 13.75671100616455 relative L2 0.17980045080184937\n",
      "training 13.717009544372559 relative L2 0.1795637309551239\n",
      "training 13.680822372436523 relative L2 0.17935289442539215\n",
      "training 13.648636817932129 relative L2 0.17916519939899445\n",
      "training 13.62000846862793 relative L2 0.1789902299642563\n",
      "training 13.593344688415527 relative L2 0.1788378655910492\n",
      "training 13.570149421691895 relative L2 0.17870080471038818\n",
      "training 13.549293518066406 relative L2 0.1785741001367569\n",
      "training 13.530022621154785 relative L2 0.17845654487609863\n",
      "training 13.512151718139648 relative L2 0.1783486008644104\n",
      "training 13.495759963989258 relative L2 0.1782485395669937\n",
      "training 13.480570793151855 relative L2 0.1781548261642456\n",
      "training 13.466341972351074 relative L2 0.1780669242143631\n",
      "training 13.453001976013184 relative L2 0.1779840737581253\n",
      "training 13.440448760986328 relative L2 0.17790356278419495\n",
      "training 13.428261756896973 relative L2 0.17782270908355713\n",
      "training 13.416024208068848 relative L2 0.17774152755737305\n",
      "training 13.403736114501953 relative L2 0.17765672504901886\n",
      "training 13.390894889831543 relative L2 0.17756633460521698\n",
      "training 13.377232551574707 relative L2 0.1774689108133316\n",
      "training 13.362515449523926 relative L2 0.17736922204494476\n",
      "training 13.347458839416504 relative L2 0.1773018091917038\n",
      "training 13.337295532226562 relative L2 0.17726421356201172\n",
      "training 13.331622123718262 relative L2 0.1772277057170868\n",
      "training 13.326118469238281 relative L2 0.17719222605228424\n",
      "training 13.320765495300293 relative L2 0.1771562397480011\n",
      "training 13.315343856811523 relative L2 0.1771208643913269\n",
      "training 13.310017585754395 relative L2 0.17708593606948853\n",
      "training 13.304757118225098 relative L2 0.1770510971546173\n",
      "training 13.299517631530762 relative L2 0.17701677978038788\n",
      "training 13.294354438781738 relative L2 0.17698325216770172\n",
      "training 13.289307594299316 relative L2 0.17694951593875885\n",
      "training 13.284235000610352 relative L2 0.1769164800643921\n",
      "training 13.27926254272461 relative L2 0.17688360810279846\n",
      "training 13.274316787719727 relative L2 0.17685097455978394\n",
      "training 13.269416809082031 relative L2 0.17681889235973358\n",
      "training 13.26460075378418 relative L2 0.1767866313457489\n",
      "training 13.259757041931152 relative L2 0.1767549365758896\n",
      "training 13.255001068115234 relative L2 0.1767234355211258\n",
      "training 13.250275611877441 relative L2 0.17669185996055603\n",
      "training 13.245539665222168 relative L2 0.1766606867313385\n",
      "training 13.240861892700195 relative L2 0.17662964761257172\n",
      "training 13.236209869384766 relative L2 0.17659875750541687\n",
      "training 13.231575012207031 relative L2 0.17656797170639038\n",
      "training 13.226964950561523 relative L2 0.17653729021549225\n",
      "training 13.222368240356445 relative L2 0.17650672793388367\n",
      "training 13.217792510986328 relative L2 0.17647631466388702\n",
      "training 13.213239669799805 relative L2 0.1764460802078247\n",
      "training 13.208714485168457 relative L2 0.17641595005989075\n",
      "training 13.204204559326172 relative L2 0.17638593912124634\n",
      "training 13.199711799621582 relative L2 0.17635595798492432\n",
      "training 13.195229530334473 relative L2 0.17632614076137543\n",
      "training 13.190771102905273 relative L2 0.1762964278459549\n",
      "training 13.18632698059082 relative L2 0.17626677453517914\n",
      "training 13.181897163391113 relative L2 0.17623716592788696\n",
      "training 13.177469253540039 relative L2 0.17620761692523956\n",
      "training 13.173054695129395 relative L2 0.17617812752723694\n",
      "training 13.168652534484863 relative L2 0.17614860832691193\n",
      "training 13.164241790771484 relative L2 0.17611907422542572\n",
      "training 13.159832954406738 relative L2 0.176089346408844\n",
      "training 13.155397415161133 relative L2 0.17605939507484436\n",
      "training 13.150927543640137 relative L2 0.17602889239788055\n",
      "training 13.146379470825195 relative L2 0.17599734663963318\n",
      "training 13.141679763793945 relative L2 0.17596374452114105\n",
      "training 13.136672019958496 relative L2 0.17592622339725494\n",
      "training 13.131086349487305 relative L2 0.17588108777999878\n",
      "training 13.124354362487793 relative L2 0.17582504451274872\n",
      "training 13.115989685058594 relative L2 0.1757960468530655\n",
      "training 13.111678123474121 relative L2 0.1757676750421524\n",
      "training 13.107460975646973 relative L2 0.17573949694633484\n",
      "training 13.103272438049316 relative L2 0.17571118474006653\n",
      "training 13.0990571975708 relative L2 0.1756828874349594\n",
      "training 13.094846725463867 relative L2 0.1756545752286911\n",
      "training 13.0906343460083 relative L2 0.17562630772590637\n",
      "training 13.08642864227295 relative L2 0.17559818923473358\n",
      "training 13.082242012023926 relative L2 0.17557016015052795\n",
      "training 13.07807445526123 relative L2 0.17554184794425964\n",
      "training 13.073869705200195 relative L2 0.17551369965076447\n",
      "training 13.06968879699707 relative L2 0.1754857301712036\n",
      "training 13.065529823303223 relative L2 0.17545771598815918\n",
      "training 13.061363220214844 relative L2 0.17542962729930878\n",
      "training 13.057186126708984 relative L2 0.17540159821510315\n",
      "training 13.053022384643555 relative L2 0.17537374794483185\n",
      "training 13.048879623413086 relative L2 0.175345778465271\n",
      "training 13.044722557067871 relative L2 0.17531785368919373\n",
      "training 13.040573120117188 relative L2 0.17529000341892242\n",
      "training 13.036436080932617 relative L2 0.17526216804981232\n",
      "training 13.032301902770996 relative L2 0.17523439228534698\n",
      "training 13.028176307678223 relative L2 0.17520661652088165\n",
      "training 13.02405071258545 relative L2 0.1751788705587387\n",
      "training 13.019927978515625 relative L2 0.17515116930007935\n",
      "training 13.015815734863281 relative L2 0.17512351274490356\n",
      "training 13.011709213256836 relative L2 0.17509590089321136\n",
      "training 13.007612228393555 relative L2 0.17506831884384155\n",
      "training 13.003517150878906 relative L2 0.17504078149795532\n",
      "training 12.999429702758789 relative L2 0.17501328885555267\n",
      "training 12.99534797668457 relative L2 0.1749858558177948\n",
      "training 12.991276741027832 relative L2 0.17495842278003693\n",
      "training 12.987205505371094 relative L2 0.17493103444576263\n",
      "training 12.983144760131836 relative L2 0.17490367591381073\n",
      "training 12.979086875915527 relative L2 0.1748763471841812\n",
      "training 12.975037574768066 relative L2 0.17484907805919647\n",
      "training 12.970992088317871 relative L2 0.17482180893421173\n",
      "training 12.966951370239258 relative L2 0.17479458451271057\n",
      "training 12.962913513183594 relative L2 0.1747673898935318\n",
      "training 12.958887100219727 relative L2 0.17474019527435303\n",
      "training 12.954858779907227 relative L2 0.17471307516098022\n",
      "training 12.95083999633789 relative L2 0.17468592524528503\n",
      "training 12.946822166442871 relative L2 0.17465883493423462\n",
      "training 12.94281005859375 relative L2 0.1746317744255066\n",
      "training 12.938800811767578 relative L2 0.17460468411445618\n",
      "training 12.934793472290039 relative L2 0.17457762360572815\n",
      "training 12.930787086486816 relative L2 0.17455057799816132\n",
      "training 12.926787376403809 relative L2 0.17452356219291687\n",
      "training 12.922788619995117 relative L2 0.17449654638767242\n",
      "training 12.918794631958008 relative L2 0.17446954548358917\n",
      "training 12.914798736572266 relative L2 0.17444255948066711\n",
      "training 12.910809516906738 relative L2 0.17441558837890625\n",
      "training 12.906821250915527 relative L2 0.174388587474823\n",
      "training 12.902830123901367 relative L2 0.17436161637306213\n",
      "training 12.898843765258789 relative L2 0.1743345856666565\n",
      "training 12.89484977722168 relative L2 0.17430755496025085\n",
      "training 12.890856742858887 relative L2 0.17428052425384521\n",
      "training 12.886861801147461 relative L2 0.1742534637451172\n",
      "training 12.882868766784668 relative L2 0.17422635853290558\n",
      "training 12.878866195678711 relative L2 0.17419922351837158\n",
      "training 12.874860763549805 relative L2 0.17417199909687042\n",
      "training 12.870844841003418 relative L2 0.17414472997188568\n",
      "training 12.8668212890625 relative L2 0.1741173416376114\n",
      "training 12.862776756286621 relative L2 0.17408974468708038\n",
      "training 12.858707427978516 relative L2 0.17406193912029266\n",
      "training 12.854607582092285 relative L2 0.1740337759256363\n",
      "training 12.850456237792969 relative L2 0.17400503158569336\n",
      "training 12.84621810913086 relative L2 0.17397540807724\n",
      "training 12.841852188110352 relative L2 0.17394419014453888\n",
      "training 12.837252616882324 relative L2 0.1739102005958557\n",
      "training 12.832242965698242 relative L2 0.17387057840824127\n",
      "training 12.826401710510254 relative L2 0.17381907999515533\n",
      "training 12.81879997253418 relative L2 0.17377591133117676\n",
      "training 12.812440872192383 relative L2 0.17374953627586365\n",
      "training 12.808565139770508 relative L2 0.1737232655286789\n",
      "training 12.80470085144043 relative L2 0.17369695007801056\n",
      "training 12.80083179473877 relative L2 0.17367060482501984\n",
      "training 12.796957015991211 relative L2 0.17364424467086792\n",
      "training 12.793079376220703 relative L2 0.17361783981323242\n",
      "training 12.78919506072998 relative L2 0.17359143495559692\n",
      "training 12.78531265258789 relative L2 0.17356497049331665\n",
      "training 12.781418800354004 relative L2 0.17353850603103638\n",
      "training 12.777524948120117 relative L2 0.17351193726062775\n",
      "training 12.773619651794434 relative L2 0.17348532378673553\n",
      "training 12.769704818725586 relative L2 0.1734587699174881\n",
      "training 12.765799522399902 relative L2 0.17343206703662872\n",
      "training 12.761873245239258 relative L2 0.17340540885925293\n",
      "training 12.757954597473145 relative L2 0.17337870597839355\n",
      "training 12.754029273986816 relative L2 0.1733519434928894\n",
      "training 12.750097274780273 relative L2 0.17332518100738525\n",
      "training 12.746162414550781 relative L2 0.1732984334230423\n",
      "training 12.742232322692871 relative L2 0.17327328026294708\n",
      "training 12.738534927368164 relative L2 0.17324528098106384\n",
      "training 12.734423637390137 relative L2 0.17321880161762238\n",
      "training 12.730538368225098 relative L2 0.17319218814373016\n",
      "training 12.726632118225098 relative L2 0.17316555976867676\n",
      "training 12.722724914550781 relative L2 0.17313897609710693\n",
      "training 12.718819618225098 relative L2 0.17311233282089233\n",
      "training 12.7149076461792 relative L2 0.17308558523654938\n",
      "training 12.710982322692871 relative L2 0.17305885255336761\n",
      "training 12.707061767578125 relative L2 0.17303207516670227\n",
      "training 12.703133583068848 relative L2 0.17300532758235931\n",
      "training 12.69920825958252 relative L2 0.17297852039337158\n",
      "training 12.695274353027344 relative L2 0.17295166850090027\n",
      "training 12.691337585449219 relative L2 0.17292484641075134\n",
      "training 12.687403678894043 relative L2 0.17289793491363525\n",
      "training 12.683459281921387 relative L2 0.17287106812000275\n",
      "training 12.679519653320312 relative L2 0.17284420132637024\n",
      "training 12.675581932067871 relative L2 0.17281728982925415\n",
      "training 12.671637535095215 relative L2 0.17279034852981567\n",
      "training 12.667688369750977 relative L2 0.17276343703269958\n",
      "training 12.663742065429688 relative L2 0.17273646593093872\n",
      "training 12.659795761108398 relative L2 0.17270955443382263\n",
      "training 12.65585708618164 relative L2 0.17268267273902893\n",
      "training 12.651918411254883 relative L2 0.17265571653842926\n",
      "training 12.647972106933594 relative L2 0.17262867093086243\n",
      "training 12.644014358520508 relative L2 0.1726018637418747\n",
      "training 12.640088081359863 relative L2 0.17257468402385712\n",
      "training 12.636113166809082 relative L2 0.17254772782325745\n",
      "training 12.632169723510742 relative L2 0.172520712018013\n",
      "training 12.628216743469238 relative L2 0.17249369621276855\n",
      "training 12.624265670776367 relative L2 0.17246659100055695\n",
      "training 12.620305061340332 relative L2 0.17243948578834534\n",
      "training 12.616338729858398 relative L2 0.17241229116916656\n",
      "training 12.612364768981934 relative L2 0.17238499224185944\n",
      "training 12.60837459564209 relative L2 0.17235761880874634\n",
      "training 12.604376792907715 relative L2 0.17233017086982727\n",
      "training 12.60036849975586 relative L2 0.17230254411697388\n",
      "training 12.596332550048828 relative L2 0.17227472364902496\n",
      "training 12.592273712158203 relative L2 0.17224667966365814\n",
      "training 12.588176727294922 relative L2 0.1722182184457779\n",
      "training 12.584027290344238 relative L2 0.17218923568725586\n",
      "training 12.579798698425293 relative L2 0.17215950787067413\n",
      "training 12.57546615600586 relative L2 0.17212863266468048\n",
      "training 12.570964813232422 relative L2 0.17209619283676147\n",
      "training 12.56623363494873 relative L2 0.17206129431724548\n",
      "training 12.561150550842285 relative L2 0.17202281951904297\n",
      "training 12.555540084838867 relative L2 0.17198032140731812\n",
      "training 12.549337387084961 relative L2 0.17193473875522614\n",
      "training 12.542684555053711 relative L2 0.17189037799835205\n",
      "training 12.536214828491211 relative L2 0.1718532145023346\n",
      "training 12.530817031860352 relative L2 0.17182385921478271\n",
      "training 12.526545524597168 relative L2 0.17179547250270844\n",
      "training 12.522414207458496 relative L2 0.1717669665813446\n",
      "training 12.518268585205078 relative L2 0.1717383861541748\n",
      "training 12.51411247253418 relative L2 0.17170973122119904\n",
      "training 12.509942054748535 relative L2 0.1716810017824173\n",
      "training 12.505764961242676 relative L2 0.17165222764015198\n",
      "training 12.501578330993652 relative L2 0.1716233491897583\n",
      "training 12.497379302978516 relative L2 0.17159436643123627\n",
      "training 12.493162155151367 relative L2 0.17156527936458588\n",
      "training 12.488935470581055 relative L2 0.17153610289096832\n",
      "training 12.48469352722168 relative L2 0.1715068370103836\n",
      "training 12.48044204711914 relative L2 0.17147737741470337\n",
      "training 12.476157188415527 relative L2 0.17144767940044403\n",
      "training 12.471844673156738 relative L2 0.1714177429676056\n",
      "training 12.46749496459961 relative L2 0.1713874191045761\n",
      "training 12.463089942932129 relative L2 0.1713564693927765\n",
      "training 12.458597183227539 relative L2 0.17132464051246643\n",
      "training 12.453971862792969 relative L2 0.1712912917137146\n",
      "training 12.449131965637207 relative L2 0.1712552011013031\n",
      "training 12.443892478942871 relative L2 0.1712145358324051\n",
      "training 12.437984466552734 relative L2 0.1711656153202057\n",
      "training 12.430878639221191 relative L2 0.17111507058143616\n",
      "training 12.423537254333496 relative L2 0.1710864007472992\n",
      "training 12.419382095336914 relative L2 0.17105679214000702\n",
      "training 12.415094375610352 relative L2 0.1710275560617447\n",
      "training 12.410857200622559 relative L2 0.17099808156490326\n",
      "training 12.406584739685059 relative L2 0.17096856236457825\n",
      "training 12.402300834655762 relative L2 0.17093883454799652\n",
      "training 12.397994995117188 relative L2 0.1709091067314148\n",
      "training 12.39368724822998 relative L2 0.17087920010089874\n",
      "training 12.389351844787598 relative L2 0.1708492487668991\n",
      "training 12.385013580322266 relative L2 0.17081917822360992\n",
      "training 12.380660057067871 relative L2 0.17078904807567596\n",
      "training 12.376289367675781 relative L2 0.17075879871845245\n",
      "training 12.371910095214844 relative L2 0.17072847485542297\n",
      "training 12.367521286010742 relative L2 0.17069831490516663\n",
      "training 12.36315631866455 relative L2 0.17066793143749237\n",
      "training 12.358756065368652 relative L2 0.1706385314464569\n",
      "training 12.354501724243164 relative L2 0.17060743272304535\n",
      "training 12.35000228881836 relative L2 0.17057731747627258\n",
      "training 12.345646858215332 relative L2 0.1705470085144043\n",
      "training 12.341264724731445 relative L2 0.17051662504673004\n",
      "training 12.336869239807129 relative L2 0.17048616707324982\n",
      "training 12.332464218139648 relative L2 0.17045560479164124\n",
      "training 12.328045845031738 relative L2 0.17042510211467743\n",
      "training 12.32363510131836 relative L2 0.1703944057226181\n",
      "training 12.319197654724121 relative L2 0.17036370933055878\n",
      "training 12.314764976501465 relative L2 0.17033295333385468\n",
      "training 12.310317039489746 relative L2 0.1703021079301834\n",
      "training 12.305862426757812 relative L2 0.1702711582183838\n",
      "training 12.301389694213867 relative L2 0.17024016380310059\n",
      "training 12.296915054321289 relative L2 0.17020940780639648\n",
      "training 12.29247760772705 relative L2 0.17017816007137299\n",
      "training 12.287965774536133 relative L2 0.170147106051445\n",
      "training 12.283487319946289 relative L2 0.1701159030199051\n",
      "training 12.278986930847168 relative L2 0.17008458077907562\n",
      "training 12.274469375610352 relative L2 0.170053169131279\n",
      "training 12.269942283630371 relative L2 0.17002162337303162\n",
      "training 12.265395164489746 relative L2 0.16999000310897827\n",
      "training 12.26083755493164 relative L2 0.16995832324028015\n",
      "training 12.256272315979004 relative L2 0.16992653906345367\n",
      "training 12.251694679260254 relative L2 0.16989462077617645\n",
      "training 12.247096061706543 relative L2 0.16986261308193207\n",
      "training 12.242486000061035 relative L2 0.16983041167259216\n",
      "training 12.237852096557617 relative L2 0.16979800164699554\n",
      "training 12.233186721801758 relative L2 0.16976554691791534\n",
      "training 12.22851848602295 relative L2 0.1697329431772232\n",
      "training 12.22382926940918 relative L2 0.16970007121562958\n",
      "training 12.219099998474121 relative L2 0.16966696083545685\n",
      "training 12.214338302612305 relative L2 0.1696336269378662\n",
      "training 12.209545135498047 relative L2 0.1696000099182129\n",
      "training 12.20471477508545 relative L2 0.16956612467765808\n",
      "training 12.199847221374512 relative L2 0.16953180730342865\n",
      "training 12.194916725158691 relative L2 0.16949720680713654\n",
      "training 12.189947128295898 relative L2 0.16946223378181458\n",
      "training 12.184922218322754 relative L2 0.16942672431468964\n",
      "training 12.179826736450195 relative L2 0.16939067840576172\n",
      "training 12.174656867980957 relative L2 0.16935408115386963\n",
      "training 12.169413566589355 relative L2 0.16931688785552979\n",
      "training 12.164088249206543 relative L2 0.16927871108055115\n",
      "training 12.158629417419434 relative L2 0.169239804148674\n",
      "training 12.153067588806152 relative L2 0.16919995844364166\n",
      "training 12.147371292114258 relative L2 0.16915884613990784\n",
      "training 12.141500473022461 relative L2 0.16911651194095612\n",
      "training 12.135459899902344 relative L2 0.16907253861427307\n",
      "training 12.12918472290039 relative L2 0.16902685165405273\n",
      "training 12.122668266296387 relative L2 0.16897915303707123\n",
      "training 12.115867614746094 relative L2 0.1689291149377823\n",
      "training 12.10872745513916 relative L2 0.16887637972831726\n",
      "training 12.101204872131348 relative L2 0.16882005333900452\n",
      "training 12.093171119689941 relative L2 0.16875997185707092\n",
      "training 12.08459758758545 relative L2 0.16869521141052246\n",
      "training 12.075353622436523 relative L2 0.16862450540065765\n",
      "training 12.065279960632324 relative L2 0.1685469150543213\n",
      "training 12.054229736328125 relative L2 0.1684609055519104\n",
      "training 12.041993141174316 relative L2 0.16836470365524292\n",
      "training 12.028298377990723 relative L2 0.16825652122497559\n",
      "training 12.012866020202637 relative L2 0.16813363134860992\n",
      "training 11.995349884033203 relative L2 0.1679943948984146\n",
      "training 11.975502014160156 relative L2 0.1678367406129837\n",
      "training 11.953012466430664 relative L2 0.16766111552715302\n",
      "training 11.927993774414062 relative L2 0.1674778014421463\n",
      "training 11.901925086975098 relative L2 0.16732889413833618\n",
      "training 11.880813598632812 relative L2 0.16725532710552216\n",
      "training 11.870471954345703 relative L2 0.16720552742481232\n",
      "training 11.863483428955078 relative L2 0.16715902090072632\n",
      "training 11.856921195983887 relative L2 0.16710761189460754\n",
      "training 11.849635124206543 relative L2 0.16704842448234558\n",
      "training 11.841251373291016 relative L2 0.16698303818702698\n",
      "training 11.831974983215332 relative L2 0.16691246628761292\n",
      "training 11.82197380065918 relative L2 0.16683261096477509\n",
      "training 11.810657501220703 relative L2 0.16673865914344788\n",
      "training 11.797338485717773 relative L2 0.16662350296974182\n",
      "training 11.781012535095215 relative L2 0.16653892397880554\n",
      "training 11.769034385681152 relative L2 0.16645489633083344\n",
      "training 11.757137298583984 relative L2 0.16638539731502533\n",
      "training 11.747312545776367 relative L2 0.16633737087249756\n",
      "training 11.74052619934082 relative L2 0.16628776490688324\n",
      "training 11.733514785766602 relative L2 0.1662372499704361\n",
      "training 11.726381301879883 relative L2 0.16618852317333221\n",
      "training 11.719507217407227 relative L2 0.16613486409187317\n",
      "training 11.711942672729492 relative L2 0.16608206927776337\n",
      "training 11.70450496673584 relative L2 0.1660308539867401\n",
      "training 11.697287559509277 relative L2 0.16598039865493774\n",
      "training 11.690184593200684 relative L2 0.16593106091022491\n",
      "training 11.683239936828613 relative L2 0.16588178277015686\n",
      "training 11.676301956176758 relative L2 0.16583259403705597\n",
      "training 11.669370651245117 relative L2 0.1657818704843521\n",
      "training 11.662225723266602 relative L2 0.1657339632511139\n",
      "training 11.655481338500977 relative L2 0.16568563878536224\n",
      "training 11.648683547973633 relative L2 0.1656373292207718\n",
      "training 11.641890525817871 relative L2 0.16558937728405\n",
      "training 11.635150909423828 relative L2 0.1655411422252655\n",
      "training 11.628375053405762 relative L2 0.16549299657344818\n",
      "training 11.621614456176758 relative L2 0.165445014834404\n",
      "training 11.614880561828613 relative L2 0.16539694368839264\n",
      "training 11.608136177062988 relative L2 0.16534875333309174\n",
      "training 11.601377487182617 relative L2 0.16530044376850128\n",
      "training 11.5946044921875 relative L2 0.16525216400623322\n",
      "training 11.587841987609863 relative L2 0.16520369052886963\n",
      "training 11.5810546875 relative L2 0.16515514254570007\n",
      "training 11.574260711669922 relative L2 0.16510652005672455\n",
      "training 11.567458152770996 relative L2 0.16505786776542664\n",
      "training 11.560653686523438 relative L2 0.16500914096832275\n",
      "training 11.553839683532715 relative L2 0.1649603545665741\n",
      "training 11.547019958496094 relative L2 0.16491155326366425\n",
      "training 11.540202140808105 relative L2 0.16486267745494843\n",
      "training 11.533374786376953 relative L2 0.16481372714042664\n",
      "training 11.526538848876953 relative L2 0.1647646725177765\n",
      "training 11.519694328308105 relative L2 0.16471551358699799\n",
      "training 11.512834548950195 relative L2 0.16466620564460754\n",
      "training 11.505958557128906 relative L2 0.16461649537086487\n",
      "training 11.499027252197266 relative L2 0.16456611454486847\n",
      "training 11.492003440856934 relative L2 0.16451720893383026\n",
      "training 11.485185623168945 relative L2 0.16446152329444885\n",
      "training 11.477431297302246 relative L2 0.1644037663936615\n",
      "training 11.469389915466309 relative L2 0.16434124112129211\n",
      "training 11.460677146911621 relative L2 0.16429419815540314\n",
      "training 11.454130172729492 relative L2 0.16424569487571716\n",
      "training 11.44738483428955 relative L2 0.1641966998577118\n",
      "training 11.440571784973145 relative L2 0.16414791345596313\n",
      "training 11.433783531188965 relative L2 0.16409865021705627\n",
      "training 11.426929473876953 relative L2 0.1640489250421524\n",
      "training 11.42000675201416 relative L2 0.16399911046028137\n",
      "training 11.413077354431152 relative L2 0.1639493703842163\n",
      "training 11.406155586242676 relative L2 0.1638995260000229\n",
      "training 11.399224281311035 relative L2 0.16385038197040558\n",
      "training 11.39239501953125 relative L2 0.16380055248737335\n",
      "training 11.385480880737305 relative L2 0.1637507677078247\n",
      "training 11.378569602966309 relative L2 0.1637011170387268\n",
      "training 11.37167739868164 relative L2 0.16365112364292145\n",
      "training 11.364738464355469 relative L2 0.1636008620262146\n",
      "training 11.357763290405273 relative L2 0.16355031728744507\n",
      "training 11.350754737854004 relative L2 0.16350015997886658\n",
      "training 11.343802452087402 relative L2 0.16344991326332092\n",
      "training 11.33683967590332 relative L2 0.16339926421642303\n",
      "training 11.329830169677734 relative L2 0.16334836184978485\n",
      "training 11.322792053222656 relative L2 0.16329717636108398\n",
      "training 11.315711975097656 relative L2 0.16324590146541595\n",
      "training 11.30861759185791 relative L2 0.16319359838962555\n",
      "training 11.301386833190918 relative L2 0.16314177215099335\n",
      "training 11.294224739074707 relative L2 0.16308754682540894\n",
      "training 11.286739349365234 relative L2 0.16303274035453796\n",
      "training 11.27917194366455 relative L2 0.1629759669303894\n",
      "training 11.27133846282959 relative L2 0.16291655600070953\n",
      "training 11.26313591003418 relative L2 0.16285362839698792\n",
      "training 11.254449844360352 relative L2 0.1627853363752365\n",
      "training 11.245024681091309 relative L2 0.16270963847637177\n",
      "training 11.234574317932129 relative L2 0.16262589395046234\n",
      "training 11.22301959991455 relative L2 0.16256341338157654\n",
      "training 11.214412689208984 relative L2 0.16251234710216522\n",
      "training 11.207383155822754 relative L2 0.1624612659215927\n",
      "training 11.200346946716309 relative L2 0.1624094843864441\n",
      "training 11.193220138549805 relative L2 0.1623574197292328\n",
      "training 11.186052322387695 relative L2 0.16230462491512299\n",
      "training 11.178780555725098 relative L2 0.1622520089149475\n",
      "training 11.171539306640625 relative L2 0.16219943761825562\n",
      "training 11.164311408996582 relative L2 0.1621464639902115\n",
      "training 11.157033920288086 relative L2 0.1620938628911972\n",
      "training 11.149803161621094 relative L2 0.16204042732715607\n",
      "training 11.142463684082031 relative L2 0.1619875133037567\n",
      "training 11.135194778442383 relative L2 0.16193653643131256\n",
      "training 11.12818717956543 relative L2 0.16188272833824158\n",
      "training 11.120818138122559 relative L2 0.1618301272392273\n",
      "training 11.113619804382324 relative L2 0.16177774965763092\n",
      "training 11.106438636779785 relative L2 0.16172491014003754\n",
      "training 11.099194526672363 relative L2 0.1616717278957367\n",
      "training 11.09189510345459 relative L2 0.16161885857582092\n",
      "training 11.084640502929688 relative L2 0.16156567633152008\n",
      "training 11.077349662780762 relative L2 0.16151291131973267\n",
      "training 11.070125579833984 relative L2 0.16145965456962585\n",
      "training 11.062846183776855 relative L2 0.1614065170288086\n",
      "training 11.055582046508789 relative L2 0.1613532453775406\n",
      "training 11.04830265045166 relative L2 0.16129982471466064\n",
      "training 11.040998458862305 relative L2 0.16124628484249115\n",
      "training 11.033675193786621 relative L2 0.16119304299354553\n",
      "training 11.026394844055176 relative L2 0.16113939881324768\n",
      "training 11.019070625305176 relative L2 0.16108570992946625\n",
      "training 11.011743545532227 relative L2 0.1610320508480072\n",
      "training 11.004425048828125 relative L2 0.1609782725572586\n",
      "training 10.997084617614746 relative L2 0.16092467308044434\n",
      "training 10.98976993560791 relative L2 0.1608705073595047\n",
      "training 10.982383728027344 relative L2 0.16081666946411133\n",
      "training 10.975046157836914 relative L2 0.16076277196407318\n",
      "training 10.96770191192627 relative L2 0.16070863604545593\n",
      "training 10.96033000946045 relative L2 0.1606544405221939\n",
      "training 10.95295238494873 relative L2 0.16060012578964233\n",
      "training 10.945558547973633 relative L2 0.1605459302663803\n",
      "training 10.93818187713623 relative L2 0.16049155592918396\n",
      "training 10.9307861328125 relative L2 0.16043730080127716\n",
      "training 10.923409461975098 relative L2 0.1603829264640808\n",
      "training 10.916017532348633 relative L2 0.16032834351062775\n",
      "training 10.908600807189941 relative L2 0.16027382016181946\n",
      "training 10.9011869430542 relative L2 0.16021907329559326\n",
      "training 10.893753051757812 relative L2 0.1601642221212387\n",
      "training 10.886314392089844 relative L2 0.16010935604572296\n",
      "training 10.878870964050293 relative L2 0.16005438566207886\n",
      "training 10.871417045593262 relative L2 0.1599995344877243\n",
      "training 10.863974571228027 relative L2 0.15994445979595184\n",
      "training 10.856508255004883 relative L2 0.1598893702030182\n",
      "training 10.849041938781738 relative L2 0.15983417630195618\n",
      "training 10.841561317443848 relative L2 0.15977893769741058\n",
      "training 10.83408260345459 relative L2 0.15972352027893066\n",
      "training 10.826583862304688 relative L2 0.15966805815696716\n",
      "training 10.819079399108887 relative L2 0.15961246192455292\n",
      "training 10.811556816101074 relative L2 0.1595570296049118\n",
      "training 10.804059982299805 relative L2 0.15950123965740204\n",
      "training 10.796517372131348 relative L2 0.15944553911685944\n",
      "training 10.78899097442627 relative L2 0.15938971936702728\n",
      "training 10.781452178955078 relative L2 0.159333735704422\n",
      "training 10.77388858795166 relative L2 0.1592778116464615\n",
      "training 10.766339302062988 relative L2 0.15922169387340546\n",
      "training 10.758767127990723 relative L2 0.15916544198989868\n",
      "training 10.751176834106445 relative L2 0.1591092199087143\n",
      "training 10.743595123291016 relative L2 0.15905329585075378\n",
      "training 10.73605728149414 relative L2 0.1589966118335724\n",
      "training 10.728423118591309 relative L2 0.1589403599500656\n",
      "training 10.720846176147461 relative L2 0.158883735537529\n",
      "training 10.713216781616211 relative L2 0.15882720053195953\n",
      "training 10.70560359954834 relative L2 0.15877071022987366\n",
      "training 10.69800090789795 relative L2 0.15871384739875793\n",
      "training 10.690354347229004 relative L2 0.15865692496299744\n",
      "training 10.682706832885742 relative L2 0.15859998762607574\n",
      "training 10.675054550170898 relative L2 0.1585429459810257\n",
      "training 10.667386054992676 relative L2 0.15848581492900848\n",
      "training 10.659709930419922 relative L2 0.1584286242723465\n",
      "training 10.652029991149902 relative L2 0.15837135910987854\n",
      "training 10.644339561462402 relative L2 0.15831390023231506\n",
      "training 10.636630058288574 relative L2 0.1582563817501068\n",
      "training 10.62891960144043 relative L2 0.15819872915744781\n",
      "training 10.62119197845459 relative L2 0.15814101696014404\n",
      "training 10.613459587097168 relative L2 0.15808317065238953\n",
      "training 10.605707168579102 relative L2 0.15802529454231262\n",
      "training 10.597951889038086 relative L2 0.15796734392642975\n",
      "training 10.590189933776855 relative L2 0.15790940821170807\n",
      "training 10.58243465423584 relative L2 0.15785112977027893\n",
      "training 10.574645042419434 relative L2 0.1577928513288498\n",
      "training 10.566852569580078 relative L2 0.15773455798625946\n",
      "training 10.559063911437988 relative L2 0.15767613053321838\n",
      "training 10.551254272460938 relative L2 0.15761762857437134\n",
      "training 10.543436050415039 relative L2 0.15755902230739594\n",
      "training 10.53560733795166 relative L2 0.15750034153461456\n",
      "training 10.527771949768066 relative L2 0.15744155645370483\n",
      "training 10.519930839538574 relative L2 0.1573825478553772\n",
      "training 10.512064933776855 relative L2 0.15732358396053314\n",
      "training 10.50420093536377 relative L2 0.15726444125175476\n",
      "training 10.496319770812988 relative L2 0.1572052240371704\n",
      "training 10.488430976867676 relative L2 0.1571459323167801\n",
      "training 10.480533599853516 relative L2 0.1570865362882614\n",
      "training 10.472625732421875 relative L2 0.15702702105045319\n",
      "training 10.464703559875488 relative L2 0.15696735680103302\n",
      "training 10.456767082214355 relative L2 0.15690763294696808\n",
      "training 10.44882583618164 relative L2 0.1568477749824524\n",
      "training 10.44086742401123 relative L2 0.15678778290748596\n",
      "training 10.432893753051758 relative L2 0.15672774612903595\n",
      "training 10.424920082092285 relative L2 0.15666750073432922\n",
      "training 10.41692066192627 relative L2 0.15660719573497772\n",
      "training 10.408917427062988 relative L2 0.15654674172401428\n",
      "training 10.400895118713379 relative L2 0.1564861238002777\n",
      "training 10.392855644226074 relative L2 0.15642540156841278\n",
      "training 10.384805679321289 relative L2 0.1563645303249359\n",
      "training 10.376736640930176 relative L2 0.1563034951686859\n",
      "training 10.368654251098633 relative L2 0.15624231100082397\n",
      "training 10.360548973083496 relative L2 0.1561809778213501\n",
      "training 10.352426528930664 relative L2 0.15611939132213593\n",
      "training 10.344278335571289 relative L2 0.15605758130550385\n",
      "training 10.336104393005371 relative L2 0.15599557757377625\n",
      "training 10.327905654907227 relative L2 0.15593324601650238\n",
      "training 10.319669723510742 relative L2 0.15587058663368225\n",
      "training 10.311394691467285 relative L2 0.1558075249195099\n",
      "training 10.303071022033691 relative L2 0.15574392676353455\n",
      "training 10.294682502746582 relative L2 0.15567976236343384\n",
      "training 10.286222457885742 relative L2 0.15561485290527344\n",
      "training 10.277668952941895 relative L2 0.15554894506931305\n",
      "training 10.268986701965332 relative L2 0.15548166632652283\n",
      "training 10.260129928588867 relative L2 0.1554126888513565\n",
      "training 10.251053810119629 relative L2 0.15534134209156036\n",
      "training 10.241668701171875 relative L2 0.15526682138442993\n",
      "training 10.231870651245117 relative L2 0.15518818795681\n",
      "training 10.221531867980957 relative L2 0.155103862285614\n",
      "training 10.210450172424316 relative L2 0.15501192212104797\n",
      "training 10.198371887207031 relative L2 0.1549099236726761\n",
      "training 10.184971809387207 relative L2 0.1547955721616745\n",
      "training 10.169947624206543 relative L2 0.15467458963394165\n",
      "training 10.154069900512695 relative L2 0.15459859371185303\n",
      "training 10.144126892089844 relative L2 0.15453535318374634\n",
      "training 10.135854721069336 relative L2 0.15447109937667847\n",
      "training 10.127445220947266 relative L2 0.15440574288368225\n",
      "training 10.118884086608887 relative L2 0.15434005856513977\n",
      "training 10.110275268554688 relative L2 0.1542738974094391\n",
      "training 10.101612091064453 relative L2 0.15420681238174438\n",
      "training 10.092843055725098 relative L2 0.15413931012153625\n",
      "training 10.08403205871582 relative L2 0.15407171845436096\n",
      "training 10.07520580291748 relative L2 0.15400394797325134\n",
      "training 10.066349983215332 relative L2 0.15393663942813873\n",
      "training 10.0575532913208 relative L2 0.15386907756328583\n",
      "training 10.048734664916992 relative L2 0.1538013219833374\n",
      "training 10.039900779724121 relative L2 0.153733029961586\n",
      "training 10.031007766723633 relative L2 0.15366460382938385\n",
      "training 10.022101402282715 relative L2 0.1535964161157608\n",
      "training 10.013223648071289 relative L2 0.1535281389951706\n",
      "training 10.004337310791016 relative L2 0.15345966815948486\n",
      "training 9.995428085327148 relative L2 0.15339113771915436\n",
      "training 9.986507415771484 relative L2 0.1533225029706955\n",
      "training 9.97757625579834 relative L2 0.15325380861759186\n",
      "training 9.968648910522461 relative L2 0.1531846970319748\n",
      "training 9.959677696228027 relative L2 0.15311534702777863\n",
      "training 9.950685501098633 relative L2 0.15304577350616455\n",
      "training 9.941671371459961 relative L2 0.1529761701822281\n",
      "training 9.932649612426758 relative L2 0.1529064178466797\n",
      "training 9.923613548278809 relative L2 0.15283654630184174\n",
      "training 9.914565086364746 relative L2 0.15276652574539185\n",
      "training 9.905497550964355 relative L2 0.1526963710784912\n",
      "training 9.896416664123535 relative L2 0.15262609720230103\n",
      "training 9.887327194213867 relative L2 0.15255559980869293\n",
      "training 9.878214836120605 relative L2 0.1524849385023117\n",
      "training 9.869086265563965 relative L2 0.15241415798664093\n",
      "training 9.859942436218262 relative L2 0.15234331786632538\n",
      "training 9.85079288482666 relative L2 0.15227234363555908\n",
      "training 9.841632843017578 relative L2 0.15220120549201965\n",
      "training 9.832459449768066 relative L2 0.1521299183368683\n",
      "training 9.82326602935791 relative L2 0.15205855667591095\n",
      "training 9.814065933227539 relative L2 0.1519869565963745\n",
      "training 9.804844856262207 relative L2 0.15191525220870972\n",
      "training 9.795613288879395 relative L2 0.15184332430362701\n",
      "training 9.786356925964355 relative L2 0.15177129209041595\n",
      "training 9.777093887329102 relative L2 0.15169909596443176\n",
      "training 9.767810821533203 relative L2 0.15162673592567444\n",
      "training 9.758512496948242 relative L2 0.151554137468338\n",
      "training 9.749184608459473 relative L2 0.15148143470287323\n",
      "training 9.739846229553223 relative L2 0.15140853822231293\n",
      "training 9.73049259185791 relative L2 0.1513354778289795\n",
      "training 9.721120834350586 relative L2 0.15126223862171173\n",
      "training 9.71173095703125 relative L2 0.15118885040283203\n",
      "training 9.702324867248535 relative L2 0.15111535787582397\n",
      "training 9.69290828704834 relative L2 0.1510416567325592\n",
      "training 9.683473587036133 relative L2 0.1509677916765213\n",
      "training 9.67402172088623 relative L2 0.15089371800422668\n",
      "training 9.664546012878418 relative L2 0.15081945061683655\n",
      "training 9.655048370361328 relative L2 0.15074484050273895\n",
      "training 9.645515441894531 relative L2 0.1506700962781906\n",
      "training 9.635968208312988 relative L2 0.15059514343738556\n",
      "training 9.626398086547852 relative L2 0.15052005648612976\n",
      "training 9.616817474365234 relative L2 0.15044473111629486\n",
      "training 9.60721206665039 relative L2 0.15036921203136444\n",
      "training 9.59758472442627 relative L2 0.1502934992313385\n",
      "training 9.58793830871582 relative L2 0.1502174735069275\n",
      "training 9.578254699707031 relative L2 0.15014106035232544\n",
      "training 9.568528175354004 relative L2 0.15006442368030548\n",
      "training 9.558778762817383 relative L2 0.14998753368854523\n",
      "training 9.54900074005127 relative L2 0.1499103307723999\n",
      "training 9.539191246032715 relative L2 0.14983269572257996\n",
      "training 9.52933120727539 relative L2 0.14975397288799286\n",
      "training 9.519335746765137 relative L2 0.1496751308441162\n",
      "training 9.50933837890625 relative L2 0.14959675073623657\n",
      "training 9.49940013885498 relative L2 0.14951835572719574\n",
      "training 9.489463806152344 relative L2 0.1494394987821579\n",
      "training 9.47947883605957 relative L2 0.14935991168022156\n",
      "training 9.469407081604004 relative L2 0.14927969872951508\n",
      "training 9.459263801574707 relative L2 0.14919938147068024\n",
      "training 9.449113845825195 relative L2 0.14911819994449615\n",
      "training 9.438861846923828 relative L2 0.1490364968776703\n",
      "training 9.42855167388916 relative L2 0.1489540934562683\n",
      "training 9.418160438537598 relative L2 0.14887113869190216\n",
      "training 9.407703399658203 relative L2 0.1487874835729599\n",
      "training 9.39716625213623 relative L2 0.1487027108669281\n",
      "training 9.38649845123291 relative L2 0.1486174613237381\n",
      "training 9.375778198242188 relative L2 0.14853094518184662\n",
      "training 9.364900588989258 relative L2 0.14844435453414917\n",
      "training 9.354022979736328 relative L2 0.14835572242736816\n",
      "training 9.342897415161133 relative L2 0.1482662409543991\n",
      "training 9.331676483154297 relative L2 0.1481754183769226\n",
      "training 9.320296287536621 relative L2 0.14808262884616852\n",
      "training 9.308674812316895 relative L2 0.14798803627490997\n",
      "training 9.296836853027344 relative L2 0.14789126813411713\n",
      "training 9.284737586975098 relative L2 0.14779195189476013\n",
      "training 9.272322654724121 relative L2 0.14768965542316437\n",
      "training 9.259550094604492 relative L2 0.14758412539958954\n",
      "training 9.246373176574707 relative L2 0.14747470617294312\n",
      "training 9.232725143432617 relative L2 0.14736156165599823\n",
      "training 9.21861743927002 relative L2 0.14724257588386536\n",
      "training 9.203786849975586 relative L2 0.1471182256937027\n",
      "training 9.188298225402832 relative L2 0.1469871997833252\n",
      "training 9.172003746032715 relative L2 0.14684844017028809\n",
      "training 9.154763221740723 relative L2 0.1467006653547287\n",
      "training 9.136402130126953 relative L2 0.14654262363910675\n",
      "training 9.116758346557617 relative L2 0.14637212455272675\n",
      "training 9.095587730407715 relative L2 0.14618684351444244\n",
      "training 9.072640419006348 relative L2 0.14598539471626282\n",
      "training 9.047709465026855 relative L2 0.14576442539691925\n",
      "training 9.020365715026855 relative L2 0.1455220729112625\n",
      "training 8.990410804748535 relative L2 0.1452571004629135\n",
      "training 8.957742691040039 relative L2 0.144977867603302\n",
      "training 8.923351287841797 relative L2 0.14472782611846924\n",
      "training 8.89261245727539 relative L2 0.1445501744747162\n",
      "training 8.870829582214355 relative L2 0.14436103403568268\n",
      "training 8.847612380981445 relative L2 0.14415591955184937\n",
      "training 8.822460174560547 relative L2 0.1439417004585266\n",
      "training 8.796232223510742 relative L2 0.14372803270816803\n",
      "training 8.77013874053955 relative L2 0.14354436099529266\n",
      "training 8.747764587402344 relative L2 0.14342765510082245\n",
      "training 8.733636856079102 relative L2 0.14331844449043274\n",
      "training 8.720369338989258 relative L2 0.1432017683982849\n",
      "training 8.706141471862793 relative L2 0.14308634400367737\n",
      "training 8.692047119140625 relative L2 0.14296868443489075\n",
      "training 8.677736282348633 relative L2 0.1428488790988922\n",
      "training 8.663241386413574 relative L2 0.14273132383823395\n",
      "training 8.649038314819336 relative L2 0.1426137089729309\n",
      "training 8.634815216064453 relative L2 0.14249788224697113\n",
      "training 8.620783805847168 relative L2 0.1423874795436859\n",
      "training 8.607418060302734 relative L2 0.14227820932865143\n",
      "training 8.594232559204102 relative L2 0.14216899871826172\n",
      "training 8.581092834472656 relative L2 0.1420609951019287\n",
      "training 8.568098068237305 relative L2 0.14195263385772705\n",
      "training 8.5550537109375 relative L2 0.1418435126543045\n",
      "training 8.541918754577637 relative L2 0.14173389971256256\n",
      "training 8.528735160827637 relative L2 0.1416243314743042\n",
      "training 8.51557445526123 relative L2 0.14151476323604584\n",
      "training 8.502436637878418 relative L2 0.14140479266643524\n",
      "training 8.48926830291748 relative L2 0.1412947177886963\n",
      "training 8.476103782653809 relative L2 0.1411844789981842\n",
      "training 8.462924003601074 relative L2 0.1410733461380005\n",
      "training 8.449641227722168 relative L2 0.14095786213874817\n",
      "training 8.435835838317871 relative L2 0.14084850251674652\n",
      "training 8.422761917114258 relative L2 0.140738382935524\n",
      "training 8.40961742401123 relative L2 0.14062680304050446\n",
      "training 8.396323204040527 relative L2 0.14051499962806702\n",
      "training 8.383021354675293 relative L2 0.14040343463420868\n",
      "training 8.36974811553955 relative L2 0.14029158651828766\n",
      "training 8.356441497802734 relative L2 0.14017918705940247\n",
      "training 8.343073844909668 relative L2 0.14006759226322174\n",
      "training 8.329816818237305 relative L2 0.13995413482189178\n",
      "training 8.31635570526123 relative L2 0.13984256982803345\n",
      "training 8.303138732910156 relative L2 0.13972996175289154\n",
      "training 8.289819717407227 relative L2 0.1396164447069168\n",
      "training 8.276395797729492 relative L2 0.13950246572494507\n",
      "training 8.262917518615723 relative L2 0.1393911987543106\n",
      "training 8.249753952026367 relative L2 0.13927552103996277\n",
      "training 8.236076354980469 relative L2 0.1391623616218567\n",
      "training 8.22271728515625 relative L2 0.1390485018491745\n",
      "training 8.209300994873047 relative L2 0.13893401622772217\n",
      "training 8.195833206176758 relative L2 0.1388193666934967\n",
      "training 8.182353973388672 relative L2 0.1387043446302414\n",
      "training 8.168831825256348 relative L2 0.13858889043331146\n",
      "training 8.155257225036621 relative L2 0.13847318291664124\n",
      "training 8.141655921936035 relative L2 0.13835787773132324\n",
      "training 8.128119468688965 relative L2 0.13824160397052765\n",
      "training 8.114497184753418 relative L2 0.13812552392482758\n",
      "training 8.100915908813477 relative L2 0.1380091905593872\n",
      "training 8.087320327758789 relative L2 0.1378924399614334\n",
      "training 8.073678970336914 relative L2 0.13777518272399902\n",
      "training 8.059978485107422 relative L2 0.1376577764749527\n",
      "training 8.046262741088867 relative L2 0.13753989338874817\n",
      "training 8.032509803771973 relative L2 0.13742129504680634\n",
      "training 8.018695831298828 relative L2 0.13729970157146454\n",
      "training 8.004551887512207 relative L2 0.13717955350875854\n",
      "training 7.990596771240234 relative L2 0.13706082105636597\n",
      "training 7.976805210113525 relative L2 0.1369415521621704\n",
      "training 7.962949752807617 relative L2 0.13682125508785248\n",
      "training 7.948988437652588 relative L2 0.13670051097869873\n",
      "training 7.934986114501953 relative L2 0.1365787386894226\n",
      "training 7.920883655548096 relative L2 0.13645625114440918\n",
      "training 7.906718730926514 relative L2 0.13633282482624054\n",
      "training 7.892464637756348 relative L2 0.1362086683511734\n",
      "training 7.878130912780762 relative L2 0.1360834538936615\n",
      "training 7.863679885864258 relative L2 0.13595733046531677\n",
      "training 7.849134922027588 relative L2 0.13582994043827057\n",
      "training 7.834457874298096 relative L2 0.13570602238178253\n",
      "training 7.8202009201049805 relative L2 0.135574609041214\n",
      "training 7.805108547210693 relative L2 0.13544303178787231\n",
      "training 7.790008544921875 relative L2 0.13531212508678436\n",
      "training 7.774982452392578 relative L2 0.1351788341999054\n",
      "training 7.759681224822998 relative L2 0.1350429654121399\n",
      "training 7.744105339050293 relative L2 0.13490526378154755\n",
      "training 7.728344917297363 relative L2 0.13477125763893127\n",
      "training 7.713034152984619 relative L2 0.13464784622192383\n",
      "training 7.698957443237305 relative L2 0.13452401757240295\n",
      "training 7.684839725494385 relative L2 0.1343993991613388\n",
      "training 7.670639991760254 relative L2 0.1342747062444687\n",
      "training 7.656440258026123 relative L2 0.13414889574050903\n",
      "training 7.642124652862549 relative L2 0.13402429223060608\n",
      "training 7.627963542938232 relative L2 0.13389870524406433\n",
      "training 7.61371374130249 relative L2 0.1337713599205017\n",
      "training 7.599282264709473 relative L2 0.1336451768875122\n",
      "training 7.584984302520752 relative L2 0.13351821899414062\n",
      "training 7.570598602294922 relative L2 0.13339118659496307\n",
      "training 7.5562214851379395 relative L2 0.13326330482959747\n",
      "training 7.541769981384277 relative L2 0.13313470780849457\n",
      "training 7.52726411819458 relative L2 0.13300569355487823\n",
      "training 7.512723445892334 relative L2 0.13287702202796936\n",
      "training 7.498231887817383 relative L2 0.13274778425693512\n",
      "training 7.48369026184082 relative L2 0.13261778652668\n",
      "training 7.469070911407471 relative L2 0.13248775899410248\n",
      "training 7.4544596672058105 relative L2 0.13235753774642944\n",
      "training 7.4398345947265625 relative L2 0.13222680985927582\n",
      "training 7.42517614364624 relative L2 0.13209539651870728\n",
      "training 7.410464286804199 relative L2 0.1319636106491089\n",
      "training 7.395727157592773 relative L2 0.13183186948299408\n",
      "training 7.381009101867676 relative L2 0.1316995620727539\n",
      "training 7.366239070892334 relative L2 0.13156701624393463\n",
      "training 7.351452350616455 relative L2 0.1314341127872467\n",
      "training 7.33663272857666 relative L2 0.13130071759223938\n",
      "training 7.321774005889893 relative L2 0.13116681575775146\n",
      "training 7.306883335113525 relative L2 0.13103245198726654\n",
      "training 7.291965007781982 relative L2 0.13089752197265625\n",
      "training 7.277003765106201 relative L2 0.13076235353946686\n",
      "training 7.262023448944092 relative L2 0.13062675297260284\n",
      "training 7.247002124786377 relative L2 0.13049070537090302\n",
      "training 7.231941223144531 relative L2 0.13035424053668976\n",
      "training 7.216849327087402 relative L2 0.13021709024906158\n",
      "training 7.201708793640137 relative L2 0.13007918000221252\n",
      "training 7.186509132385254 relative L2 0.12994076311588287\n",
      "training 7.17126989364624 relative L2 0.12980172038078308\n",
      "training 7.155972480773926 relative L2 0.12966197729110718\n",
      "training 7.140605926513672 relative L2 0.12952139973640442\n",
      "training 7.1251630783081055 relative L2 0.1293799728155136\n",
      "training 7.109646797180176 relative L2 0.1292375773191452\n",
      "training 7.094046115875244 relative L2 0.12909404933452606\n",
      "training 7.078341007232666 relative L2 0.12894929945468903\n",
      "training 7.0625224113464355 relative L2 0.12880326807498932\n",
      "training 7.0465779304504395 relative L2 0.12865570187568665\n",
      "training 7.03048038482666 relative L2 0.12850652635097504\n",
      "training 7.01422119140625 relative L2 0.128355473279953\n",
      "training 6.997774600982666 relative L2 0.12820233404636383\n",
      "training 6.9811272621154785 relative L2 0.12804682552814484\n",
      "training 6.964247226715088 relative L2 0.12788879871368408\n",
      "training 6.947113990783691 relative L2 0.12772871553897858\n",
      "training 6.929776668548584 relative L2 0.12756578624248505\n",
      "training 6.912144660949707 relative L2 0.12739869952201843\n",
      "training 6.894079685211182 relative L2 0.12722599506378174\n",
      "training 6.875424861907959 relative L2 0.12705044448375702\n",
      "training 6.856494903564453 relative L2 0.1268720030784607\n",
      "training 6.837283611297607 relative L2 0.1266932636499405\n",
      "training 6.81806755065918 relative L2 0.1265224814414978\n",
      "training 6.799726486206055 relative L2 0.12636728584766388\n",
      "training 6.783100605010986 relative L2 0.12621845304965973\n",
      "training 6.7671685218811035 relative L2 0.1260695606470108\n",
      "training 6.751245498657227 relative L2 0.1259191632270813\n",
      "training 6.735178470611572 relative L2 0.12576739490032196\n",
      "training 6.718987464904785 relative L2 0.12561644613742828\n",
      "training 6.702908515930176 relative L2 0.1254620999097824\n",
      "training 6.686494827270508 relative L2 0.1253093034029007\n",
      "training 6.670264720916748 relative L2 0.1251571923494339\n",
      "training 6.65411901473999 relative L2 0.12500520050525665\n",
      "training 6.637994766235352 relative L2 0.12485264241695404\n",
      "training 6.621834754943848 relative L2 0.12469905614852905\n",
      "training 6.605592727661133 relative L2 0.12454477697610855\n",
      "training 6.589305400848389 relative L2 0.12439002841711044\n",
      "training 6.572990894317627 relative L2 0.12423506379127502\n",
      "training 6.556665897369385 relative L2 0.1240798681974411\n",
      "training 6.540328502655029 relative L2 0.12392441183328629\n",
      "training 6.5239787101745605 relative L2 0.12376846373081207\n",
      "training 6.507602214813232 relative L2 0.12361369282007217\n",
      "training 6.491377353668213 relative L2 0.12345590442419052\n",
      "training 6.474864959716797 relative L2 0.12329930812120438\n",
      "training 6.458499431610107 relative L2 0.12314265966415405\n",
      "training 6.442138671875 relative L2 0.1229856014251709\n",
      "training 6.425751209259033 relative L2 0.12282810360193253\n",
      "training 6.409337520599365 relative L2 0.12266988307237625\n",
      "training 6.392877101898193 relative L2 0.12251114845275879\n",
      "training 6.376388072967529 relative L2 0.1223520040512085\n",
      "training 6.359875679016113 relative L2 0.12219264358282089\n",
      "training 6.343358516693115 relative L2 0.12203355878591537\n",
      "training 6.326892375946045 relative L2 0.12187349051237106\n",
      "training 6.3103485107421875 relative L2 0.12171350419521332\n",
      "training 6.293832302093506 relative L2 0.12155299633741379\n",
      "training 6.277283191680908 relative L2 0.12139192968606949\n",
      "training 6.260697364807129 relative L2 0.12123043090105057\n",
      "training 6.2440900802612305 relative L2 0.12106846272945404\n",
      "training 6.227457523345947 relative L2 0.12090684473514557\n",
      "training 6.210885047912598 relative L2 0.12074405699968338\n",
      "training 6.194216728210449 relative L2 0.12058139592409134\n",
      "training 6.177581310272217 relative L2 0.12041819840669632\n",
      "training 6.160910606384277 relative L2 0.12025461345911026\n",
      "training 6.144220352172852 relative L2 0.12009084969758987\n",
      "training 6.127536296844482 relative L2 0.11992642283439636\n",
      "training 6.110811710357666 relative L2 0.11976183950901031\n",
      "training 6.094094753265381 relative L2 0.11959686130285263\n",
      "training 6.077358722686768 relative L2 0.11943131685256958\n",
      "training 6.060583591461182 relative L2 0.11926566064357758\n",
      "training 6.043822765350342 relative L2 0.11909937858581543\n",
      "training 6.027024269104004 relative L2 0.11893279105424881\n",
      "training 6.010221004486084 relative L2 0.11876589059829712\n",
      "training 5.993408203125 relative L2 0.11859854310750961\n",
      "training 5.976568698883057 relative L2 0.1184309795498848\n",
      "training 5.959731101989746 relative L2 0.11826282739639282\n",
      "training 5.942859649658203 relative L2 0.11809439212083817\n",
      "training 5.925985336303711 relative L2 0.11792545765638351\n",
      "training 5.909088611602783 relative L2 0.11775621026754379\n",
      "training 5.892182350158691 relative L2 0.11758656799793243\n",
      "training 5.875260353088379 relative L2 0.11741654574871063\n",
      "training 5.85832405090332 relative L2 0.11724615097045898\n",
      "training 5.841374397277832 relative L2 0.1170753762125969\n",
      "training 5.824410915374756 relative L2 0.11690417677164078\n",
      "training 5.807432174682617 relative L2 0.11673254519701004\n",
      "training 5.790438652038574 relative L2 0.11656057834625244\n",
      "training 5.773433208465576 relative L2 0.11638820916414261\n",
      "training 5.756409645080566 relative L2 0.11621547490358353\n",
      "training 5.739378452301025 relative L2 0.11604229360818863\n",
      "training 5.7223286628723145 relative L2 0.11586873978376389\n",
      "training 5.705268383026123 relative L2 0.11569474637508392\n",
      "training 5.688190937042236 relative L2 0.1155204102396965\n",
      "training 5.671104431152344 relative L2 0.11534564942121506\n",
      "training 5.654000282287598 relative L2 0.11517050862312317\n",
      "training 5.636886119842529 relative L2 0.11499496549367905\n",
      "training 5.619758605957031 relative L2 0.11481902003288269\n",
      "training 5.602616786956787 relative L2 0.1146426871418953\n",
      "training 5.585464954376221 relative L2 0.11446590721607208\n",
      "training 5.568297386169434 relative L2 0.11428878456354141\n",
      "training 5.551120281219482 relative L2 0.1141112670302391\n",
      "training 5.53393030166626 relative L2 0.11393332481384277\n",
      "training 5.516727447509766 relative L2 0.11375496536493301\n",
      "training 5.499512195587158 relative L2 0.11357621848583221\n",
      "training 5.482285976409912 relative L2 0.113397017121315\n",
      "training 5.465043544769287 relative L2 0.11321744322776794\n",
      "training 5.4477925300598145 relative L2 0.11303748935461044\n",
      "training 5.4305315017700195 relative L2 0.11285708844661713\n",
      "training 5.413254261016846 relative L2 0.11267625540494919\n",
      "training 5.3959641456604 relative L2 0.11249503493309021\n",
      "training 5.378665447235107 relative L2 0.1123133972287178\n",
      "training 5.361353397369385 relative L2 0.11213133484125137\n",
      "training 5.34403133392334 relative L2 0.11194884777069092\n",
      "training 5.326693534851074 relative L2 0.11176594346761703\n",
      "training 5.30934476852417 relative L2 0.11158260703086853\n",
      "training 5.291985034942627 relative L2 0.11139882355928421\n",
      "training 5.274610996246338 relative L2 0.11121465265750885\n",
      "training 5.257229328155518 relative L2 0.11102999001741409\n",
      "training 5.239830017089844 relative L2 0.1108449324965477\n",
      "training 5.222421646118164 relative L2 0.1106593981385231\n",
      "training 5.20499849319458 relative L2 0.11047342419624329\n",
      "training 5.18756103515625 relative L2 0.11028700321912766\n",
      "training 5.170112133026123 relative L2 0.11010010540485382\n",
      "training 5.15264892578125 relative L2 0.10991273075342178\n",
      "training 5.135170936584473 relative L2 0.10972485691308975\n",
      "training 5.117676734924316 relative L2 0.10953649878501892\n",
      "training 5.1001667976379395 relative L2 0.10934767127037048\n",
      "training 5.082643032073975 relative L2 0.10915827006101608\n",
      "training 5.065097332000732 relative L2 0.10896837711334229\n",
      "training 5.047535419464111 relative L2 0.10877793282270432\n",
      "training 5.029953479766846 relative L2 0.1085868701338768\n",
      "training 5.012344837188721 relative L2 0.10839524865150452\n",
      "training 4.994714736938477 relative L2 0.10820295661687851\n",
      "training 4.977055072784424 relative L2 0.10800999402999878\n",
      "training 4.959365367889404 relative L2 0.10781634598970413\n",
      "training 4.941643238067627 relative L2 0.1076219230890274\n",
      "training 4.923881530761719 relative L2 0.10742664337158203\n",
      "training 4.906074047088623 relative L2 0.10723049938678741\n",
      "training 4.888219833374023 relative L2 0.1070333868265152\n",
      "training 4.8703107833862305 relative L2 0.10683519393205643\n",
      "training 4.8523359298706055 relative L2 0.10663577169179916\n",
      "training 4.834287166595459 relative L2 0.10643503814935684\n",
      "training 4.816155433654785 relative L2 0.10623281449079514\n",
      "training 4.797922611236572 relative L2 0.10602891445159912\n",
      "training 4.779572010040283 relative L2 0.10582315921783447\n",
      "training 4.7610883712768555 relative L2 0.10561522841453552\n",
      "training 4.742442607879639 relative L2 0.10540483146905899\n",
      "training 4.723611354827881 relative L2 0.10519151389598846\n",
      "training 4.7045578956604 relative L2 0.1049749106168747\n",
      "training 4.685249328613281 relative L2 0.10475436598062515\n",
      "training 4.665637493133545 relative L2 0.10452932864427567\n",
      "training 4.6456685066223145 relative L2 0.10429900139570236\n",
      "training 4.625272274017334 relative L2 0.10406259447336197\n",
      "training 4.604369163513184 relative L2 0.10381890833377838\n",
      "training 4.582869529724121 relative L2 0.10356706380844116\n",
      "training 4.560694217681885 relative L2 0.10330644994974136\n",
      "training 4.537804126739502 relative L2 0.10303830355405807\n",
      "training 4.5143208503723145 relative L2 0.10277016460895538\n",
      "training 4.490894794464111 relative L2 0.10252764821052551\n",
      "training 4.469764709472656 relative L2 0.10231470316648483\n",
      "training 4.451263427734375 relative L2 0.10210300981998444\n",
      "training 4.432882308959961 relative L2 0.10189200192689896\n",
      "training 4.4145941734313965 relative L2 0.10168047249317169\n",
      "training 4.396317958831787 relative L2 0.10146702080965042\n",
      "training 4.377939224243164 relative L2 0.10125191509723663\n",
      "training 4.359468460083008 relative L2 0.1010359525680542\n",
      "training 4.340947151184082 relative L2 0.10081963986158371\n",
      "training 4.322414875030518 relative L2 0.10060296207666397\n",
      "training 4.303884029388428 relative L2 0.1003856211900711\n",
      "training 4.285348415374756 relative L2 0.10016739368438721\n",
      "training 4.26678991317749 relative L2 0.09994765371084213\n",
      "training 4.248149394989014 relative L2 0.09972673654556274\n",
      "training 4.229441165924072 relative L2 0.0995037853717804\n",
      "training 4.2105913162231445 relative L2 0.09928019344806671\n",
      "training 4.191723823547363 relative L2 0.0990646556019783\n",
      "training 4.173579692840576 relative L2 0.09884796291589737\n",
      "training 4.155384063720703 relative L2 0.09862921386957169\n",
      "training 4.137060642242432 relative L2 0.0984107032418251\n",
      "training 4.118801593780518 relative L2 0.09819032996892929\n",
      "training 4.100422382354736 relative L2 0.09796793013811111\n",
      "training 4.0819091796875 relative L2 0.09774334728717804\n",
      "training 4.063252925872803 relative L2 0.09751743823289871\n",
      "training 4.044534683227539 relative L2 0.09728917479515076\n",
      "training 4.025667190551758 relative L2 0.09705669432878494\n",
      "training 4.006494998931885 relative L2 0.0968213826417923\n",
      "training 3.9871370792388916 relative L2 0.09658081084489822\n",
      "training 3.967397928237915 relative L2 0.09633569419384003\n",
      "training 3.947333335876465 relative L2 0.09608408063650131\n",
      "training 3.9267780780792236 relative L2 0.09581813216209412\n",
      "training 3.9050979614257812 relative L2 0.09554382413625717\n",
      "training 3.882802963256836 relative L2 0.09531581401824951\n",
      "training 3.864333391189575 relative L2 0.09510541707277298\n",
      "training 3.8473129272460938 relative L2 0.09489113837480545\n",
      "training 3.830009937286377 relative L2 0.09467557072639465\n",
      "training 3.812657356262207 relative L2 0.09445691853761673\n",
      "training 3.7951159477233887 relative L2 0.09423472732305527\n",
      "training 3.777336835861206 relative L2 0.0940113365650177\n",
      "training 3.7594873905181885 relative L2 0.09378613531589508\n",
      "training 3.7415223121643066 relative L2 0.09355909377336502\n",
      "training 3.7234559059143066 relative L2 0.0933302789926529\n",
      "training 3.705305576324463 relative L2 0.0930994376540184\n",
      "training 3.6870486736297607 relative L2 0.09286696463823318\n",
      "training 3.6687090396881104 relative L2 0.09263303875923157\n",
      "training 3.650294065475464 relative L2 0.09239861369132996\n",
      "training 3.6318817138671875 relative L2 0.09216420352458954\n",
      "training 3.6135191917419434 relative L2 0.09192836284637451\n",
      "training 3.595100164413452 relative L2 0.09169851988554001\n",
      "training 3.5771920680999756 relative L2 0.09148251265287399\n",
      "training 3.5604114532470703 relative L2 0.09125818312168121\n",
      "training 3.5430099964141846 relative L2 0.0910254716873169\n",
      "training 3.524989366531372 relative L2 0.09078869223594666\n",
      "training 3.5066967010498047 relative L2 0.09055663645267487\n",
      "training 3.4888293743133545 relative L2 0.09032995998859406\n",
      "training 3.4714224338531494 relative L2 0.09010197967290878\n",
      "training 3.4539546966552734 relative L2 0.08987420052289963\n",
      "training 3.436544895172119 relative L2 0.08964535593986511\n",
      "training 3.419102668762207 relative L2 0.08941444754600525\n",
      "training 3.4015512466430664 relative L2 0.08918177336454391\n",
      "training 3.3839123249053955 relative L2 0.08894739300012589\n",
      "training 3.3661868572235107 relative L2 0.0887141302227974\n",
      "training 3.348583698272705 relative L2 0.08848529309034348\n",
      "training 3.3313629627227783 relative L2 0.08825571835041046\n",
      "training 3.314138650894165 relative L2 0.08802424371242523\n",
      "training 3.2968156337738037 relative L2 0.0877913236618042\n",
      "training 3.2794249057769775 relative L2 0.0875580832362175\n",
      "training 3.2620532512664795 relative L2 0.08732523024082184\n",
      "training 3.2447540760040283 relative L2 0.08709345757961273\n",
      "training 3.2275848388671875 relative L2 0.08686111122369766\n",
      "training 3.210426092147827 relative L2 0.08662785589694977\n",
      "training 3.1932473182678223 relative L2 0.08639448881149292\n",
      "training 3.1761021614074707 relative L2 0.08616165071725845\n",
      "training 3.1590359210968018 relative L2 0.08592849969863892\n",
      "training 3.1419923305511475 relative L2 0.085694320499897\n",
      "training 3.1249237060546875 relative L2 0.08545996248722076\n",
      "training 3.1078898906707764 relative L2 0.08522607386112213\n",
      "training 3.0909368991851807 relative L2 0.08499230444431305\n",
      "training 3.0740346908569336 relative L2 0.08475824445486069\n",
      "training 3.057155132293701 relative L2 0.08452367782592773\n",
      "training 3.0402894020080566 relative L2 0.08428868651390076\n",
      "training 3.0234427452087402 relative L2 0.08405353128910065\n",
      "training 3.0066330432891846 relative L2 0.08381844311952591\n",
      "training 2.9898722171783447 relative L2 0.08358333259820938\n",
      "training 2.973154306411743 relative L2 0.08334801346063614\n",
      "training 2.9564669132232666 relative L2 0.08311232924461365\n",
      "training 2.939803123474121 relative L2 0.08287656307220459\n",
      "training 2.92318058013916 relative L2 0.08264069259166718\n",
      "training 2.906597852706909 relative L2 0.08240462094545364\n",
      "training 2.8900458812713623 relative L2 0.08216831088066101\n",
      "training 2.873523712158203 relative L2 0.08193175494670868\n",
      "training 2.857032537460327 relative L2 0.08169496804475784\n",
      "training 2.840574264526367 relative L2 0.08145805448293686\n",
      "training 2.824155569076538 relative L2 0.08122090995311737\n",
      "training 2.807767152786255 relative L2 0.08098343759775162\n",
      "training 2.79140305519104 relative L2 0.0807456448674202\n",
      "training 2.7750630378723145 relative L2 0.08050740510225296\n",
      "training 2.7587413787841797 relative L2 0.08026866614818573\n",
      "training 2.74243426322937 relative L2 0.08002921938896179\n",
      "training 2.726128578186035 relative L2 0.07978902757167816\n",
      "training 2.709820508956909 relative L2 0.07954772561788559\n",
      "training 2.6934845447540283 relative L2 0.0793050080537796\n",
      "training 2.6771018505096436 relative L2 0.07906030118465424\n",
      "training 2.6606361865997314 relative L2 0.07881302386522293\n",
      "training 2.644050359725952 relative L2 0.07856202125549316\n",
      "training 2.627267837524414 relative L2 0.07830598950386047\n",
      "training 2.6101977825164795 relative L2 0.07804340124130249\n",
      "training 2.5927464962005615 relative L2 0.07777722179889679\n",
      "training 2.5751190185546875 relative L2 0.07754002511501312\n",
      "training 2.5594656467437744 relative L2 0.07730914652347565\n",
      "training 2.544260263442993 relative L2 0.07707597315311432\n",
      "training 2.528949022293091 relative L2 0.07684051990509033\n",
      "training 2.5135459899902344 relative L2 0.07660311460494995\n",
      "training 2.4980740547180176 relative L2 0.0763644427061081\n",
      "training 2.48256516456604 relative L2 0.07612459361553192\n",
      "training 2.467017650604248 relative L2 0.07588371634483337\n",
      "training 2.4514477252960205 relative L2 0.0756417065858841\n",
      "training 2.4358599185943604 relative L2 0.07539864629507065\n",
      "training 2.420264720916748 relative L2 0.07515483349561691\n",
      "training 2.4046738147735596 relative L2 0.07491054385900497\n",
      "training 2.389096260070801 relative L2 0.07466576993465424\n",
      "training 2.373530626296997 relative L2 0.07442052662372589\n",
      "training 2.357988119125366 relative L2 0.0741756483912468\n",
      "training 2.342526912689209 relative L2 0.0739416778087616\n",
      "training 2.327808141708374 relative L2 0.07370717823505402\n",
      "training 2.3131024837493896 relative L2 0.07346726953983307\n",
      "training 2.2980923652648926 relative L2 0.07322318851947784\n",
      "training 2.282862424850464 relative L2 0.07297845184803009\n",
      "training 2.267643928527832 relative L2 0.07273844629526138\n",
      "training 2.2527811527252197 relative L2 0.07250172644853592\n",
      "training 2.2381675243377686 relative L2 0.07226387411355972\n",
      "training 2.2235283851623535 relative L2 0.07202497124671936\n",
      "training 2.2088708877563477 relative L2 0.07178504019975662\n",
      "training 2.1942014694213867 relative L2 0.07154414802789688\n",
      "training 2.1795248985290527 relative L2 0.07130230218172073\n",
      "training 2.164841413497925 relative L2 0.07106021046638489\n",
      "training 2.150188446044922 relative L2 0.07081989198923111\n",
      "training 2.1356914043426514 relative L2 0.0705789253115654\n",
      "training 2.121208429336548 relative L2 0.07033339887857437\n",
      "training 2.1065006256103516 relative L2 0.07008413225412369\n",
      "training 2.0916194915771484 relative L2 0.06984971463680267\n",
      "training 2.0776689052581787 relative L2 0.06961443275213242\n",
      "training 2.0637080669403076 relative L2 0.06937851011753082\n",
      "training 2.04975962638855 relative L2 0.0691404789686203\n",
      "training 2.0357441902160645 relative L2 0.06890024989843369\n",
      "training 2.0216522216796875 relative L2 0.06865879893302917\n",
      "training 2.0075318813323975 relative L2 0.06841763854026794\n",
      "training 1.9934715032577515 relative L2 0.06818608194589615\n",
      "training 1.9800199270248413 relative L2 0.06795182824134827\n",
      "training 1.9664660692214966 relative L2 0.06771285831928253\n",
      "training 1.952684760093689 relative L2 0.06747127324342728\n",
      "training 1.938793659210205 relative L2 0.06723663955926895\n",
      "training 1.9253464937210083 relative L2 0.06700319796800613\n",
      "training 1.9120128154754639 relative L2 0.06676850467920303\n",
      "training 1.8986618518829346 relative L2 0.0665326714515686\n",
      "training 1.8852993249893188 relative L2 0.06629594415426254\n",
      "training 1.8719305992126465 relative L2 0.06605858355760574\n",
      "training 1.8585669994354248 relative L2 0.06582462787628174\n",
      "training 1.8454416990280151 relative L2 0.06559255719184875\n",
      "training 1.832476019859314 relative L2 0.06535770744085312\n",
      "training 1.819403052330017 relative L2 0.06512166559696198\n",
      "training 1.8063037395477295 relative L2 0.06488917022943497\n",
      "training 1.7934417724609375 relative L2 0.06465759873390198\n",
      "training 1.780674934387207 relative L2 0.06442497670650482\n",
      "training 1.767904281616211 relative L2 0.06419146060943604\n",
      "training 1.7551367282867432 relative L2 0.06395820528268814\n",
      "training 1.7424256801605225 relative L2 0.06372753530740738\n",
      "training 1.7298952341079712 relative L2 0.06349693983793259\n",
      "training 1.7174127101898193 relative L2 0.06326525658369064\n",
      "training 1.704919695854187 relative L2 0.06303410977125168\n",
      "training 1.6925036907196045 relative L2 0.06280487775802612\n",
      "training 1.6802321672439575 relative L2 0.06257542967796326\n",
      "training 1.6679904460906982 relative L2 0.062345389276742935\n",
      "training 1.6557637453079224 relative L2 0.06211601197719574\n",
      "training 1.6436213254928589 relative L2 0.06188799440860748\n",
      "training 1.6315971612930298 relative L2 0.06165993958711624\n",
      "training 1.6196120977401733 relative L2 0.06143169105052948\n",
      "training 1.60765540599823 relative L2 0.061204127967357635\n",
      "training 1.5957787036895752 relative L2 0.06097767502069473\n",
      "training 1.5840063095092773 relative L2 0.060751158744096756\n",
      "training 1.5722777843475342 relative L2 0.06052444875240326\n",
      "training 1.5605815649032593 relative L2 0.06029883772134781\n",
      "training 1.5489822626113892 relative L2 0.06007388234138489\n",
      "training 1.5374598503112793 relative L2 0.05984896048903465\n",
      "training 1.525983452796936 relative L2 0.059624239802360535\n",
      "training 1.5145609378814697 relative L2 0.05940015986561775\n",
      "training 1.503212332725525 relative L2 0.05917687341570854\n",
      "training 1.491944670677185 relative L2 0.05895368382334709\n",
      "training 1.4807246923446655 relative L2 0.05873064696788788\n",
      "training 1.4695571660995483 relative L2 0.058508288115262985\n",
      "training 1.4584674835205078 relative L2 0.05828641355037689\n",
      "training 1.4474422931671143 relative L2 0.058064743876457214\n",
      "training 1.436466097831726 relative L2 0.05784337967634201\n",
      "training 1.4255461692810059 relative L2 0.05762230232357979\n",
      "training 1.4146842956542969 relative L2 0.05740169808268547\n",
      "training 1.4038879871368408 relative L2 0.057181138545274734\n",
      "training 1.3931338787078857 relative L2 0.056960683315992355\n",
      "training 1.3824247121810913 relative L2 0.05674032121896744\n",
      "training 1.371760606765747 relative L2 0.05651988461613655\n",
      "training 1.3611359596252441 relative L2 0.05629900470376015\n",
      "training 1.350530743598938 relative L2 0.0560774952173233\n",
      "training 1.3399362564086914 relative L2 0.05585482716560364\n",
      "training 1.3293280601501465 relative L2 0.05563028156757355\n",
      "training 1.318676233291626 relative L2 0.05540263652801514\n",
      "training 1.3079200983047485 relative L2 0.055169932544231415\n",
      "training 1.2969659566879272 relative L2 0.05492900311946869\n",
      "training 1.2856699228286743 relative L2 0.054679207503795624\n",
      "training 1.2740129232406616 relative L2 0.05446525663137436\n",
      "training 1.2640745639801025 relative L2 0.05427045747637749\n",
      "training 1.2550386190414429 relative L2 0.05407093092799187\n",
      "training 1.2458159923553467 relative L2 0.053866226226091385\n",
      "training 1.2364147901535034 relative L2 0.05365779623389244\n",
      "training 1.2268929481506348 relative L2 0.0534469373524189\n",
      "training 1.217284083366394 relative L2 0.053233981132507324\n",
      "training 1.2075995206832886 relative L2 0.05301951617002487\n",
      "training 1.1978906393051147 relative L2 0.052802979946136475\n",
      "training 1.1881474256515503 relative L2 0.05258551612496376\n",
      "training 1.178407073020935 relative L2 0.05236763879656792\n",
      "training 1.1686733961105347 relative L2 0.05216542258858681\n",
      "training 1.1596673727035522 relative L2 0.051973018795251846\n",
      "training 1.1511443853378296 relative L2 0.05177434906363487\n",
      "training 1.1423797607421875 relative L2 0.05156950280070305\n",
      "training 1.1333662271499634 relative L2 0.05136016756296158\n",
      "training 1.124182105064392 relative L2 0.05114959552884102\n",
      "training 1.1149832010269165 relative L2 0.05094596743583679\n",
      "training 1.1061359643936157 relative L2 0.050752922892570496\n",
      "training 1.0977753400802612 relative L2 0.05055873095989227\n",
      "training 1.0893882513046265 relative L2 0.05036260932683945\n",
      "training 1.0809550285339355 relative L2 0.05016457289457321\n",
      "training 1.0724834203720093 relative L2 0.049965355545282364\n",
      "training 1.0639957189559937 relative L2 0.049765340983867645\n",
      "training 1.0555000305175781 relative L2 0.04956584423780441\n",
      "training 1.0470556020736694 relative L2 0.04937295615673065\n",
      "training 1.0389295816421509 relative L2 0.04918293654918671\n",
      "training 1.030967116355896 relative L2 0.04899231716990471\n",
      "training 1.0230021476745605 relative L2 0.04880012199282646\n",
      "training 1.0149898529052734 relative L2 0.0486072301864624\n",
      "training 1.0069787502288818 relative L2 0.048415519297122955\n",
      "training 0.9990590214729309 relative L2 0.048226844519376755\n",
      "training 0.9912996888160706 relative L2 0.04804138094186783\n",
      "training 0.9836918711662292 relative L2 0.047855228185653687\n",
      "training 0.9760816693305969 relative L2 0.047667793929576874\n",
      "training 0.9684563875198364 relative L2 0.04748068004846573\n",
      "training 0.9608798027038574 relative L2 0.04729635640978813\n",
      "training 0.953441858291626 relative L2 0.047113820910453796\n",
      "training 0.9460980892181396 relative L2 0.046932149678468704\n",
      "training 0.9388173818588257 relative L2 0.04675045236945152\n",
      "training 0.9315683841705322 relative L2 0.046569064259529114\n",
      "training 0.9243612885475159 relative L2 0.04638869687914848\n",
      "training 0.9172182679176331 relative L2 0.04620978608727455\n",
      "training 0.9101563692092896 relative L2 0.04603216424584389\n",
      "training 0.9031752943992615 relative L2 0.045855700969696045\n",
      "training 0.8962701559066772 relative L2 0.04567950591444969\n",
      "training 0.8894007802009583 relative L2 0.04550386965274811\n",
      "training 0.8825757503509521 relative L2 0.04532960057258606\n",
      "training 0.8758294582366943 relative L2 0.045156437903642654\n",
      "training 0.8691548705101013 relative L2 0.04498429223895073\n",
      "training 0.8625454306602478 relative L2 0.04481307044625282\n",
      "training 0.8559938669204712 relative L2 0.04464268311858177\n",
      "training 0.8494975566864014 relative L2 0.044473033398389816\n",
      "training 0.8430554270744324 relative L2 0.044304315000772476\n",
      "training 0.8366751074790955 relative L2 0.04413666948676109\n",
      "training 0.8303584456443787 relative L2 0.04397013783454895\n",
      "training 0.8241053819656372 relative L2 0.043804626911878586\n",
      "training 0.8179143667221069 relative L2 0.04363995045423508\n",
      "training 0.811779260635376 relative L2 0.04347619786858559\n",
      "training 0.8057017922401428 relative L2 0.04331338033080101\n",
      "training 0.799679696559906 relative L2 0.043151553720235825\n",
      "training 0.7937156558036804 relative L2 0.042990703135728836\n",
      "training 0.7878111004829407 relative L2 0.04283082112669945\n",
      "training 0.7819649577140808 relative L2 0.042671944946050644\n",
      "training 0.7761768698692322 relative L2 0.04251409322023392\n",
      "training 0.7704457640647888 relative L2 0.042357187718153\n",
      "training 0.7647701501846313 relative L2 0.042201150208711624\n",
      "training 0.759148120880127 relative L2 0.042046114802360535\n",
      "training 0.7535828948020935 relative L2 0.04189207777380943\n",
      "training 0.7480731010437012 relative L2 0.041739046573638916\n",
      "training 0.7426183223724365 relative L2 0.041586969047784805\n",
      "training 0.7372183203697205 relative L2 0.04143588989973068\n",
      "training 0.7318738102912903 relative L2 0.041285812854766846\n",
      "training 0.7265832424163818 relative L2 0.041136667132377625\n",
      "training 0.7213438749313354 relative L2 0.04098855331540108\n",
      "training 0.7161595225334167 relative L2 0.04084135591983795\n",
      "training 0.7110262513160706 relative L2 0.04069516435265541\n",
      "training 0.705946147441864 relative L2 0.04054998233914375\n",
      "training 0.700918436050415 relative L2 0.040405821055173874\n",
      "training 0.6959434747695923 relative L2 0.04026259481906891\n",
      "training 0.6910189390182495 relative L2 0.04012034833431244\n",
      "training 0.6861456632614136 relative L2 0.039979103952646255\n",
      "training 0.6813231706619263 relative L2 0.03983882814645767\n",
      "training 0.6765502691268921 relative L2 0.039699580520391464\n",
      "training 0.6718292236328125 relative L2 0.03956126421689987\n",
      "training 0.6671563386917114 relative L2 0.03942394629120827\n",
      "training 0.662533164024353 relative L2 0.03928761929273605\n",
      "training 0.6579588651657104 relative L2 0.03915230557322502\n",
      "training 0.6534342765808105 relative L2 0.03901790454983711\n",
      "training 0.648955762386322 relative L2 0.038884494453668594\n",
      "training 0.6445255279541016 relative L2 0.03875209763646126\n",
      "training 0.6401437520980835 relative L2 0.03862065449357033\n",
      "training 0.6358079314231873 relative L2 0.038490179926157\n",
      "training 0.6315187215805054 relative L2 0.03836073726415634\n",
      "training 0.6272779107093811 relative L2 0.03823221102356911\n",
      "training 0.6230810880661011 relative L2 0.03810463100671768\n",
      "training 0.6189287304878235 relative L2 0.03797805681824684\n",
      "training 0.614823043346405 relative L2 0.03785247728228569\n",
      "training 0.6107631921768188 relative L2 0.03772778436541557\n",
      "training 0.6067452430725098 relative L2 0.03760409355163574\n",
      "training 0.6027723550796509 relative L2 0.03748135641217232\n",
      "training 0.5988430976867676 relative L2 0.03735959157347679\n",
      "training 0.5949576497077942 relative L2 0.03723878040909767\n",
      "training 0.5911149978637695 relative L2 0.03711890056729317\n",
      "training 0.5873140692710876 relative L2 0.036999981850385666\n",
      "training 0.5835557579994202 relative L2 0.03688202053308487\n",
      "training 0.5798395276069641 relative L2 0.036764975637197495\n",
      "training 0.5761639475822449 relative L2 0.03664889559149742\n",
      "training 0.5725297331809998 relative L2 0.036533769220113754\n",
      "training 0.5689368844032288 relative L2 0.03641956299543381\n",
      "training 0.5653838515281677 relative L2 0.036306291818618774\n",
      "training 0.561870813369751 relative L2 0.03619391843676567\n",
      "training 0.5583962202072144 relative L2 0.036082521080970764\n",
      "training 0.5549623370170593 relative L2 0.03597201779484749\n",
      "training 0.551566481590271 relative L2 0.03586244583129883\n",
      "training 0.5482094287872314 relative L2 0.0357537642121315\n",
      "training 0.5448897480964661 relative L2 0.03564602509140968\n",
      "training 0.5416086316108704 relative L2 0.03553919866681099\n",
      "training 0.5383650660514832 relative L2 0.03543322905898094\n",
      "training 0.5351570844650269 relative L2 0.035328201949596405\n",
      "training 0.5319870114326477 relative L2 0.035224076360464096\n",
      "training 0.5288532972335815 relative L2 0.03512080758810043\n",
      "training 0.5257545113563538 relative L2 0.03501845896244049\n",
      "training 0.5226923227310181 relative L2 0.034916963428258896\n",
      "training 0.5196645855903625 relative L2 0.03481636568903923\n",
      "training 0.5166720151901245 relative L2 0.0347166508436203\n",
      "training 0.5137144327163696 relative L2 0.034617770463228226\n",
      "training 0.5107899308204651 relative L2 0.03451976925134659\n",
      "training 0.5078995227813721 relative L2 0.03442266583442688\n",
      "training 0.5050438046455383 relative L2 0.034326378256082535\n",
      "training 0.5022196173667908 relative L2 0.03423098102211952\n",
      "training 0.49942952394485474 relative L2 0.03413638845086098\n",
      "training 0.49667054414749146 relative L2 0.03404269367456436\n",
      "training 0.49394527077674866 relative L2 0.03394975885748863\n",
      "training 0.49124935269355774 relative L2 0.033857718110084534\n",
      "training 0.4885866940021515 relative L2 0.03376644849777222\n",
      "training 0.48595356941223145 relative L2 0.03367603197693825\n",
      "training 0.4833517074584961 relative L2 0.03358646482229233\n",
      "training 0.4807814061641693 relative L2 0.03349766507744789\n",
      "training 0.4782397449016571 relative L2 0.03340968117117882\n",
      "training 0.47572794556617737 relative L2 0.03332250192761421\n",
      "training 0.47324568033218384 relative L2 0.03323614224791527\n",
      "training 0.4707930386066437 relative L2 0.03315054997801781\n",
      "training 0.4683683514595032 relative L2 0.03306574746966362\n",
      "training 0.4659722149372101 relative L2 0.03298171982169151\n",
      "training 0.4636039435863495 relative L2 0.0328984372317791\n",
      "training 0.4612625539302826 relative L2 0.03281598165631294\n",
      "training 0.4589502215385437 relative L2 0.03273426741361618\n",
      "training 0.45666423439979553 relative L2 0.03265329822897911\n",
      "training 0.45440474152565 relative L2 0.03257307410240173\n",
      "training 0.4521716237068176 relative L2 0.03249363228678703\n",
      "training 0.4499654471874237 relative L2 0.03241488337516785\n",
      "training 0.4477839767932892 relative L2 0.032336894422769547\n",
      "training 0.44562867283821106 relative L2 0.03225964307785034\n",
      "training 0.44349873065948486 relative L2 0.03218308836221695\n",
      "training 0.4413931965827942 relative L2 0.032107263803482056\n",
      "training 0.4393123984336853 relative L2 0.03203216940164566\n",
      "training 0.4372565448284149 relative L2 0.03195773437619209\n",
      "training 0.43522343039512634 relative L2 0.03188404440879822\n",
      "training 0.43321529030799866 relative L2 0.03181098774075508\n",
      "training 0.4312288463115692 relative L2 0.031738657504320145\n",
      "training 0.42926663160324097 relative L2 0.03166702017188072\n",
      "training 0.42732757329940796 relative L2 0.03159605711698532\n",
      "training 0.4254109263420105 relative L2 0.03152574598789215\n",
      "training 0.42351627349853516 relative L2 0.03145609423518181\n",
      "training 0.4216434359550476 relative L2 0.03138710558414459\n",
      "training 0.4197925925254822 relative L2 0.0313187800347805\n",
      "training 0.4179633557796478 relative L2 0.03125108405947685\n",
      "training 0.4161548316478729 relative L2 0.031184028834104538\n",
      "training 0.4143674969673157 relative L2 0.03111758455634117\n",
      "training 0.41260015964508057 relative L2 0.031051792204380035\n",
      "training 0.410853773355484 relative L2 0.030986594036221504\n",
      "training 0.40912681818008423 relative L2 0.03092203661799431\n",
      "training 0.4074205458164215 relative L2 0.030858078971505165\n",
      "training 0.40573370456695557 relative L2 0.03079470805823803\n",
      "training 0.4040655493736267 relative L2 0.030731936916708946\n",
      "training 0.4024167060852051 relative L2 0.03066975623369217\n",
      "training 0.40078645944595337 relative L2 0.03060816414654255\n",
      "training 0.3991750180721283 relative L2 0.030547158792614937\n",
      "training 0.3975820243358612 relative L2 0.030486710369586945\n",
      "training 0.39600664377212524 relative L2 0.030426818877458572\n",
      "training 0.39444881677627563 relative L2 0.030367502942681313\n",
      "training 0.3929090201854706 relative L2 0.03030872717499733\n",
      "training 0.39138609170913696 relative L2 0.03025052323937416\n",
      "training 0.38988062739372253 relative L2 0.030192846432328224\n",
      "training 0.3883917033672333 relative L2 0.030135691165924072\n",
      "training 0.38691914081573486 relative L2 0.030079081654548645\n",
      "training 0.38546326756477356 relative L2 0.030022967606782913\n",
      "training 0.38402292132377625 relative L2 0.02996739186346531\n",
      "training 0.3825989365577698 relative L2 0.029912332072854042\n",
      "training 0.3811910152435303 relative L2 0.029857734218239784\n",
      "training 0.37979748845100403 relative L2 0.029803654178977013\n",
      "training 0.37841978669166565 relative L2 0.02975006215274334\n",
      "training 0.3770570755004883 relative L2 0.029696937650442123\n",
      "training 0.37570858001708984 relative L2 0.029644306749105453\n",
      "training 0.3743750751018524 relative L2 0.02959214709699154\n",
      "training 0.37305545806884766 relative L2 0.029540488496422768\n",
      "training 0.3717508614063263 relative L2 0.02948928251862526\n",
      "training 0.3704596757888794 relative L2 0.029438545927405357\n",
      "training 0.3691824972629547 relative L2 0.02938828058540821\n",
      "training 0.36791905760765076 relative L2 0.029338449239730835\n",
      "training 0.36666902899742126 relative L2 0.02928907983005047\n",
      "training 0.3654325306415558 relative L2 0.02924017608165741\n",
      "training 0.36420977115631104 relative L2 0.029191693291068077\n",
      "training 0.3629993796348572 relative L2 0.029143646359443665\n",
      "training 0.36180195212364197 relative L2 0.029096033424139023\n",
      "training 0.36061719059944153 relative L2 0.02904883585870266\n",
      "training 0.3594447672367096 relative L2 0.029002027586102486\n",
      "training 0.3582839071750641 relative L2 0.028955627232789993\n",
      "training 0.3571351170539856 relative L2 0.028909634798765182\n",
      "training 0.35599833726882935 relative L2 0.028864046558737755\n",
      "training 0.3548731803894043 relative L2 0.028818847611546516\n",
      "training 0.35375964641571045 relative L2 0.02877401001751423\n",
      "training 0.3526567220687866 relative L2 0.028729576617479324\n",
      "training 0.35156527161598206 relative L2 0.028685513883829117\n",
      "training 0.35048484802246094 relative L2 0.028641842305660248\n",
      "training 0.34941551089286804 relative L2 0.028598535805940628\n",
      "training 0.34835687279701233 relative L2 0.0285556111484766\n",
      "training 0.3473089039325714 relative L2 0.02851303294301033\n",
      "training 0.34627100825309753 relative L2 0.028470801189541817\n",
      "training 0.3452431261539459 relative L2 0.028428953140974045\n",
      "training 0.34422600269317627 relative L2 0.02838745154440403\n",
      "training 0.34321874380111694 relative L2 0.028346288949251175\n",
      "training 0.34222108125686646 relative L2 0.02830546721816063\n",
      "training 0.34123316407203674 relative L2 0.028264978900551796\n",
      "training 0.3402547538280487 relative L2 0.028224818408489227\n",
      "training 0.33928555250167847 relative L2 0.028184978291392326\n",
      "training 0.3383253514766693 relative L2 0.028145477175712585\n",
      "training 0.3373746871948242 relative L2 0.028106266632676125\n",
      "training 0.3364325165748596 relative L2 0.02806735783815384\n",
      "training 0.3354988694190979 relative L2 0.028028782457113266\n",
      "training 0.3345744013786316 relative L2 0.027990521863102913\n",
      "training 0.3336588144302368 relative L2 0.02795252948999405\n",
      "training 0.3327508568763733 relative L2 0.02791483886539936\n",
      "training 0.3318513333797455 relative L2 0.027877427637577057\n",
      "training 0.33095958828926086 relative L2 0.02784033864736557\n",
      "training 0.3300767242908478 relative L2 0.027803510427474976\n",
      "training 0.3292011618614197 relative L2 0.02776697650551796\n",
      "training 0.3283337950706482 relative L2 0.02773069590330124\n",
      "training 0.3274734616279602 relative L2 0.02769472450017929\n",
      "training 0.3266216516494751 relative L2 0.027658989652991295\n",
      "training 0.3257766366004944 relative L2 0.027623524889349937\n",
      "training 0.3249390423297882 relative L2 0.02758832275867462\n",
      "training 0.32410863041877747 relative L2 0.027553370222449303\n",
      "training 0.32328516244888306 relative L2 0.02751867100596428\n",
      "training 0.3224688470363617 relative L2 0.027484212070703506\n",
      "training 0.3216589689254761 relative L2 0.027449995279312134\n",
      "training 0.3208557665348053 relative L2 0.027416031807661057\n",
      "training 0.320059597492218 relative L2 0.027382316067814827\n",
      "training 0.31927019357681274 relative L2 0.027348816394805908\n",
      "training 0.3184867799282074 relative L2 0.02731555700302124\n",
      "training 0.31770995259284973 relative L2 0.02728250063955784\n",
      "training 0.31693872809410095 relative L2 0.027249695733189583\n",
      "training 0.3161744177341461 relative L2 0.027217106893658638\n",
      "training 0.3154159188270569 relative L2 0.02718469686806202\n",
      "training 0.3146624267101288 relative L2 0.02715253084897995\n",
      "training 0.3139156103134155 relative L2 0.027120569720864296\n",
      "training 0.31317436695098877 relative L2 0.027088822796940804\n",
      "training 0.31243896484375 relative L2 0.027057267725467682\n",
      "training 0.31170880794525146 relative L2 0.027025902643799782\n",
      "training 0.3109840452671051 relative L2 0.026994740590453148\n",
      "training 0.31026479601860046 relative L2 0.02696377970278263\n",
      "training 0.3095510005950928 relative L2 0.026933005079627037\n",
      "training 0.308842271566391 relative L2 0.026902418583631516\n",
      "training 0.3081386685371399 relative L2 0.026872018352150917\n",
      "training 0.30744001269340515 relative L2 0.026841789484024048\n",
      "training 0.3067462742328644 relative L2 0.026811745017766953\n",
      "training 0.30605748295783997 relative L2 0.02678186446428299\n",
      "training 0.30537328124046326 relative L2 0.026752160862088203\n",
      "training 0.3046937584877014 relative L2 0.0267226193100214\n",
      "training 0.3040187954902649 relative L2 0.026693252846598625\n",
      "training 0.3033485412597656 relative L2 0.02666402980685234\n",
      "training 0.3026823103427887 relative L2 0.026634976267814636\n",
      "training 0.3020206093788147 relative L2 0.02660607546567917\n",
      "training 0.3013632893562317 relative L2 0.026577314361929893\n",
      "training 0.3007097840309143 relative L2 0.0265487190335989\n",
      "training 0.3000606894493103 relative L2 0.026520254090428352\n",
      "training 0.29941537976264954 relative L2 0.026491928845643997\n",
      "training 0.29877394437789917 relative L2 0.026463745161890984\n",
      "training 0.29813623428344727 relative L2 0.026435695588588715\n",
      "training 0.2975022792816162 relative L2 0.026407774537801743\n",
      "training 0.29687196016311646 relative L2 0.026379963383078575\n",
      "training 0.29624488949775696 relative L2 0.026352273300290108\n",
      "training 0.2956211566925049 relative L2 0.026324696838855743\n",
      "training 0.2950003743171692 relative L2 0.026297220960259438\n",
      "training 0.2943824231624603 relative L2 0.02626984380185604\n",
      "training 0.2937673032283783 relative L2 0.026242531836032867\n",
      "training 0.2931542992591858 relative L2 0.026215285062789917\n",
      "training 0.2925432026386261 relative L2 0.026188069954514503\n",
      "training 0.29193314909935 relative L2 0.026160884648561478\n",
      "training 0.2913244664669037 relative L2 0.026133662089705467\n",
      "training 0.29071545600891113 relative L2 0.026106378063559532\n",
      "training 0.2901054322719574 relative L2 0.026078948751091957\n",
      "training 0.2894930839538574 relative L2 0.026051310822367668\n",
      "training 0.2888762354850769 relative L2 0.026023264974355698\n",
      "training 0.2882508337497711 relative L2 0.02599467895925045\n",
      "training 0.28761470317840576 relative L2 0.02596530131995678\n",
      "training 0.28696170449256897 relative L2 0.025934772565960884\n",
      "training 0.28628379106521606 relative L2 0.025902701541781425\n",
      "training 0.2855721116065979 relative L2 0.02586839720606804\n",
      "training 0.2848146855831146 relative L2 0.025831308215856552\n",
      "training 0.2839970588684082 relative L2 0.025790590792894363\n",
      "training 0.283102810382843 relative L2 0.025745874270796776\n",
      "training 0.28212136030197144 relative L2 0.025697246193885803\n",
      "training 0.28105852007865906 relative L2 0.025646302849054337\n",
      "training 0.27994784712791443 relative L2 0.025595946237444878\n",
      "training 0.27885717153549194 relative L2 0.02555026300251484\n",
      "training 0.27788978815078735 relative L2 0.025512034073472023\n",
      "training 0.27708593010902405 relative L2 0.025480439886450768\n",
      "training 0.27642086148262024 relative L2 0.02545374259352684\n",
      "training 0.2758583128452301 relative L2 0.025430399924516678\n",
      "training 0.2753582000732422 relative L2 0.02540872059762478\n",
      "training 0.27489081025123596 relative L2 0.02538726106286049\n",
      "training 0.27443164587020874 relative L2 0.025365285575389862\n",
      "training 0.2739618420600891 relative L2 0.02534235268831253\n",
      "training 0.2734701633453369 relative L2 0.02531830407679081\n",
      "training 0.2729523479938507 relative L2 0.025293007493019104\n",
      "training 0.27240675687789917 relative L2 0.025266552343964577\n",
      "training 0.2718363106250763 relative L2 0.025239184498786926\n",
      "training 0.27124595642089844 relative L2 0.0252111554145813\n",
      "training 0.27064064145088196 relative L2 0.025182748213410378\n",
      "training 0.27002692222595215 relative L2 0.02515426278114319\n",
      "training 0.269411563873291 relative L2 0.025125958025455475\n",
      "training 0.2687999904155731 relative L2 0.02509806677699089\n",
      "training 0.26819756627082825 relative L2 0.0250706784427166\n",
      "training 0.2676089406013489 relative L2 0.025043915957212448\n",
      "training 0.2670355439186096 relative L2 0.025017939507961273\n",
      "training 0.26647835969924927 relative L2 0.024992674589157104\n",
      "training 0.26593634486198425 relative L2 0.024968046694993973\n",
      "training 0.2654079794883728 relative L2 0.024943936616182327\n",
      "training 0.26489052176475525 relative L2 0.02492019347846508\n",
      "training 0.26438021659851074 relative L2 0.02489657700061798\n",
      "training 0.2638740837574005 relative L2 0.024872947484254837\n",
      "training 0.2633693218231201 relative L2 0.02484920434653759\n",
      "training 0.2628636658191681 relative L2 0.024825265631079674\n",
      "training 0.262355238199234 relative L2 0.024801105260849\n",
      "training 0.2618435025215149 relative L2 0.024776708334684372\n",
      "training 0.26132795214653015 relative L2 0.024752097204327583\n",
      "training 0.26080918312072754 relative L2 0.024727318435907364\n",
      "training 0.26028797030448914 relative L2 0.02470245398581028\n",
      "training 0.25976529717445374 relative L2 0.024677466601133347\n",
      "training 0.2592408061027527 relative L2 0.024652421474456787\n",
      "training 0.2587149143218994 relative L2 0.024627286940813065\n",
      "training 0.2581866681575775 relative L2 0.02460198663175106\n",
      "training 0.2576543688774109 relative L2 0.02457643300294876\n",
      "training 0.25711625814437866 relative L2 0.024550464004278183\n",
      "training 0.2565694749355316 relative L2 0.024523787200450897\n",
      "training 0.2560085356235504 relative L2 0.02449624240398407\n",
      "training 0.2554292380809784 relative L2 0.02446739748120308\n",
      "training 0.25482338666915894 relative L2 0.024436695501208305\n",
      "training 0.25418028235435486 relative L2 0.024403559044003487\n",
      "training 0.25348734855651855 relative L2 0.02436712570488453\n",
      "training 0.25272637605667114 relative L2 0.024326352402567863\n",
      "training 0.25187769532203674 relative L2 0.024279871955513954\n",
      "training 0.2509182393550873 relative L2 0.024226607754826546\n",
      "training 0.2498214840888977 relative L2 0.02416551671922207\n",
      "training 0.2485695630311966 relative L2 0.02409619279205799\n",
      "training 0.24715854227542877 relative L2 0.024019775912165642\n",
      "training 0.24560612440109253 relative L2 0.02393859438598156\n",
      "training 0.24396252632141113 relative L2 0.02385718934237957\n",
      "training 0.2423231601715088 relative L2 0.023782813921570778\n",
      "training 0.24083061516284943 relative L2 0.023722685873508453\n",
      "training 0.2396230250597 relative L2 0.02367868646979332\n",
      "training 0.23875756561756134 relative L2 0.023646321147680283\n",
      "training 0.23812803626060486 relative L2 0.0236197616904974\n",
      "training 0.23760291934013367 relative L2 0.023593708872795105\n",
      "training 0.23708100616931915 relative L2 0.0235656276345253\n",
      "training 0.23650768399238586 relative L2 0.023534363135695457\n",
      "training 0.23587524890899658 relative L2 0.02350119687616825\n",
      "training 0.23520109057426453 relative L2 0.023468017578125\n",
      "training 0.23452571034431458 relative L2 0.023437105119228363\n",
      "training 0.2338947057723999 relative L2 0.023409150540828705\n",
      "training 0.23333454132080078 relative L2 0.023383913561701775\n",
      "training 0.23282864689826965 relative L2 0.023360176011919975\n",
      "training 0.23235604166984558 relative L2 0.023337233811616898\n",
      "training 0.23189814388751984 relative L2 0.02331441640853882\n",
      "training 0.2314421385526657 relative L2 0.02329127863049507\n",
      "training 0.23098057508468628 relative L2 0.02326761558651924\n",
      "training 0.2305098921060562 relative L2 0.02324344776570797\n",
      "training 0.23002807796001434 relative L2 0.023218557238578796\n",
      "training 0.2295321375131607 relative L2 0.02319294586777687\n",
      "training 0.22902315855026245 relative L2 0.023166606202721596\n",
      "training 0.2284998893737793 relative L2 0.02313951402902603\n",
      "training 0.22796356678009033 relative L2 0.023111751303076744\n",
      "training 0.2274157702922821 relative L2 0.023083556443452835\n",
      "training 0.2268601506948471 relative L2 0.023055240511894226\n",
      "training 0.22630147635936737 relative L2 0.0230270829051733\n",
      "training 0.22574694454669952 relative L2 0.022999374195933342\n",
      "training 0.2252046763896942 relative L2 0.02297264337539673\n",
      "training 0.22468052804470062 relative L2 0.02294696494936943\n",
      "training 0.22417816519737244 relative L2 0.022922242060303688\n",
      "training 0.22369983792304993 relative L2 0.022898316383361816\n",
      "training 0.2232394814491272 relative L2 0.02287493273615837\n",
      "training 0.22278854250907898 relative L2 0.02285161241889\n",
      "training 0.2223387062549591 relative L2 0.022828083485364914\n",
      "training 0.22188347578048706 relative L2 0.022804176434874535\n",
      "training 0.22141948342323303 relative L2 0.02277982234954834\n",
      "training 0.2209462970495224 relative L2 0.022754987701773643\n",
      "training 0.22046366333961487 relative L2 0.02272982709109783\n",
      "training 0.21997396647930145 relative L2 0.02270442061126232\n",
      "training 0.2194790542125702 relative L2 0.02267886884510517\n",
      "training 0.21898159384727478 relative L2 0.022653287276625633\n",
      "training 0.21848349273204803 relative L2 0.022627703845500946\n",
      "training 0.21798549592494965 relative L2 0.0226021409034729\n",
      "training 0.21748881042003632 relative L2 0.02257654443383217\n",
      "training 0.21699213981628418 relative L2 0.02255086414515972\n",
      "training 0.21649512648582458 relative L2 0.022524984553456306\n",
      "training 0.2159964144229889 relative L2 0.02249884605407715\n",
      "training 0.2154952436685562 relative L2 0.02247246913611889\n",
      "training 0.2149907499551773 relative L2 0.02244592271745205\n",
      "training 0.2144833505153656 relative L2 0.022419286891818047\n",
      "training 0.21397601068019867 relative L2 0.022392798215150833\n",
      "training 0.21347321569919586 relative L2 0.02236688882112503\n",
      "training 0.21298153698444366 relative L2 0.022341841831803322\n",
      "training 0.21250803768634796 relative L2 0.02231810800731182\n",
      "training 0.21206040680408478 relative L2 0.0222958792001009\n",
      "training 0.2116425782442093 relative L2 0.0222746804356575\n",
      "training 0.211244136095047 relative L2 0.02225388027727604\n",
      "training 0.21085399389266968 relative L2 0.022233303636312485\n",
      "training 0.21046796441078186 relative L2 0.022212879732251167\n",
      "training 0.2100839763879776 relative L2 0.022192442789673805\n",
      "training 0.2096996307373047 relative L2 0.02217191271483898\n",
      "training 0.20931333303451538 relative L2 0.022151317447423935\n",
      "training 0.20892535150051117 relative L2 0.022130649536848068\n",
      "training 0.20853570103645325 relative L2 0.022109931334853172\n",
      "training 0.20814526081085205 relative L2 0.022089190781116486\n",
      "training 0.2077542096376419 relative L2 0.022068506106734276\n",
      "training 0.20736397802829742 relative L2 0.022047867998480797\n",
      "training 0.20697514712810516 relative L2 0.022027339786291122\n",
      "training 0.20658843219280243 relative L2 0.022006956860423088\n",
      "training 0.2062048763036728 relative L2 0.021986596286296844\n",
      "training 0.20582252740859985 relative L2 0.02196621522307396\n",
      "training 0.20543988049030304 relative L2 0.02194587141275406\n",
      "training 0.20505794882774353 relative L2 0.02192557044327259\n",
      "training 0.20467759668827057 relative L2 0.021905293688178062\n",
      "training 0.20429855585098267 relative L2 0.02188505046069622\n",
      "training 0.20392046868801117 relative L2 0.021864844486117363\n",
      "training 0.20354358851909637 relative L2 0.02184465155005455\n",
      "training 0.20316757261753082 relative L2 0.021824469789862633\n",
      "training 0.202792227268219 relative L2 0.02180430293083191\n",
      "training 0.20241756737232208 relative L2 0.021784167736768723\n",
      "training 0.20204417407512665 relative L2 0.021764008328318596\n",
      "training 0.20167122781276703 relative L2 0.021743901073932648\n",
      "training 0.20130012929439545 relative L2 0.02172384038567543\n",
      "training 0.20092980563640594 relative L2 0.02170383930206299\n",
      "training 0.20056046545505524 relative L2 0.021683845669031143\n",
      "training 0.2001914083957672 relative L2 0.02166387438774109\n",
      "training 0.1998228132724762 relative L2 0.02164393849670887\n",
      "training 0.19945497810840607 relative L2 0.021624036133289337\n",
      "training 0.19908800721168518 relative L2 0.021604182198643684\n",
      "training 0.1987220048904419 relative L2 0.021584326401352882\n",
      "training 0.19835610687732697 relative L2 0.02156449295580387\n",
      "training 0.19799108803272247 relative L2 0.0215446874499321\n",
      "training 0.19762659072875977 relative L2 0.021524930372834206\n",
      "training 0.19726286828517914 relative L2 0.021505160257220268\n",
      "training 0.1968994438648224 relative L2 0.02148539014160633\n",
      "training 0.19653624296188354 relative L2 0.021465662866830826\n",
      "training 0.1961740255355835 relative L2 0.021445900201797485\n",
      "training 0.19581206142902374 relative L2 0.021426113322377205\n",
      "training 0.19545017182826996 relative L2 0.021406343206763268\n",
      "training 0.19508911669254303 relative L2 0.02138654887676239\n",
      "training 0.19472748041152954 relative L2 0.021366756409406662\n",
      "training 0.19436585903167725 relative L2 0.02134665660560131\n",
      "training 0.1940002292394638 relative L2 0.021323813125491142\n",
      "training 0.1935807317495346 relative L2 0.021301040425896645\n",
      "training 0.19317205250263214 relative L2 0.021278316155076027\n",
      "training 0.1927601546049118 relative L2 0.021255705505609512\n",
      "training 0.19234560430049896 relative L2 0.021232469007372856\n",
      "training 0.19192823767662048 relative L2 0.021209292113780975\n",
      "training 0.19151166081428528 relative L2 0.02118641324341297\n",
      "training 0.19109472632408142 relative L2 0.02116318605840206\n",
      "training 0.19067755341529846 relative L2 0.02113991230726242\n",
      "training 0.19026200473308563 relative L2 0.02111700177192688\n",
      "training 0.18984757363796234 relative L2 0.021093932911753654\n",
      "training 0.18943414092063904 relative L2 0.021070707589387894\n",
      "training 0.1890215128660202 relative L2 0.0210478026419878\n",
      "training 0.18860994279384613 relative L2 0.021024886518716812\n",
      "training 0.18820001184940338 relative L2 0.021001771092414856\n",
      "training 0.18779097497463226 relative L2 0.02097891829907894\n",
      "training 0.18738268315792084 relative L2 0.020956173539161682\n",
      "training 0.18697623908519745 relative L2 0.020933251827955246\n",
      "training 0.18657059967517853 relative L2 0.020910564810037613\n",
      "training 0.18616646528244019 relative L2 0.02088797092437744\n",
      "training 0.1857636719942093 relative L2 0.020865166559815407\n",
      "training 0.18536177277565002 relative L2 0.02084251679480076\n",
      "training 0.184961199760437 relative L2 0.02082000859081745\n",
      "training 0.18456195294857025 relative L2 0.020797371864318848\n",
      "training 0.18416401743888855 relative L2 0.0207748394459486\n",
      "training 0.18376769125461578 relative L2 0.02075246535241604\n",
      "training 0.18337200582027435 relative L2 0.0207300316542387\n",
      "training 0.18297795951366425 relative L2 0.02070765011012554\n",
      "training 0.1825852245092392 relative L2 0.02068547159433365\n",
      "training 0.1821940839290619 relative L2 0.020663205534219742\n",
      "training 0.18180373311042786 relative L2 0.02064097672700882\n",
      "training 0.18141481280326843 relative L2 0.020618904381990433\n",
      "training 0.1810269057750702 relative L2 0.020596779882907867\n",
      "training 0.18063980340957642 relative L2 0.020574670284986496\n",
      "training 0.18025410175323486 relative L2 0.020552661269903183\n",
      "training 0.1798691600561142 relative L2 0.02053062990307808\n",
      "training 0.17948530614376068 relative L2 0.020508557558059692\n",
      "training 0.17910194396972656 relative L2 0.02048662304878235\n",
      "training 0.17871975898742676 relative L2 0.020464671775698662\n",
      "training 0.17833833396434784 relative L2 0.020442673936486244\n",
      "training 0.177957683801651 relative L2 0.020420793443918228\n",
      "training 0.17757786810398102 relative L2 0.02039891853928566\n",
      "training 0.1771988570690155 relative L2 0.020376993343234062\n",
      "training 0.17682045698165894 relative L2 0.020355146378278732\n",
      "training 0.17644289135932922 relative L2 0.020333306863904\n",
      "training 0.17606611549854279 relative L2 0.02031145989894867\n",
      "training 0.17569030821323395 relative L2 0.020289653912186623\n",
      "training 0.17531493306159973 relative L2 0.020267857238650322\n",
      "training 0.17494045197963715 relative L2 0.020246027037501335\n",
      "training 0.17456664144992828 relative L2 0.02022424153983593\n",
      "training 0.17419332265853882 relative L2 0.02020248770713806\n",
      "training 0.17382121086120605 relative L2 0.0201807152479887\n",
      "training 0.17344984412193298 relative L2 0.020158935338258743\n",
      "training 0.17307822406291962 relative L2 0.020137188956141472\n",
      "training 0.17270761728286743 relative L2 0.020115388557314873\n",
      "training 0.17233707010746002 relative L2 0.020093625411391258\n",
      "training 0.17196714878082275 relative L2 0.020071882754564285\n",
      "training 0.1715974509716034 relative L2 0.02005012333393097\n",
      "training 0.17122814059257507 relative L2 0.02002842165529728\n",
      "training 0.17085997760295868 relative L2 0.020006678998470306\n",
      "training 0.17049162089824677 relative L2 0.019984889775514603\n",
      "training 0.17012354731559753 relative L2 0.01996311917901039\n",
      "training 0.16975560784339905 relative L2 0.019941363483667374\n",
      "training 0.16938820481300354 relative L2 0.019919566810131073\n",
      "training 0.1690211445093155 relative L2 0.019897744059562683\n",
      "training 0.16865405440330505 relative L2 0.019875938072800636\n",
      "training 0.16828744113445282 relative L2 0.019854100421071053\n",
      "training 0.16792118549346924 relative L2 0.0198323056101799\n",
      "training 0.1675555408000946 relative L2 0.019810492172837257\n",
      "training 0.16719000041484833 relative L2 0.019788691774010658\n",
      "training 0.1668248325586319 relative L2 0.01976688578724861\n",
      "training 0.16645945608615875 relative L2 0.019745100289583206\n",
      "training 0.16609425842761993 relative L2 0.019723284989595413\n",
      "training 0.16572906076908112 relative L2 0.01970142126083374\n",
      "training 0.16536380350589752 relative L2 0.019679557532072067\n",
      "training 0.16499914228916168 relative L2 0.019657647237181664\n",
      "training 0.16463448107242584 relative L2 0.01963571086525917\n",
      "training 0.1642696112394333 relative L2 0.019613755866885185\n",
      "training 0.16390478610992432 relative L2 0.019591769203543663\n",
      "training 0.16353961825370789 relative L2 0.01956976391375065\n",
      "training 0.1631743311882019 relative L2 0.019547779113054276\n",
      "training 0.16280941665172577 relative L2 0.01952582411468029\n",
      "training 0.16244487464427948 relative L2 0.019503850489854813\n",
      "training 0.1620800793170929 relative L2 0.019481830298900604\n",
      "training 0.1617150753736496 relative L2 0.019459731876850128\n",
      "training 0.1613498330116272 relative L2 0.019437599927186966\n",
      "training 0.16098439693450928 relative L2 0.019415467977523804\n",
      "training 0.16061906516551971 relative L2 0.019393334165215492\n",
      "training 0.16025389730930328 relative L2 0.01937117800116539\n",
      "training 0.1598888784646988 relative L2 0.01934894546866417\n",
      "training 0.15952332317829132 relative L2 0.019326698035001755\n",
      "training 0.15915803611278534 relative L2 0.019304420799016953\n",
      "training 0.15879260003566742 relative L2 0.019282078370451927\n",
      "training 0.1584264487028122 relative L2 0.0192597433924675\n",
      "training 0.158060684800148 relative L2 0.019237397238612175\n",
      "training 0.15769453346729279 relative L2 0.01921498216688633\n",
      "training 0.1573277711868286 relative L2 0.019192539155483246\n",
      "training 0.15696093440055847 relative L2 0.019170032814145088\n",
      "training 0.15659351646900177 relative L2 0.01914752461016178\n",
      "training 0.15622645616531372 relative L2 0.0191249568015337\n",
      "training 0.15585878491401672 relative L2 0.01910235546529293\n",
      "training 0.15549100935459137 relative L2 0.019079705700278282\n",
      "training 0.15512290596961975 relative L2 0.019056975841522217\n",
      "training 0.1547541320323944 relative L2 0.019034186378121376\n",
      "training 0.1543850600719452 relative L2 0.019011367112398148\n",
      "training 0.15401560068130493 relative L2 0.018988480791449547\n",
      "training 0.15364544093608856 relative L2 0.018965531140565872\n",
      "training 0.15327483415603638 relative L2 0.01894245855510235\n",
      "training 0.1529027819633484 relative L2 0.01891935057938099\n",
      "training 0.15253031253814697 relative L2 0.01889612339437008\n",
      "training 0.15215656161308289 relative L2 0.018872786313295364\n",
      "training 0.15178167819976807 relative L2 0.01884935051202774\n",
      "training 0.15140561759471893 relative L2 0.01882578805088997\n",
      "training 0.15102766454219818 relative L2 0.018802080303430557\n",
      "training 0.1506478637456894 relative L2 0.018778212368488312\n",
      "training 0.15026558935642242 relative L2 0.018754206597805023\n",
      "training 0.14988088607788086 relative L2 0.01873004622757435\n",
      "training 0.1494940221309662 relative L2 0.018705612048506737\n",
      "training 0.14910387992858887 relative L2 0.01868104562163353\n",
      "training 0.14871171116828918 relative L2 0.018656350672245026\n",
      "training 0.14831726253032684 relative L2 0.01863149181008339\n",
      "training 0.14792075753211975 relative L2 0.018606608733534813\n",
      "training 0.1475246101617813 relative L2 0.01858169585466385\n",
      "training 0.14712855219841003 relative L2 0.018556902185082436\n",
      "training 0.14673472940921783 relative L2 0.018532345071434975\n",
      "training 0.14634498953819275 relative L2 0.018508171662688255\n",
      "training 0.14596034586429596 relative L2 0.018484383821487427\n",
      "training 0.14558176696300507 relative L2 0.018460892140865326\n",
      "training 0.1452079713344574 relative L2 0.01843753457069397\n",
      "training 0.14483733475208282 relative L2 0.018414100632071495\n",
      "training 0.14446735382080078 relative L2 0.01839045248925686\n",
      "training 0.1440953016281128 relative L2 0.018366465345025063\n",
      "training 0.14371953904628754 relative L2 0.018342142924666405\n",
      "training 0.1433396339416504 relative L2 0.01831751875579357\n",
      "training 0.1429559886455536 relative L2 0.018292700871825218\n",
      "training 0.14257042109966278 relative L2 0.018267802894115448\n",
      "training 0.14218464493751526 relative L2 0.018242841586470604\n",
      "training 0.14179891347885132 relative L2 0.018217923119664192\n",
      "training 0.14141474664211273 relative L2 0.018192993476986885\n",
      "training 0.1410309076309204 relative L2 0.01816805824637413\n",
      "training 0.1406475156545639 relative L2 0.018143100664019585\n",
      "training 0.14026370644569397 relative L2 0.018118107691407204\n",
      "training 0.13987907767295837 relative L2 0.018093112856149673\n",
      "training 0.1394938826560974 relative L2 0.018068023025989532\n",
      "training 0.13910774886608124 relative L2 0.0180427897721529\n",
      "training 0.13872046768665314 relative L2 0.018017545342445374\n",
      "training 0.13833294808864594 relative L2 0.017992207780480385\n",
      "training 0.13794443011283875 relative L2 0.01796679198741913\n",
      "training 0.13755549490451813 relative L2 0.01794133335351944\n",
      "training 0.13716667890548706 relative L2 0.017915762960910797\n",
      "training 0.13677699863910675 relative L2 0.01789010316133499\n",
      "training 0.136386439204216 relative L2 0.017864368855953217\n",
      "training 0.13599500060081482 relative L2 0.01783854328095913\n",
      "training 0.13560274243354797 relative L2 0.017812617123126984\n",
      "training 0.135209321975708 relative L2 0.017786620184779167\n",
      "training 0.1348147988319397 relative L2 0.017760571092367172\n",
      "training 0.13441962003707886 relative L2 0.01773441769182682\n",
      "training 0.1340230405330658 relative L2 0.017708227038383484\n",
      "training 0.1336265653371811 relative L2 0.017681950703263283\n",
      "training 0.13322918117046356 relative L2 0.01765560731291771\n",
      "training 0.13283118605613708 relative L2 0.017629152163863182\n",
      "training 0.13243260979652405 relative L2 0.01760260947048664\n",
      "training 0.13203299045562744 relative L2 0.017576022073626518\n",
      "training 0.1316327154636383 relative L2 0.01754932478070259\n",
      "training 0.13123121857643127 relative L2 0.017522549256682396\n",
      "training 0.1308296024799347 relative L2 0.01749565452337265\n",
      "training 0.13042700290679932 relative L2 0.0174686461687088\n",
      "training 0.13002341985702515 relative L2 0.017441540956497192\n",
      "training 0.12961873412132263 relative L2 0.017414376139640808\n",
      "training 0.12921388447284698 relative L2 0.017387066036462784\n",
      "training 0.1288076490163803 relative L2 0.0173596553504467\n",
      "training 0.12840044498443604 relative L2 0.01733214780688286\n",
      "training 0.12799246609210968 relative L2 0.01730455830693245\n",
      "training 0.12758447229862213 relative L2 0.017276881262660027\n",
      "training 0.12717591226100922 relative L2 0.017249077558517456\n",
      "training 0.1267663985490799 relative L2 0.017221197485923767\n",
      "training 0.1263563185930252 relative L2 0.017193205654621124\n",
      "training 0.12594519555568695 relative L2 0.017165139317512512\n",
      "training 0.12553328275680542 relative L2 0.017137009650468826\n",
      "training 0.12512053549289703 relative L2 0.017108796164393425\n",
      "training 0.12470728158950806 relative L2 0.01708046905696392\n",
      "training 0.12429339438676834 relative L2 0.01705205999314785\n",
      "training 0.1238790899515152 relative L2 0.017023572698235512\n",
      "training 0.12346482276916504 relative L2 0.01699496991932392\n",
      "training 0.12304964661598206 relative L2 0.016966285184025764\n",
      "training 0.12263403832912445 relative L2 0.016937505453824997\n",
      "training 0.12221747636795044 relative L2 0.016908638179302216\n",
      "training 0.12180015444755554 relative L2 0.016879715025424957\n",
      "training 0.12138288468122482 relative L2 0.016850732266902924\n",
      "training 0.12096571922302246 relative L2 0.01682165265083313\n",
      "training 0.12054812908172607 relative L2 0.016792498528957367\n",
      "training 0.12013020366430283 relative L2 0.016763243824243546\n",
      "training 0.11971218138933182 relative L2 0.016733907163143158\n",
      "training 0.1192939430475235 relative L2 0.016704516485333443\n",
      "training 0.1188756600022316 relative L2 0.01667502149939537\n",
      "training 0.11845685541629791 relative L2 0.016645479947328568\n",
      "training 0.11803741753101349 relative L2 0.016615932807326317\n",
      "training 0.11761833727359772 relative L2 0.016586311161518097\n",
      "training 0.11719886213541031 relative L2 0.016556687653064728\n",
      "training 0.11677964776754379 relative L2 0.01652703806757927\n",
      "training 0.11636032909154892 relative L2 0.016497289761900902\n",
      "training 0.11594091355800629 relative L2 0.016467448323965073\n",
      "training 0.11552035063505173 relative L2 0.016437577083706856\n",
      "training 0.1151004284620285 relative L2 0.016407646238803864\n",
      "training 0.11468098312616348 relative L2 0.016377633437514305\n",
      "training 0.11426188051700592 relative L2 0.016347544267773628\n",
      "training 0.11384245753288269 relative L2 0.01631743274629116\n",
      "training 0.11342376470565796 relative L2 0.01628725789487362\n",
      "training 0.11300569027662277 relative L2 0.016257040202617645\n",
      "training 0.11258788406848907 relative L2 0.01622684672474861\n",
      "training 0.11217077821493149 relative L2 0.016196589916944504\n",
      "training 0.11175349354743958 relative L2 0.01616634987294674\n",
      "training 0.1113368347287178 relative L2 0.01613609865307808\n",
      "training 0.1109205111861229 relative L2 0.016105851158499718\n",
      "training 0.11050515621900558 relative L2 0.016075586900115013\n",
      "training 0.11009083688259125 relative L2 0.016045251861214638\n",
      "training 0.10967712849378586 relative L2 0.01601489447057247\n",
      "training 0.10926396399736404 relative L2 0.015984535217285156\n",
      "training 0.10885164141654968 relative L2 0.01595415733754635\n",
      "training 0.10843998938798904 relative L2 0.015923822298645973\n",
      "training 0.10802927613258362 relative L2 0.015893476083874702\n",
      "training 0.10761909186840057 relative L2 0.015863128006458282\n",
      "training 0.107209712266922 relative L2 0.01583281345665455\n",
      "training 0.10680122673511505 relative L2 0.01580253802239895\n",
      "training 0.10639382898807526 relative L2 0.01577231101691723\n",
      "training 0.10598796606063843 relative L2 0.01574213057756424\n",
      "training 0.10558341443538666 relative L2 0.015711959451436996\n",
      "training 0.10517992079257965 relative L2 0.015681836754083633\n",
      "training 0.10477742552757263 relative L2 0.015651719644665718\n",
      "training 0.10437561571598053 relative L2 0.01562168076634407\n",
      "training 0.1039753183722496 relative L2 0.015591690316796303\n",
      "training 0.10357591509819031 relative L2 0.015561782754957676\n",
      "training 0.10317827761173248 relative L2 0.015531851910054684\n",
      "training 0.1027815043926239 relative L2 0.01550196297466755\n",
      "training 0.1023862212896347 relative L2 0.015472112223505974\n",
      "training 0.10199230164289474 relative L2 0.015442361123859882\n",
      "training 0.10160017013549805 relative L2 0.01541261188685894\n",
      "training 0.10120975226163864 relative L2 0.015382983721792698\n",
      "training 0.10082089900970459 relative L2 0.015353451482951641\n",
      "training 0.10043361037969589 relative L2 0.0153239406645298\n",
      "training 0.10004734247922897 relative L2 0.015294454991817474\n",
      "training 0.0996626615524292 relative L2 0.015265034511685371\n",
      "training 0.09927978366613388 relative L2 0.015235694125294685\n",
      "training 0.0988985151052475 relative L2 0.015206485986709595\n",
      "training 0.0985192358493805 relative L2 0.01517738401889801\n",
      "training 0.09814178198575974 relative L2 0.015148364938795567\n",
      "training 0.09776610881090164 relative L2 0.015119422227144241\n",
      "training 0.09739221632480621 relative L2 0.015090573579072952\n",
      "training 0.09702077507972717 relative L2 0.01506178267300129\n",
      "training 0.09665060043334961 relative L2 0.015033097937703133\n",
      "training 0.09628242999315262 relative L2 0.015004515647888184\n",
      "training 0.0959162637591362 relative L2 0.014976030215620995\n",
      "training 0.09555196762084961 relative L2 0.014947665855288506\n",
      "training 0.09518980234861374 relative L2 0.014919397421181202\n",
      "training 0.09482929110527039 relative L2 0.014891262166202068\n",
      "training 0.09447101503610611 relative L2 0.014863249845802784\n",
      "training 0.094114750623703 relative L2 0.014835327863693237\n",
      "training 0.09376012533903122 relative L2 0.014807618223130703\n",
      "training 0.09340809285640717 relative L2 0.014780014753341675\n",
      "training 0.09305828809738159 relative L2 0.014752517454326153\n",
      "training 0.09271065145730972 relative L2 0.014725103974342346\n",
      "training 0.09236470609903336 relative L2 0.014697803184390068\n",
      "training 0.09202088415622711 relative L2 0.014670606702566147\n",
      "training 0.09167902171611786 relative L2 0.014643514528870583\n",
      "training 0.09133923798799515 relative L2 0.014616526663303375\n",
      "training 0.09100142121315002 relative L2 0.014589697122573853\n",
      "training 0.09066599607467651 relative L2 0.014562985859811306\n",
      "training 0.09033273160457611 relative L2 0.014536422677338123\n",
      "training 0.09000169485807419 relative L2 0.014510007575154305\n",
      "training 0.08967297524213791 relative L2 0.014483710750937462\n",
      "training 0.08934658765792847 relative L2 0.014457553625106812\n",
      "training 0.08902257680892944 relative L2 0.014431498944759369\n",
      "training 0.08870050311088562 relative L2 0.01440558210015297\n",
      "training 0.08838099241256714 relative L2 0.014379814267158508\n",
      "training 0.08806374669075012 relative L2 0.014354194514453411\n",
      "training 0.08774889260530472 relative L2 0.01432871539145708\n",
      "training 0.08743645250797272 relative L2 0.014303387142717838\n",
      "training 0.08712628483772278 relative L2 0.014278172515332699\n",
      "training 0.08681787550449371 relative L2 0.014253129251301289\n",
      "training 0.08651169389486313 relative L2 0.01422820519655943\n",
      "training 0.08620759844779968 relative L2 0.014203405007719994\n",
      "training 0.0859055146574974 relative L2 0.014178743585944176\n",
      "training 0.08560574799776077 relative L2 0.014154237695038319\n",
      "training 0.08530817180871964 relative L2 0.014129881747066975\n",
      "training 0.08501257747411728 relative L2 0.014105691574513912\n",
      "training 0.08471959084272385 relative L2 0.014081654138863087\n",
      "training 0.08442874997854233 relative L2 0.01405775360763073\n",
      "training 0.08414020389318466 relative L2 0.01403395738452673\n",
      "training 0.08385348320007324 relative L2 0.01401031669229269\n",
      "training 0.08356916159391403 relative L2 0.013986787758767605\n",
      "training 0.08328662812709808 relative L2 0.013963370583951473\n",
      "training 0.08300603181123734 relative L2 0.013940117321908474\n",
      "training 0.08272788673639297 relative L2 0.013916975818574429\n",
      "training 0.0824514850974083 relative L2 0.013893991708755493\n",
      "training 0.08217745274305344 relative L2 0.013871129602193832\n",
      "training 0.08190535753965378 relative L2 0.013848400674760342\n",
      "training 0.08163540065288544 relative L2 0.013825805857777596\n",
      "training 0.081367626786232 relative L2 0.013803328387439251\n",
      "training 0.0811019241809845 relative L2 0.013780992478132248\n",
      "training 0.08083826303482056 relative L2 0.013758772984147072\n",
      "training 0.08057643473148346 relative L2 0.013736694119870663\n",
      "training 0.08031676709651947 relative L2 0.013714744709432125\n",
      "training 0.08005889505147934 relative L2 0.013692913576960564\n",
      "training 0.07980305701494217 relative L2 0.0136712109670043\n",
      "training 0.07954908162355423 relative L2 0.013649635016918182\n",
      "training 0.07929706573486328 relative L2 0.013628206215798855\n",
      "training 0.07904703170061111 relative L2 0.013606918044388294\n",
      "training 0.0787990391254425 relative L2 0.013585753738880157\n",
      "training 0.07855295389890671 relative L2 0.01356472261250019\n",
      "training 0.07830879837274551 relative L2 0.013543796725571156\n",
      "training 0.07806630432605743 relative L2 0.013523025438189507\n",
      "training 0.07782594859600067 relative L2 0.01350231934338808\n",
      "training 0.07758679240942001 relative L2 0.013481725007295609\n",
      "training 0.0773494690656662 relative L2 0.013461248017847538\n",
      "training 0.07711411267518997 relative L2 0.013440894894301891\n",
      "training 0.07688050717115402 relative L2 0.013420633040368557\n",
      "training 0.07664822041988373 relative L2 0.013400498777627945\n",
      "training 0.07641785591840744 relative L2 0.013380494900047779\n",
      "training 0.07618921995162964 relative L2 0.013360577635467052\n",
      "training 0.075962133705616 relative L2 0.013340766541659832\n",
      "training 0.07573672384023666 relative L2 0.013321081176400185\n",
      "training 0.07551305741071701 relative L2 0.013301495462656021\n",
      "training 0.07529070973396301 relative L2 0.013282030820846558\n",
      "training 0.07506991177797318 relative L2 0.013262650929391384\n",
      "training 0.0748506411910057 relative L2 0.013243379071354866\n",
      "training 0.07463262230157852 relative L2 0.013224192894995213\n",
      "training 0.07441604882478714 relative L2 0.013205097056925297\n",
      "training 0.07420075684785843 relative L2 0.013186076655983925\n",
      "training 0.0739867240190506 relative L2 0.01316716056317091\n",
      "training 0.07377418875694275 relative L2 0.013148341327905655\n",
      "training 0.07356299459934235 relative L2 0.013129598461091518\n",
      "training 0.07335309684276581 relative L2 0.013110977597534657\n",
      "training 0.07314465939998627 relative L2 0.013092403300106525\n",
      "training 0.07293722778558731 relative L2 0.01307389885187149\n",
      "training 0.0727309137582779 relative L2 0.013055517338216305\n",
      "training 0.07252620160579681 relative L2 0.013037202879786491\n",
      "training 0.07232243567705154 relative L2 0.013018971309065819\n",
      "training 0.07211992889642715 relative L2 0.013000821694731712\n",
      "training 0.07191850244998932 relative L2 0.01298274751752615\n",
      "training 0.07171826809644699 relative L2 0.0129647646099329\n",
      "training 0.07151921838521957 relative L2 0.012946829199790955\n",
      "training 0.07132107019424438 relative L2 0.012928984127938747\n",
      "training 0.07112415134906769 relative L2 0.012911212630569935\n",
      "training 0.0709281861782074 relative L2 0.01289349514991045\n",
      "training 0.07073303312063217 relative L2 0.01287586148828268\n",
      "training 0.07053899765014648 relative L2 0.012858283706009388\n",
      "training 0.070345938205719 relative L2 0.012840768322348595\n",
      "training 0.07015381008386612 relative L2 0.012823297642171383\n",
      "training 0.0699625238776207 relative L2 0.012805885635316372\n",
      "training 0.06977209448814392 relative L2 0.012788536958396435\n",
      "training 0.06958264112472534 relative L2 0.01277125719934702\n",
      "training 0.0693940594792366 relative L2 0.012754027731716633\n",
      "training 0.0692063644528389 relative L2 0.012736869044601917\n",
      "training 0.0690196305513382 relative L2 0.01271976437419653\n",
      "training 0.06883367151021957 relative L2 0.012702721171081066\n",
      "training 0.06864858418703079 relative L2 0.012685708701610565\n",
      "training 0.06846418231725693 relative L2 0.012668758630752563\n",
      "training 0.06828074157238007 relative L2 0.012651842087507248\n",
      "training 0.06809800863265991 relative L2 0.0126350037753582\n",
      "training 0.06791635602712631 relative L2 0.012618194334208965\n",
      "training 0.06773536652326584 relative L2 0.01260143518447876\n",
      "training 0.06755498796701431 relative L2 0.012584715150296688\n",
      "training 0.06737548857927322 relative L2 0.0125680360943079\n",
      "training 0.06719662249088287 relative L2 0.012551391497254372\n",
      "training 0.0670185536146164 relative L2 0.01253479439765215\n",
      "training 0.06684122234582901 relative L2 0.012518234550952911\n",
      "training 0.06666456162929535 relative L2 0.012501722201704979\n",
      "training 0.06648866087198257 relative L2 0.012485233135521412\n",
      "training 0.06631329655647278 relative L2 0.012468783184885979\n",
      "training 0.06613846123218536 relative L2 0.01245237234979868\n",
      "training 0.06596435606479645 relative L2 0.012436010874807835\n",
      "training 0.06579088419675827 relative L2 0.01241967175155878\n",
      "training 0.06561800837516785 relative L2 0.012403368018567562\n",
      "training 0.06544563919305801 relative L2 0.01238708384335041\n",
      "training 0.06527400761842728 relative L2 0.012370864860713482\n",
      "training 0.06510304659605026 relative L2 0.012354640290141106\n",
      "training 0.06493263691663742 relative L2 0.01233847439289093\n",
      "training 0.06476268172264099 relative L2 0.012322315946221352\n",
      "training 0.06459347158670425 relative L2 0.012306232936680317\n",
      "training 0.06442470848560333 relative L2 0.012290103361010551\n",
      "training 0.06425627321004868 relative L2 0.012274076230823994\n",
      "training 0.06408843398094177 relative L2 0.012258006259799004\n",
      "training 0.06392116099596024 relative L2 0.012242044322192669\n",
      "training 0.06375422328710556 relative L2 0.012225992977619171\n",
      "training 0.06358781456947327 relative L2 0.012210105545818806\n",
      "training 0.06342196464538574 relative L2 0.012194078415632248\n",
      "training 0.06325677782297134 relative L2 0.01217828318476677\n",
      "training 0.06309197098016739 relative L2 0.01216222159564495\n",
      "training 0.06292770802974701 relative L2 0.01214661542326212\n",
      "training 0.06276436150074005 relative L2 0.01213050726801157\n",
      "training 0.06260129064321518 relative L2 0.01211514137685299\n",
      "training 0.06243904307484627 relative L2 0.012098941020667553\n",
      "training 0.06227763369679451 relative L2 0.012084043584764004\n",
      "training 0.062117330729961395 relative L2 0.012067689560353756\n",
      "training 0.06195911392569542 relative L2 0.012053861282765865\n",
      "training 0.06180432438850403 relative L2 0.012037637643516064\n",
      "training 0.06165577843785286 relative L2 0.012026483193039894\n",
      "training 0.06151768937706947 relative L2 0.012011619284749031\n",
      "training 0.06139766424894333 relative L2 0.012006416916847229\n",
      "training 0.06130210682749748 relative L2 0.011994539760053158\n",
      "training 0.061234887689352036 relative L2 0.011994676664471626\n",
      "training 0.061170388013124466 relative L2 0.01197634544223547\n",
      "training 0.06105668842792511 relative L2 0.01196108479052782\n",
      "training 0.06083023175597191 relative L2 0.011924750171601772\n",
      "training 0.060518424957990646 relative L2 0.011900407262146473\n",
      "training 0.06024068221449852 relative L2 0.011885379441082478\n",
      "training 0.06008869782090187 relative L2 0.01187665481120348\n",
      "training 0.060027770698070526 relative L2 0.011872985400259495\n",
      "training 0.05994493141770363 relative L2 0.011850366368889809\n",
      "training 0.05976521223783493 relative L2 0.01183018647134304\n",
      "training 0.059524744749069214 relative L2 0.011808814480900764\n",
      "training 0.059324875473976135 relative L2 0.011796131730079651\n",
      "training 0.05920674651861191 relative L2 0.011789826676249504\n",
      "training 0.059114452451467514 relative L2 0.011772125028073788\n",
      "training 0.05897356942296028 relative L2 0.011755818501114845\n",
      "training 0.05877889320254326 relative L2 0.011735329404473305\n",
      "training 0.05859069153666496 relative L2 0.011721150949597359\n",
      "training 0.05845227465033531 relative L2 0.011711875908076763\n",
      "training 0.058340512216091156 relative L2 0.011695594526827335\n",
      "training 0.058206018060445786 relative L2 0.01168103702366352\n",
      "training 0.05803612247109413 relative L2 0.01166216004639864\n",
      "training 0.057863544672727585 relative L2 0.011647585779428482\n",
      "training 0.057719551026821136 relative L2 0.011636372655630112\n",
      "training 0.05759511515498161 relative L2 0.011620637960731983\n",
      "training 0.05746031925082207 relative L2 0.011606823652982712\n",
      "training 0.057303912937641144 relative L2 0.011589248664677143\n",
      "training 0.05714309588074684 relative L2 0.01157461665570736\n",
      "training 0.05699784681200981 relative L2 0.011562123894691467\n",
      "training 0.05686585232615471 relative L2 0.011546730063855648\n",
      "training 0.05673021823167801 relative L2 0.011533214710652828\n",
      "training 0.05658187344670296 relative L2 0.011516516096889973\n",
      "training 0.056429069489240646 relative L2 0.011501934379339218\n",
      "training 0.056284099817276 relative L2 0.011488610878586769\n",
      "training 0.05614827573299408 relative L2 0.01147346943616867\n",
      "training 0.05601206049323082 relative L2 0.01146004069596529\n",
      "training 0.055868860334157944 relative L2 0.01144392043352127\n",
      "training 0.05572149157524109 relative L2 0.011429443024098873\n",
      "training 0.05557764694094658 relative L2 0.011415490880608559\n",
      "training 0.055439598858356476 relative L2 0.011400561779737473\n",
      "training 0.055303044617176056 relative L2 0.011387075297534466\n",
      "training 0.055163271725177765 relative L2 0.011371402069926262\n",
      "training 0.05502020940184593 relative L2 0.011357082054018974\n",
      "training 0.05487808585166931 relative L2 0.011342709884047508\n",
      "training 0.05473927780985832 relative L2 0.011327954940497875\n",
      "training 0.054602254182100296 relative L2 0.011314311064779758\n",
      "training 0.05446416884660721 relative L2 0.01129898801445961\n",
      "training 0.054324109107255936 relative L2 0.011284791864454746\n",
      "training 0.05418359115719795 relative L2 0.011270127259194851\n",
      "training 0.054044634103775024 relative L2 0.01125552598387003\n",
      "training 0.053907547146081924 relative L2 0.011241658590734005\n",
      "training 0.05377078428864479 relative L2 0.011226605623960495\n",
      "training 0.0536329485476017 relative L2 0.01121252216398716\n",
      "training 0.05349447950720787 relative L2 0.011197707615792751\n",
      "training 0.05335628613829613 relative L2 0.011183244176208973\n",
      "training 0.05321915075182915 relative L2 0.01116910669952631\n",
      "training 0.0530826635658741 relative L2 0.011154276318848133\n",
      "training 0.052946120500564575 relative L2 0.011140226386487484\n",
      "training 0.05280926078557968 relative L2 0.01112534198909998\n",
      "training 0.052671853452920914 relative L2 0.011110968887805939\n",
      "training 0.052534863352775574 relative L2 0.011096542701125145\n",
      "training 0.05239851772785187 relative L2 0.011081867851316929\n",
      "training 0.05226242542266846 relative L2 0.011067728511989117\n",
      "training 0.05212659016251564 relative L2 0.011052940972149372\n",
      "training 0.0519905760884285 relative L2 0.011038665659725666\n",
      "training 0.0518544502556324 relative L2 0.011024080216884613\n",
      "training 0.05171859264373779 relative L2 0.011009580455720425\n",
      "training 0.051583148539066315 relative L2 0.010995293967425823\n",
      "training 0.051447831094264984 relative L2 0.010980572551488876\n",
      "training 0.0513124093413353 relative L2 0.010966324247419834\n",
      "training 0.0511772558093071 relative L2 0.010951600037515163\n",
      "training 0.05104181170463562 relative L2 0.010937179438769817\n",
      "training 0.05090682581067085 relative L2 0.010922674089670181\n",
      "training 0.05077192187309265 relative L2 0.010908054187893867\n",
      "training 0.050637245178222656 relative L2 0.010893715545535088\n",
      "training 0.05050274729728699 relative L2 0.010878977365791798\n",
      "training 0.05036802962422371 relative L2 0.010864535346627235\n",
      "training 0.05023328959941864 relative L2 0.010849847458302975\n",
      "training 0.05009879916906357 relative L2 0.010835259221494198\n",
      "training 0.04996449500322342 relative L2 0.01082069706171751\n",
      "training 0.04983038827776909 relative L2 0.010805926285684109\n",
      "training 0.049696266651153564 relative L2 0.010791373439133167\n",
      "training 0.04956212267279625 relative L2 0.01077655702829361\n",
      "training 0.04942813515663147 relative L2 0.010761968791484833\n",
      "training 0.04929421842098236 relative L2 0.01074725016951561\n",
      "training 0.04916055500507355 relative L2 0.010732571594417095\n",
      "training 0.04902690649032593 relative L2 0.01071794144809246\n",
      "training 0.0488932803273201 relative L2 0.010703179985284805\n",
      "training 0.04875976964831352 relative L2 0.010688584297895432\n",
      "training 0.04862616956233978 relative L2 0.010673796758055687\n",
      "training 0.04849265515804291 relative L2 0.010659152641892433\n",
      "training 0.048359159380197525 relative L2 0.010644386522471905\n",
      "training 0.04822571203112602 relative L2 0.01062963530421257\n",
      "training 0.04809233173727989 relative L2 0.010614887811243534\n",
      "training 0.04795902594923973 relative L2 0.010600035078823566\n",
      "training 0.04782579094171524 relative L2 0.010585298761725426\n",
      "training 0.047692570835351944 relative L2 0.010570396669209003\n",
      "training 0.04755937308073044 relative L2 0.010555586777627468\n",
      "training 0.047426190227270126 relative L2 0.010540656745433807\n",
      "training 0.047293100506067276 relative L2 0.010525754652917385\n",
      "training 0.047160036861896515 relative L2 0.010510839521884918\n",
      "training 0.047026973217725754 relative L2 0.010495868511497974\n",
      "training 0.04689381644129753 relative L2 0.010480967350304127\n",
      "training 0.04676072299480438 relative L2 0.010465964674949646\n",
      "training 0.046627532690763474 relative L2 0.010451036505401134\n",
      "training 0.046494413167238235 relative L2 0.010436021722853184\n",
      "training 0.04636138677597046 relative L2 0.010421046987175941\n",
      "training 0.046228330582380295 relative L2 0.010406010784208775\n",
      "training 0.04609518498182297 relative L2 0.010390953160822392\n",
      "training 0.04596185311675072 relative L2 0.010375915095210075\n",
      "training 0.04582861065864563 relative L2 0.01036082487553358\n",
      "training 0.045695360749959946 relative L2 0.010345772840082645\n",
      "training 0.04556198790669441 relative L2 0.010330632328987122\n",
      "training 0.04542861133813858 relative L2 0.010315514169633389\n",
      "training 0.04529513418674469 relative L2 0.010300331749022007\n",
      "training 0.04516168683767319 relative L2 0.01028513815253973\n",
      "training 0.04502817243337631 relative L2 0.01026991382241249\n",
      "training 0.04489472508430481 relative L2 0.010254655964672565\n",
      "training 0.04476122930645943 relative L2 0.010239353403449059\n",
      "training 0.044627558439970016 relative L2 0.01022398378700018\n",
      "training 0.04449399188160896 relative L2 0.01020863838493824\n",
      "training 0.04436047747731209 relative L2 0.01019321195781231\n",
      "training 0.04422678425908089 relative L2 0.010177831165492535\n",
      "training 0.04409318417310715 relative L2 0.010162359103560448\n",
      "training 0.043959394097328186 relative L2 0.010146908462047577\n",
      "training 0.043825410306453705 relative L2 0.010131393559277058\n",
      "training 0.0436914786696434 relative L2 0.010115901939570904\n",
      "training 0.04355752840638161 relative L2 0.010100345127284527\n",
      "training 0.04342363029718399 relative L2 0.010084751062095165\n",
      "training 0.043289534747600555 relative L2 0.010069116950035095\n",
      "training 0.04315539076924324 relative L2 0.010053453035652637\n",
      "training 0.0430213063955307 relative L2 0.010037780739367008\n",
      "training 0.04288708046078682 relative L2 0.010022040456533432\n",
      "training 0.04275282099843025 relative L2 0.010006319731473923\n",
      "training 0.042618654668331146 relative L2 0.009990492835640907\n",
      "training 0.04248428717255592 relative L2 0.009974687360227108\n",
      "training 0.0423496775329113 relative L2 0.009958812966942787\n",
      "training 0.042215168476104736 relative L2 0.009942973032593727\n",
      "training 0.04208078607916832 relative L2 0.00992705300450325\n",
      "training 0.04194633290171623 relative L2 0.00991115253418684\n",
      "training 0.041811831295490265 relative L2 0.009895178489387035\n",
      "training 0.04167715087532997 relative L2 0.009879212826490402\n",
      "training 0.04154220595955849 relative L2 0.009863201528787613\n",
      "training 0.04140743985772133 relative L2 0.009847179055213928\n",
      "training 0.04127238318324089 relative L2 0.009831071831285954\n",
      "training 0.041137389838695526 relative L2 0.009815003722906113\n",
      "training 0.04100235551595688 relative L2 0.009798841550946236\n",
      "training 0.04086725786328316 relative L2 0.009782732464373112\n",
      "training 0.04073223099112511 relative L2 0.009766521863639355\n",
      "training 0.04059707745909691 relative L2 0.009750344790518284\n",
      "training 0.04046177491545677 relative L2 0.009734070859849453\n",
      "training 0.04032636061310768 relative L2 0.009717842563986778\n",
      "training 0.04019083082675934 relative L2 0.009701511822640896\n",
      "training 0.04005530849099159 relative L2 0.009685203433036804\n",
      "training 0.03991978242993355 relative L2 0.009668800048530102\n",
      "training 0.03978433459997177 relative L2 0.009652411565184593\n",
      "training 0.03964882344007492 relative L2 0.009635895490646362\n",
      "training 0.03951316699385643 relative L2 0.009619449265301228\n",
      "training 0.039377614855766296 relative L2 0.009602885693311691\n",
      "training 0.03924202919006348 relative L2 0.009586398489773273\n",
      "training 0.03910653293132782 relative L2 0.009569751098752022\n",
      "training 0.0389709509909153 relative L2 0.009553219191730022\n",
      "training 0.038835253566503525 relative L2 0.009536491706967354\n",
      "training 0.03869988024234772 relative L2 0.009519953280687332\n",
      "training 0.038564421236515045 relative L2 0.009503121487796307\n",
      "training 0.03842909261584282 relative L2 0.009486584924161434\n",
      "training 0.03829360753297806 relative L2 0.009469586424529552\n",
      "training 0.0381583534181118 relative L2 0.009453133679926395\n",
      "training 0.03802339360117912 relative L2 0.009435958229005337\n",
      "training 0.037888478487730026 relative L2 0.009419680573046207\n",
      "training 0.03775399923324585 relative L2 0.009402396157383919\n",
      "training 0.03762039914727211 relative L2 0.009386611171066761\n",
      "training 0.037487976253032684 relative L2 0.009369255043566227\n",
      "training 0.037357378751039505 relative L2 0.009354510344564915\n",
      "training 0.0372299998998642 relative L2 0.00933755747973919\n",
      "training 0.03710775077342987 relative L2 0.009325250051915646\n",
      "training 0.03699394315481186 relative L2 0.009309894405305386\n",
      "training 0.0368928499519825 relative L2 0.009302794933319092\n",
      "training 0.036810800433158875 relative L2 0.009291253052651882\n",
      "training 0.036751750856637955 relative L2 0.009291214868426323\n",
      "training 0.036712490022182465 relative L2 0.009279776364564896\n",
      "training 0.03666717931628227 relative L2 0.009273779578506947\n",
      "training 0.03657103329896927 relative L2 0.009242353960871696\n",
      "training 0.03637094050645828 relative L2 0.009211178869009018\n",
      "training 0.036086615175008774 relative L2 0.009171880781650543\n",
      "training 0.03580499067902565 relative L2 0.00914947222918272\n",
      "training 0.03562195971608162 relative L2 0.009141189977526665\n",
      "training 0.03554774075746536 relative L2 0.009133529849350452\n",
      "training 0.03551042824983597 relative L2 0.009126117452979088\n",
      "training 0.03542247787117958 relative L2 0.00909972470253706\n",
      "training 0.035247042775154114 relative L2 0.009074767120182514\n",
      "training 0.03503169119358063 relative L2 0.00905047170817852\n",
      "training 0.03485362231731415 relative L2 0.00903599988669157\n",
      "training 0.034745391458272934 relative L2 0.009028289467096329\n",
      "training 0.03467052802443504 relative L2 0.009012483060359955\n",
      "training 0.034569766372442245 relative L2 0.008995605632662773\n",
      "training 0.03441895917057991 relative L2 0.00897086039185524\n",
      "training 0.034245092421770096 relative L2 0.008951983414590359\n",
      "training 0.03409470617771149 relative L2 0.00893787294626236\n",
      "training 0.033983342349529266 relative L2 0.00892370380461216\n",
      "training 0.03388712927699089 relative L2 0.008910751901566982\n",
      "training 0.03377300873398781 relative L2 0.008890029974281788\n",
      "training 0.03363131359219551 relative L2 0.008871561847627163\n",
      "training 0.03348159044981003 relative L2 0.008853297680616379\n",
      "training 0.03334758058190346 relative L2 0.00883768405765295\n",
      "training 0.03323423117399216 relative L2 0.008824513293802738\n",
      "training 0.033126335591077805 relative L2 0.008807005360722542\n",
      "training 0.03300703316926956 relative L2 0.008790596388280392\n",
      "training 0.03287392482161522 relative L2 0.008771457709372044\n",
      "training 0.032738491892814636 relative L2 0.008754773996770382\n",
      "training 0.032612770795822144 relative L2 0.008739682845771313\n",
      "training 0.032497722655534744 relative L2 0.008723459206521511\n",
      "training 0.03238482028245926 relative L2 0.008708489127457142\n",
      "training 0.03226568177342415 relative L2 0.008690297603607178\n",
      "training 0.032139454036951065 relative L2 0.008673765696585178\n",
      "training 0.03201254457235336 relative L2 0.00865702610462904\n",
      "training 0.03189093619585037 relative L2 0.008640938438475132\n",
      "training 0.031775232404470444 relative L2 0.008626045659184456\n",
      "training 0.0316610261797905 relative L2 0.008609157055616379\n",
      "training 0.03154386952519417 relative L2 0.008593453094363213\n",
      "training 0.03142315149307251 relative L2 0.0085762245580554\n",
      "training 0.031301792711019516 relative L2 0.008560126647353172\n",
      "training 0.031183162704110146 relative L2 0.00854441151022911\n",
      "training 0.031067978590726852 relative L2 0.008528250269591808\n",
      "training 0.030954252928495407 relative L2 0.008513083681464195\n",
      "training 0.03083989955484867 relative L2 0.008496362715959549\n",
      "training 0.030723826959729195 relative L2 0.008480685763061047\n",
      "training 0.03060709685087204 relative L2 0.008464379236102104\n",
      "training 0.03049161098897457 relative L2 0.008448593318462372\n",
      "training 0.03037811629474163 relative L2 0.008433259092271328\n",
      "training 0.030266162008047104 relative L2 0.008417257107794285\n",
      "training 0.030154403299093246 relative L2 0.00840207003057003\n",
      "training 0.03004218265414238 relative L2 0.008385883644223213\n",
      "training 0.029929637908935547 relative L2 0.008370460011065006\n",
      "training 0.029817186295986176 relative L2 0.008354696445167065\n",
      "training 0.029705682769417763 relative L2 0.008339177817106247\n",
      "training 0.02959561161696911 relative L2 0.008324034512043\n",
      "training 0.02948642522096634 relative L2 0.008308377116918564\n",
      "training 0.029377641156315804 relative L2 0.008293374441564083\n",
      "training 0.029268911108374596 relative L2 0.008277668617665768\n",
      "training 0.029160242527723312 relative L2 0.008262552320957184\n",
      "training 0.029051991179585457 relative L2 0.008247176185250282\n",
      "training 0.02894449792802334 relative L2 0.008231974206864834\n",
      "training 0.028837846592068672 relative L2 0.008217002265155315\n",
      "training 0.02873186022043228 relative L2 0.008201715536415577\n",
      "training 0.028626449406147003 relative L2 0.00818693358451128\n",
      "training 0.028521498665213585 relative L2 0.00817165244370699\n",
      "training 0.028416741639375687 relative L2 0.008156849071383476\n",
      "training 0.028312543407082558 relative L2 0.008141791447997093\n",
      "training 0.028208857402205467 relative L2 0.008126940578222275\n",
      "training 0.028105899691581726 relative L2 0.008112193085253239\n",
      "training 0.0280035100877285 relative L2 0.008097353391349316\n",
      "training 0.02790175750851631 relative L2 0.008082836866378784\n",
      "training 0.027800533920526505 relative L2 0.00806803721934557\n",
      "training 0.027699869126081467 relative L2 0.00805361196398735\n",
      "training 0.027599645778536797 relative L2 0.008038940839469433\n",
      "training 0.027499882504343987 relative L2 0.00802454724907875\n",
      "training 0.027400724589824677 relative L2 0.008010081946849823\n",
      "training 0.027301928028464317 relative L2 0.007995715364813805\n",
      "training 0.027203863486647606 relative L2 0.007981497794389725\n",
      "training 0.02710629627108574 relative L2 0.007967167533934116\n",
      "training 0.027009261772036552 relative L2 0.007953105494379997\n",
      "training 0.02691274881362915 relative L2 0.007938875816762447\n",
      "training 0.02681688405573368 relative L2 0.007924910634756088\n",
      "training 0.026721514761447906 relative L2 0.007910811342298985\n",
      "training 0.026626674458384514 relative L2 0.007896906696259975\n",
      "training 0.02653244137763977 relative L2 0.007882986217737198\n",
      "training 0.02643858827650547 relative L2 0.007869117893278599\n",
      "training 0.026345286518335342 relative L2 0.007855350151658058\n",
      "training 0.026252392679452896 relative L2 0.007841513492166996\n",
      "training 0.026160065084695816 relative L2 0.00782788172364235\n",
      "training 0.026068340986967087 relative L2 0.007814154028892517\n",
      "training 0.02597709931433201 relative L2 0.007800639607012272\n",
      "training 0.025886503979563713 relative L2 0.007787065580487251\n",
      "training 0.02579648420214653 relative L2 0.007773631252348423\n",
      "training 0.025707054883241653 relative L2 0.007760209962725639\n",
      "training 0.025618135929107666 relative L2 0.007746829651296139\n",
      "training 0.02552974782884121 relative L2 0.007733566220849752\n",
      "training 0.025441879406571388 relative L2 0.007720273919403553\n",
      "training 0.02535443753004074 relative L2 0.007707146927714348\n",
      "training 0.025267623364925385 relative L2 0.007693964522331953\n",
      "training 0.025181351229548454 relative L2 0.007680939976125956\n",
      "training 0.025095485150814056 relative L2 0.007667879108339548\n",
      "training 0.025010239332914352 relative L2 0.007654985412955284\n",
      "training 0.02492550201714039 relative L2 0.0076420484110713005\n",
      "training 0.024841342121362686 relative L2 0.007629224099218845\n",
      "training 0.024757565930485725 relative L2 0.007616408634930849\n",
      "training 0.024674389511346817 relative L2 0.007603664882481098\n",
      "training 0.02459162287414074 relative L2 0.007590951398015022\n",
      "training 0.024509331211447716 relative L2 0.007578257471323013\n",
      "training 0.02442736178636551 relative L2 0.007565650157630444\n",
      "training 0.02434593252837658 relative L2 0.007553035859018564\n",
      "training 0.02426481433212757 relative L2 0.007540528662502766\n",
      "training 0.02418423816561699 relative L2 0.007528012618422508\n",
      "training 0.024104135110974312 relative L2 0.007515615317970514\n",
      "training 0.02402440831065178 relative L2 0.007503206841647625\n",
      "training 0.02394525334239006 relative L2 0.007490950170904398\n",
      "training 0.023866605013608932 relative L2 0.007478651124984026\n",
      "training 0.023788416758179665 relative L2 0.007466523442417383\n",
      "training 0.02371085435152054 relative L2 0.007454357109963894\n",
      "training 0.02363370731472969 relative L2 0.007442345842719078\n",
      "training 0.023557055741548538 relative L2 0.007430271711200476\n",
      "training 0.023480771109461784 relative L2 0.00741835730150342\n",
      "training 0.023404985666275024 relative L2 0.0074063874781131744\n",
      "training 0.02332952618598938 relative L2 0.007394556887447834\n",
      "training 0.023254547268152237 relative L2 0.0073826988227665424\n",
      "training 0.023180002346634865 relative L2 0.007370927836745977\n",
      "training 0.023105673491954803 relative L2 0.00735915033146739\n",
      "training 0.023031869903206825 relative L2 0.0073474859818816185\n",
      "training 0.022958416491746902 relative L2 0.007335805334150791\n",
      "training 0.022885428741574287 relative L2 0.007324245758354664\n",
      "training 0.022812845185399055 relative L2 0.007312643341720104\n",
      "training 0.022740565240383148 relative L2 0.007301162928342819\n",
      "training 0.02266876958310604 relative L2 0.0072896783240139484\n",
      "training 0.02259749546647072 relative L2 0.0072782887145876884\n",
      "training 0.02252649888396263 relative L2 0.00726689537987113\n",
      "training 0.022456027567386627 relative L2 0.007255597040057182\n",
      "training 0.022385818883776665 relative L2 0.007244282402098179\n",
      "training 0.022316083312034607 relative L2 0.007233080454170704\n",
      "training 0.022246651351451874 relative L2 0.007221825886517763\n",
      "training 0.0221775621175766 relative L2 0.0072107273153960705\n",
      "training 0.022108905017375946 relative L2 0.00719955051317811\n",
      "training 0.022040581330657005 relative L2 0.007188568823039532\n",
      "training 0.021972723305225372 relative L2 0.007177455350756645\n",
      "training 0.021905099973082542 relative L2 0.00716658728197217\n",
      "training 0.021837951615452766 relative L2 0.007155534811317921\n",
      "training 0.02177112177014351 relative L2 0.007144796196371317\n",
      "training 0.021704716607928276 relative L2 0.0071337842382490635\n",
      "training 0.021638641133904457 relative L2 0.007123217452317476\n",
      "training 0.02157307043671608 relative L2 0.007112211547791958\n",
      "training 0.021507762372493744 relative L2 0.0071018412709236145\n",
      "training 0.021443026140332222 relative L2 0.00709083816036582\n",
      "training 0.021378735080361366 relative L2 0.0070808036252856255\n",
      "training 0.02131517231464386 relative L2 0.00706979725509882\n",
      "training 0.021252376958727837 relative L2 0.0070603350177407265\n",
      "training 0.021190643310546875 relative L2 0.007049364037811756\n",
      "training 0.021130429580807686 relative L2 0.007040996104478836\n",
      "training 0.02107256092131138 relative L2 0.0070304060354828835\n",
      "training 0.021018462255597115 relative L2 0.007024449296295643\n",
      "training 0.020970318466424942 relative L2 0.007015392649918795\n",
      "training 0.020931625738739967 relative L2 0.007014880422502756\n",
      "training 0.02090812474489212 relative L2 0.007010373752564192\n",
      "training 0.020906461402773857 relative L2 0.007020515855401754\n",
      "training 0.020934415981173515 relative L2 0.007023133337497711\n",
      "training 0.0209890715777874 relative L2 0.007041802629828453\n",
      "training 0.02105364017188549 relative L2 0.007035135757178068\n",
      "training 0.021065179258584976 relative L2 0.007027217652648687\n",
      "training 0.020965153351426125 relative L2 0.0069793169386684895\n",
      "training 0.020726468414068222 relative L2 0.006938815116882324\n",
      "training 0.02045387215912342 relative L2 0.006907210685312748\n",
      "training 0.020282326266169548 relative L2 0.0069034527987241745\n",
      "training 0.020265230908989906 relative L2 0.006918214727193117\n",
      "training 0.020329609513282776 relative L2 0.006916341371834278\n",
      "training 0.020350316539406776 relative L2 0.006906525697559118\n",
      "training 0.020258786156773567 relative L2 0.006873077247291803\n",
      "training 0.0200897715985775 relative L2 0.006851791404187679\n",
      "training 0.019951678812503815 relative L2 0.006844419054687023\n",
      "training 0.019906895235180855 relative L2 0.006844117771834135\n",
      "training 0.019919931888580322 relative L2 0.006846184842288494\n",
      "training 0.019908521324396133 relative L2 0.006828182376921177\n",
      "training 0.019827812910079956 relative L2 0.006811164319515228\n",
      "training 0.01971079222857952 relative L2 0.006794307846575975\n",
      "training 0.019620954990386963 relative L2 0.006787370890378952\n",
      "training 0.01958446390926838 relative L2 0.006787224672734737\n",
      "training 0.019570183008909225 relative L2 0.006777340546250343\n",
      "training 0.01953084208071232 relative L2 0.006767014507204294\n",
      "training 0.01945417933166027 relative L2 0.006750030443072319\n",
      "training 0.01936778612434864 relative L2 0.006739655043929815\n",
      "training 0.019305098801851273 relative L2 0.006734421942383051\n",
      "training 0.01926949806511402 relative L2 0.006726793944835663\n",
      "training 0.01923702284693718 relative L2 0.006720233242958784\n",
      "training 0.019185759127140045 relative L2 0.006706148851662874\n",
      "training 0.019116850569844246 relative L2 0.006695537827908993\n",
      "training 0.019049614667892456 relative L2 0.0066864993423223495\n",
      "training 0.018998194485902786 relative L2 0.006678587757050991\n",
      "training 0.018958698958158493 relative L2 0.006672819145023823\n",
      "training 0.01891646720468998 relative L2 0.006661592982709408\n",
      "training 0.018862687051296234 relative L2 0.006652286276221275\n",
      "training 0.018802018836140633 relative L2 0.006641570013016462\n",
      "training 0.018745075911283493 relative L2 0.006632919888943434\n",
      "training 0.018697084859013557 relative L2 0.00662618363276124\n",
      "training 0.018654007464647293 relative L2 0.0066167558543384075\n",
      "training 0.01860797591507435 relative L2 0.006608809810131788\n",
      "training 0.01855587400496006 relative L2 0.0065981014631688595\n",
      "training 0.018501034006476402 relative L2 0.006589251104742289\n",
      "training 0.01844898797571659 relative L2 0.006581051275134087\n",
      "training 0.01840183511376381 relative L2 0.006572330836206675\n",
      "training 0.018356796354055405 relative L2 0.0065649389289319515\n",
      "training 0.018310027197003365 relative L2 0.0065550729632377625\n",
      "training 0.018260162323713303 relative L2 0.0065466370433568954\n",
      "training 0.018209118396043777 relative L2 0.006537462584674358\n",
      "training 0.01815962605178356 relative L2 0.006528893485665321\n",
      "training 0.01811264641582966 relative L2 0.006521180272102356\n",
      "training 0.018066944554448128 relative L2 0.006512131076306105\n",
      "training 0.01802057772874832 relative L2 0.006504209712147713\n",
      "training 0.017972640693187714 relative L2 0.006494854111224413\n",
      "training 0.017923913896083832 relative L2 0.006486458703875542\n",
      "training 0.01787590980529785 relative L2 0.006478061433881521\n",
      "training 0.017829308286309242 relative L2 0.006469454616308212\n",
      "training 0.017783580347895622 relative L2 0.00646164221689105\n",
      "training 0.01773778162896633 relative L2 0.006452650297433138\n",
      "training 0.017691323533654213 relative L2 0.006444577593356371\n",
      "training 0.017644377425312996 relative L2 0.006435743533074856\n",
      "training 0.017597492784261703 relative L2 0.006427398417145014\n",
      "training 0.017551373690366745 relative L2 0.006419248413294554\n",
      "training 0.017505943775177002 relative L2 0.006410673260688782\n",
      "training 0.0174607764929533 relative L2 0.00640275189653039\n",
      "training 0.017415370792150497 relative L2 0.006394002586603165\n",
      "training 0.017369724810123444 relative L2 0.0063858903013169765\n",
      "training 0.01732400804758072 relative L2 0.006377403624355793\n",
      "training 0.017278600484132767 relative L2 0.006369117647409439\n",
      "training 0.017233723774552345 relative L2 0.006361039355397224\n",
      "training 0.017189042642712593 relative L2 0.006352555472403765\n",
      "training 0.01714446023106575 relative L2 0.006344594061374664\n",
      "training 0.017099864780902863 relative L2 0.006336074322462082\n",
      "training 0.01705523580312729 relative L2 0.006327993236482143\n",
      "training 0.017010681331157684 relative L2 0.006319696083664894\n",
      "training 0.01696638949215412 relative L2 0.006311468314379454\n",
      "training 0.01692231185734272 relative L2 0.006303426343947649\n",
      "training 0.016878435388207436 relative L2 0.006295088678598404\n",
      "training 0.016834693029522896 relative L2 0.006287135183811188\n",
      "training 0.016790954396128654 relative L2 0.006278772372752428\n",
      "training 0.01674724370241165 relative L2 0.006270754151046276\n",
      "training 0.01670362986624241 relative L2 0.006262544076889753\n",
      "training 0.016660209745168686 relative L2 0.006254424341022968\n",
      "training 0.01661694049835205 relative L2 0.006246390752494335\n",
      "training 0.016573816537857056 relative L2 0.006238189060240984\n",
      "training 0.016530834138393402 relative L2 0.006230246741324663\n",
      "training 0.016487931832671165 relative L2 0.006222038064152002\n",
      "training 0.01644512638449669 relative L2 0.006214078050106764\n",
      "training 0.01640240103006363 relative L2 0.006205955985933542\n",
      "training 0.016359815374016762 relative L2 0.00619793264195323\n",
      "training 0.01631733775138855 relative L2 0.0061899395659565926\n",
      "training 0.01627504639327526 relative L2 0.006181865464895964\n",
      "training 0.016232848167419434 relative L2 0.006173953879624605\n",
      "training 0.01619076542556286 relative L2 0.006165869068354368\n",
      "training 0.01614878699183464 relative L2 0.006157961208373308\n",
      "training 0.016106825321912766 relative L2 0.006149915978312492\n",
      "training 0.016065053641796112 relative L2 0.00614200159907341\n",
      "training 0.016023404896259308 relative L2 0.006134027149528265\n",
      "training 0.01598181203007698 relative L2 0.006126069463789463\n",
      "training 0.0159403458237648 relative L2 0.006118173245340586\n",
      "training 0.015898991376161575 relative L2 0.0061102076433598995\n",
      "training 0.015857810154557228 relative L2 0.006102345418184996\n",
      "training 0.015816615894436836 relative L2 0.0060943858698010445\n",
      "training 0.015775609761476517 relative L2 0.0060865445993840694\n",
      "training 0.01573467068374157 relative L2 0.006078616250306368\n",
      "training 0.015693845227360725 relative L2 0.006070774979889393\n",
      "training 0.01565314084291458 relative L2 0.006062903441488743\n",
      "training 0.01561256218701601 relative L2 0.006055046338587999\n",
      "training 0.015572055242955685 relative L2 0.006047219969332218\n",
      "training 0.015531640499830246 relative L2 0.006039354018867016\n",
      "training 0.015491344034671783 relative L2 0.006031575612723827\n",
      "training 0.015451164916157722 relative L2 0.006023708265274763\n",
      "training 0.015411031432449818 relative L2 0.006015947088599205\n",
      "training 0.015371030196547508 relative L2 0.006008119788020849\n",
      "training 0.015331174246966839 relative L2 0.006000361870974302\n",
      "training 0.01529136672616005 relative L2 0.00599255645647645\n",
      "training 0.015251635573804379 relative L2 0.00598479900509119\n",
      "training 0.015212043188512325 relative L2 0.005977048072963953\n",
      "training 0.015172579325735569 relative L2 0.0059692757204174995\n",
      "training 0.015133129432797432 relative L2 0.005961564369499683\n",
      "training 0.01509386207908392 relative L2 0.005953797604888678\n",
      "training 0.015054631978273392 relative L2 0.005946113727986813\n",
      "training 0.015015535056591034 relative L2 0.005938360933214426\n",
      "training 0.014976510778069496 relative L2 0.005930699408054352\n",
      "training 0.014937636442482471 relative L2 0.005922975018620491\n",
      "training 0.014898818917572498 relative L2 0.005915307905524969\n",
      "training 0.014860065653920174 relative L2 0.0059076156467199326\n",
      "training 0.014821442775428295 relative L2 0.0058999634347856045\n",
      "training 0.014782939106225967 relative L2 0.00589230190962553\n",
      "training 0.014744492247700691 relative L2 0.005884641315788031\n",
      "training 0.014706119894981384 relative L2 0.0058770147152245045\n",
      "training 0.014667884446680546 relative L2 0.005869363900274038\n",
      "training 0.014629718847572803 relative L2 0.005861756391823292\n",
      "training 0.014591625891625881 relative L2 0.005854129791259766\n",
      "training 0.014553723856806755 relative L2 0.005846546497195959\n",
      "training 0.014515820890665054 relative L2 0.005838924553245306\n",
      "training 0.014478061348199844 relative L2 0.0058313640765845776\n",
      "training 0.01444035954773426 relative L2 0.005823757499456406\n",
      "training 0.014402778819203377 relative L2 0.00581620866432786\n",
      "training 0.014365230686962605 relative L2 0.0058086165226995945\n",
      "training 0.014327816665172577 relative L2 0.005801085848361254\n",
      "training 0.014290457591414452 relative L2 0.005793503485620022\n",
      "training 0.014253207482397556 relative L2 0.005786001216620207\n",
      "training 0.014216093346476555 relative L2 0.005778448190540075\n",
      "training 0.014179064892232418 relative L2 0.005770939402282238\n",
      "training 0.014142035506665707 relative L2 0.0057634045369923115\n",
      "training 0.014105175621807575 relative L2 0.005755917634814978\n",
      "training 0.014068365097045898 relative L2 0.00574839673936367\n",
      "training 0.01403165515512228 relative L2 0.005740921013057232\n",
      "training 0.013995024375617504 relative L2 0.005733430851250887\n",
      "training 0.013958512805402279 relative L2 0.005725952330976725\n",
      "training 0.013922015205025673 relative L2 0.0057184877805411816\n",
      "training 0.013885666616261005 relative L2 0.005711032543331385\n",
      "training 0.013849437236785889 relative L2 0.0057035828940570354\n",
      "training 0.01381321344524622 relative L2 0.005696132779121399\n",
      "training 0.013777128420770168 relative L2 0.00568870035931468\n",
      "training 0.013741068542003632 relative L2 0.005681267473846674\n",
      "training 0.013705175369977951 relative L2 0.005673850420862436\n",
      "training 0.013669279403984547 relative L2 0.005666431039571762\n",
      "training 0.013633545488119125 relative L2 0.005659043788909912\n",
      "training 0.013597897253930569 relative L2 0.005651627201586962\n",
      "training 0.013562250882387161 relative L2 0.005644249729812145\n",
      "training 0.01352677121758461 relative L2 0.00563686341047287\n",
      "training 0.013491347432136536 relative L2 0.005629491992294788\n",
      "training 0.013456005603075027 relative L2 0.005622122436761856\n",
      "training 0.013420750387012959 relative L2 0.005614760331809521\n",
      "training 0.013385545462369919 relative L2 0.00560740428045392\n",
      "training 0.013350464403629303 relative L2 0.0056000808253884315\n",
      "training 0.01331550907343626 relative L2 0.00559272663667798\n",
      "training 0.013280552811920643 relative L2 0.005585413426160812\n",
      "training 0.013245717622339725 relative L2 0.005578071344643831\n",
      "training 0.013210923410952091 relative L2 0.0055707781575620174\n",
      "training 0.013176260516047478 relative L2 0.00556345097720623\n",
      "training 0.013141637668013573 relative L2 0.005556174088269472\n",
      "training 0.013107150793075562 relative L2 0.0055488720536231995\n",
      "training 0.013072735629975796 relative L2 0.005541606340557337\n",
      "training 0.013038376346230507 relative L2 0.005534302443265915\n",
      "training 0.01300404779613018 relative L2 0.005527067463845015\n",
      "training 0.012969921343028545 relative L2 0.005519780330359936\n",
      "training 0.0129357585683465 relative L2 0.005512561183422804\n",
      "training 0.01290179044008255 relative L2 0.005505294539034367\n",
      "training 0.012867828831076622 relative L2 0.005498080514371395\n",
      "training 0.012833959423005581 relative L2 0.005490810610353947\n",
      "training 0.012800092808902264 relative L2 0.005483632441610098\n",
      "training 0.012766418047249317 relative L2 0.005476381164044142\n",
      "training 0.01273281592875719 relative L2 0.0054692369885742664\n",
      "training 0.012699279002845287 relative L2 0.005461948923766613\n",
      "training 0.012665722519159317 relative L2 0.005454861093312502\n",
      "training 0.012632372789084911 relative L2 0.005447568837553263\n",
      "training 0.012599080801010132 relative L2 0.005440559703856707\n",
      "training 0.012565926648676395 relative L2 0.005433212034404278\n",
      "training 0.012532824650406837 relative L2 0.005426337011158466\n",
      "training 0.01249991450458765 relative L2 0.005418931134045124\n",
      "training 0.012467228807508945 relative L2 0.005412318743765354\n",
      "training 0.012434827163815498 relative L2 0.005404825322329998\n",
      "training 0.01240287534892559 relative L2 0.005398768000304699\n",
      "training 0.012371715158224106 relative L2 0.0053913164883852005\n",
      "training 0.012341869994997978 relative L2 0.0053865485824644566\n",
      "training 0.012314229272305965 relative L2 0.005379729438573122\n",
      "training 0.012290618382394314 relative L2 0.005378345027565956\n",
      "training 0.012274079024791718 relative L2 0.005374498199671507\n",
      "training 0.012269921600818634 relative L2 0.005382228642702103\n",
      "training 0.012287222780287266 relative L2 0.005388423800468445\n",
      "training 0.012339101172983646 relative L2 0.005417913664132357\n",
      "training 0.01244337111711502 relative L2 0.005444714799523354\n",
      "training 0.012606114149093628 relative L2 0.005498069804161787\n",
      "training 0.012805734761059284 relative L2 0.005511814262717962\n",
      "training 0.012924643233418465 relative L2 0.005502593703567982\n",
      "training 0.01282542571425438 relative L2 0.005411256570369005\n",
      "training 0.012450921349227428 relative L2 0.00532930064946413\n",
      "training 0.012045823968946934 relative L2 0.005294536706060171\n",
      "training 0.011897657997906208 relative L2 0.005322441924363375\n",
      "training 0.012037578038871288 relative L2 0.005370185244828463\n",
      "training 0.012223100289702415 relative L2 0.005356327630579472\n",
      "training 0.012197170406579971 relative L2 0.005312006454914808\n",
      "training 0.011964906938374043 relative L2 0.005262652412056923\n",
      "training 0.011759870685636997 relative L2 0.005261985119432211\n",
      "training 0.011759425513446331 relative L2 0.005291460547596216\n",
      "training 0.011872490867972374 relative L2 0.005289566703140736\n",
      "training 0.011890889145433903 relative L2 0.00526569876819849\n",
      "training 0.011759069748222828 relative L2 0.005230123642832041\n",
      "training 0.011614680290222168 relative L2 0.005224952939897776\n",
      "training 0.011592398397624493 relative L2 0.005240880884230137\n",
      "training 0.011649291962385178 relative L2 0.00523668248206377\n",
      "training 0.011650944128632545 relative L2 0.005220408085733652\n",
      "training 0.011559910140931606 relative L2 0.005196465644985437\n",
      "training 0.011464325711131096 relative L2 0.005191644188016653\n",
      "training 0.01144419889897108 relative L2 0.00519949896261096\n",
      "training 0.011467697098851204 relative L2 0.005192579235881567\n",
      "training 0.011452856473624706 relative L2 0.005180160515010357\n",
      "training 0.011384147219359875 relative L2 0.005163684953004122\n",
      "training 0.011318887583911419 relative L2 0.0051590558141469955\n",
      "training 0.011300302110612392 relative L2 0.005161790177226067\n",
      "training 0.011303141713142395 relative L2 0.0051536341197788715\n",
      "training 0.01127956435084343 relative L2 0.005143691319972277\n",
      "training 0.0112256845459342 relative L2 0.005131623707711697\n",
      "training 0.011177822016179562 relative L2 0.005126688163727522\n",
      "training 0.01115828100591898 relative L2 0.005126214120537043\n",
      "training 0.011148766614496708 relative L2 0.005117854103446007\n",
      "training 0.011121826246380806 relative L2 0.005109559278935194\n",
      "training 0.011078002862632275 relative L2 0.005099931266158819\n",
      "training 0.01103959046304226 relative L2 0.005094520747661591\n",
      "training 0.011017988435924053 relative L2 0.005092015024274588\n",
      "training 0.011001243256032467 relative L2 0.005083967931568623\n",
      "training 0.0109737329185009 relative L2 0.005076754372566938\n",
      "training 0.010936670936644077 relative L2 0.005068417638540268\n",
      "training 0.010903227142989635 relative L2 0.005062593147158623\n",
      "training 0.010879608802497387 relative L2 0.005058822687715292\n",
      "training 0.01085885614156723 relative L2 0.005051237065345049\n",
      "training 0.010831878520548344 relative L2 0.005044692195951939\n",
      "training 0.010799184441566467 relative L2 0.00503702973946929\n",
      "training 0.010768400505185127 relative L2 0.005030959378927946\n",
      "training 0.010743398219347 relative L2 0.005026377271860838\n",
      "training 0.010720443911850452 relative L2 0.005019206088036299\n",
      "training 0.01069413311779499 relative L2 0.0050130570307374\n",
      "training 0.010664239525794983 relative L2 0.0050057778134942055\n",
      "training 0.010635067708790302 relative L2 0.004999594762921333\n",
      "training 0.010609232820570469 relative L2 0.00499446177855134\n",
      "training 0.010585062205791473 relative L2 0.004987617954611778\n",
      "training 0.01055927574634552 relative L2 0.004981708712875843\n",
      "training 0.010531265288591385 relative L2 0.004974662326276302\n",
      "training 0.010503151454031467 relative L2 0.004968451336026192\n",
      "training 0.010476890951395035 relative L2 0.0049629067070782185\n",
      "training 0.010451965034008026 relative L2 0.004956322256475687\n",
      "training 0.010426566004753113 relative L2 0.0049505229108035564\n",
      "training 0.010399793274700642 relative L2 0.00494369026273489\n",
      "training 0.010372619144618511 relative L2 0.004937517456710339\n",
      "training 0.01034634280949831 relative L2 0.004931696690618992\n",
      "training 0.010321062058210373 relative L2 0.004925271030515432\n",
      "training 0.010295793414115906 relative L2 0.004919515457004309\n",
      "training 0.010269856080412865 relative L2 0.00491285277530551\n",
      "training 0.010243434458971024 relative L2 0.0049067530781030655\n",
      "training 0.010217375122010708 relative L2 0.004900736268609762\n",
      "training 0.010191953741014004 relative L2 0.004894438199698925\n",
      "training 0.010166800580918789 relative L2 0.004888659343123436\n",
      "training 0.010141362436115742 relative L2 0.00488214660435915\n",
      "training 0.010115571320056915 relative L2 0.0048761190846562386\n",
      "training 0.010089820250868797 relative L2 0.0048699830658733845\n",
      "training 0.010064450092613697 relative L2 0.0048637851141393185\n",
      "training 0.010039394721388817 relative L2 0.004857947118580341\n",
      "training 0.01001427136361599 relative L2 0.004851568024605513\n",
      "training 0.009988999925553799 relative L2 0.004845611751079559\n",
      "training 0.009963641874492168 relative L2 0.0048393988981842995\n",
      "training 0.00993841327726841 relative L2 0.004833289887756109\n",
      "training 0.009913463145494461 relative L2 0.004827368073165417\n",
      "training 0.009888559579849243 relative L2 0.004821119364351034\n",
      "training 0.009863719344139099 relative L2 0.004815204534679651\n",
      "training 0.009838749654591084 relative L2 0.0048089888878166676\n",
      "training 0.009813806042075157 relative L2 0.0048029483295977116\n",
      "training 0.009789004921913147 relative L2 0.004796961322426796\n",
      "training 0.009764336980879307 relative L2 0.0047908020205795765\n",
      "training 0.009739731438457966 relative L2 0.004784907680004835\n",
      "training 0.00971513893455267 relative L2 0.004778722301125526\n",
      "training 0.009690524078905582 relative L2 0.00477274926379323\n",
      "training 0.00966593623161316 relative L2 0.0047667003236711025\n",
      "training 0.00964148249477148 relative L2 0.0047606416046619415\n",
      "training 0.009617109782993793 relative L2 0.004754711873829365\n",
      "training 0.009592771530151367 relative L2 0.004748604726046324\n",
      "training 0.00956850778311491 relative L2 0.004742678254842758\n",
      "training 0.009544234722852707 relative L2 0.004736616276204586\n",
      "training 0.009520026855170727 relative L2 0.004730615299195051\n",
      "training 0.009495830163359642 relative L2 0.004724666476249695\n",
      "training 0.009471781551837921 relative L2 0.0047186207957565784\n",
      "training 0.009447751566767693 relative L2 0.004712697584182024\n",
      "training 0.009423731826245785 relative L2 0.004706653766334057\n",
      "training 0.009399760514497757 relative L2 0.004700726363807917\n",
      "training 0.0093759186565876 relative L2 0.004694745410233736\n",
      "training 0.009352056309580803 relative L2 0.0046887584030628204\n",
      "training 0.009328258223831654 relative L2 0.004682831931859255\n",
      "training 0.009304496459662914 relative L2 0.0046768393367528915\n",
      "training 0.009280836209654808 relative L2 0.004670938942581415\n",
      "training 0.009257222525775433 relative L2 0.004664967767894268\n",
      "training 0.009233632124960423 relative L2 0.004659042693674564\n",
      "training 0.009210108779370785 relative L2 0.004653122741729021\n",
      "training 0.009186641313135624 relative L2 0.004647175315767527\n",
      "training 0.009163213893771172 relative L2 0.004641279578208923\n",
      "training 0.009139814414083958 relative L2 0.004635322839021683\n",
      "training 0.009116445668041706 relative L2 0.004629450850188732\n",
      "training 0.009093212895095348 relative L2 0.00462351692840457\n",
      "training 0.009069947525858879 relative L2 0.004617631435394287\n",
      "training 0.009046784602105618 relative L2 0.004611730109900236\n",
      "training 0.009023615159094334 relative L2 0.004605830181390047\n",
      "training 0.009000541642308235 relative L2 0.004599961917847395\n",
      "training 0.00897749699652195 relative L2 0.004594058264046907\n",
      "training 0.008954520337283611 relative L2 0.004588220734149218\n",
      "training 0.00893165823072195 relative L2 0.004582329653203487\n",
      "training 0.008908754214644432 relative L2 0.004576491191983223\n",
      "training 0.008885987102985382 relative L2 0.004570631310343742\n",
      "training 0.008863206952810287 relative L2 0.00456477515399456\n",
      "training 0.008840473368763924 relative L2 0.004558939021080732\n",
      "training 0.008817782625555992 relative L2 0.004553083796054125\n",
      "training 0.008795151486992836 relative L2 0.004547266289591789\n",
      "training 0.008772572502493858 relative L2 0.004541425034403801\n",
      "training 0.00875005591660738 relative L2 0.004535604268312454\n",
      "training 0.008727543987333775 relative L2 0.004529779311269522\n",
      "training 0.008705121465027332 relative L2 0.00452396972104907\n",
      "training 0.008682746440172195 relative L2 0.004518154542893171\n",
      "training 0.008660360239446163 relative L2 0.004512338899075985\n",
      "training 0.008638077415525913 relative L2 0.004506549797952175\n",
      "training 0.008615825325250626 relative L2 0.004500752314925194\n",
      "training 0.00859365239739418 relative L2 0.004494967870414257\n",
      "training 0.008571496233344078 relative L2 0.004489179700613022\n",
      "training 0.008549394086003304 relative L2 0.004483402706682682\n",
      "training 0.008527341298758984 relative L2 0.004477630369365215\n",
      "training 0.008505348116159439 relative L2 0.004471864551305771\n",
      "training 0.00848340056836605 relative L2 0.004466100130230188\n",
      "training 0.008461483754217625 relative L2 0.004460339434444904\n",
      "training 0.008439612574875355 relative L2 0.00445457873865962\n",
      "training 0.008417761884629726 relative L2 0.004448826424777508\n",
      "training 0.008395991288125515 relative L2 0.004443090409040451\n",
      "training 0.008374286815524101 relative L2 0.004437336232513189\n",
      "training 0.008352568373084068 relative L2 0.004431611858308315\n",
      "training 0.008330954238772392 relative L2 0.004425882827490568\n",
      "training 0.008309389464557171 relative L2 0.004420168232172728\n",
      "training 0.008287874981760979 relative L2 0.004414458759129047\n",
      "training 0.008266435004770756 relative L2 0.004408735316246748\n",
      "training 0.008244955912232399 relative L2 0.004403039813041687\n",
      "training 0.008223602548241615 relative L2 0.0043973177671432495\n",
      "training 0.008202211000025272 relative L2 0.0043916255235672\n",
      "training 0.008180920965969563 relative L2 0.004385942593216896\n",
      "training 0.00815972313284874 relative L2 0.004380254074931145\n",
      "training 0.008138514123857021 relative L2 0.004374565556645393\n",
      "training 0.008117330260574818 relative L2 0.0043688868172466755\n",
      "training 0.008096223697066307 relative L2 0.0043632155284285545\n",
      "training 0.008075132966041565 relative L2 0.004357544705271721\n",
      "training 0.008054131641983986 relative L2 0.004351889714598656\n",
      "training 0.008033150807023048 relative L2 0.0043462226167321205\n",
      "training 0.00801222026348114 relative L2 0.004340591840445995\n",
      "training 0.007991377264261246 relative L2 0.004334922879934311\n",
      "training 0.007970510050654411 relative L2 0.004329295828938484\n",
      "training 0.007949725724756718 relative L2 0.004323646891862154\n",
      "training 0.007928977720439434 relative L2 0.004318028688430786\n",
      "training 0.007908288389444351 relative L2 0.004312403500080109\n",
      "training 0.007887669838964939 relative L2 0.00430678203701973\n",
      "training 0.007867053151130676 relative L2 0.004301164299249649\n",
      "training 0.007846470922231674 relative L2 0.0042955512180924416\n",
      "training 0.007825974375009537 relative L2 0.00428994745016098\n",
      "training 0.00780547596514225 relative L2 0.004284335765987635\n",
      "training 0.007785055786371231 relative L2 0.004278757143765688\n",
      "training 0.007764684967696667 relative L2 0.004273148253560066\n",
      "training 0.007744325790554285 relative L2 0.0042675817385315895\n",
      "training 0.00772405881434679 relative L2 0.004261994734406471\n",
      "training 0.007703830488026142 relative L2 0.004256421700119972\n",
      "training 0.0076835849322378635 relative L2 0.004250851459801197\n",
      "training 0.007663470692932606 relative L2 0.004245286341756582\n",
      "training 0.007643317803740501 relative L2 0.004239724017679691\n",
      "training 0.007623265031725168 relative L2 0.004234176594763994\n",
      "training 0.007603244390338659 relative L2 0.004228627774864435\n",
      "training 0.007583276368677616 relative L2 0.004223085939884186\n",
      "training 0.007563336752355099 relative L2 0.004217544104903936\n",
      "training 0.007543434388935566 relative L2 0.004212015308439732\n",
      "training 0.007523603271692991 relative L2 0.004206488374620676\n",
      "training 0.007503793574869633 relative L2 0.0042009674943983555\n",
      "training 0.007484052330255508 relative L2 0.004195450339466333\n",
      "training 0.007464324589818716 relative L2 0.004189932253211737\n",
      "training 0.00744465971365571 relative L2 0.004184431862086058\n",
      "training 0.007425026502460241 relative L2 0.00417892774567008\n",
      "training 0.007405475713312626 relative L2 0.004173435736447573\n",
      "training 0.0073859309777617455 relative L2 0.004167929757386446\n",
      "training 0.007366412319242954 relative L2 0.0041624451987445354\n",
      "training 0.007346956059336662 relative L2 0.004156957846134901\n",
      "training 0.0073275514878332615 relative L2 0.004151483532041311\n",
      "training 0.00730818510055542 relative L2 0.004146005026996136\n",
      "training 0.007288865279406309 relative L2 0.004140549339354038\n",
      "training 0.007269613910466433 relative L2 0.004135075490921736\n",
      "training 0.007250362541526556 relative L2 0.004129627253860235\n",
      "training 0.007231193594634533 relative L2 0.004124167375266552\n",
      "training 0.007212044671177864 relative L2 0.004118720535188913\n",
      "training 0.007192929275333881 relative L2 0.0041132825426757336\n",
      "training 0.0071738967671990395 relative L2 0.004107850603759289\n",
      "training 0.007154901511967182 relative L2 0.004102421458810568\n",
      "training 0.007135920226573944 relative L2 0.004096981603652239\n",
      "training 0.007116980850696564 relative L2 0.004091584589332342\n",
      "training 0.007098126225173473 relative L2 0.004086142405867577\n",
      "training 0.007079280912876129 relative L2 0.004080763086676598\n",
      "training 0.007060484495013952 relative L2 0.004075330216437578\n",
      "training 0.00704175652936101 relative L2 0.004069951828569174\n",
      "training 0.007022981531918049 relative L2 0.004064525943249464\n",
      "training 0.007004348561167717 relative L2 0.004059186205267906\n",
      "training 0.006985727231949568 relative L2 0.004053746350109577\n",
      "training 0.006967124994844198 relative L2 0.004048420116305351\n",
      "training 0.0069485739804804325 relative L2 0.004042993299663067\n",
      "training 0.0069301011972129345 relative L2 0.004037694074213505\n",
      "training 0.006911660078912973 relative L2 0.004032262600958347\n",
      "training 0.0068932645954191685 relative L2 0.004026991315186024\n",
      "training 0.006874914281070232 relative L2 0.004021547734737396\n",
      "training 0.006856594234704971 relative L2 0.00401631323620677\n",
      "training 0.006838337983936071 relative L2 0.004010866861790419\n",
      "training 0.006820162758231163 relative L2 0.00400568637996912\n",
      "training 0.006802019663155079 relative L2 0.004000202752649784\n",
      "training 0.006783911492675543 relative L2 0.003995089326053858\n",
      "training 0.006765870377421379 relative L2 0.003989584278315306\n",
      "training 0.006747928913682699 relative L2 0.003984579350799322\n",
      "training 0.006730076856911182 relative L2 0.003979029133915901\n",
      "training 0.006712313275784254 relative L2 0.003974201623350382\n",
      "training 0.006694752257317305 relative L2 0.0039686099626123905\n",
      "training 0.006677328143268824 relative L2 0.003964071162045002\n",
      "training 0.006660255137830973 relative L2 0.003958503715693951\n",
      "training 0.006643625441938639 relative L2 0.003954543266445398\n",
      "training 0.0066276914440095425 relative L2 0.003949192352592945\n",
      "training 0.006612905766814947 relative L2 0.003946490585803986\n",
      "training 0.006599839776754379 relative L2 0.00394195131957531\n",
      "training 0.0065895672887563705 relative L2 0.003941982053220272\n",
      "training 0.0065834312699735165 relative L2 0.003939871210604906\n",
      "training 0.00658407574519515 relative L2 0.003945899195969105\n",
      "training 0.006594525184482336 relative L2 0.003949934616684914\n",
      "training 0.006620034109801054 relative L2 0.003968013916164637\n",
      "training 0.006665830500423908 relative L2 0.003983625676482916\n",
      "training 0.006736630108207464 relative L2 0.004017740022391081\n",
      "training 0.0068305740132927895 relative L2 0.004038860555738211\n",
      "training 0.006927992217242718 relative L2 0.004065350163727999\n",
      "training 0.006991058122366667 relative L2 0.004047533962875605\n",
      "training 0.006958452984690666 relative L2 0.004011703655123711\n",
      "training 0.00680919224396348 relative L2 0.003938823472708464\n",
      "training 0.006584727670997381 relative L2 0.0038864531088620424\n",
      "training 0.006397681776434183 relative L2 0.0038654962554574013\n",
      "training 0.006332284305244684 relative L2 0.0038798858877271414\n",
      "training 0.006385229527950287 relative L2 0.003911896143108606\n",
      "training 0.006477988790720701 relative L2 0.003918909467756748\n",
      "training 0.00651836721226573 relative L2 0.003908195067197084\n",
      "training 0.006465238053351641 relative L2 0.0038687943015247583\n",
      "training 0.006349507719278336 relative L2 0.003841072553768754\n",
      "training 0.006250476930290461 relative L2 0.003832000307738781\n",
      "training 0.00622209208086133 relative L2 0.0038401451893150806\n",
      "training 0.006254045758396387 relative L2 0.0038546971045434475\n",
      "training 0.006291224155575037 relative L2 0.0038488483987748623\n",
      "training 0.0062843565829098225 relative L2 0.0038354089483618736\n",
      "training 0.006229191552847624 relative L2 0.0038121365942060947\n",
      "training 0.006161532364785671 relative L2 0.003801001701503992\n",
      "training 0.006122602615505457 relative L2 0.0038016142789274454\n",
      "training 0.006122400984168053 relative L2 0.0038043633103370667\n",
      "training 0.006137465592473745 relative L2 0.0038067521527409554\n",
      "training 0.006136700976639986 relative L2 0.003795238444581628\n",
      "training 0.006108051631599665 relative L2 0.003783929394558072\n",
      "training 0.006065066903829575 relative L2 0.0037721958942711353\n",
      "training 0.00603079330176115 relative L2 0.0037678219377994537\n",
      "training 0.006017070263624191 relative L2 0.003768844297155738\n",
      "training 0.006016780622303486 relative L2 0.003766085719689727\n",
      "training 0.0060135372914373875 relative L2 0.0037627695128321648\n",
      "training 0.005996684543788433 relative L2 0.003752312855795026\n",
      "training 0.005968677345663309 relative L2 0.003744615940377116\n",
      "training 0.00594078004360199 relative L2 0.003738456405699253\n",
      "training 0.005922055337578058 relative L2 0.0037349879276007414\n",
      "training 0.005912629887461662 relative L2 0.003733815858140588\n",
      "training 0.00590542471036315 relative L2 0.0037285417784005404\n",
      "training 0.005893082357943058 relative L2 0.0037238134536892176\n",
      "training 0.005873845890164375 relative L2 0.0037157656624913216\n",
      "training 0.005851649213582277 relative L2 0.0037100634071975946\n",
      "training 0.005832029040902853 relative L2 0.0037055520806461573\n",
      "training 0.005817626602947712 relative L2 0.003701508045196533\n",
      "training 0.005806637927889824 relative L2 0.003698737593367696\n",
      "training 0.005795103032141924 relative L2 0.003693044651299715\n",
      "training 0.005780395586043596 relative L2 0.0036883188877254725\n",
      "training 0.005762715358287096 relative L2 0.003681874368339777\n",
      "training 0.0057445187121629715 relative L2 0.003676915541291237\n",
      "training 0.005728196818381548 relative L2 0.003672611201182008\n",
      "training 0.0057143461890518665 relative L2 0.003668122924864292\n",
      "training 0.005701741669327021 relative L2 0.003664547111839056\n",
      "training 0.005688558332622051 relative L2 0.0036591128446161747\n",
      "training 0.005673840641975403 relative L2 0.003654600353911519\n",
      "training 0.005657875444740057 relative L2 0.003649001009762287\n",
      "training 0.005641821771860123 relative L2 0.0036442868877202272\n",
      "training 0.005626671016216278 relative L2 0.003639856120571494\n",
      "training 0.0056126792915165424 relative L2 0.0036352169699966908\n",
      "training 0.005599267315119505 relative L2 0.0036312821321189404\n",
      "training 0.0055857314728200436 relative L2 0.0036261812783777714\n",
      "training 0.005571509711444378 relative L2 0.0036218110471963882\n",
      "training 0.005556658376008272 relative L2 0.0036166179925203323\n",
      "training 0.005541703198105097 relative L2 0.0036120309960097075\n",
      "training 0.005527078174054623 relative L2 0.0036074419040232897\n",
      "training 0.005512989591807127 relative L2 0.003602829063311219\n",
      "training 0.005499341990798712 relative L2 0.003598656738176942\n",
      "training 0.005485725123435259 relative L2 0.003593810135498643\n",
      "training 0.005471923854202032 relative L2 0.0035895195323973894\n",
      "training 0.005457816179841757 relative L2 0.003584567690268159\n",
      "training 0.005443555302917957 relative L2 0.003580095013603568\n",
      "training 0.005429373122751713 relative L2 0.003575423499569297\n",
      "training 0.005415386985987425 relative L2 0.0035708670038729906\n",
      "training 0.005401659291237593 relative L2 0.003566526109352708\n",
      "training 0.005388092715293169 relative L2 0.003561846213415265\n",
      "training 0.005374514032155275 relative L2 0.0035575744695961475\n",
      "training 0.005360898096114397 relative L2 0.0035528005100786686\n",
      "training 0.005347151774913073 relative L2 0.0035484284162521362\n",
      "training 0.005333390552550554 relative L2 0.0035437443293631077\n",
      "training 0.005319662857800722 relative L2 0.0035392800346016884\n",
      "training 0.005306044593453407 relative L2 0.0035347978118807077\n",
      "training 0.005292539019137621 relative L2 0.003530253889039159\n",
      "training 0.005279119126498699 relative L2 0.003525912994518876\n",
      "training 0.005265744403004646 relative L2 0.0035213055089116096\n",
      "training 0.0052523803897202015 relative L2 0.00351699348539114\n",
      "training 0.0052390326745808125 relative L2 0.003512362716719508\n",
      "training 0.005225608590990305 relative L2 0.003507985733449459\n",
      "training 0.005212225019931793 relative L2 0.0035034457687288523\n",
      "training 0.005198904778808355 relative L2 0.003499024547636509\n",
      "training 0.005185662768781185 relative L2 0.003494588891044259\n",
      "training 0.005172448698431253 relative L2 0.0034901131875813007\n",
      "training 0.005159296561032534 relative L2 0.0034857625141739845\n",
      "training 0.0051461937837302685 relative L2 0.0034812488593161106\n",
      "training 0.005133079830557108 relative L2 0.003476910525932908\n",
      "training 0.005119995679706335 relative L2 0.0034724120050668716\n",
      "training 0.005106947850435972 relative L2 0.0034680627286434174\n",
      "training 0.005093924235552549 relative L2 0.0034636042546480894\n",
      "training 0.005080928094685078 relative L2 0.0034592256415635347\n",
      "training 0.005067977588623762 relative L2 0.0034548286348581314\n",
      "training 0.0050550661981105804 relative L2 0.003450424177572131\n",
      "training 0.0050421967171132565 relative L2 0.003446082817390561\n",
      "training 0.005029368679970503 relative L2 0.003441642504185438\n",
      "training 0.005016514100134373 relative L2 0.0034373330418020487\n",
      "training 0.005003757309168577 relative L2 0.003432905999943614\n",
      "training 0.004991000052541494 relative L2 0.0034285951405763626\n",
      "training 0.004978273995220661 relative L2 0.0034241927787661552\n",
      "training 0.004965588450431824 relative L2 0.0034198688808828592\n",
      "training 0.004952913615852594 relative L2 0.0034155002795159817\n",
      "training 0.004940292797982693 relative L2 0.0034111670684069395\n",
      "training 0.004927688743919134 relative L2 0.0034068268723785877\n",
      "training 0.004915106575936079 relative L2 0.0034024869091808796\n",
      "training 0.004902593791484833 relative L2 0.0033981800079345703\n",
      "training 0.004890079610049725 relative L2 0.003393827937543392\n",
      "training 0.004877607338130474 relative L2 0.00338954059407115\n",
      "training 0.004865157883614302 relative L2 0.003385195741429925\n",
      "training 0.004852744750678539 relative L2 0.0033809118904173374\n",
      "training 0.004840342793613672 relative L2 0.0033765945117920637\n",
      "training 0.004828012082725763 relative L2 0.003372310660779476\n",
      "training 0.004815686494112015 relative L2 0.003368005622178316\n",
      "training 0.00480337580665946 relative L2 0.0033637143205851316\n",
      "training 0.004791106563061476 relative L2 0.0033594463020563126\n",
      "training 0.004778886679559946 relative L2 0.0033551491796970367\n",
      "training 0.004766672383993864 relative L2 0.003350903047248721\n",
      "training 0.0047545176930725574 relative L2 0.003346612909808755\n",
      "training 0.004742380231618881 relative L2 0.0033423732966184616\n",
      "training 0.004730253480374813 relative L2 0.0033380889799445868\n",
      "training 0.0047181714326143265 relative L2 0.003333860309794545\n",
      "training 0.004706110805273056 relative L2 0.003329595783725381\n",
      "training 0.004694105591624975 relative L2 0.0033253722358494997\n",
      "training 0.004682107362896204 relative L2 0.0033211272675544024\n",
      "training 0.0046701617538928986 relative L2 0.0033169046510010958\n",
      "training 0.004658231511712074 relative L2 0.0033126797061413527\n",
      "training 0.004646334797143936 relative L2 0.0033084473107010126\n",
      "training 0.00463444460183382 relative L2 0.0033042440190911293\n",
      "training 0.004622607957571745 relative L2 0.0033000106923282146\n",
      "training 0.004610774572938681 relative L2 0.003295821836218238\n",
      "training 0.004598989151418209 relative L2 0.003291604109108448\n",
      "training 0.004587236791849136 relative L2 0.0032874243333935738\n",
      "training 0.0045755053870379925 relative L2 0.0032832222059369087\n",
      "training 0.004563816357403994 relative L2 0.003279048018157482\n",
      "training 0.004552145954221487 relative L2 0.0032748559024184942\n",
      "training 0.004540495574474335 relative L2 0.0032706898637115955\n",
      "training 0.004528903868049383 relative L2 0.003266511484980583\n",
      "training 0.0045173028483986855 relative L2 0.003262344980612397\n",
      "training 0.004505753517150879 relative L2 0.0032581959385424852\n",
      "training 0.004494244232773781 relative L2 0.003254024311900139\n",
      "training 0.004482729360461235 relative L2 0.003249882720410824\n",
      "training 0.004471256397664547 relative L2 0.003245722269639373\n",
      "training 0.0044598085805773735 relative L2 0.0032416002359241247\n",
      "training 0.00444840919226408 relative L2 0.0032374525908380747\n",
      "training 0.004437033087015152 relative L2 0.0032333354465663433\n",
      "training 0.004425667691975832 relative L2 0.0032292031683027744\n",
      "training 0.004414369352161884 relative L2 0.0032251046504825354\n",
      "training 0.004403072874993086 relative L2 0.0032209635246545076\n",
      "training 0.00439178803935647 relative L2 0.0032168757170438766\n",
      "training 0.004380539059638977 relative L2 0.003212748561054468\n",
      "training 0.004369333852082491 relative L2 0.003208670997992158\n",
      "training 0.004358130507171154 relative L2 0.0032045457046478987\n",
      "training 0.004346981178969145 relative L2 0.003200487233698368\n",
      "training 0.004335841163992882 relative L2 0.003196367993950844\n",
      "training 0.004324756562709808 relative L2 0.003192333737388253\n",
      "training 0.004313690587878227 relative L2 0.0031882186885923147\n",
      "training 0.004302657674998045 relative L2 0.003184194676578045\n",
      "training 0.004291631747037172 relative L2 0.003180081257596612\n",
      "training 0.0042806644923985004 relative L2 0.0031760791316628456\n",
      "training 0.004269690718501806 relative L2 0.003171953372657299\n",
      "training 0.00425875885412097 relative L2 0.003168009454384446\n",
      "training 0.004247906152158976 relative L2 0.00316387671045959\n",
      "training 0.004237068351358175 relative L2 0.003159973071888089\n",
      "training 0.004226250573992729 relative L2 0.003155822167173028\n",
      "training 0.004215519409626722 relative L2 0.0031520258635282516\n",
      "training 0.004204857163131237 relative L2 0.0031478474847972393\n",
      "training 0.004194284789264202 relative L2 0.003144227433949709\n",
      "training 0.004183867014944553 relative L2 0.0031400739680975676\n",
      "training 0.004173704888671637 relative L2 0.003136797808110714\n",
      "training 0.00416382635012269 relative L2 0.003132746322080493\n",
      "training 0.00415446562692523 relative L2 0.003130200318992138\n",
      "training 0.004145905841141939 relative L2 0.0031266403384506702\n",
      "training 0.0041386885568499565 relative L2 0.0031257898081094027\n",
      "training 0.004133588634431362 relative L2 0.00312383403070271\n",
      "training 0.004131984896957874 relative L2 0.0031271418556571007\n",
      "training 0.004136191215366125 relative L2 0.0031299940310418606\n",
      "training 0.004149546846747398 relative L2 0.0031433498952537775\n",
      "training 0.004177715163677931 relative L2 0.0031586848199367523\n",
      "training 0.004228033125400543 relative L2 0.0031938408501446247\n",
      "training 0.004311040043830872 relative L2 0.0032328611705452204\n",
      "training 0.004431929439306259 relative L2 0.0032958847004920244\n",
      "training 0.004588901065289974 relative L2 0.0033414121717214584\n",
      "training 0.004737436305731535 relative L2 0.0033743749372661114\n",
      "training 0.004809126257896423 relative L2 0.0033291897270828485\n",
      "training 0.004702659323811531 relative L2 0.0032391478307545185\n",
      "training 0.004432677756994963 relative L2 0.003122020745649934\n",
      "training 0.004129847511649132 relative L2 0.0030664424411952496\n",
      "training 0.003978437278419733 relative L2 0.003087932476773858\n",
      "training 0.0040319254621863365 relative L2 0.003142785746604204\n",
      "training 0.004186216741800308 relative L2 0.0031830742955207825\n",
      "training 0.004280971363186836 relative L2 0.0031553476583212614\n",
      "training 0.00422038696706295 relative L2 0.003098532324656844\n",
      "training 0.00405840203166008 relative L2 0.0030474772211164236\n",
      "training 0.003931256942451 relative L2 0.003045652760192752\n",
      "training 0.003926776349544525 relative L2 0.0030783547554165125\n",
      "training 0.004005966242402792 relative L2 0.0030959108844399452\n",
      "training 0.004061158746480942 relative L2 0.003086802316829562\n",
      "training 0.004027362447232008 relative L2 0.0030477673280984163\n",
      "training 0.00393361272290349 relative L2 0.0030226046219468117\n",
      "training 0.0038649258203804493 relative L2 0.0030237019527703524\n",
      "training 0.0038669798523187637 relative L2 0.003038208233192563\n",
      "training 0.0039089638739824295 relative L2 0.00304824928753078\n",
      "training 0.003927956335246563 relative L2 0.0030327446293085814\n",
      "training 0.0038949644658714533 relative L2 0.0030127360951155424\n",
      "training 0.003838488133624196 relative L2 0.0029988684691488743\n",
      "training 0.003805135376751423 relative L2 0.003000491764396429\n",
      "training 0.003810587339103222 relative L2 0.0030095914844423532\n",
      "training 0.0038298028521239758 relative L2 0.0030072974041104317\n",
      "training 0.0038291767705231905 relative L2 0.0029983632266521454\n",
      "training 0.003801473183557391 relative L2 0.0029836150351911783\n",
      "training 0.003767306450754404 relative L2 0.002977209398522973\n",
      "training 0.003750203177332878 relative L2 0.0029787938110530376\n",
      "training 0.0037526970263570547 relative L2 0.002979653887450695\n",
      "training 0.0037580837961286306 relative L2 0.002978128846734762\n",
      "training 0.003750317031517625 relative L2 0.0029683737084269524\n",
      "training 0.0037291308399289846 relative L2 0.0029606076423078775\n",
      "training 0.0037074796855449677 relative L2 0.002956041134893894\n",
      "training 0.003696327330544591 relative L2 0.002954858588054776\n",
      "training 0.0036946984473615885 relative L2 0.0029551575426012278\n",
      "training 0.0036929375492036343 relative L2 0.0029502452816814184\n",
      "training 0.003683486022055149 relative L2 0.002944859443232417\n",
      "training 0.003667557844892144 relative L2 0.0029381634667515755\n",
      "training 0.0036522471345961094 relative L2 0.002934420946985483\n",
      "training 0.0036427860613912344 relative L2 0.002932985546067357\n",
      "training 0.003638132009655237 relative L2 0.002930194605141878\n",
      "training 0.0036330395378172398 relative L2 0.0029273261316120625\n",
      "training 0.003623796859756112 relative L2 0.002921530744060874\n",
      "training 0.003611181164160371 relative L2 0.002917027333751321\n",
      "training 0.003598972922191024 relative L2 0.002913303906098008\n",
      "training 0.003589865518733859 relative L2 0.002910352312028408\n",
      "training 0.003583302488550544 relative L2 0.0029082130640745163\n",
      "training 0.003576681949198246 relative L2 0.002904075663536787\n",
      "training 0.003568000392988324 relative L2 0.0029003522358834743\n",
      "training 0.0035574764478951693 relative L2 0.0028957086615264416\n",
      "training 0.0035468994174152613 relative L2 0.002892079995945096\n",
      "training 0.0035377764143049717 relative L2 0.002889140509068966\n",
      "training 0.0035301349125802517 relative L2 0.0028857854194939137\n",
      "training 0.003522791899740696 relative L2 0.002882879227399826\n",
      "training 0.0035145736765116453 relative L2 0.002878662431612611\n",
      "training 0.003505290951579809 relative L2 0.002875029807910323\n",
      "training 0.003495672484859824 relative L2 0.002871190197765827\n",
      "training 0.0034866167698055506 relative L2 0.00286777107976377\n",
      "training 0.0034784297458827496 relative L2 0.0028648097068071365\n",
      "training 0.003470692550763488 relative L2 0.0028612040914595127\n",
      "training 0.0034626787528395653 relative L2 0.0028580010402947664\n",
      "training 0.003454106394201517 relative L2 0.0028540475759655237\n",
      "training 0.003445168724283576 relative L2 0.0028505674563348293\n",
      "training 0.003436356084421277 relative L2 0.0028470493853092194\n",
      "training 0.0034279150422662497 relative L2 0.002843632362782955\n",
      "training 0.0034198842477053404 relative L2 0.002840504515916109\n",
      "training 0.003411889309063554 relative L2 0.0028368777129799128\n",
      "training 0.003403691342100501 relative L2 0.00283361435867846\n",
      "training 0.003395308740437031 relative L2 0.0028299156110733747\n",
      "training 0.003386810189113021 relative L2 0.00282651255838573\n",
      "training 0.0033784296829253435 relative L2 0.00282309390604496\n",
      "training 0.003370222169905901 relative L2 0.0028196722269058228\n",
      "training 0.0033622169867157936 relative L2 0.002816461957991123\n",
      "training 0.003354210639372468 relative L2 0.002812917809933424\n",
      "training 0.003346120472997427 relative L2 0.002809629775583744\n",
      "training 0.0033379211090505123 relative L2 0.002806070726364851\n",
      "training 0.0033297091722488403 relative L2 0.0028027088847011328\n",
      "training 0.003321573603898287 relative L2 0.0027993274852633476\n",
      "training 0.003313532331958413 relative L2 0.002795913489535451\n",
      "training 0.003305565332993865 relative L2 0.002792669227346778\n",
      "training 0.003297652117908001 relative L2 0.0027891946956515312\n",
      "training 0.003289690474048257 relative L2 0.0027859213296324015\n",
      "training 0.003281693672761321 relative L2 0.002782442606985569\n",
      "training 0.0032736717257648706 relative L2 0.002779122209176421\n",
      "training 0.0032657133415341377 relative L2 0.002775764325633645\n",
      "training 0.003257821314036846 relative L2 0.0027724008541554213\n",
      "training 0.003249957226216793 relative L2 0.0027691295836120844\n",
      "training 0.003242131322622299 relative L2 0.0027657418977469206\n",
      "training 0.003234337782487273 relative L2 0.002762472489848733\n",
      "training 0.0032264895271509886 relative L2 0.0027590689714998007\n",
      "training 0.003218660829588771 relative L2 0.002755782101303339\n",
      "training 0.00321086123585701 relative L2 0.002752429572865367\n",
      "training 0.003203070955350995 relative L2 0.002749114064499736\n",
      "training 0.003195337485522032 relative L2 0.002745830686762929\n",
      "training 0.0031876200810074806 relative L2 0.002742493525147438\n",
      "training 0.0031799389980733395 relative L2 0.0027392450720071793\n",
      "training 0.003172272350639105 relative L2 0.002735900692641735\n",
      "training 0.0031646019779145718 relative L2 0.0027326419949531555\n",
      "training 0.003156948136165738 relative L2 0.0027293208986520767\n",
      "training 0.003149312688037753 relative L2 0.0027260545175522566\n",
      "training 0.0031417154241353273 relative L2 0.0027227764949202538\n",
      "training 0.0031341295689344406 relative L2 0.002719487529247999\n",
      "training 0.003126563271507621 relative L2 0.0027162465266883373\n",
      "training 0.0031190249137580395 relative L2 0.002712957328185439\n",
      "training 0.003111521014943719 relative L2 0.0027097281999886036\n",
      "training 0.0031040008179843426 relative L2 0.0027064329478889704\n",
      "training 0.0030965048354119062 relative L2 0.0027032080106437206\n",
      "training 0.003089027013629675 relative L2 0.0026999383699148893\n",
      "training 0.003081576433032751 relative L2 0.00269670644775033\n",
      "training 0.003074142150580883 relative L2 0.002693461487069726\n",
      "training 0.0030667230021208525 relative L2 0.002690220018848777\n",
      "training 0.0030593317933380604 relative L2 0.0026870223227888346\n",
      "training 0.003051991807296872 relative L2 0.0026837794575840235\n",
      "training 0.00304464902728796 relative L2 0.0026805761735886335\n",
      "training 0.0030372925102710724 relative L2 0.0026773405261337757\n",
      "training 0.0030299874488264322 relative L2 0.0026741456240415573\n",
      "training 0.0030226765666157007 relative L2 0.0026709260419011116\n",
      "training 0.003015403402969241 relative L2 0.0026677290443331003\n",
      "training 0.0030081255827099085 relative L2 0.002664529951289296\n",
      "training 0.0030008952599018812 relative L2 0.0026613317895680666\n",
      "training 0.002993661444634199 relative L2 0.002658153884112835\n",
      "training 0.0029864665120840073 relative L2 0.002654965268447995\n",
      "training 0.0029792962595820427 relative L2 0.002651793882250786\n",
      "training 0.0029721148312091827 relative L2 0.0026486010756343603\n",
      "training 0.00296496762894094 relative L2 0.00264545320533216\n",
      "training 0.0029578469693660736 relative L2 0.0026422624941915274\n",
      "training 0.0029507263097912073 relative L2 0.0026391192805022\n",
      "training 0.002943634521216154 relative L2 0.002635948359966278\n",
      "training 0.002936564851552248 relative L2 0.0026328093372285366\n",
      "training 0.002929509151726961 relative L2 0.0026296537835150957\n",
      "training 0.0029224834870547056 relative L2 0.0026265177875757217\n",
      "training 0.0029154643416404724 relative L2 0.002623368753120303\n",
      "training 0.002908450784161687 relative L2 0.0026202371809631586\n",
      "training 0.0029014702886343002 relative L2 0.0026171025820076466\n",
      "training 0.0028944951482117176 relative L2 0.002613971708342433\n",
      "training 0.0028875493444502354 relative L2 0.0026108617894351482\n",
      "training 0.0028806307818740606 relative L2 0.002607732778415084\n",
      "training 0.0028737145476043224 relative L2 0.002604640321806073\n",
      "training 0.0028668418526649475 relative L2 0.0026015143375843763\n",
      "training 0.0028599565848708153 relative L2 0.0025984279345721006\n",
      "training 0.002853109035640955 relative L2 0.0025953047443181276\n",
      "training 0.0028462489135563374 relative L2 0.0025922348722815514\n",
      "training 0.002839444437995553 relative L2 0.0025891230907291174\n",
      "training 0.00283263367600739 relative L2 0.0025860602036118507\n",
      "training 0.00282586133107543 relative L2 0.0025829626247286797\n",
      "training 0.0028190957382321358 relative L2 0.0025798974093049765\n",
      "training 0.002812332706525922 relative L2 0.002576812170445919\n",
      "training 0.002805607160553336 relative L2 0.0025737618561834097\n",
      "training 0.002798892790451646 relative L2 0.002570678712800145\n",
      "training 0.002792191458866 relative L2 0.002567624906077981\n",
      "training 0.0027854847721755505 relative L2 0.0025645610876381397\n",
      "training 0.00277883792296052 relative L2 0.002561527071520686\n",
      "training 0.002772195963189006 relative L2 0.002558470703661442\n",
      "training 0.0027655710000544786 relative L2 0.0025554352905601263\n",
      "training 0.0027589478995651007 relative L2 0.0025523845106363297\n",
      "training 0.0027523464523255825 relative L2 0.0025493537541478872\n",
      "training 0.002745762001723051 relative L2 0.002546323463320732\n",
      "training 0.0027392071206122637 relative L2 0.002543304581195116\n",
      "training 0.0027326736599206924 relative L2 0.002540281740948558\n",
      "training 0.002726144390180707 relative L2 0.0025372724048793316\n",
      "training 0.0027196554001420736 relative L2 0.002534258645027876\n",
      "training 0.002713149646297097 relative L2 0.0025312479119747877\n",
      "training 0.0027066795155406 relative L2 0.0025282471906393766\n",
      "training 0.00270021241158247 relative L2 0.0025252385530620813\n",
      "training 0.002693765563890338 relative L2 0.002522250171750784\n",
      "training 0.0026873357128351927 relative L2 0.0025192515458911657\n",
      "training 0.0026809319388121367 relative L2 0.002516280859708786\n",
      "training 0.0026745463255792856 relative L2 0.0025132845621556044\n",
      "training 0.002668175846338272 relative L2 0.0025103280786424875\n",
      "training 0.002661820501089096 relative L2 0.0025073292199522257\n",
      "training 0.002655474469065666 relative L2 0.002504390897229314\n",
      "training 0.0026491570752114058 relative L2 0.002501393435522914\n",
      "training 0.0026428501587361097 relative L2 0.0024984790943562984\n",
      "training 0.002636573975905776 relative L2 0.0024954795371741056\n",
      "training 0.002630302682518959 relative L2 0.0024925870820879936\n",
      "training 0.0026240581646561623 relative L2 0.0024895898532122374\n",
      "training 0.002617837395519018 relative L2 0.002486719749867916\n",
      "training 0.0026116271037608385 relative L2 0.0024837167002260685\n",
      "training 0.0026054412592202425 relative L2 0.002480877097696066\n",
      "training 0.00259926775470376 relative L2 0.002477866830304265\n",
      "training 0.0025931273121386766 relative L2 0.0024750810116529465\n",
      "training 0.002587029244750738 relative L2 0.0024720579385757446\n",
      "training 0.002580934902653098 relative L2 0.002469331957399845\n",
      "training 0.002574906451627612 relative L2 0.00246631377376616\n",
      "training 0.002568921772763133 relative L2 0.0024636934977024794\n",
      "training 0.002563027199357748 relative L2 0.002460685558617115\n",
      "training 0.002557196654379368 relative L2 0.0024582266341894865\n",
      "training 0.002551509067416191 relative L2 0.0024552904069423676\n",
      "training 0.0025460131000727415 relative L2 0.002453153720125556\n",
      "training 0.002540792105719447 relative L2 0.0024503946769982576\n",
      "training 0.0025359466671943665 relative L2 0.0024488826747983694\n",
      "training 0.0025317021645605564 relative L2 0.0024466479662805796\n",
      "training 0.002528358716517687 relative L2 0.0024464535526931286\n",
      "training 0.002526360098272562 relative L2 0.0024455501697957516\n",
      "training 0.002526399679481983 relative L2 0.0024482158478349447\n",
      "training 0.0025295729283243418 relative L2 0.0024505872279405594\n",
      "training 0.0025373545940965414 relative L2 0.0024594038259238005\n",
      "training 0.00255220802500844 relative L2 0.00246925326064229\n",
      "training 0.002577061066403985 relative L2 0.0024905959144234657\n",
      "training 0.002616753103211522 relative L2 0.0025149225257337093\n",
      "training 0.0026746843941509724 relative L2 0.0025559067726135254\n",
      "training 0.002755309920758009 relative L2 0.002596102422103286\n",
      "training 0.002851976780220866 relative L2 0.002645807806402445\n",
      "training 0.0029524227138608694 relative L2 0.0026684654876589775\n",
      "training 0.0030144762713462114 relative L2 0.0026677679270505905\n",
      "training 0.003001611214131117 relative L2 0.0026087667793035507\n",
      "training 0.0028801318258047104 relative L2 0.0025259447284042835\n",
      "training 0.0026909192092716694 relative L2 0.0024381624534726143\n",
      "training 0.00251204171217978 relative L2 0.0023966808803379536\n",
      "training 0.0024243767838925123 relative L2 0.0024077643174678087\n",
      "training 0.0024460486602038145 relative L2 0.0024473241064697504\n",
      "training 0.0025313987862318754 relative L2 0.0024867812171578407\n",
      "training 0.002608059672638774 relative L2 0.0024880983401089907\n",
      "training 0.0026176178362220526 relative L2 0.0024611493572592735\n",
      "training 0.002554623642936349 relative L2 0.0024126528296619654\n",
      "training 0.002459251321852207 relative L2 0.002380998572334647\n",
      "training 0.002392222872003913 relative L2 0.002376981545239687\n",
      "training 0.00238421862013638 relative L2 0.0023938429076224566\n",
      "training 0.0024205215740948915 relative L2 0.002414938760921359\n",
      "training 0.0024597349110990763 relative L2 0.002415641210973263\n",
      "training 0.00246573518961668 relative L2 0.0024017964024096727\n",
      "training 0.0024330769665539265 relative L2 0.0023758632596582174\n",
      "training 0.0023838276974856853 relative L2 0.0023595644161105156\n",
      "training 0.002349340356886387 relative L2 0.0023572356440126896\n",
      "training 0.002344645792618394 relative L2 0.002364603104069829\n",
      "training 0.0023610161151736975 relative L2 0.0023738739546388388\n",
      "training 0.0023769340477883816 relative L2 0.002371503971517086\n",
      "training 0.002375322859734297 relative L2 0.0023629560600966215\n",
      "training 0.0023551920894533396 relative L2 0.0023485333658754826\n",
      "training 0.0023285571951419115 relative L2 0.0023399065248668194\n",
      "training 0.002310309326276183 relative L2 0.0023380923084914684\n",
      "training 0.002306578913703561 relative L2 0.0023404222447425127\n",
      "training 0.0023123836144804955 relative L2 0.002343860687687993\n",
      "training 0.002317303791642189 relative L2 0.0023408797569572926\n",
      "training 0.0023135729134082794 relative L2 0.002335579600185156\n",
      "training 0.002301011234521866 relative L2 0.002326960675418377\n",
      "training 0.002285480033606291 relative L2 0.0023214712273329496\n",
      "training 0.0022738762199878693 relative L2 0.002319088438525796\n",
      "training 0.002269138814881444 relative L2 0.0023187040351331234\n",
      "training 0.0022691453341394663 relative L2 0.0023193524684756994\n",
      "training 0.002269138814881444 relative L2 0.00231666280888021\n",
      "training 0.0022653480991721153 relative L2 0.0023133191280066967\n",
      "training 0.0022573338355869055 relative L2 0.0023076687939465046\n",
      "training 0.0022473896387964487 relative L2 0.00230356864631176\n",
      "training 0.0022387008648365736 relative L2 0.0023005856201052666\n",
      "training 0.002232996979728341 relative L2 0.002298810752108693\n",
      "training 0.002229913603514433 relative L2 0.002297988859936595\n",
      "training 0.002227537101134658 relative L2 0.0022956435568630695\n",
      "training 0.0022239519748836756 relative L2 0.0022933045402169228\n",
      "training 0.002218373119831085 relative L2 0.002289260970428586\n",
      "training 0.0022113914601504803 relative L2 0.0022859543096274137\n",
      "training 0.002204357413575053 relative L2 0.002282730769366026\n",
      "training 0.002198382280766964 relative L2 0.0022803039755672216\n",
      "training 0.002193754306063056 relative L2 0.002278537256643176\n",
      "training 0.002189963823184371 relative L2 0.00227625691331923\n",
      "training 0.0021861293353140354 relative L2 0.002274272497743368\n",
      "training 0.0021816217340528965 relative L2 0.002271185163408518\n",
      "training 0.002176328795030713 relative L2 0.0022684780415147543\n",
      "training 0.0021705550607293844 relative L2 0.002265327610075474\n",
      "training 0.0021648735273629427 relative L2 0.002262701280415058\n",
      "training 0.0021596571896225214 relative L2 0.002260299865156412\n",
      "training 0.0021549994125962257 relative L2 0.0022579266224056482\n",
      "training 0.002150679938495159 relative L2 0.0022558639757335186\n",
      "training 0.002146361628547311 relative L2 0.0022532595321536064\n",
      "training 0.002141810255125165 relative L2 0.0022509521804749966\n",
      "training 0.0021369620226323605 relative L2 0.002248094417154789\n",
      "training 0.0021318872459232807 relative L2 0.002245571929961443\n",
      "training 0.0021267894189804792 relative L2 0.002242883900180459\n",
      "training 0.002121834084391594 relative L2 0.0022404242772608995\n",
      "training 0.00211711716838181 relative L2 0.0022380833979696035\n",
      "training 0.0021125844214111567 relative L2 0.002235639141872525\n",
      "training 0.002108126413077116 relative L2 0.002233408857136965\n",
      "training 0.002103646518662572 relative L2 0.0022308439947664738\n",
      "training 0.0020990639459341764 relative L2 0.00222851918078959\n",
      "training 0.002094388473778963 relative L2 0.0022258867975324392\n",
      "training 0.0020896545611321926 relative L2 0.0022234765347093344\n",
      "training 0.0020849136635661125 relative L2 0.002220926806330681\n",
      "training 0.002080219332128763 relative L2 0.0022185109555721283\n",
      "training 0.0020756253506988287 relative L2 0.002216129098087549\n",
      "training 0.0020711065735667944 relative L2 0.002213707659393549\n",
      "training 0.0020666485652327538 relative L2 0.0022114100866019726\n",
      "training 0.0020621875301003456 relative L2 0.002208937192335725\n",
      "training 0.002057716716080904 relative L2 0.0022066314704716206\n",
      "training 0.0020532167982310057 relative L2 0.0022041245829313993\n",
      "training 0.002048690803349018 relative L2 0.002201785333454609\n",
      "training 0.0020441683009266853 relative L2 0.0021993110422044992\n",
      "training 0.002039659768342972 relative L2 0.0021969482768326998\n",
      "training 0.0020351754501461983 relative L2 0.0021945384796708822\n",
      "training 0.002030715811997652 relative L2 0.002192158717662096\n",
      "training 0.002026289701461792 relative L2 0.0021898197010159492\n",
      "training 0.00202189851552248 relative L2 0.0021874287631362677\n",
      "training 0.002017520135268569 relative L2 0.002185117220506072\n",
      "training 0.002013147808611393 relative L2 0.0021827053278684616\n",
      "training 0.0020087657030671835 relative L2 0.002180397277697921\n",
      "training 0.002004400361329317 relative L2 0.002177993068471551\n",
      "training 0.0020000336226075888 relative L2 0.002175675006583333\n",
      "training 0.0019956824835389853 relative L2 0.0021732982713729143\n",
      "training 0.0019913464784622192 relative L2 0.0021709734573960304\n",
      "training 0.001987023511901498 relative L2 0.0021686230320483446\n",
      "training 0.0019827077630907297 relative L2 0.002166284481063485\n",
      "training 0.001978407148271799 relative L2 0.002163978759199381\n",
      "training 0.0019741489086300135 relative L2 0.002161639044061303\n",
      "training 0.001969881122931838 relative L2 0.0021593442652374506\n",
      "training 0.001965631963685155 relative L2 0.002156999194994569\n",
      "training 0.001961380010470748 relative L2 0.0021547151263803244\n",
      "training 0.001957153668627143 relative L2 0.0021523828618228436\n",
      "training 0.0019529347773641348 relative L2 0.002150102984160185\n",
      "training 0.0019487290410324931 relative L2 0.002147782128304243\n",
      "training 0.0019445284269750118 relative L2 0.0021454908419400454\n",
      "training 0.0019403306068852544 relative L2 0.002143196063116193\n",
      "training 0.0019361627055332065 relative L2 0.0021409010514616966\n",
      "training 0.0019319916609674692 relative L2 0.0021386307198554277\n",
      "training 0.0019278536783531308 relative L2 0.0021363398991525173\n",
      "training 0.0019237175583839417 relative L2 0.002134079346433282\n",
      "training 0.0019195893546566367 relative L2 0.0021317878272384405\n",
      "training 0.0019154745386913419 relative L2 0.0021295410115271807\n",
      "training 0.001911371131427586 relative L2 0.0021272548474371433\n",
      "training 0.0019072805298492312 relative L2 0.0021250154823064804\n",
      "training 0.0019031958654522896 relative L2 0.002122740726917982\n",
      "training 0.0018991307588294148 relative L2 0.002120506949722767\n",
      "training 0.0018950706580653787 relative L2 0.0021182410418987274\n",
      "training 0.0018910245271399617 relative L2 0.0021160095930099487\n",
      "training 0.0018869857303798199 relative L2 0.0021137543953955173\n",
      "training 0.0018829564796760678 relative L2 0.00211152876727283\n",
      "training 0.0018789471359923482 relative L2 0.0021092842798680067\n",
      "training 0.001874939538538456 relative L2 0.0021070647053420544\n",
      "training 0.0018709565047174692 relative L2 0.0021048339549452066\n",
      "training 0.0018669728888198733 relative L2 0.0021026122849434614\n",
      "training 0.0018629998667165637 relative L2 0.002100397599861026\n",
      "training 0.0018590493127703667 relative L2 0.0020981815177947283\n",
      "training 0.001855101902037859 relative L2 0.0020959721878170967\n",
      "training 0.0018511639209464192 relative L2 0.002093764254823327\n",
      "training 0.0018472411902621388 relative L2 0.00209156796336174\n",
      "training 0.0018433327786624432 relative L2 0.002089361660182476\n",
      "training 0.0018394232029095292 relative L2 0.002087174914777279\n",
      "training 0.0018355374922975898 relative L2 0.002084979321807623\n",
      "training 0.0018316545756533742 relative L2 0.002082795836031437\n",
      "training 0.0018277809722349048 relative L2 0.0020806121174246073\n",
      "training 0.0018239323981106281 relative L2 0.0020784439984709024\n",
      "training 0.0018200884805992246 relative L2 0.002076259581372142\n",
      "training 0.0018162463093176484 relative L2 0.0020740919280797243\n",
      "training 0.001812413102015853 relative L2 0.0020719156600534916\n",
      "training 0.0018085928168147802 relative L2 0.002069764072075486\n",
      "training 0.0018047998892143369 relative L2 0.0020676052663475275\n",
      "training 0.0018010101048275828 relative L2 0.0020654527470469475\n",
      "training 0.0017972270725294948 relative L2 0.0020633020903915167\n",
      "training 0.0017934567295014858 relative L2 0.002061154693365097\n",
      "training 0.0017896919744089246 relative L2 0.0020590159110724926\n",
      "training 0.0017859491053968668 relative L2 0.002056874567642808\n",
      "training 0.001782207633368671 relative L2 0.0020547390449792147\n",
      "training 0.0017784734955057502 relative L2 0.0020526049192994833\n",
      "training 0.0017747536767274141 relative L2 0.002050486160442233\n",
      "training 0.001771054696291685 relative L2 0.002048357855528593\n",
      "training 0.0017673546681180596 relative L2 0.0020462353713810444\n",
      "training 0.0017636596458032727 relative L2 0.0020441189408302307\n",
      "training 0.0017599869752302766 relative L2 0.002042009960860014\n",
      "training 0.0017563201254233718 relative L2 0.002039894461631775\n",
      "training 0.0017526614246889949 relative L2 0.002037801081314683\n",
      "training 0.0017490294994786382 relative L2 0.00203569489531219\n",
      "training 0.0017453941982239485 relative L2 0.002033596159890294\n",
      "training 0.0017417572671547532 relative L2 0.0020314992871135473\n",
      "training 0.0017381437355652452 relative L2 0.002029412891715765\n",
      "training 0.0017345387022942305 relative L2 0.0020273160189390182\n",
      "training 0.0017309343675151467 relative L2 0.002025239635258913\n",
      "training 0.001727350172586739 relative L2 0.0020231609232723713\n",
      "training 0.001723791123367846 relative L2 0.002021098043769598\n",
      "training 0.0017202269518747926 relative L2 0.002019009552896023\n",
      "training 0.0017166663892567158 relative L2 0.0020169636700302362\n",
      "training 0.0017131287604570389 relative L2 0.002014880534261465\n",
      "training 0.0017095997463911772 relative L2 0.0020128637552261353\n",
      "training 0.001706097973510623 relative L2 0.0020107796881347895\n",
      "training 0.0017026040004566312 relative L2 0.0020087985321879387\n",
      "training 0.0016991348238661885 relative L2 0.0020067207515239716\n",
      "training 0.0016957044135779142 relative L2 0.0020048313308507204\n",
      "training 0.00169233581982553 relative L2 0.002002800116315484\n",
      "training 0.0016890743281692266 relative L2 0.0020011377055197954\n",
      "training 0.0016859844326972961 relative L2 0.0019992815796285868\n",
      "training 0.0016831759130582213 relative L2 0.0019982093945145607\n",
      "training 0.0016808934742584825 relative L2 0.001997046172618866\n",
      "training 0.0016795292031019926 relative L2 0.001997716724872589\n",
      "training 0.001679847831837833 relative L2 0.0019990333821624517\n",
      "training 0.0016831636894494295 relative L2 0.0020050748717039824\n",
      "training 0.0016919678309932351 relative L2 0.00201487448066473\n",
      "training 0.001710622338578105 relative L2 0.002037632977589965\n",
      "training 0.0017471458995714784 relative L2 0.002074107062071562\n",
      "training 0.0018142379121854901 relative L2 0.0021433215588331223\n",
      "training 0.0019334807293489575 relative L2 0.002244242699816823\n",
      "training 0.0021273368038237095 relative L2 0.002395039889961481\n",
      "training 0.002416465198621154 relative L2 0.0025473833084106445\n",
      "training 0.0027452444192022085 relative L2 0.002659687539562583\n",
      "training 0.0029826145619153976 relative L2 0.00259776902385056\n",
      "training 0.0028554177843034267 relative L2 0.0023656210396438837\n",
      "training 0.002357200952246785 relative L2 0.002066968008875847\n",
      "training 0.0018016512040048838 relative L2 0.0019688387401401997\n",
      "training 0.001632007653824985 relative L2 0.002114070812240243\n",
      "training 0.0018807604210451245 relative L2 0.0022752403747290373\n",
      "training 0.00218685669824481 relative L2 0.0022815498523414135\n",
      "training 0.002191913314163685 relative L2 0.0021126174833625555\n",
      "training 0.0018830004846677184 relative L2 0.001967185875400901\n",
      "training 0.001628163387067616 relative L2 0.0019940626807510853\n",
      "training 0.0016727577894926071 relative L2 0.002108906628564\n",
      "training 0.0018763262778520584 relative L2 0.002147594466805458\n",
      "training 0.001941024442203343 relative L2 0.002054664073511958\n",
      "training 0.001780052436515689 relative L2 0.0019576677586883307\n",
      "training 0.0016123111126944423 relative L2 0.001965515548363328\n",
      "training 0.0016251391498371959 relative L2 0.0020358297042548656\n",
      "training 0.0017472216859459877 relative L2 0.0020597120746970177\n",
      "training 0.0017847901908680797 relative L2 0.001999573316425085\n",
      "training 0.0016847793012857437 relative L2 0.0019423490157350898\n",
      "training 0.0015871882205829024 relative L2 0.0019517296459525824\n",
      "training 0.0016023400239646435 relative L2 0.0019938121549785137\n",
      "training 0.0016750111244618893 relative L2 0.0020027784630656242\n",
      "training 0.0016871525440365076 relative L2 0.0019617697689682245\n",
      "training 0.0016208280576393008 relative L2 0.001929991296492517\n",
      "training 0.0015670951688662171 relative L2 0.0019399782177060843\n",
      "training 0.0015830181073397398 relative L2 0.001963886432349682\n",
      "training 0.0016244723228737712 relative L2 0.0019646415021270514\n",
      "training 0.001623349147848785 relative L2 0.0019372296519577503\n",
      "training 0.0015799242537468672 relative L2 0.0019200989045202732\n",
      "training 0.0015510711818933487 relative L2 0.001928200013935566\n",
      "training 0.0015637752367183566 relative L2 0.0019409451633691788\n",
      "training 0.0015862206928431988 relative L2 0.0019386648200452328\n",
      "training 0.0015806189039722085 relative L2 0.0019207509467378259\n",
      "training 0.0015527276555076241 relative L2 0.001911194296553731\n",
      "training 0.0015366616426035762 relative L2 0.0019165241392329335\n",
      "training 0.0015448244521394372 relative L2 0.0019228503806516528\n",
      "training 0.0015563436318188906 relative L2 0.0019200595561414957\n",
      "training 0.0015503744361922145 relative L2 0.001908381236717105\n",
      "training 0.001532475813291967 relative L2 0.001902525662444532\n",
      "training 0.0015226490795612335 relative L2 0.0019054070580750704\n",
      "training 0.0015268881106749177 relative L2 0.0019082220969721675\n",
      "training 0.001532388268969953 relative L2 0.001905829762108624\n",
      "training 0.00152742734644562 relative L2 0.001898055779747665\n",
      "training 0.0015156948938965797 relative L2 0.0018939374713227153\n",
      "training 0.0015087929787114263 relative L2 0.0018949760124087334\n",
      "training 0.0015101478202268481 relative L2 0.0018959317822009325\n",
      "training 0.0015124010387808084 relative L2 0.001894085668027401\n",
      "training 0.0015085976338014007 relative L2 0.0018887032056227326\n",
      "training 0.0015005937311798334 relative L2 0.001885424368083477\n",
      "training 0.0014951016055420041 relative L2 0.0018852028297260404\n",
      "training 0.001494540716521442 relative L2 0.001885146601125598\n",
      "training 0.0014949735486879945 relative L2 0.0018837328534573317\n",
      "training 0.0014920711982995272 relative L2 0.0018798393430188298\n",
      "training 0.0014863659162074327 relative L2 0.00187703687697649\n",
      "training 0.0014816668117418885 relative L2 0.0018759998492896557\n",
      "training 0.0014799053315073252 relative L2 0.001875324291177094\n",
      "training 0.0014791995054110885 relative L2 0.0018741389503702521\n",
      "training 0.001476823352277279 relative L2 0.0018711816519498825\n",
      "training 0.0014725441578775644 relative L2 0.0018687457777559757\n",
      "training 0.0014684542547911406 relative L2 0.0018672299338504672\n",
      "training 0.0014660174492746592 relative L2 0.0018661466892808676\n",
      "training 0.0014645461924374104 relative L2 0.0018650229321792722\n",
      "training 0.001462402637116611 relative L2 0.0018626927630975842\n",
      "training 0.001459059538319707 relative L2 0.001860549091361463\n",
      "training 0.0014554521767422557 relative L2 0.0018587573431432247\n",
      "training 0.0014526573941111565 relative L2 0.001857397030107677\n",
      "training 0.0014506472507491708 relative L2 0.001856225891970098\n",
      "training 0.0014485485153272748 relative L2 0.001854287926107645\n",
      "training 0.0014457673532888293 relative L2 0.0018524114275351167\n",
      "training 0.0014426070265471935 relative L2 0.0018505118787288666\n",
      "training 0.0014397046761587262 relative L2 0.0018489704234525561\n",
      "training 0.001437333063222468 relative L2 0.0018476734403520823\n",
      "training 0.001435140031389892 relative L2 0.0018459678394719958\n",
      "training 0.0014326696982607245 relative L2 0.0018442959990352392\n",
      "training 0.0014298614114522934 relative L2 0.0018424170557409525\n",
      "training 0.0014270336832851171 relative L2 0.0018407879397273064\n",
      "training 0.0014244717312976718 relative L2 0.0018393403152003884\n",
      "training 0.001422130037099123 relative L2 0.0018377364613115788\n",
      "training 0.0014197619166225195 relative L2 0.001836187089793384\n",
      "training 0.0014171972870826721 relative L2 0.0018343966221436858\n",
      "training 0.0014145215973258018 relative L2 0.0018327591242268682\n",
      "training 0.0014119194820523262 relative L2 0.001831201952882111\n",
      "training 0.0014094797661527991 relative L2 0.0018296325579285622\n",
      "training 0.001407118048518896 relative L2 0.0018281427910551429\n",
      "training 0.0014046917203813791 relative L2 0.001826442196033895\n",
      "training 0.0014021636452525854 relative L2 0.0018248409032821655\n",
      "training 0.0013996054185554385 relative L2 0.0018232150468975306\n",
      "training 0.0013971137814223766 relative L2 0.0018216468160972\n",
      "training 0.0013947132974863052 relative L2 0.0018201566999778152\n",
      "training 0.0013923423830419779 relative L2 0.00181854167021811\n",
      "training 0.0013899309560656548 relative L2 0.0018169914837926626\n",
      "training 0.001387458760291338 relative L2 0.001815351890400052\n",
      "training 0.0013849821407347918 relative L2 0.0018137864535674453\n",
      "training 0.0013825567439198494 relative L2 0.0018122551264241338\n",
      "training 0.0013801740715280175 relative L2 0.0018106814241036773\n",
      "training 0.001377803971990943 relative L2 0.0018091709353029728\n",
      "training 0.0013754183892160654 relative L2 0.0018075673142448068\n",
      "training 0.001373013132251799 relative L2 0.0018060310976579785\n",
      "training 0.0013706182362511754 relative L2 0.0018044683383777738\n",
      "training 0.0013682316057384014 relative L2 0.0018029165221378207\n",
      "training 0.0013658744283020496 relative L2 0.0018014158122241497\n",
      "training 0.001363537274301052 relative L2 0.001799846300855279\n",
      "training 0.0013611831236630678 relative L2 0.001798335462808609\n",
      "training 0.0013588351430371404 relative L2 0.001796781667508185\n",
      "training 0.0013564910041168332 relative L2 0.0017952567432075739\n",
      "training 0.0013541533844545484 relative L2 0.0017937393859028816\n",
      "training 0.0013518318301066756 relative L2 0.0017922065453603864\n",
      "training 0.001349522266536951 relative L2 0.0017907049041241407\n",
      "training 0.0013472061837092042 relative L2 0.001789164380170405\n",
      "training 0.0013448928948491812 relative L2 0.0017876598285511136\n",
      "training 0.0013425846118479967 relative L2 0.0017861381638795137\n",
      "training 0.0013402870390564203 relative L2 0.0017846296541392803\n",
      "training 0.0013379983138293028 relative L2 0.001783133251592517\n",
      "training 0.0013357207644730806 relative L2 0.0017816310282796621\n",
      "training 0.0013334632385522127 relative L2 0.0017801447538658977\n",
      "training 0.001331195468083024 relative L2 0.0017786311218515038\n",
      "training 0.0013289247872307897 relative L2 0.0017771403072401881\n",
      "training 0.0013266594614833593 relative L2 0.0017756408778950572\n",
      "training 0.0013244014699012041 relative L2 0.001774151111021638\n",
      "training 0.0013221584958955646 relative L2 0.0017726720543578267\n",
      "training 0.0013199228560552 relative L2 0.0017711817054077983\n",
      "training 0.0013176912907510996 relative L2 0.001769713475368917\n",
      "training 0.0013154690386727452 relative L2 0.0017682330217212439\n",
      "training 0.0013132556341588497 relative L2 0.0017667560605332255\n",
      "training 0.0013110290747135878 relative L2 0.0017652850365266204\n",
      "training 0.001308824634179473 relative L2 0.0017638165736570954\n",
      "training 0.001306625665165484 relative L2 0.0017623549792915583\n",
      "training 0.0013044301886111498 relative L2 0.0017608864000067115\n",
      "training 0.0013022404164075851 relative L2 0.0017594328382983804\n",
      "training 0.001300061005167663 relative L2 0.0017579755512997508\n",
      "training 0.0012978912563994527 relative L2 0.0017565203597769141\n",
      "training 0.0012957133585587144 relative L2 0.0017550671473145485\n",
      "training 0.0012935464037582278 relative L2 0.001753617194481194\n",
      "training 0.0012913885293528438 relative L2 0.0017521721310913563\n",
      "training 0.001289231819100678 relative L2 0.001750732073560357\n",
      "training 0.0012870929203927517 relative L2 0.0017492969054728746\n",
      "training 0.0012849557679146528 relative L2 0.001747853122651577\n",
      "training 0.0012828157050535083 relative L2 0.0017464194679632783\n",
      "training 0.001280684839002788 relative L2 0.001744984881952405\n",
      "training 0.0012785573489964008 relative L2 0.001743555418215692\n",
      "training 0.0012764402199536562 relative L2 0.001742131425999105\n",
      "training 0.0012743300758302212 relative L2 0.001740706735290587\n",
      "training 0.0012722258688881993 relative L2 0.0017392841400578618\n",
      "training 0.001270120614208281 relative L2 0.0017378624761477113\n",
      "training 0.0012680223444476724 relative L2 0.0017364445375278592\n",
      "training 0.0012659301282837987 relative L2 0.0017350321868434548\n",
      "training 0.0012638474581763148 relative L2 0.0017336184391751885\n",
      "training 0.0012617673492059112 relative L2 0.0017322137719020247\n",
      "training 0.0012596974847838283 relative L2 0.0017308061942458153\n",
      "training 0.0012576280860230327 relative L2 0.0017293990822508931\n",
      "training 0.0012555606663227081 relative L2 0.0017280012834817171\n",
      "training 0.0012535069836303592 relative L2 0.0017266026698052883\n",
      "training 0.0012514595873653889 relative L2 0.0017252117395401\n",
      "training 0.0012494164984673262 relative L2 0.0017238147556781769\n",
      "training 0.0012473755050450563 relative L2 0.0017224299954250455\n",
      "training 0.0012453453382477164 relative L2 0.0017210382502526045\n",
      "training 0.001243316219188273 relative L2 0.0017196567496284842\n",
      "training 0.001241291523911059 relative L2 0.0017182669835165143\n",
      "training 0.001239268807694316 relative L2 0.0017168890917673707\n",
      "training 0.0012372578494250774 relative L2 0.0017155116656795144\n",
      "training 0.001235251547768712 relative L2 0.0017141405260190368\n",
      "training 0.0012332549085840583 relative L2 0.00171277008485049\n",
      "training 0.0012312574544921517 relative L2 0.001711395219899714\n",
      "training 0.00122926221229136 relative L2 0.0017100331606343389\n",
      "training 0.0012272789608687162 relative L2 0.0017086705192923546\n",
      "training 0.001225304207764566 relative L2 0.0017073128838092089\n",
      "training 0.0012233313173055649 relative L2 0.0017059564124792814\n",
      "training 0.0012213651789352298 relative L2 0.0017046016873791814\n",
      "training 0.0012194017181172967 relative L2 0.0017032470786944032\n",
      "training 0.0012174397706985474 relative L2 0.0017018929356709123\n",
      "training 0.001215485855937004 relative L2 0.0017005517147481441\n",
      "training 0.0012135424185544252 relative L2 0.0016992052551358938\n",
      "training 0.001211604569107294 relative L2 0.0016978646162897348\n",
      "training 0.0012096647405996919 relative L2 0.001696522580459714\n",
      "training 0.0012077369028702378 relative L2 0.001695192069746554\n",
      "training 0.0012058152351528406 relative L2 0.0016938596963882446\n",
      "training 0.001203901949338615 relative L2 0.001692532910965383\n",
      "training 0.0012019893620163202 relative L2 0.0016912034479901195\n",
      "training 0.0012000834103673697 relative L2 0.0016898801550269127\n",
      "training 0.0011981790885329247 relative L2 0.0016885563964024186\n",
      "training 0.0011962841963395476 relative L2 0.0016872406704351306\n",
      "training 0.0011943929130211473 relative L2 0.001685920637100935\n",
      "training 0.001192506868392229 relative L2 0.0016846141079440713\n",
      "training 0.0011906302534043789 relative L2 0.0016833001282066107\n",
      "training 0.001188756781630218 relative L2 0.0016820019809529185\n",
      "training 0.001186894252896309 relative L2 0.0016806898638606071\n",
      "training 0.001185026252642274 relative L2 0.001679393113590777\n",
      "training 0.001183169661089778 relative L2 0.0016780924052000046\n",
      "training 0.0011813201708719134 relative L2 0.001676794607192278\n",
      "training 0.001179467304609716 relative L2 0.0016754958778619766\n",
      "training 0.0011776200262829661 relative L2 0.001674204715527594\n",
      "training 0.001175784389488399 relative L2 0.0016729184426367283\n",
      "training 0.0011739538749679923 relative L2 0.0016716307727620006\n",
      "training 0.001172127784229815 relative L2 0.0016703499713912606\n",
      "training 0.0011703046038746834 relative L2 0.0016690671909600496\n",
      "training 0.001168491318821907 relative L2 0.0016677951207384467\n",
      "training 0.0011666821083053946 relative L2 0.0016665178118273616\n",
      "training 0.0011648795334622264 relative L2 0.001665249583311379\n",
      "training 0.0011630768422037363 relative L2 0.0016639763489365578\n",
      "training 0.0011612839298322797 relative L2 0.0016627125442028046\n",
      "training 0.0011594939278438687 relative L2 0.0016614451305940747\n",
      "training 0.0011577063705772161 relative L2 0.0016601852839812636\n",
      "training 0.001155927893705666 relative L2 0.001658924506045878\n",
      "training 0.0011541511630639434 relative L2 0.001657667919062078\n",
      "training 0.001152380951680243 relative L2 0.0016564142424613237\n",
      "training 0.0011506163282319903 relative L2 0.001655162894167006\n",
      "training 0.0011488569434732199 relative L2 0.001653917133808136\n",
      "training 0.0011471054749563336 relative L2 0.0016526752151548862\n",
      "training 0.0011453626211732626 relative L2 0.0016514365561306477\n",
      "training 0.0011436218628659844 relative L2 0.0016501958016306162\n",
      "training 0.0011418858775869012 relative L2 0.0016489564441144466\n",
      "training 0.0011401469819247723 relative L2 0.0016477197641506791\n",
      "training 0.0011384180979803205 relative L2 0.0016464919317513704\n",
      "training 0.001136697013862431 relative L2 0.0016452588606625795\n",
      "training 0.0011349759297445416 relative L2 0.0016440337058156729\n",
      "training 0.0011332618305459619 relative L2 0.0016428136732429266\n",
      "training 0.0011315608862787485 relative L2 0.0016415934078395367\n",
      "training 0.0011298565659672022 relative L2 0.001640373026020825\n",
      "training 0.0011281585320830345 relative L2 0.0016391639364883304\n",
      "training 0.0011264700442552567 relative L2 0.0016379456501454115\n",
      "training 0.0011247799266129732 relative L2 0.0016367393545806408\n",
      "training 0.0011230955133214593 relative L2 0.0016355238622054458\n",
      "training 0.0011214141268283129 relative L2 0.0016343261813744903\n",
      "training 0.001119741820730269 relative L2 0.001633116160519421\n",
      "training 0.0011180707952007651 relative L2 0.0016319204587489367\n",
      "training 0.0011164041934534907 relative L2 0.0016307167243212461\n",
      "training 0.0011147449258714914 relative L2 0.0016295291716232896\n",
      "training 0.0011130886850878596 relative L2 0.0016283264849334955\n",
      "training 0.001111439778469503 relative L2 0.0016271541826426983\n",
      "training 0.0011098009999841452 relative L2 0.0016259561525657773\n",
      "training 0.0011081657139584422 relative L2 0.0016247922321781516\n",
      "training 0.001106534036807716 relative L2 0.0016235962975770235\n",
      "training 0.0011049112072214484 relative L2 0.0016224471619352698\n",
      "training 0.0011032955953851342 relative L2 0.0016212535556405783\n",
      "training 0.0011016889475286007 relative L2 0.0016201226972043514\n",
      "training 0.0011000866070389748 relative L2 0.0016189299058169127\n",
      "training 0.0010984973050653934 relative L2 0.0016178239602595568\n",
      "training 0.0010969167342409492 relative L2 0.0016166411805897951\n",
      "training 0.001095360261388123 relative L2 0.001615585875697434\n",
      "training 0.0010938317282125354 relative L2 0.0016144224209710956\n",
      "training 0.0010923289228230715 relative L2 0.0016134345205500722\n",
      "training 0.0010908636031672359 relative L2 0.0016123107634484768\n",
      "training 0.0010894518345594406 relative L2 0.0016114466125145555\n",
      "training 0.0010881148045882583 relative L2 0.0016104194801300764\n",
      "training 0.0010868896497413516 relative L2 0.0016097977058961987\n",
      "training 0.0010858241003006697 relative L2 0.0016089982818812132\n",
      "training 0.0010849853279069066 relative L2 0.0016088404227048159\n",
      "training 0.0010844641365110874 relative L2 0.0016085575334727764\n",
      "training 0.0010844379430636764 relative L2 0.0016093638259917498\n",
      "training 0.0010851024417206645 relative L2 0.001610233448445797\n",
      "training 0.0010868095559999347 relative L2 0.0016130066942423582\n",
      "training 0.0010899740736931562 relative L2 0.0016162993852049112\n",
      "training 0.001095231855288148 relative L2 0.0016229194588959217\n",
      "training 0.001103428192436695 relative L2 0.0016310744686052203\n",
      "training 0.0011157564586028457 relative L2 0.0016450562980026007\n",
      "training 0.0011339066550135612 relative L2 0.0016620211536064744\n",
      "training 0.0011592195369303226 relative L2 0.001688067801296711\n",
      "training 0.0011944477446377277 relative L2 0.0017176870023831725\n",
      "training 0.0012393200304359198 relative L2 0.0017578501719981432\n",
      "training 0.0012961266329512 relative L2 0.0017961127450689673\n",
      "training 0.0013565331464633346 relative L2 0.001837089192122221\n",
      "training 0.0014166680630296469 relative L2 0.0018576242728158832\n",
      "training 0.0014520715922117233 relative L2 0.0018608010141178966\n",
      "training 0.0014537827810272574 relative L2 0.001823942526243627\n",
      "training 0.0013993490720167756 relative L2 0.0017639694269746542\n",
      "training 0.0013052267022430897 relative L2 0.0016845166683197021\n",
      "training 0.0011912814807146788 relative L2 0.0016198885859921575\n",
      "training 0.0010991402668878436 relative L2 0.0015853493241593242\n",
      "training 0.001052908948622644 relative L2 0.001588097307831049\n",
      "training 0.0010566941928118467 relative L2 0.001616846420802176\n",
      "training 0.0010949597926810384 relative L2 0.0016497804317623377\n",
      "training 0.0011419650400057435 relative L2 0.0016739987768232822\n",
      "training 0.0011743782088160515 relative L2 0.0016731409123167396\n",
      "training 0.0011750160483643413 relative L2 0.0016546845436096191\n",
      "training 0.0011471998877823353 relative L2 0.0016213356284424663\n",
      "training 0.0011023260885849595 relative L2 0.001592293381690979\n",
      "training 0.001061704708263278 relative L2 0.0015750613529235125\n",
      "training 0.001039066701196134 relative L2 0.0015744941774755716\n",
      "training 0.0010383245535194874 relative L2 0.0015860006678849459\n",
      "training 0.001053261337801814 relative L2 0.0015994829591363668\n",
      "training 0.0010723313316702843 relative L2 0.0016095414757728577\n",
      "training 0.001084943418391049 relative L2 0.0016081739449873567\n",
      "training 0.0010842365445569158 relative L2 0.0015998051967471838\n",
      "training 0.0010717518161982298 relative L2 0.0015851091593503952\n",
      "training 0.001052812789566815 relative L2 0.0015727729769423604\n",
      "training 0.0010356451384723186 relative L2 0.0015649438137188554\n",
      "training 0.0010255558881908655 relative L2 0.0015637832693755627\n",
      "training 0.00102400709874928 relative L2 0.001567574799992144\n",
      "training 0.001028764178045094 relative L2 0.0015723080141469836\n",
      "training 0.001035572262480855 relative L2 0.0015763656701892614\n",
      "training 0.0010403393534943461 relative L2 0.0015759773086756468\n",
      "training 0.0010405369102954865 relative L2 0.001573102781549096\n",
      "training 0.0010359996231272817 relative L2 0.0015670352149754763\n",
      "training 0.0010285318130627275 relative L2 0.0015614076983183622\n",
      "training 0.0010205925209447742 relative L2 0.0015565023059025407\n",
      "training 0.0010144093539565802 relative L2 0.0015541078755632043\n",
      "training 0.0010111316805705428 relative L2 0.0015537736471742392\n",
      "training 0.0010106366826221347 relative L2 0.001554564805701375\n",
      "training 0.0010118841892108321 relative L2 0.0015559898456558585\n",
      "training 0.0010134449694305658 relative L2 0.0015562254702672362\n",
      "training 0.0010141462553292513 relative L2 0.0015559097519144416\n",
      "training 0.001013306900858879 relative L2 0.0015538492007181048\n",
      "training 0.001011001062579453 relative L2 0.0015515798004344106\n",
      "training 0.0010076428297907114 relative L2 0.001548603642731905\n",
      "training 0.0010040272027254105 relative L2 0.001546302461065352\n",
      "training 0.0010007928358390927 relative L2 0.0015443681040778756\n",
      "training 0.000998375820927322 relative L2 0.00154322967864573\n",
      "training 0.0009968570666387677 relative L2 0.0015426650643348694\n",
      "training 0.0009960585739463568 relative L2 0.001542260404676199\n",
      "training 0.000995658803731203 relative L2 0.0015421329298987985\n",
      "training 0.0009953060653060675 relative L2 0.0015415368834510446\n",
      "training 0.0009947491344064474 relative L2 0.001540989731438458\n",
      "training 0.0009937870781868696 relative L2 0.0015397635288536549\n",
      "training 0.0009924329351633787 relative L2 0.0015386411687359214\n",
      "training 0.0009907298954203725 relative L2 0.0015370360342785716\n",
      "training 0.0009888469940051436 relative L2 0.0015356849180534482\n",
      "training 0.0009869053028523922 relative L2 0.0015341569669544697\n",
      "training 0.0009850558126345277 relative L2 0.0015329353045672178\n",
      "training 0.0009833669755607843 relative L2 0.0015317433280870318\n",
      "training 0.0009818741818889976 relative L2 0.0015307511202991009\n",
      "training 0.0009805649751797318 relative L2 0.0015298710204660892\n",
      "training 0.0009794039651751518 relative L2 0.0015290179289877415\n",
      "training 0.000978344352915883 relative L2 0.0015282975509762764\n",
      "training 0.000977333402261138 relative L2 0.0015274538891389966\n",
      "training 0.0009763362468220294 relative L2 0.001526752719655633\n",
      "training 0.0009753115009516478 relative L2 0.001525841886177659\n",
      "training 0.0009742584661580622 relative L2 0.001525095314718783\n",
      "training 0.0009731567115522921 relative L2 0.001524114515632391\n",
      "training 0.0009720252128317952 relative L2 0.0015233148587867618\n",
      "training 0.0009708497091196477 relative L2 0.0015222857473418117\n",
      "training 0.0009696590714156628 relative L2 0.0015214415034279227\n",
      "training 0.0009684293181635439 relative L2 0.001520374440588057\n",
      "training 0.0009671872248873115 relative L2 0.0015194960869848728\n",
      "training 0.0009659225470386446 relative L2 0.0015184108633548021\n",
      "training 0.0009646484977565706 relative L2 0.0015175064327195287\n",
      "training 0.0009633638546802104 relative L2 0.001516432617790997\n",
      "training 0.0009620912023819983 relative L2 0.0015155362198129296\n",
      "training 0.0009608338586986065 relative L2 0.001514493371360004\n",
      "training 0.0009595879819244146 relative L2 0.0015136078000068665\n",
      "training 0.0009583604987710714 relative L2 0.0015125962672755122\n",
      "training 0.0009571440168656409 relative L2 0.0015117251314222813\n",
      "training 0.0009559456957504153 relative L2 0.0015107450308278203\n",
      "training 0.0009547633235342801 relative L2 0.0015098885633051395\n",
      "training 0.0009535911958664656 relative L2 0.001508934423327446\n",
      "training 0.0009524397901259363 relative L2 0.0015081046149134636\n",
      "training 0.0009513041586615145 relative L2 0.0015071789966896176\n",
      "training 0.0009501909371465445 relative L2 0.0015063798055052757\n",
      "training 0.0009490929078310728 relative L2 0.001505488296970725\n",
      "training 0.0009480324806645513 relative L2 0.0015047516208142042\n",
      "training 0.0009470044751651585 relative L2 0.001503898878581822\n",
      "training 0.0009460098226554692 relative L2 0.0015032329829409719\n",
      "training 0.0009450532961636782 relative L2 0.0015024302992969751\n",
      "training 0.0009441447327844799 relative L2 0.0015018766280263662\n",
      "training 0.0009433074737899005 relative L2 0.001501196646131575\n",
      "training 0.0009425841853953898 relative L2 0.0015008493792265654\n",
      "training 0.0009419773705303669 relative L2 0.001500385464169085\n",
      "training 0.0009415695094503462 relative L2 0.0015003858134150505\n",
      "training 0.0009413572261109948 relative L2 0.001500287326052785\n",
      "training 0.0009414679952897131 relative L2 0.0015008713817223907\n",
      "training 0.0009419359848834574 relative L2 0.0015014300588518381\n",
      "training 0.0009429570054635406 relative L2 0.0015029830392450094\n",
      "training 0.0009445782052353024 relative L2 0.001504646148532629\n",
      "training 0.0009471034281887114 relative L2 0.0015078582800924778\n",
      "training 0.0009507434442639351 relative L2 0.0015114173293113708\n",
      "training 0.0009558260790072381 relative L2 0.0015172624262049794\n",
      "training 0.0009627321269363165 relative L2 0.0015237632906064391\n",
      "training 0.0009718000655993819 relative L2 0.001533631468191743\n",
      "training 0.0009838187834247947 relative L2 0.001544385333545506\n",
      "training 0.000998734962195158 relative L2 0.0015595145523548126\n",
      "training 0.0010176659561693668 relative L2 0.0015749058220535517\n",
      "training 0.0010392292169854045 relative L2 0.001594639616087079\n",
      "training 0.0010645408183336258 relative L2 0.0016118638450279832\n",
      "training 0.0010892973514273763 relative L2 0.0016308086924254894\n",
      "training 0.0011139212874695659 relative L2 0.001641234615817666\n",
      "training 0.0011299027828499675 relative L2 0.0016475676093250513\n",
      "training 0.0011371822329238057 relative L2 0.0016387375071644783\n",
      "training 0.0011264169588685036 relative L2 0.001621853094547987\n",
      "training 0.001101582427509129 relative L2 0.0015906892949715257\n",
      "training 0.001060462323948741 relative L2 0.0015568301314488053\n",
      "training 0.0010140967788174748 relative L2 0.0015215114690363407\n",
      "training 0.0009688902646303177 relative L2 0.001495318254455924\n",
      "training 0.0009347301092930138 relative L2 0.0014795763418078423\n",
      "training 0.000915215874556452 relative L2 0.001475857337936759\n",
      "training 0.0009104755008593202 relative L2 0.0014813998714089394\n",
      "training 0.0009172479039989412 relative L2 0.0014917518710717559\n",
      "training 0.0009306970168836415 relative L2 0.0015041857259348035\n",
      "training 0.0009459298453293741 relative L2 0.001512897782959044\n",
      "training 0.0009577679447829723 relative L2 0.0015183190116658807\n",
      "training 0.0009639759664423764 relative L2 0.0015163004864007235\n",
      "training 0.0009621581993997097 relative L2 0.0015107477083802223\n",
      "training 0.0009542740881443024 relative L2 0.0015000745188444853\n",
      "training 0.0009413267835043371 relative L2 0.0014894520863890648\n",
      "training 0.000927275512367487 relative L2 0.0014787997351959348\n",
      "training 0.0009143127244897187 relative L2 0.0014714052667841315\n",
      "training 0.000904748507309705 relative L2 0.0014668306102976203\n",
      "training 0.0008992133662104607 relative L2 0.0014654768165200949\n",
      "training 0.0008974999655038118 relative L2 0.0014665573835372925\n",
      "training 0.0008987478213384748 relative L2 0.0014688015216961503\n",
      "training 0.0009017468546517193 relative L2 0.0014719063183292747\n",
      "training 0.0009053255198523402 relative L2 0.0014740771148353815\n",
      "training 0.0009083977784030139 relative L2 0.001475934055633843\n",
      "training 0.0009103164193220437 relative L2 0.001475767232477665\n",
      "training 0.0009105355711653829 relative L2 0.0014750229893252254\n",
      "training 0.0009091732790693641 relative L2 0.001472461735829711\n",
      "training 0.0009063883917406201 relative L2 0.0014698378508910537\n",
      "training 0.0009027255000546575 relative L2 0.0014662935864180326\n",
      "training 0.0008986613829620183 relative L2 0.0014632786624133587\n",
      "training 0.000894614146091044 relative L2 0.001460139756090939\n",
      "training 0.0008909693569876254 relative L2 0.001457778736948967\n",
      "training 0.0008878561202436686 relative L2 0.0014556855894625187\n",
      "training 0.0008854088955558836 relative L2 0.0014542639255523682\n",
      "training 0.0008835685439407825 relative L2 0.0014531827764585614\n",
      "training 0.0008822741219773889 relative L2 0.0014524816069751978\n",
      "training 0.0008814168977551162 relative L2 0.0014520753175020218\n",
      "training 0.0008808705024421215 relative L2 0.001451726770028472\n",
      "training 0.0008805229444988072 relative L2 0.0014516104711219668\n",
      "training 0.0008802690426819026 relative L2 0.00145133922342211\n",
      "training 0.0008800717769190669 relative L2 0.001451290911063552\n",
      "training 0.0008798539638519287 relative L2 0.0014509591273963451\n",
      "training 0.0008796231122687459 relative L2 0.0014508581953123212\n",
      "training 0.0008793064625933766 relative L2 0.001450423034839332\n",
      "training 0.0008789755520410836 relative L2 0.0014502624981105328\n",
      "training 0.0008785618119873106 relative L2 0.001449756557121873\n",
      "training 0.0008781663491390646 relative L2 0.0014495933428406715\n",
      "training 0.0008777297916822135 relative L2 0.0014490777393803\n",
      "training 0.0008773404988460243 relative L2 0.0014489538734778762\n",
      "training 0.0008769360138103366 relative L2 0.0014484915882349014\n",
      "training 0.0008766287937760353 relative L2 0.001448491821065545\n",
      "training 0.000876359234098345 relative L2 0.0014481641119346023\n",
      "training 0.0008762364159338176 relative L2 0.0014483636477962136\n",
      "training 0.0008761912467889488 relative L2 0.0014482495607808232\n",
      "training 0.0008763535879552364 relative L2 0.0014487702865153551\n",
      "training 0.0008766737300902605 relative L2 0.0014489832101389766\n",
      "training 0.0008772712317295372 relative L2 0.0014499438693746924\n",
      "training 0.0008780988864600658 relative L2 0.0014506072038784623\n",
      "training 0.0008792894659563899 relative L2 0.001452201628126204\n",
      "training 0.0008808530983515084 relative L2 0.0014534895308315754\n",
      "training 0.0008828658028505743 relative L2 0.001455908757634461\n",
      "training 0.0008853968465700746 relative L2 0.0014580043498426676\n",
      "training 0.0008884714916348457 relative L2 0.001461502630263567\n",
      "training 0.0008922847337089479 relative L2 0.001464619766920805\n",
      "training 0.000896710786037147 relative L2 0.0014694109559059143\n",
      "training 0.0009020772995427251 relative L2 0.0014735934091731906\n",
      "training 0.0009079346782527864 relative L2 0.0014795507304370403\n",
      "training 0.0009147195378318429 relative L2 0.0014843945391476154\n",
      "training 0.0009215304162353277 relative L2 0.0014909282326698303\n",
      "training 0.0009290162706747651 relative L2 0.0014955529477447271\n",
      "training 0.0009356743539683521 relative L2 0.001501434831880033\n",
      "training 0.0009423195151612163 relative L2 0.0015041123842820525\n",
      "training 0.0009465930052101612 relative L2 0.0015070317313075066\n",
      "training 0.0009494469850324094 relative L2 0.0015054610557854176\n",
      "training 0.0009483207250013947 relative L2 0.0015033738454803824\n",
      "training 0.0009447855991311371 relative L2 0.0014963583089411259\n",
      "training 0.0009367052116431296 relative L2 0.0014888235600665212\n",
      "training 0.0009263615356758237 relative L2 0.001477510086260736\n",
      "training 0.0009128740639425814 relative L2 0.0014667365467175841\n",
      "training 0.0008987435721792281 relative L2 0.0014543583383783698\n",
      "training 0.0008840010850690305 relative L2 0.0014439938822761178\n",
      "training 0.0008707587840035558 relative L2 0.0014343068469315767\n",
      "training 0.000859339430462569 relative L2 0.001427335781045258\n",
      "training 0.000850571901537478 relative L2 0.0014220596058294177\n",
      "training 0.0008444085251539946 relative L2 0.0014190271031111479\n",
      "training 0.000840630498714745 relative L2 0.0014175428077578545\n",
      "training 0.0008388886926695704 relative L2 0.0014173855306580663\n",
      "training 0.0008387312991544604 relative L2 0.0014182546874508262\n",
      "training 0.00083968136459589 relative L2 0.0014194607501849532\n",
      "training 0.0008412885945290327 relative L2 0.0014211840461939573\n",
      "training 0.000843163812533021 relative L2 0.0014226107159629464\n",
      "training 0.0008451275643892586 relative L2 0.0014244953636080027\n",
      "training 0.0008471294422633946 relative L2 0.001425746246241033\n",
      "training 0.000848948722705245 relative L2 0.0014274410204961896\n",
      "training 0.000850670738145709 relative L2 0.0014283026102930307\n",
      "training 0.0008520656847395003 relative L2 0.0014296326553449035\n",
      "training 0.0008533131331205368 relative L2 0.0014300356851890683\n",
      "training 0.0008541831630282104 relative L2 0.0014309299876913428\n",
      "training 0.0008548792102374136 relative L2 0.0014308597892522812\n",
      "training 0.0008551946957595646 relative L2 0.0014313702704384923\n",
      "training 0.000855411752127111 relative L2 0.001430973527021706\n",
      "training 0.0008553393417969346 relative L2 0.0014312310377135873\n",
      "training 0.0008552415529266 relative L2 0.0014305959921330214\n",
      "training 0.0008548874175176024 relative L2 0.0014306664234027267\n",
      "training 0.0008545577293261886 relative L2 0.001429881900548935\n",
      "training 0.000854024023283273 relative L2 0.0014298439491540194\n",
      "training 0.0008535635424777865 relative L2 0.0014289740938693285\n",
      "training 0.000852926925290376 relative L2 0.0014288837555795908\n",
      "training 0.0008524034055881202 relative L2 0.0014279823517426848\n",
      "training 0.0008517283131368458 relative L2 0.00142790621612221\n",
      "training 0.0008512227213941514 relative L2 0.0014270429965108633\n",
      "training 0.0008505930891260505 relative L2 0.0014270251849666238\n",
      "training 0.0008501588017679751 relative L2 0.0014262152835726738\n",
      "training 0.0008495950605720282 relative L2 0.0014262503245845437\n",
      "training 0.0008492240449413657 relative L2 0.0014255099231377244\n",
      "training 0.0008487451123073697 relative L2 0.001425682450644672\n",
      "training 0.0008485388825647533 relative L2 0.001425099791958928\n",
      "training 0.0008482521516270936 relative L2 0.001425450318492949\n",
      "training 0.000848259252961725 relative L2 0.0014249829109758139\n",
      "training 0.000848116003908217 relative L2 0.0014254306443035603\n",
      "training 0.0008482342236675322 relative L2 0.0014249924570322037\n",
      "training 0.0008481303229928017 relative L2 0.0014254741836339235\n",
      "training 0.0008482856792397797 relative L2 0.0014250223757699132\n",
      "training 0.0008481708355247974 relative L2 0.0014254951383918524\n",
      "training 0.0008483105921186507 relative L2 0.0014249806990846992\n",
      "training 0.0008481218246743083 relative L2 0.0014253855915740132\n",
      "training 0.0008481754921376705 relative L2 0.0014247344806790352\n",
      "training 0.0008478268282487988 relative L2 0.0014250040985643864\n",
      "training 0.0008477140800096095 relative L2 0.0014241490280255675\n",
      "training 0.0008471218752674758 relative L2 0.0014241988537833095\n",
      "training 0.0008467418956570327 relative L2 0.001423100009560585\n",
      "training 0.0008458554511889815 relative L2 0.0014229381922632456\n",
      "training 0.0008452226174995303 relative L2 0.0014215765986591578\n",
      "training 0.0008440157980658114 relative L2 0.0014210836961865425\n",
      "training 0.0008429906447418034 relative L2 0.0014194156974554062\n",
      "training 0.0008414085023105145 relative L2 0.0014186389744281769\n",
      "training 0.0008400536607950926 relative L2 0.001416736631654203\n",
      "training 0.0008381796069443226 relative L2 0.0014156886609271169\n",
      "training 0.0008365144021809101 relative L2 0.0014135813107714057\n",
      "training 0.0008343848749063909 relative L2 0.0014123502187430859\n",
      "training 0.0008325202506966889 relative L2 0.0014101763954386115\n",
      "training 0.0008302985806949437 relative L2 0.0014087322633713484\n",
      "training 0.0008282020571641624 relative L2 0.0014064511051401496\n",
      "training 0.0008258383022621274 relative L2 0.0014047921868041158\n",
      "training 0.0008235126151703298 relative L2 0.0014024582924321294\n",
      "training 0.0008210701053030789 relative L2 0.0014007750432938337\n",
      "training 0.0008187488419935107 relative L2 0.0013985466212034225\n",
      "training 0.0008164126193150878 relative L2 0.0013970001600682735\n",
      "training 0.0008142843144014478 relative L2 0.0013950254069641232\n",
      "training 0.0008122303406707942 relative L2 0.001393851824104786\n",
      "training 0.0008105711312964559 relative L2 0.0013923195656388998\n",
      "training 0.0008090256014838815 relative L2 0.0013915672898292542\n",
      "training 0.0008078818791545928 relative L2 0.0013904842780902982\n",
      "training 0.0008068567258305848 relative L2 0.001390149467624724\n",
      "training 0.0008062145207077265 relative L2 0.0013895295560359955\n",
      "training 0.0008057328523136675 relative L2 0.0013896855525672436\n",
      "training 0.0008056680089794099 relative L2 0.001389582292176783\n",
      "training 0.0008058000821620226 relative L2 0.001390318269841373\n",
      "training 0.0008064097492024302 relative L2 0.0013908372493460774\n",
      "training 0.0008072903146967292 relative L2 0.0013923572842031717\n",
      "training 0.0008088053436949849 relative L2 0.0013937236508354545\n",
      "training 0.0008107149042189121 relative L2 0.0013963193632662296\n",
      "training 0.000813475635368377 relative L2 0.0013988366117700934\n",
      "training 0.0008167914929799736 relative L2 0.00140288844704628\n",
      "training 0.0008212527027353644 relative L2 0.0014068696182221174\n",
      "training 0.000826377363409847 relative L2 0.0014127148315310478\n",
      "training 0.0008329601841978729 relative L2 0.0014182935701683164\n",
      "training 0.0008400980732403696 relative L2 0.00142589770257473\n",
      "training 0.0008488011662848294 relative L2 0.0014326960081234574\n",
      "training 0.0008575482643209398 relative L2 0.0014414471806958318\n",
      "training 0.0008676809375174344 relative L2 0.0014482997357845306\n",
      "training 0.0008766474202275276 relative L2 0.0014563917648047209\n",
      "training 0.0008860216476023197 relative L2 0.001460823230445385\n",
      "training 0.0008921233820728958 relative L2 0.0014652116224169731\n",
      "training 0.000896938145160675 relative L2 0.0014640764566138387\n",
      "training 0.0008961659041233361 relative L2 0.0014615794643759727\n",
      "training 0.000892436015419662 relative L2 0.0014526275917887688\n",
      "training 0.0008819838403724134 relative L2 0.0014422137755900621\n",
      "training 0.0008686170913279057 relative L2 0.001426994800567627\n",
      "training 0.0008506273152306676 relative L2 0.0014122374122962356\n",
      "training 0.0008323827059939504 relative L2 0.0013963270466774702\n",
      "training 0.0008138316916301847 relative L2 0.0013830927200615406\n",
      "training 0.0007979047950357199 relative L2 0.0013720982242375612\n",
      "training 0.0007852981216274202 relative L2 0.0013650379842147231\n",
      "training 0.0007769418880343437 relative L2 0.001361208502203226\n",
      "training 0.0007725960458628833 relative L2 0.0013604277046397328\n",
      "training 0.0007716858526691794 relative L2 0.0013619076926261187\n",
      "training 0.0007733353995718062 relative L2 0.0013645995641127229\n",
      "training 0.0007765642367303371 relative L2 0.0013682302087545395\n",
      "training 0.0007806071080267429 relative L2 0.001371474820189178\n",
      "training 0.0007845844957046211 relative L2 0.0013747167540714145\n",
      "training 0.0007881232886575162 relative L2 0.0013765885960310698\n",
      "training 0.0007905703387223184 relative L2 0.0013781774323433638\n",
      "training 0.0007921511423774064 relative L2 0.0013781272573396564\n",
      "training 0.0007923782686702907 relative L2 0.0013780371518805623\n",
      "training 0.0007919868803583086 relative L2 0.001376472762785852\n",
      "training 0.0007904440280981362 relative L2 0.0013751209480687976\n",
      "training 0.0007885904633440077 relative L2 0.001372643164359033\n",
      "training 0.0007859698962420225 relative L2 0.0013706102035939693\n",
      "training 0.0007833523559384048 relative L2 0.0013678382383659482\n",
      "training 0.0007803720072843134 relative L2 0.0013656531227752566\n",
      "training 0.0007776188431307673 relative L2 0.001363065093755722\n",
      "training 0.0007748292991891503 relative L2 0.0013611121103167534\n",
      "training 0.0007723853341303766 relative L2 0.0013589554000645876\n",
      "training 0.0007700692513026297 relative L2 0.001357418717816472\n",
      "training 0.0007681426941417158 relative L2 0.0013557381462305784\n",
      "training 0.0007663526921533048 relative L2 0.0013545629335567355\n",
      "training 0.0007648724131286144 relative L2 0.00135326711460948\n",
      "training 0.0007635040092281997 relative L2 0.0013523860834538937\n",
      "training 0.0007623832789249718 relative L2 0.0013513874728232622\n",
      "training 0.0007613410707563162 relative L2 0.0013507212279364467\n",
      "training 0.0007604819256812334 relative L2 0.0013499363558366895\n",
      "training 0.0007596745854243636 relative L2 0.001349442987702787\n",
      "training 0.0007590207969769835 relative L2 0.0013488407712429762\n",
      "training 0.000758419104386121 relative L2 0.0013485192321240902\n",
      "training 0.0007579638622701168 relative L2 0.0013480987399816513\n",
      "training 0.0007575712515972555 relative L2 0.0013479877961799502\n",
      "training 0.0007573523325845599 relative L2 0.0013477945467457175\n",
      "training 0.0007572275353595614 relative L2 0.001347969751805067\n",
      "training 0.0007573242182843387 relative L2 0.001348121091723442\n",
      "training 0.0007576088537462056 relative L2 0.0013488002587109804\n",
      "training 0.0007582612452097237 relative L2 0.0013495625462383032\n",
      "training 0.0007592711481265724 relative L2 0.0013511204160749912\n",
      "training 0.0007608995656482875 relative L2 0.001352991908788681\n",
      "training 0.0007632214110344648 relative L2 0.0013561351224780083\n",
      "training 0.0007666313904337585 relative L2 0.0013599059311673045\n",
      "training 0.000771205872297287 relative L2 0.0013657051604241133\n",
      "training 0.0007776442798785865 relative L2 0.001372602884657681\n",
      "training 0.0007859607576392591 relative L2 0.001382701564580202\n",
      "training 0.000797411659732461 relative L2 0.0013944520615041256\n",
      "training 0.0008116551907733083 relative L2 0.0014109060866758227\n",
      "training 0.000830775941722095 relative L2 0.0014292339328676462\n",
      "training 0.0008533779182471335 relative L2 0.00145324959885329\n",
      "training 0.0008821515366435051 relative L2 0.001477353973314166\n",
      "training 0.000912776100449264 relative L2 0.0015055063413456082\n",
      "training 0.0009476657141931355 relative L2 0.0015275564510375261\n",
      "training 0.0009768279269337654 relative L2 0.0015470109647139907\n",
      "training 0.001001355005428195 relative L2 0.0015498213469982147\n",
      "training 0.001005916972644627 relative L2 0.0015413241926580667\n",
      "training 0.0009939057053998113 relative L2 0.0015106324572116137\n",
      "training 0.0009549939422868192 relative L2 0.0014698620652779937\n",
      "training 0.0009027085616253316 relative L2 0.0014194402610883117\n",
      "training 0.0008415260235778987 relative L2 0.0013759228168055415\n",
      "training 0.0007894590380601585 relative L2 0.0013449686812236905\n",
      "training 0.0007540226797573268 relative L2 0.0013332117814570665\n",
      "training 0.0007405515061691403 relative L2 0.0013384275371208787\n",
      "training 0.00074639858212322 relative L2 0.001354035222902894\n",
      "training 0.000764432072173804 relative L2 0.0013732793740928173\n",
      "training 0.0007863651844672859 relative L2 0.001387236756272614\n",
      "training 0.0008031165925785899 relative L2 0.0013945162063464522\n",
      "training 0.0008112486684694886 relative L2 0.001390302088111639\n",
      "training 0.0008067311719059944 relative L2 0.001379644963890314\n",
      "training 0.0007937732734717429 relative L2 0.0013629599707201123\n",
      "training 0.0007747381459921598 relative L2 0.0013473685830831528\n",
      "training 0.0007565152482129633 relative L2 0.0013349954970180988\n",
      "training 0.0007426354568451643 relative L2 0.001329019432887435\n",
      "training 0.0007357865106314421 relative L2 0.0013290824135765433\n",
      "training 0.0007358472794294357 relative L2 0.0013333611423149705\n",
      "training 0.0007407806115224957 relative L2 0.0013395986752584577\n",
      "training 0.0007476767059415579 relative L2 0.0013445719378069043\n",
      "training 0.0007535670301876962 relative L2 0.0013477603206411004\n",
      "training 0.000756947323679924 relative L2 0.0013471882557496428\n",
      "training 0.0007565626292489469 relative L2 0.0013447081437334418\n",
      "training 0.0007534686010330915 relative L2 0.0013396769063547254\n",
      "training 0.000747977290302515 relative L2 0.0013346184277907014\n",
      "training 0.0007420368492603302 relative L2 0.001329542021267116\n",
      "training 0.0007364571210928261 relative L2 0.0013259672559797764\n",
      "training 0.0007323239115066826 relative L2 0.0013238390674814582\n",
      "training 0.0007299864664673805 relative L2 0.0013233175268396735\n",
      "training 0.0007293956005014479 relative L2 0.001324007986113429\n",
      "training 0.0007301297737285495 relative L2 0.0013252426870167255\n",
      "training 0.000731595151592046 relative L2 0.0013268012553453445\n",
      "training 0.0007332378881983459 relative L2 0.0013278477126732469\n",
      "training 0.0007345457561314106 relative L2 0.001328734215348959\n",
      "training 0.0007353995461016893 relative L2 0.0013286805478855968\n",
      "training 0.0007354894769378006 relative L2 0.0013284152373671532\n",
      "training 0.0007350349333137274 relative L2 0.001327304751612246\n",
      "training 0.000733932654839009 relative L2 0.001326187513768673\n",
      "training 0.0007325334008783102 relative L2 0.0013245686423033476\n",
      "training 0.0007308424101211131 relative L2 0.0013231546618044376\n",
      "training 0.0007291370420716703 relative L2 0.0013215679209679365\n",
      "training 0.0007274584495462477 relative L2 0.0013203152921050787\n",
      "training 0.0007259660051204264 relative L2 0.0013190839672461152\n",
      "training 0.0007246575551107526 relative L2 0.001318171969614923\n",
      "training 0.0007235812372528017 relative L2 0.0013173623010516167\n",
      "training 0.0007227142341434956 relative L2 0.0013167631113901734\n",
      "training 0.0007220159168355167 relative L2 0.001316248788498342\n",
      "training 0.0007214552024379373 relative L2 0.0013158464571461082\n",
      "training 0.000721000658813864 relative L2 0.0013155171182006598\n",
      "training 0.0007206249865703285 relative L2 0.001315220259130001\n",
      "training 0.0007203065324574709 relative L2 0.0013149926671758294\n",
      "training 0.00072002864908427 relative L2 0.0013147431891411543\n",
      "training 0.0007197783561423421 relative L2 0.001314586028456688\n",
      "training 0.0007195632788352668 relative L2 0.001314383465796709\n",
      "training 0.0007193807978183031 relative L2 0.0013142907992005348\n",
      "training 0.0007192232878878713 relative L2 0.0013141323579475284\n",
      "training 0.000719104427844286 relative L2 0.0013141355011612177\n",
      "training 0.0007190395845100284 relative L2 0.0013140622759237885\n",
      "training 0.0007190298056229949 relative L2 0.0013141927774995565\n",
      "training 0.0007190919131971896 relative L2 0.001314238877967\n",
      "training 0.0007192313787527382 relative L2 0.0013145592529326677\n",
      "training 0.0007194888312369585 relative L2 0.0013148097787052393\n",
      "training 0.0007198740495368838 relative L2 0.0013154458720237017\n",
      "training 0.0007204629364423454 relative L2 0.0013160442467778921\n",
      "training 0.0007212602067738771 relative L2 0.001317214802838862\n",
      "training 0.0007224208675324917 relative L2 0.0013184348354116082\n",
      "training 0.0007239427068270743 relative L2 0.0013204714050516486\n",
      "training 0.0007260424899868667 relative L2 0.0013225874863564968\n",
      "training 0.0007286116015166044 relative L2 0.0013258992694318295\n",
      "training 0.0007321088924072683 relative L2 0.001329468679614365\n",
      "training 0.0007363740587607026 relative L2 0.0013347923522815108\n",
      "training 0.0007421138579957187 relative L2 0.0013405433855950832\n",
      "training 0.0007489458075724542 relative L2 0.0013487935066223145\n",
      "training 0.0007580125238746405 relative L2 0.0013576298952102661\n",
      "training 0.0007685375167056918 relative L2 0.001369738020002842\n",
      "training 0.0007821180042810738 relative L2 0.0013821335742250085\n",
      "training 0.0007970595615915954 relative L2 0.001398227526806295\n",
      "training 0.0008155168034136295 relative L2 0.001413258258253336\n",
      "training 0.0008340153144672513 relative L2 0.0014311519917100668\n",
      "training 0.0008549796184524894 relative L2 0.001444679219275713\n",
      "training 0.0008721500053070486 relative L2 0.001458051148802042\n",
      "training 0.000887906237039715 relative L2 0.0014618599088862538\n",
      "training 0.0008933549979701638 relative L2 0.0014613043749704957\n",
      "training 0.0008919222746044397 relative L2 0.001447395421564579\n",
      "training 0.0008754844311624765 relative L2 0.0014278910821303725\n",
      "training 0.0008510060724802315 relative L2 0.001398196560330689\n",
      "training 0.0008160271099768579 relative L2 0.0013682892313227057\n",
      "training 0.0007803930202499032 relative L2 0.0013386363862082362\n",
      "training 0.0007467645918950438 relative L2 0.0013168719597160816\n",
      "training 0.000721944437827915 relative L2 0.0013039400801062584\n",
      "training 0.0007077324553392828 relative L2 0.001300955656915903\n",
      "training 0.0007043925579637289 relative L2 0.0013057455653324723\n",
      "training 0.0007096040062606335 relative L2 0.001314713852480054\n",
      "training 0.0007197581580840051 relative L2 0.0013253032229840755\n",
      "training 0.0007313452078960836 relative L2 0.001333278720267117\n",
      "training 0.0007406693766824901 relative L2 0.001338769681751728\n",
      "training 0.0007465229136869311 relative L2 0.0013387149665504694\n",
      "training 0.0007468453259207308 relative L2 0.0013358293799683452\n",
      "training 0.0007431904668919742 relative L2 0.001328665530309081\n",
      "training 0.000735450885258615 relative L2 0.001320830313488841\n",
      "training 0.0007263249717652798 relative L2 0.0013120403746142983\n",
      "training 0.0007167750154621899 relative L2 0.0013050257693976164\n",
      "training 0.0007087804260663688 relative L2 0.0012996891746297479\n",
      "training 0.0007030245615169406 relative L2 0.0012969590025022626\n",
      "training 0.0006999433971941471 relative L2 0.0012963691260665655\n",
      "training 0.0006993034039624035 relative L2 0.0012973872944712639\n",
      "training 0.0007004745420999825 relative L2 0.0012994605349376798\n",
      "training 0.0007026573293842375 relative L2 0.001301520736888051\n",
      "training 0.0007050720159895718 relative L2 0.0013036540476605296\n",
      "training 0.000707257364410907 relative L2 0.0013047638349235058\n",
      "training 0.0007086795521900058 relative L2 0.0013055962044745684\n",
      "training 0.000709395098965615 relative L2 0.0013051691930741072\n",
      "training 0.0007091336883604527 relative L2 0.0013046698877587914\n",
      "training 0.0007083714008331299 relative L2 0.0013031595153734088\n",
      "training 0.0007069048006087542 relative L2 0.0013018455356359482\n",
      "training 0.0007052615401335061 relative L2 0.0012999364407733083\n",
      "training 0.0007033331203274429 relative L2 0.0012984330533072352\n",
      "training 0.0007015158189460635 relative L2 0.0012966619106009603\n",
      "training 0.0006997109740041196 relative L2 0.0012953435070812702\n",
      "training 0.0006981358164921403 relative L2 0.0012939359294250607\n",
      "training 0.000696699193213135 relative L2 0.0012929339427500963\n",
      "training 0.0006955090211704373 relative L2 0.0012919491855427623\n",
      "training 0.0006945045897737145 relative L2 0.0012912657111883163\n",
      "training 0.0006936954450793564 relative L2 0.0012906150659546256\n",
      "training 0.0006930292584002018 relative L2 0.0012901456793770194\n",
      "training 0.0006924827466718853 relative L2 0.001289709354750812\n",
      "training 0.0006920260493643582 relative L2 0.0012893651146441698\n",
      "training 0.0006916383863426745 relative L2 0.0012890601065009832\n",
      "training 0.0006913056713528931 relative L2 0.0012887814082205296\n",
      "training 0.0006910081137903035 relative L2 0.0012885509058833122\n",
      "training 0.0006907418719492853 relative L2 0.0012883155141025782\n",
      "training 0.0006905051996000111 relative L2 0.0012881482252851129\n",
      "training 0.0006902947789058089 relative L2 0.0012879559071734548\n",
      "training 0.000690119166392833 relative L2 0.001287877676077187\n",
      "training 0.0006899912259541452 relative L2 0.0012877688277512789\n",
      "training 0.0006899220170453191 relative L2 0.001287827966734767\n",
      "training 0.0006899272557348013 relative L2 0.0012878825655207038\n",
      "training 0.0006900555454194546 relative L2 0.00128821877297014\n",
      "training 0.0006903418689034879 relative L2 0.0012885835021734238\n",
      "training 0.0006908344221301377 relative L2 0.0012893818784505129\n",
      "training 0.0006915965932421386 relative L2 0.0012902902672067285\n",
      "training 0.000692716974299401 relative L2 0.001291943364776671\n",
      "training 0.0006943789776414633 relative L2 0.0012938972795382142\n",
      "training 0.0006966944201849401 relative L2 0.0012971062678843737\n",
      "training 0.0007000180776230991 relative L2 0.0013009426183998585\n",
      "training 0.0007044798112474382 relative L2 0.0013069453416392207\n",
      "training 0.0007108459249138832 relative L2 0.0013141875388100743\n",
      "training 0.0007192155462689698 relative L2 0.001325057353824377\n",
      "training 0.0007310140645131469 relative L2 0.001338075497187674\n",
      "training 0.0007461490458808839 relative L2 0.0013568102149292827\n",
      "training 0.0007670663180761039 relative L2 0.0013785103801637888\n",
      "training 0.0007928228587843478 relative L2 0.0014079270185902715\n",
      "training 0.0008269285317510366 relative L2 0.0014391106087714434\n",
      "training 0.0008653480326756835 relative L2 0.0014771274290978909\n",
      "training 0.0009115204447880387 relative L2 0.0015098981093615294\n",
      "training 0.0009539967868477106 relative L2 0.0015414651716127992\n",
      "training 0.0009938203729689121 relative L2 0.0015530771343037486\n",
      "training 0.0010101549560204148 relative L2 0.0015496789710596204\n",
      "training 0.0010045724920928478 relative L2 0.0015144479693844914\n",
      "training 0.0009598442120477557 relative L2 0.0014619831927120686\n",
      "training 0.0008926403825171292 relative L2 0.0013927804538980126\n",
      "training 0.0008096317760646343 relative L2 0.0013316846452653408\n",
      "training 0.0007384322234429419 relative L2 0.001290314132347703\n",
      "training 0.0006927602225914598 relative L2 0.0012792609632015228\n",
      "training 0.0006806133897043765 relative L2 0.0012939544394612312\n",
      "training 0.0006965069915167987 relative L2 0.0013210270553827286\n",
      "training 0.0007268823101185262 relative L2 0.0013484652154147625\n",
      "training 0.0007574557093903422 relative L2 0.0013626153813675046\n",
      "training 0.0007743153255432844 relative L2 0.0013630707981064916\n",
      "training 0.0007742215530015528 relative L2 0.0013463945360854268\n",
      "training 0.0007556434720754623 relative L2 0.0013230537297204137\n",
      "training 0.0007287051412276924 relative L2 0.0012982110492885113\n",
      "training 0.0007014595903456211 relative L2 0.0012817723909392953\n",
      "training 0.0006832380313426256 relative L2 0.001276517636142671\n",
      "training 0.0006776034133508801 relative L2 0.0012814856600016356\n",
      "training 0.0006830778438597918 relative L2 0.0012921122834086418\n",
      "training 0.0006944675114937127 relative L2 0.0013018158497288823\n",
      "training 0.0007054562447592616 relative L2 0.0013078444171696901\n",
      "training 0.0007117631612345576 relative L2 0.0013066347455605865\n",
      "training 0.0007108080317266285 relative L2 0.0013010022230446339\n",
      "training 0.0007042109500616789 relative L2 0.001291528227739036\n",
      "training 0.0006941074971109629 relative L2 0.0012827658792957664\n",
      "training 0.0006842972943559289 relative L2 0.001276282942853868\n",
      "training 0.0006774102803319693 relative L2 0.0012738872319459915\n",
      "training 0.0006747646839357913 relative L2 0.001275021000765264\n",
      "training 0.0006759550306014717 relative L2 0.0012781460536643863\n",
      "training 0.0006794590735808015 relative L2 0.001281931297853589\n",
      "training 0.0006833926890976727 relative L2 0.0012842256110161543\n",
      "training 0.00068611279129982 relative L2 0.001285139936953783\n",
      "training 0.0006868712371215224 relative L2 0.0012835764791816473\n",
      "training 0.0006854095263406634 relative L2 0.0012810988118872046\n",
      "training 0.0006824946613050997 relative L2 0.0012775924988090992\n",
      "training 0.0006788792088627815 relative L2 0.0012746363645419478\n",
      "training 0.000675535702612251 relative L2 0.0012722756946459413\n",
      "training 0.0006730807363055646 relative L2 0.0012711600866168737\n",
      "training 0.0006718356162309647 relative L2 0.001271052286028862\n",
      "training 0.00067171361297369 relative L2 0.0012716077035292983\n",
      "training 0.0006723643164150417 relative L2 0.001272603520192206\n",
      "training 0.0006733539630658925 relative L2 0.001273344038054347\n",
      "training 0.0006742681143805385 relative L2 0.0012739922385662794\n",
      "training 0.0006748366286046803 relative L2 0.0012739208759739995\n",
      "training 0.0006749017047695816 relative L2 0.0012737096985802054\n",
      "training 0.0006745308637619019 relative L2 0.0012728367000818253\n",
      "training 0.0006737250369042158 relative L2 0.0012720065424218774\n",
      "training 0.000672703143209219 relative L2 0.00127084506675601\n",
      "training 0.0006715618656016886 relative L2 0.0012699421495199203\n",
      "training 0.0006704959087073803 relative L2 0.0012690054718405008\n",
      "training 0.000669560395181179 relative L2 0.0012683665845543146\n",
      "training 0.000668820459395647 relative L2 0.0012678327038884163\n",
      "training 0.0006682787789031863 relative L2 0.0012675082543864846\n",
      "training 0.0006679163780063391 relative L2 0.001267312210984528\n",
      "training 0.0006677014753222466 relative L2 0.001267187763005495\n",
      "training 0.0006675870972685516 relative L2 0.0012671725125983357\n",
      "training 0.0006675369222648442 relative L2 0.0012671109288930893\n",
      "training 0.0006675152690149844 relative L2 0.0012671509757637978\n",
      "training 0.0006675028707832098 relative L2 0.001267068786546588\n",
      "training 0.0006674774922430515 relative L2 0.0012670947471633554\n",
      "training 0.0006674340111203492 relative L2 0.0012669559800997376\n",
      "training 0.0006673603784292936 relative L2 0.0012669460847973824\n",
      "training 0.0006672683521173894 relative L2 0.0012667571427300572\n",
      "training 0.0006671496666967869 relative L2 0.0012667217524722219\n",
      "training 0.0006670237053185701 relative L2 0.0012665028916671872\n",
      "training 0.0006668784189969301 relative L2 0.0012664622627198696\n",
      "training 0.0006667411653324962 relative L2 0.0012662463122978806\n",
      "training 0.0006666034460067749 relative L2 0.0012662257067859173\n",
      "training 0.0006664831307716668 relative L2 0.001266028848476708\n",
      "training 0.0006663717213086784 relative L2 0.0012660535285249352\n",
      "training 0.0006662943633273244 relative L2 0.0012659128988161683\n",
      "training 0.0006662498344667256 relative L2 0.0012660252396017313\n",
      "training 0.0006662578671239316 relative L2 0.0012659659842029214\n",
      "training 0.0006663113017566502 relative L2 0.0012662230292335153\n",
      "training 0.0006664622342213988 relative L2 0.0012663156958296895\n",
      "training 0.0006666925037279725 relative L2 0.0012667950941249728\n",
      "training 0.000667066196911037 relative L2 0.0012671318836510181\n",
      "training 0.000667578773573041 relative L2 0.0012680087238550186\n",
      "training 0.0006683564861305058 relative L2 0.0012687763664871454\n",
      "training 0.0006693617324344814 relative L2 0.001270290114916861\n",
      "training 0.0006707934662699699 relative L2 0.0012717406498268247\n",
      "training 0.0006725749699398875 relative L2 0.0012742603430524468\n",
      "training 0.0006750529864802957 relative L2 0.0012768390588462353\n",
      "training 0.0006781123811379075 relative L2 0.0012809752952307463\n",
      "training 0.0006822988507337868 relative L2 0.0012853837106376886\n",
      "training 0.0006874338141642511 relative L2 0.0012921756133437157\n",
      "training 0.0006944813649170101 relative L2 0.001299569383263588\n",
      "training 0.0007030345150269568 relative L2 0.001310387859120965\n",
      "training 0.0007145330891944468 relative L2 0.0013219999382272363\n",
      "training 0.0007280370919033885 relative L2 0.001338230213150382\n",
      "training 0.0007457478786818683 relative L2 0.0013549301074817777\n",
      "training 0.0007655014633201063 relative L2 0.0013768343487754464\n",
      "training 0.0007901309872977436 relative L2 0.0013971652369946241\n",
      "training 0.0008148825145326555 relative L2 0.0014211683301255107\n",
      "training 0.0008426751592196524 relative L2 0.001438387669622898\n",
      "training 0.0008645328343845904 relative L2 0.001454332028515637\n",
      "training 0.0008830715669319034 relative L2 0.0014557220274582505\n",
      "training 0.0008858385845087469 relative L2 0.0014497743686661124\n",
      "training 0.0008774556335993111 relative L2 0.0014250503154471517\n",
      "training 0.0008483126875944436 relative L2 0.0013930001296103\n",
      "training 0.0008090719929896295 relative L2 0.001350234029814601\n",
      "training 0.0007601042743772268 relative L2 0.0013108942657709122\n",
      "training 0.0007150563760660589 relative L2 0.001278255949728191\n",
      "training 0.0006796520319767296 relative L2 0.001260354183614254\n",
      "training 0.0006601273780688643 relative L2 0.0012570570688694715\n",
      "training 0.0006566466181538999 relative L2 0.0012654107995331287\n",
      "training 0.0006657227640971541 relative L2 0.0012802225537598133\n",
      "training 0.0006814209627918899 relative L2 0.0012943235924467444\n",
      "training 0.0006972363917157054 relative L2 0.001305398065596819\n",
      "training 0.000708949170075357 relative L2 0.0013082550140097737\n",
      "training 0.0007126557175070047 relative L2 0.0013057512696832418\n",
      "training 0.0007093356689438224 relative L2 0.001296008238568902\n",
      "training 0.0006990926922298968 relative L2 0.0012843833537772298\n",
      "training 0.0006859202403575182 relative L2 0.0012715234188362956\n",
      "training 0.0006723353289999068 relative L2 0.0012617672327905893\n",
      "training 0.0006615963065996766 relative L2 0.0012557124719023705\n",
      "training 0.0006552726845256984 relative L2 0.0012542323675006628\n",
      "training 0.0006536627188324928 relative L2 0.0012563408818095922\n",
      "training 0.0006558425957337022 relative L2 0.001260282821021974\n",
      "training 0.0006602049106732011 relative L2 0.0012649840209633112\n",
      "training 0.0006650186260230839 relative L2 0.0012682103551924229\n",
      "training 0.0006687603308819234 relative L2 0.0012702875537797809\n",
      "training 0.0006706951535306871 relative L2 0.0012697326019406319\n",
      "training 0.0006704102270305157 relative L2 0.0012681717053055763\n",
      "training 0.0006684268009848893 relative L2 0.0012647694675251842\n",
      "training 0.0006650524446740746 relative L2 0.0012613902799785137\n",
      "training 0.0006611863500438631 relative L2 0.0012576829176396132\n",
      "training 0.0006574275903403759 relative L2 0.00125493248924613\n",
      "training 0.0006543441559188068 relative L2 0.0012528126826509833\n",
      "training 0.0006521920440718532 relative L2 0.001251792535185814\n",
      "training 0.0006510583916679025 relative L2 0.0012515503913164139\n",
      "training 0.0006508059450425208 relative L2 0.001251873909495771\n",
      "training 0.0006511901738122106 relative L2 0.0012526571517810225\n",
      "training 0.0006519473972730339 relative L2 0.0012533816043287516\n",
      "training 0.0006528254598379135 relative L2 0.0012542628683149815\n",
      "training 0.0006536332657560706 relative L2 0.0012546844081953168\n",
      "training 0.0006542304181493819 relative L2 0.0012551674153655767\n",
      "training 0.000654586183372885 relative L2 0.0012550540268421173\n",
      "training 0.0006546314689330757 relative L2 0.001255054259672761\n",
      "training 0.0006544642965309322 relative L2 0.001254527596756816\n",
      "training 0.0006540701142512262 relative L2 0.0012542065232992172\n",
      "training 0.0006535665597766638 relative L2 0.0012534676352515817\n",
      "training 0.0006529367528855801 relative L2 0.001252983813174069\n",
      "training 0.0006522744079120457 relative L2 0.0012521916069090366\n",
      "training 0.0006515707937069237 relative L2 0.0012516940478235483\n",
      "training 0.0006509139784611762 relative L2 0.001250987290404737\n",
      "training 0.0006502827163785696 relative L2 0.001250564120709896\n",
      "training 0.0006497235735878348 relative L2 0.0012499723816290498\n",
      "training 0.0006491966196335852 relative L2 0.0012496266281232238\n",
      "training 0.0006487377686426044 relative L2 0.0012491593370214105\n",
      "training 0.0006483267061412334 relative L2 0.0012489022919908166\n",
      "training 0.0006479758303612471 relative L2 0.0012485409388318658\n",
      "training 0.0006476661656051874 relative L2 0.001248359214514494\n",
      "training 0.0006474042893387377 relative L2 0.0012480750447139144\n",
      "training 0.0006471693050116301 relative L2 0.0012479479191824794\n",
      "training 0.0006469694199040532 relative L2 0.0012477203272283077\n",
      "training 0.0006467927596531808 relative L2 0.0012476490810513496\n",
      "training 0.0006466516060754657 relative L2 0.001247480628080666\n",
      "training 0.0006465406622737646 relative L2 0.00124748598318547\n",
      "training 0.000646475178655237 relative L2 0.0012473914539441466\n",
      "training 0.0006464512553066015 relative L2 0.0012475131079554558\n",
      "training 0.000646497355774045 relative L2 0.0012475401163101196\n",
      "training 0.0006466165650635958 relative L2 0.0012478668941184878\n",
      "training 0.0006468623178079724 relative L2 0.001248127082362771\n",
      "training 0.0006472502718679607 relative L2 0.001248823944479227\n",
      "training 0.0006478605209849775 relative L2 0.001249488559551537\n",
      "training 0.0006487107602879405 relative L2 0.0012508388608694077\n",
      "training 0.0006499762530438602 relative L2 0.0012523054610937834\n",
      "training 0.000651725975330919 relative L2 0.0012549447128549218\n",
      "training 0.0006543112103827298 relative L2 0.001258007949218154\n",
      "training 0.0006578344036825001 relative L2 0.0012630777200683951\n",
      "training 0.0006629564450122416 relative L2 0.0012692242162302136\n",
      "training 0.0006699106306768954 relative L2 0.0012790367472916842\n",
      "training 0.000680106517393142 relative L2 0.0012911621015518904\n",
      "training 0.0006938108126632869 relative L2 0.0013097600312903523\n",
      "training 0.0007137614884413779 relative L2 0.001332558924332261\n",
      "training 0.0007399871828965843 relative L2 0.0013656585942953825\n",
      "training 0.0007770886295475066 relative L2 0.001403904752805829\n",
      "training 0.0008229415398091078 relative L2 0.0014545931480824947\n",
      "training 0.0008833392057567835 relative L2 0.0015051491791382432\n",
      "training 0.0009480350418016315 relative L2 0.0015611894195899367\n",
      "training 0.001019553397782147 relative L2 0.001598019851371646\n",
      "training 0.0010704107116907835 relative L2 0.0016183904372155666\n",
      "training 0.0010966192930936813 relative L2 0.0015923967584967613\n",
      "training 0.0010627956362441182 relative L2 0.0015331691829487681\n",
      "training 0.0009827943285927176 relative L2 0.0014344955561682582\n",
      "training 0.0008598302374593914 relative L2 0.0013358003925532103\n",
      "training 0.0007429064717143774 relative L2 0.0012626610696315765\n",
      "training 0.0006628368282690644 relative L2 0.0012422094587236643\n",
      "training 0.0006409490597434342 relative L2 0.0012695863842964172\n",
      "training 0.0006698896759189665 relative L2 0.0013174103805795312\n",
      "training 0.0007229188922792673 relative L2 0.001359965419396758\n",
      "training 0.0007704745512455702 relative L2 0.0013731805374845862\n",
      "training 0.0007866861415095627 relative L2 0.0013589994050562382\n",
      "training 0.0007693564984947443 relative L2 0.001318981172516942\n",
      "training 0.0007246743771247566 relative L2 0.0012763722334057093\n",
      "training 0.0006771825137548149 relative L2 0.0012467295164242387\n",
      "training 0.0006457801791839302 relative L2 0.0012415323872119188\n",
      "training 0.0006402378785423934 relative L2 0.0012565450742840767\n",
      "training 0.000655954412650317 relative L2 0.0012778155505657196\n",
      "training 0.0006792162312194705 relative L2 0.0012936843559145927\n",
      "training 0.0006960027385503054 relative L2 0.0012943808687850833\n",
      "training 0.0006973443669266999 relative L2 0.0012832251377403736\n",
      "training 0.000684599915985018 relative L2 0.0012640877394005656\n",
      "training 0.0006643685628660023 relative L2 0.0012478065909817815\n",
      "training 0.0006467194762080908 relative L2 0.0012396613601595163\n",
      "training 0.0006382570718415082 relative L2 0.001241504680365324\n",
      "training 0.0006402344442903996 relative L2 0.0012497968273237348\n",
      "training 0.0006488148355856538 relative L2 0.0012580836191773415\n",
      "training 0.0006579275359399617 relative L2 0.0012629671255126595\n",
      "training 0.000662793405354023 relative L2 0.0012610952835530043\n",
      "training 0.0006611644639633596 relative L2 0.0012552830157801509\n",
      "training 0.0006546199438162148 relative L2 0.001247104606591165\n",
      "training 0.000646212778519839 relative L2 0.0012409305199980736\n",
      "training 0.0006395185482688248 relative L2 0.0012380123371258378\n",
      "training 0.0006365262670442462 relative L2 0.0012387698516249657\n",
      "training 0.000637353106867522 relative L2 0.001241904217749834\n",
      "training 0.0006405366584658623 relative L2 0.0012450040085241199\n",
      "training 0.0006439949502237141 relative L2 0.0012471657246351242\n",
      "training 0.0006460559088736773 relative L2 0.0012467338237911463\n",
      "training 0.0006458411808125675 relative L2 0.0012449233327060938\n",
      "training 0.0006437019328586757 relative L2 0.0012417393736541271\n",
      "training 0.0006405377062037587 relative L2 0.0012390324845910072\n",
      "training 0.0006375477532856166 relative L2 0.0012370655313134193\n",
      "training 0.0006355671212077141 relative L2 0.0012364855501800776\n",
      "training 0.0006349365576170385 relative L2 0.001237009302712977\n",
      "training 0.0006354532670229673 relative L2 0.001238000812008977\n",
      "training 0.0006365781300701201 relative L2 0.0012391771888360381\n",
      "training 0.0006376957171596587 relative L2 0.0012396270176395774\n",
      "training 0.000638311670627445 relative L2 0.0012397030368447304\n",
      "training 0.0006382422870956361 relative L2 0.0012388786999508739\n",
      "training 0.0006375215016305447 relative L2 0.0012379540130496025\n",
      "training 0.000636421493254602 relative L2 0.001236719312146306\n",
      "training 0.0006352303316816688 relative L2 0.0012358408421278\n",
      "training 0.0006342346896417439 relative L2 0.0012351862387731671\n",
      "training 0.0006335917860269547 relative L2 0.001234952243976295\n",
      "training 0.0006333369528874755 relative L2 0.0012350331526249647\n",
      "training 0.0006334019126370549 relative L2 0.001235221978276968\n",
      "training 0.000633646035566926 relative L2 0.0012355457292869687\n",
      "training 0.0006339190294966102 relative L2 0.001235641771927476\n",
      "training 0.0006341013358905911 relative L2 0.0012357494561001658\n",
      "training 0.0006341250264085829 relative L2 0.0012355092912912369\n",
      "training 0.0006339646060951054 relative L2 0.001235293340869248\n",
      "training 0.0006336501683108509 relative L2 0.0012348158052191138\n",
      "training 0.0006332306074909866 relative L2 0.0012344553833827376\n",
      "training 0.0006327832816168666 relative L2 0.0012339926324784756\n",
      "training 0.0006323554553091526 relative L2 0.001233684946782887\n",
      "training 0.0006319917738437653 relative L2 0.0012333919294178486\n",
      "training 0.000631711445748806 relative L2 0.00123321614228189\n",
      "training 0.0006315171485766768 relative L2 0.0012331018224358559\n",
      "training 0.0006313949706964195 relative L2 0.0012330160243436694\n",
      "training 0.0006313204066827893 relative L2 0.0012329923920333385\n",
      "training 0.0006312705227173865 relative L2 0.0012329159071668983\n",
      "training 0.0006312247714959085 relative L2 0.0012328982120379806\n",
      "training 0.0006311664474196732 relative L2 0.0012327757431194186\n",
      "training 0.0006310833850875497 relative L2 0.0012327146250754595\n",
      "training 0.0006309726741164923 relative L2 0.0012325355783104897\n",
      "training 0.0006308341398835182 relative L2 0.0012324264971539378\n",
      "training 0.0006306732539087534 relative L2 0.0012322147376835346\n",
      "training 0.0006304971757344902 relative L2 0.0012320757377892733\n",
      "training 0.0006303113186731935 relative L2 0.0012318581575527787\n",
      "training 0.0006301216781139374 relative L2 0.001231710659340024\n",
      "training 0.0006299350643530488 relative L2 0.0012315105414018035\n",
      "training 0.0006297547952271998 relative L2 0.001231369678862393\n",
      "training 0.0006295847124420106 relative L2 0.0012311965692788363\n",
      "training 0.0006294231861829758 relative L2 0.0012310625752434134\n",
      "training 0.0006292694015428424 relative L2 0.0012309129815548658\n",
      "training 0.0006291235331445932 relative L2 0.0012307852739468217\n",
      "training 0.0006289851735346019 relative L2 0.001230656635016203\n",
      "training 0.0006288530421443284 relative L2 0.0012305323034524918\n",
      "training 0.0006287253927439451 relative L2 0.0012304187985137105\n",
      "training 0.0006286022835411131 relative L2 0.0012302943505346775\n",
      "training 0.0006284805131144822 relative L2 0.0012301887618377805\n",
      "training 0.0006283604889176786 relative L2 0.00123006256762892\n",
      "training 0.0006282422691583633 relative L2 0.0012299629161134362\n",
      "training 0.0006281239329837263 relative L2 0.0012298329966142774\n",
      "training 0.0006280053057707846 relative L2 0.0012297338107600808\n",
      "training 0.0006278849905356765 relative L2 0.0012295988854020834\n",
      "training 0.0006277631036937237 relative L2 0.0012295008637011051\n",
      "training 0.0006276423810049891 relative L2 0.001229366404004395\n",
      "training 0.0006275223568081856 relative L2 0.0012292719911783934\n",
      "training 0.0006274037295952439 relative L2 0.0012291398597881198\n",
      "training 0.0006272883620113134 relative L2 0.0012290546437725425\n",
      "training 0.0006271765450946987 relative L2 0.0012289294973015785\n",
      "training 0.0006270720041356981 relative L2 0.0012288623256608844\n",
      "training 0.0006269749137572944 relative L2 0.0012287497520446777\n",
      "training 0.0006268882425501943 relative L2 0.001228713896125555\n",
      "training 0.0006268171709962189 relative L2 0.0012286247219890356\n",
      "training 0.0006267636781558394 relative L2 0.001228646608069539\n",
      "training 0.0006267427816055715 relative L2 0.001228615059517324\n",
      "training 0.0006267614080570638 relative L2 0.001228751614689827\n",
      "training 0.0006268442375585437 relative L2 0.0012288485886529088\n",
      "training 0.0006270157755352557 relative L2 0.001229227869771421\n",
      "training 0.000627328990958631 relative L2 0.0012296080822125077\n",
      "training 0.0006278237560763955 relative L2 0.001230470254085958\n",
      "training 0.0006286075222305954 relative L2 0.0012314605992287397\n",
      "training 0.0006297811632975936 relative L2 0.0012333811027929187\n",
      "training 0.0006316219223663211 relative L2 0.0012357946252450347\n",
      "training 0.0006343527347780764 relative L2 0.0012400808045640588\n",
      "training 0.0006386096938513219 relative L2 0.0012457937700673938\n",
      "training 0.0006449328502640128 relative L2 0.0012554031563922763\n",
      "training 0.0006547665107063949 relative L2 0.0012684763642027974\n",
      "training 0.0006692070746794343 relative L2 0.0012896640691906214\n",
      "training 0.0006916577112860978 relative L2 0.0013181716203689575\n",
      "training 0.000723856792319566 relative L2 0.0013619757955893874\n",
      "training 0.0007728519849479198 relative L2 0.0014177759876474738\n",
      "training 0.0008396300254389644 relative L2 0.001495667384006083\n",
      "training 0.0009347079321742058 relative L2 0.0015813229838386178\n",
      "training 0.0010479189222678542 relative L2 0.0016798818251118064\n",
      "training 0.0011826014379039407 relative L2 0.0017522327834740281\n",
      "training 0.001289799460209906 relative L2 0.0017920088721439242\n",
      "training 0.0013475812738761306 relative L2 0.001744415727443993\n",
      "training 0.0012781985569745302 relative L2 0.0016263658180832863\n",
      "training 0.0011075959773734212 relative L2 0.0014434759505093098\n",
      "training 0.0008708660607226193 relative L2 0.0012857193360105157\n",
      "training 0.0006873455713503063 relative L2 0.001225339830853045\n",
      "training 0.0006233138265088201 relative L2 0.0012785408180207014\n",
      "training 0.0006801023846492171 relative L2 0.0013794145779684186\n",
      "training 0.0007930677384138107 relative L2 0.001446262001991272\n",
      "training 0.0008742695208638906 relative L2 0.001449864823371172\n",
      "training 0.0008774957968853414 relative L2 0.0013810653472319245\n",
      "training 0.0007959696231409907 relative L2 0.001290803891606629\n",
      "training 0.0006928686634637415 relative L2 0.0012305463897064328\n",
      "training 0.0006288213189691305 relative L2 0.0012348591117188334\n",
      "training 0.0006333607598207891 relative L2 0.0012829467887058854\n",
      "training 0.0006843072478659451 relative L2 0.0013258912367746234\n",
      "training 0.0007325097685679793 relative L2 0.001335280598141253\n",
      "training 0.0007423100178129971 relative L2 0.0013032126007601619\n",
      "training 0.0007071698782965541 relative L2 0.0012575878063216805\n",
      "training 0.0006570510449819267 relative L2 0.0012270144652575254\n",
      "training 0.000625112559646368 relative L2 0.0012289193691685796\n",
      "training 0.0006271158927120268 relative L2 0.001252987771295011\n",
      "training 0.0006521680043078959 relative L2 0.001274437760002911\n",
      "training 0.000675637973472476 relative L2 0.0012786734150722623\n",
      "training 0.0006796731613576412 relative L2 0.0012620125198736787\n",
      "training 0.0006622352520935237 relative L2 0.0012394492514431477\n",
      "training 0.0006379112601280212 relative L2 0.0012246420374140143\n",
      "training 0.0006226293044164777 relative L2 0.00122557592112571\n",
      "training 0.0006236160988919437 relative L2 0.0012372632045298815\n",
      "training 0.0006356294616125524 relative L2 0.0012478099670261145\n",
      "training 0.0006470771040767431 relative L2 0.0012503679608926177\n",
      "training 0.0006494065746665001 relative L2 0.0012425392633304\n",
      "training 0.0006414928939193487 relative L2 0.0012315871426835656\n",
      "training 0.0006297238869592547 relative L2 0.001223619095981121\n",
      "training 0.000621579063590616 relative L2 0.0012229668209329247\n",
      "training 0.0006208934355527163 relative L2 0.001227951142936945\n",
      "training 0.0006259604706428945 relative L2 0.0012333199847489595\n",
      "training 0.0006317803054116666 relative L2 0.0012357037048786879\n",
      "training 0.0006340182153508067 relative L2 0.001232936861924827\n",
      "training 0.0006313826306723058 relative L2 0.0012280242517590523\n",
      "training 0.0006260406807996333 relative L2 0.0012232913868501782\n",
      "training 0.0006212600274011493 relative L2 0.0012215548194944859\n",
      "training 0.0006194126908667386 relative L2 0.0012227990664541721\n",
      "training 0.000620662875007838 relative L2 0.0012252910528331995\n",
      "training 0.000623368367087096 relative L2 0.0012274021282792091\n",
      "training 0.0006254012696444988 relative L2 0.0012272546300664544\n",
      "training 0.0006254324107430875 relative L2 0.001225677551701665\n",
      "training 0.0006236211629584432 relative L2 0.001223091036081314\n",
      "training 0.0006210698047652841 relative L2 0.0012213066220283508\n",
      "training 0.0006191401043906808 relative L2 0.0012207272229716182\n",
      "training 0.0006185609381645918 relative L2 0.001221315935254097\n",
      "training 0.0006192087894305587 relative L2 0.0012224913807585835\n",
      "training 0.0006203448865562677 relative L2 0.0012231378350406885\n",
      "training 0.0006211299914866686 relative L2 0.0012232251465320587\n",
      "training 0.0006210964056663215 relative L2 0.0012223229277879\n",
      "training 0.00062027876265347 relative L2 0.0012213061563670635\n",
      "training 0.0006191300926730037 relative L2 0.0012203175574541092\n",
      "training 0.0006181685603223741 relative L2 0.001219907309859991\n",
      "training 0.0006177204195410013 relative L2 0.001219996134750545\n",
      "training 0.0006177989998832345 relative L2 0.0012203097576275468\n",
      "training 0.0006181696662679315 relative L2 0.0012207144172862172\n",
      "training 0.0006185209495015442 relative L2 0.0012207265244796872\n",
      "training 0.0006186143145896494 relative L2 0.0012205884559080005\n",
      "training 0.0006183897494338453 relative L2 0.0012200765777379274\n",
      "training 0.0006179335177876055 relative L2 0.0012196407187730074\n",
      "training 0.0006174257723614573 relative L2 0.0012192195281386375\n",
      "training 0.0006170259439386427 relative L2 0.0012190304696559906\n",
      "training 0.0006168180843815207 relative L2 0.0012190190609544516\n",
      "training 0.0006167963147163391 relative L2 0.0012190659763291478\n",
      "training 0.0006168741383589804 relative L2 0.0012191837886348367\n",
      "training 0.0006169536500237882 relative L2 0.0012191341957077384\n",
      "training 0.0006169534754008055 relative L2 0.001219076686538756\n",
      "training 0.000616840785369277 relative L2 0.0012188265100121498\n",
      "training 0.0006166321691125631 relative L2 0.0012186238309368491\n",
      "training 0.0006163805956020951 relative L2 0.0012183643411844969\n",
      "training 0.00061614322476089 relative L2 0.0012182032223790884\n",
      "training 0.0006159592885524035 relative L2 0.001218087854795158\n",
      "training 0.0006158445030450821 relative L2 0.0012180224293842912\n",
      "training 0.0006157855386845767 relative L2 0.0012180100893601775\n",
      "training 0.0006157538737170398 relative L2 0.0012179497862234712\n",
      "training 0.0006157188909128308 relative L2 0.0012179228942841291\n",
      "training 0.0006156596355140209 relative L2 0.0012177972821518779\n",
      "training 0.0006155630690045655 relative L2 0.001217703684233129\n",
      "training 0.0006154350703582168 relative L2 0.0012175352312624454\n",
      "training 0.0006152888527140021 relative L2 0.0012174120638519526\n",
      "training 0.0006151406560093164 relative L2 0.0012172670103609562\n",
      "training 0.0006150053231976926 relative L2 0.0012171632843092084\n",
      "training 0.0006148918764665723 relative L2 0.0012170742265880108\n",
      "training 0.0006147996173240244 relative L2 0.0012169918045401573\n",
      "training 0.0006147222593426704 relative L2 0.0012169333640486002\n",
      "training 0.0006146496743895113 relative L2 0.0012168450048193336\n",
      "training 0.0006145752849988639 relative L2 0.0012167831882834435\n",
      "training 0.0006144928047433496 relative L2 0.001216672477312386\n",
      "training 0.0006143986829556525 relative L2 0.0012165900552645326\n",
      "training 0.0006142951315268874 relative L2 0.001216467353515327\n",
      "training 0.0006141859339550138 relative L2 0.0012163742212578654\n",
      "training 0.0006140763871371746 relative L2 0.0012162607163190842\n",
      "training 0.0006139696342870593 relative L2 0.001216167351230979\n",
      "training 0.0006138679455034435 relative L2 0.0012160730548202991\n",
      "training 0.0006137730670161545 relative L2 0.0012159837642684579\n",
      "training 0.000613683310803026 relative L2 0.001215904951095581\n",
      "training 0.0006135972216725349 relative L2 0.0012158142635598779\n",
      "training 0.0006135121220722795 relative L2 0.0012157410383224487\n",
      "training 0.0006134270806796849 relative L2 0.0012156442971900105\n",
      "training 0.0006133391289040446 relative L2 0.0012155672302469611\n",
      "training 0.0006132482085376978 relative L2 0.001215465134009719\n",
      "training 0.0006131549598649144 relative L2 0.0012153821298852563\n",
      "training 0.0006130594993010163 relative L2 0.0012152799172326922\n",
      "training 0.0006129634566605091 relative L2 0.0012151934206485748\n",
      "training 0.0006128677632659674 relative L2 0.0012150972615927458\n",
      "training 0.000612773874308914 relative L2 0.0012150098336860538\n",
      "training 0.0006126821390353143 relative L2 0.0012149219401180744\n",
      "training 0.0006125919753685594 relative L2 0.0012148339301347733\n",
      "training 0.00061250344151631 relative L2 0.0012147512752562761\n",
      "training 0.0006124154897406697 relative L2 0.0012146614026278257\n",
      "training 0.0006123281782492995 relative L2 0.0012145820073783398\n",
      "training 0.0006122408085502684 relative L2 0.001214489690028131\n",
      "training 0.00061215297318995 relative L2 0.0012144094798713923\n",
      "training 0.0006120638572610915 relative L2 0.0012143151834607124\n",
      "training 0.000611974042840302 relative L2 0.0012142335763201118\n",
      "training 0.0006118841702118516 relative L2 0.0012141407933086157\n",
      "training 0.0006117944722063839 relative L2 0.001214057207107544\n",
      "training 0.0006117043667472899 relative L2 0.0012139662867411971\n",
      "training 0.0006116149597801268 relative L2 0.0012138824677094817\n",
      "training 0.0006115264259278774 relative L2 0.0012137951562181115\n",
      "training 0.0006114381831139326 relative L2 0.0012137104058638215\n",
      "training 0.0006113510462455451 relative L2 0.0012136251898482442\n",
      "training 0.0006112636183388531 relative L2 0.001213539275340736\n",
      "training 0.0006111768889240921 relative L2 0.001213456504046917\n",
      "training 0.0006110903341323137 relative L2 0.001213369658216834\n",
      "training 0.0006110036047175527 relative L2 0.001213287701830268\n",
      "training 0.0006109169917181134 relative L2 0.0012132002739235759\n",
      "training 0.0006108308443799615 relative L2 0.001213119481690228\n",
      "training 0.0006107448134571314 relative L2 0.0012130307732149959\n",
      "training 0.0006106581422500312 relative L2 0.001212951261550188\n",
      "training 0.0006105721113272011 relative L2 0.0012128629023209214\n",
      "training 0.0006104864878579974 relative L2 0.0012127828085795045\n",
      "training 0.0006104001658968627 relative L2 0.0012126944493502378\n",
      "training 0.0006103138439357281 relative L2 0.0012126141227781773\n",
      "training 0.0006102280458435416 relative L2 0.0012125265784561634\n",
      "training 0.0006101423059590161 relative L2 0.0012124466011300683\n",
      "training 0.0006100565660744905 relative L2 0.001212359289638698\n",
      "training 0.0006099710590206087 relative L2 0.0012122796615585685\n",
      "training 0.000609886075835675 relative L2 0.001212193164974451\n",
      "training 0.0006098013836890459 relative L2 0.0012121136533096433\n",
      "training 0.0006097166915424168 relative L2 0.0012120275059714913\n",
      "training 0.0006096320576034486 relative L2 0.0012119485763832927\n",
      "training 0.0006095474818721414 relative L2 0.001211862196214497\n",
      "training 0.0006094631971791387 relative L2 0.001211783499456942\n",
      "training 0.000609378854278475 relative L2 0.001211697468534112\n",
      "training 0.0006092946277931333 relative L2 0.001211619470268488\n",
      "training 0.0006092108669690788 relative L2 0.0012115339050069451\n",
      "training 0.0006091276882216334 relative L2 0.001211457303725183\n",
      "training 0.000609044567681849 relative L2 0.0012113708071410656\n",
      "training 0.0006089614471420646 relative L2 0.0012112950207665563\n",
      "training 0.0006088787922635674 relative L2 0.0012112087570130825\n",
      "training 0.0006087963120080531 relative L2 0.001211134367622435\n",
      "training 0.0006087141227908432 relative L2 0.0012110477546229959\n",
      "training 0.0006086325156502426 relative L2 0.001210976392030716\n",
      "training 0.0006085519562475383 relative L2 0.0012108897790312767\n",
      "training 0.0006084720371291041 relative L2 0.0012108207447454333\n",
      "training 0.0006083923508413136 relative L2 0.0012107340153306723\n",
      "training 0.0006083144107833505 relative L2 0.0012106734793633223\n",
      "training 0.0006082401378080249 relative L2 0.0012105919886380434\n",
      "training 0.0006081716273911297 relative L2 0.0012105506611987948\n",
      "training 0.0006081121391616762 relative L2 0.001210485934279859\n",
      "training 0.0006080680177547038 relative L2 0.0012104944325983524\n",
      "training 0.0006080495077185333 relative L2 0.001210480579175055\n",
      "training 0.0006080710445530713 relative L2 0.0012106127105653286\n",
      "training 0.0006081623141653836 relative L2 0.0012107619550079107\n",
      "training 0.0006083736661821604 relative L2 0.0012112485710531473\n",
      "training 0.000608799746260047 relative L2 0.0012119215680286288\n",
      "training 0.0006095885182730854 relative L2 0.0012134336866438389\n",
      "training 0.0006110184476710856 relative L2 0.0012157210148870945\n",
      "training 0.0006135414587333798 relative L2 0.0012203080113977194\n",
      "training 0.0006180658820085227 relative L2 0.0012276147026568651\n",
      "training 0.0006259406800381839 relative L2 0.0012415293604135513\n",
      "training 0.0006401387508958578 relative L2 0.001264187740162015\n",
      "training 0.000664727296680212 relative L2 0.001305383280850947\n",
      "training 0.0007089704158715904 relative L2 0.0013704430311918259\n",
      "training 0.0007837059092707932 relative L2 0.0014795016031712294\n",
      "training 0.000914380478207022 relative L2 0.00163318554405123\n",
      "training 0.0011188497301191092 relative L2 0.0018494520336389542\n",
      "training 0.0014363061636686325 relative L2 0.0020774351432919502\n",
      "training 0.0018186232773587108 relative L2 0.00228115264326334\n",
      "training 0.002192182932049036 relative L2 0.002295825630426407\n",
      "training 0.0022239922545850277 relative L2 0.0020860135555267334\n",
      "training 0.0018309035804122686 relative L2 0.0016313245287165046\n",
      "training 0.0011162784649059176 relative L2 0.0012474819086492062\n",
      "training 0.0006463967729359865 relative L2 0.0013003165367990732\n",
      "training 0.000703358615282923 relative L2 0.0016086118994280696\n",
      "training 0.0010850167600437999 relative L2 0.0017888409784063697\n",
      "training 0.0013427749508991838 relative L2 0.001670595956966281\n",
      "training 0.001171301817521453 relative L2 0.0013799113221466541\n",
      "training 0.000793675659224391 relative L2 0.0012087298091500998\n",
      "training 0.0006062450120225549 relative L2 0.001344042830169201\n",
      "training 0.0007532380986958742 relative L2 0.001527839107438922\n",
      "training 0.0009758935193531215 relative L2 0.0015190280973911285\n",
      "training 0.0009660354116931558 relative L2 0.0013494630111381412\n",
      "training 0.0007584629347547889 relative L2 0.0012121786130592227\n",
      "training 0.0006098495214246213 relative L2 0.0012717264471575618\n",
      "training 0.0006728373700752854 relative L2 0.0013948570704087615\n",
      "training 0.0008112266077660024 relative L2 0.0013999169459566474\n",
      "training 0.0008183756726793945 relative L2 0.001294372254051268\n",
      "training 0.0006967922090552747 relative L2 0.0012103453045710921\n",
      "training 0.000607949448749423 relative L2 0.0012478752760216594\n",
      "training 0.0006472821114584804 relative L2 0.0013236934319138527\n",
      "training 0.0007292910595424473 relative L2 0.0013230227632448077\n",
      "training 0.0007294227252714336 relative L2 0.0012555108405649662\n",
      "training 0.0006548591190949082 relative L2 0.0012084710178896785\n",
      "training 0.0006059976294636726 relative L2 0.001235004747286439\n",
      "training 0.0006336846272461116 relative L2 0.0012800679542124271\n",
      "training 0.0006812034989707172 relative L2 0.0012756733922287822\n",
      "training 0.0006771258194930851 relative L2 0.0012332892511039972\n",
      "training 0.0006314768688753247 relative L2 0.0012075809063389897\n",
      "training 0.0006050723022781312 relative L2 0.0012254866305738688\n",
      "training 0.000623720174189657 relative L2 0.0012520758900791407\n",
      "training 0.0006512211402878165 relative L2 0.0012476389529183507\n",
      "training 0.0006470519583672285 relative L2 0.0012217042967677116\n",
      "training 0.0006194747984409332 relative L2 0.0012069945223629475\n",
      "training 0.0006044793408364058 relative L2 0.00121801916975528\n",
      "training 0.0006159567856229842 relative L2 0.0012339428067207336\n",
      "training 0.0006321737309917808 relative L2 0.0012311653699725866\n",
      "training 0.0006296862848103046 relative L2 0.0012157215969637036\n",
      "training 0.0006133344722911716 relative L2 0.0012064790353178978\n",
      "training 0.00060396728804335 relative L2 0.0012124291388317943\n",
      "training 0.0006101701874285936 relative L2 0.0012221548240631819\n",
      "training 0.0006199534982442856 relative L2 0.001221271581016481\n",
      "training 0.0006193607696332037 relative L2 0.0012124584754928946\n",
      "training 0.0006100023747421801 relative L2 0.0012061079032719135\n",
      "training 0.0006036056438460946 relative L2 0.0012086777715012431\n",
      "training 0.0006062934990040958 relative L2 0.0012146920198574662\n",
      "training 0.000612287491094321 relative L2 0.001215206109918654\n",
      "training 0.0006130661349743605 relative L2 0.0012105219066143036\n",
      "training 0.0006080318707972765 relative L2 0.0012059269938617945\n",
      "training 0.0006034389371052384 relative L2 0.0012064039474353194\n",
      "training 0.0006039424915798008 relative L2 0.0012099449522793293\n",
      "training 0.0006074467673897743 relative L2 0.0012111268006265163\n",
      "training 0.000608846137765795 relative L2 0.0012090213131159544\n",
      "training 0.0006065085181035101 relative L2 0.0012058288557454944\n",
      "training 0.0006033520330674946 relative L2 0.0012052177917212248\n",
      "training 0.0006027094786986709 relative L2 0.0012070161756128073\n",
      "training 0.0006044768961146474 relative L2 0.0012082862667739391\n",
      "training 0.0006059104925952852 relative L2 0.0012077270075678825\n",
      "training 0.0006051943055354059 relative L2 0.0012057034764438868\n",
      "training 0.0006032304372638464 relative L2 0.0012046928750351071\n",
      "training 0.0006021550507284701 relative L2 0.0012052739039063454\n",
      "training 0.0006027195486240089 relative L2 0.0012062187306582928\n",
      "training 0.0006037715938873589 relative L2 0.0012064186157658696\n",
      "training 0.0006038672872819006 relative L2 0.001205370994284749\n",
      "training 0.0006028923089616001 relative L2 0.0012044706381857395\n",
      "training 0.0006019160500727594 relative L2 0.0012043481692671776\n",
      "training 0.0006017942796461284 relative L2 0.0012048190692439675\n",
      "training 0.0006023215828463435 relative L2 0.0012052390957251191\n",
      "training 0.0006026758928783238 relative L2 0.0012048672651872039\n",
      "training 0.0006023760652169585 relative L2 0.0012042924063280225\n",
      "training 0.0006017285049892962 relative L2 0.001203883090056479\n",
      "training 0.0006013364181853831 relative L2 0.0012039546854794025\n",
      "training 0.000601423264015466 relative L2 0.0012042673770338297\n",
      "training 0.0006016985862515867 relative L2 0.0012042555026710033\n",
      "training 0.0006017471896484494 relative L2 0.0012040287256240845\n",
      "training 0.0006014583050273359 relative L2 0.0012036192929372191\n",
      "training 0.0006010793731547892 relative L2 0.0012034607352688909\n",
      "training 0.0006009074859321117 relative L2 0.0012035553809255362\n",
      "training 0.0006009862991049886 relative L2 0.0012036287225782871\n",
      "training 0.0006011005607433617 relative L2 0.0012036225525662303\n",
      "training 0.0006010480574332178 relative L2 0.0012033635284751654\n",
      "training 0.0006008254713378847 relative L2 0.0012031595688313246\n",
      "training 0.0006005918839946389 relative L2 0.0012030561920255423\n",
      "training 0.0006004914175719023 relative L2 0.001203059684485197\n",
      "training 0.0006005108589306474 relative L2 0.001203104155138135\n",
      "training 0.0006005275063216686 relative L2 0.0012029949575662613\n",
      "training 0.0006004500319249928 relative L2 0.0012028721394017339\n",
      "training 0.000600296538323164 relative L2 0.0012027141638100147\n",
      "training 0.0006001523579470813 relative L2 0.0012026429176330566\n",
      "training 0.0006000788998790085 relative L2 0.001202640705741942\n",
      "training 0.0006000637076795101 relative L2 0.0012025948381051421\n",
      "training 0.0006000403664074838 relative L2 0.001202544430270791\n",
      "training 0.0005999639979563653 relative L2 0.00120240927208215\n",
      "training 0.0005998477572575212 relative L2 0.00120231241453439\n",
      "training 0.0005997372209094465 relative L2 0.0012022416340187192\n",
      "training 0.0005996663821861148 relative L2 0.00120219262316823\n",
      "training 0.0005996264517307281 relative L2 0.0012021654983982444\n",
      "training 0.000599582795985043 relative L2 0.001202077604830265\n",
      "training 0.0005995120154693723 relative L2 0.0012020028661936522\n",
      "training 0.000599420047365129 relative L2 0.0012019048444926739\n",
      "training 0.0005993300583213568 relative L2 0.0012018366251140833\n",
      "training 0.0005992594524286687 relative L2 0.0012017908738926053\n",
      "training 0.0005992066580802202 relative L2 0.0012017262633889914\n",
      "training 0.0005991532234475017 relative L2 0.0012016730615869164\n",
      "training 0.0005990859935991466 relative L2 0.0012015830725431442\n",
      "training 0.0005990061908960342 relative L2 0.0012015111278742552\n",
      "training 0.0005989262135699391 relative L2 0.0012014424428343773\n",
      "training 0.0005988576449453831 relative L2 0.0012013798113912344\n",
      "training 0.000598799204453826 relative L2 0.0012013305677101016\n",
      "training 0.0005987408221699297 relative L2 0.0012012545485049486\n",
      "training 0.0005986742908135056 relative L2 0.0012011908693239093\n",
      "training 0.0005986006581224501 relative L2 0.0012011125218123198\n",
      "training 0.0005985268508084118 relative L2 0.0012010465143248439\n",
      "training 0.0005984588642604649 relative L2 0.0012009880738332868\n",
      "training 0.0005983963492326438 relative L2 0.0012009210186079144\n",
      "training 0.000598334998358041 relative L2 0.001200863509438932\n",
      "training 0.0005982691654935479 relative L2 0.00120078818872571\n",
      "training 0.0005981997819617391 relative L2 0.0012007231125608087\n",
      "training 0.0005981297581456602 relative L2 0.0012006564065814018\n",
      "training 0.0005980629939585924 relative L2 0.001200591097585857\n",
      "training 0.0005979992565698922 relative L2 0.0012005327735096216\n",
      "training 0.0005979357520118356 relative L2 0.0012004626914858818\n",
      "training 0.0005978703848086298 relative L2 0.0012004005257040262\n",
      "training 0.0005978028639219701 relative L2 0.0012003303272649646\n",
      "training 0.0005977353430353105 relative L2 0.0012002658331766725\n",
      "training 0.0005976692191325128 relative L2 0.0012002041330561042\n",
      "training 0.0005976054235361516 relative L2 0.0012001373106613755\n",
      "training 0.0005975415697321296 relative L2 0.001200077123939991\n",
      "training 0.0005974763189442456 relative L2 0.0012000077404081821\n",
      "training 0.0005974100204184651 relative L2 0.0011999444104731083\n",
      "training 0.0005973436054773629 relative L2 0.0011998792178928852\n",
      "training 0.0005972781800664961 relative L2 0.001199814723804593\n",
      "training 0.0005972139770165086 relative L2 0.0011997538385912776\n",
      "training 0.000597149773966521 relative L2 0.0011996866669505835\n",
      "training 0.0005970851634629071 relative L2 0.0011996246175840497\n",
      "training 0.0005970196216367185 relative L2 0.0011995582608506083\n",
      "training 0.0005969541962258518 relative L2 0.0011994943488389254\n",
      "training 0.0005968895857222378 relative L2 0.0011994318338111043\n",
      "training 0.0005968251498416066 relative L2 0.001199366757646203\n",
      "training 0.0005967611796222627 relative L2 0.0011993048246949911\n",
      "training 0.0005966965109109879 relative L2 0.001199239050038159\n",
      "training 0.0005966320750303566 relative L2 0.0011991761857643723\n",
      "training 0.0005965675809420645 relative L2 0.0011991121573373675\n",
      "training 0.0005965030286461115 relative L2 0.0011990477796643972\n",
      "training 0.0005964389420114458 relative L2 0.0011989858467131853\n",
      "training 0.0005963747971691191 relative L2 0.0011989205377176404\n",
      "training 0.0005963105359114707 relative L2 0.0011988583719357848\n",
      "training 0.0005962461582385004 relative L2 0.001198793645016849\n",
      "training 0.0005961817805655301 relative L2 0.0011987301986664534\n",
      "training 0.0005961176357232034 relative L2 0.0011986673343926668\n",
      "training 0.0005960536655038595 relative L2 0.0011986029567196965\n",
      "training 0.0005959898116998374 relative L2 0.0011985409073531628\n",
      "training 0.0005959258996881545 relative L2 0.0011984764132648706\n",
      "training 0.0005958618130534887 relative L2 0.0011984134325757623\n",
      "training 0.0005957976682111621 relative L2 0.0011983498698100448\n",
      "training 0.0005957339308224618 relative L2 0.001198286539874971\n",
      "training 0.0005956703098490834 relative L2 0.0011982243740931153\n",
      "training 0.0005956065724603832 relative L2 0.0011981603456661105\n",
      "training 0.0005955430679023266 relative L2 0.0011980978306382895\n",
      "training 0.0005954793305136263 relative L2 0.0011980346171185374\n",
      "training 0.0005954157677479088 relative L2 0.0011979714035987854\n",
      "training 0.0005953523213975132 relative L2 0.001197909121401608\n",
      "training 0.0005952887004241347 relative L2 0.0011978454422205687\n",
      "training 0.000595225254073739 relative L2 0.0011977831600233912\n",
      "training 0.0005951617495156825 relative L2 0.0011977199465036392\n",
      "training 0.0005950983031652868 relative L2 0.00119765754789114\n",
      "training 0.0005950351478531957 relative L2 0.001197594916447997\n",
      "training 0.0005949720507487655 relative L2 0.0011975321685895324\n",
      "training 0.0005949088954366744 relative L2 0.0011974695371463895\n",
      "training 0.0005948456237092614 relative L2 0.001197406672872603\n",
      "training 0.0005947825266048312 relative L2 0.0011973443906754255\n",
      "training 0.0005947193712927401 relative L2 0.0011972816428169608\n",
      "training 0.0005946563323959708 relative L2 0.0011972192442044616\n",
      "training 0.0005945935845375061 relative L2 0.0011971569620072842\n",
      "training 0.0005945305456407368 relative L2 0.0011970946798101068\n",
      "training 0.0005944677395746112 relative L2 0.0011970320483669639\n",
      "training 0.0005944047588855028 relative L2 0.00119696999900043\n",
      "training 0.0005943417781963944 relative L2 0.0011969077168032527\n",
      "training 0.0005942787975072861 relative L2 0.0011968454346060753\n",
      "training 0.0005942161660641432 relative L2 0.0011967833852395415\n",
      "training 0.0005941535346210003 relative L2 0.0011967215687036514\n",
      "training 0.0005940909613855183 relative L2 0.0011966589372605085\n",
      "training 0.0005940283881500363 relative L2 0.0011965970043092966\n",
      "training 0.0005939657567068934 relative L2 0.001196534838527441\n",
      "training 0.0005939031834714115 relative L2 0.001196472905576229\n",
      "training 0.0005938406102359295 relative L2 0.0011964108562096953\n",
      "training 0.0005937778623774648 relative L2 0.0011963486904278398\n",
      "training 0.0005937156383879483 relative L2 0.0011962867574766278\n",
      "training 0.0005936531815677881 relative L2 0.0011962250573560596\n",
      "training 0.000593590724747628 relative L2 0.0011961631244048476\n",
      "training 0.0005935285589657724 relative L2 0.0011961015406996012\n",
      "training 0.0005934663349762559 relative L2 0.0011960396077483892\n",
      "training 0.0005934042856097221 relative L2 0.001195977907627821\n",
      "training 0.0005933422944508493 relative L2 0.0011959162075072527\n",
      "training 0.0005932800704613328 relative L2 0.00119585485663265\n",
      "training 0.0005932179628871381 relative L2 0.0011957932729274035\n",
      "training 0.0005931559717282653 relative L2 0.0011957314563915133\n",
      "training 0.0005930938059464097 relative L2 0.001195669756270945\n",
      "training 0.0005930319312028587 relative L2 0.001195608521811664\n",
      "training 0.0005929699400439858 relative L2 0.0011955469381064177\n",
      "training 0.0005929080653004348 relative L2 0.0011954853544011712\n",
      "training 0.000592846074141562 relative L2 0.0011954238871112466\n",
      "training 0.0005927842576056719 relative L2 0.001195362419821322\n",
      "training 0.0005927225574851036 relative L2 0.0011953010689467192\n",
      "training 0.0005926607409492135 relative L2 0.0011952397180721164\n",
      "training 0.0005925991572439671 relative L2 0.0011951784836128354\n",
      "training 0.0005925371660850942 relative L2 0.0011951173655688763\n",
      "training 0.0005924756987951696 relative L2 0.0011950560146942735\n",
      "training 0.0005924140568822622 relative L2 0.001194994430989027\n",
      "training 0.0005923520657233894 relative L2 0.0011949334293603897\n",
      "training 0.0005922905402258039 relative L2 0.001194871962070465\n",
      "training 0.0005922288401052356 relative L2 0.0011948109604418278\n",
      "training 0.0005921675474382937 relative L2 0.0011947500752285123\n",
      "training 0.0005921063711866736 relative L2 0.0011946888407692313\n",
      "training 0.0005920447874814272 relative L2 0.0011946277227252722\n",
      "training 0.0005919834366068244 relative L2 0.0011945662554353476\n",
      "training 0.000591921852901578 relative L2 0.0011945052538067102\n",
      "training 0.0005918607348576188 relative L2 0.001194444135762751\n",
      "training 0.0005917993257753551 relative L2 0.0011943831341341138\n",
      "training 0.0005917378584854305 relative L2 0.0011943223653361201\n",
      "training 0.0005916767404414713 relative L2 0.0011942611308768392\n",
      "training 0.0005916153313592076 relative L2 0.0011942003620788455\n",
      "training 0.0005915540386922657 relative L2 0.0011941397096961737\n",
      "training 0.0005914929788559675 relative L2 0.0011940785916522145\n",
      "training 0.0005914318608120084 relative L2 0.0011940181721001863\n",
      "training 0.0005913708591833711 relative L2 0.001193957170471549\n",
      "training 0.0005913097993470728 relative L2 0.0011938965180888772\n",
      "training 0.0005912487977184355 relative L2 0.0011938358657062054\n",
      "training 0.0005911875632591546 relative L2 0.001193774864077568\n",
      "training 0.0005911266198381782 relative L2 0.001193714328110218\n",
      "training 0.0005910656764172018 relative L2 0.0011936534428969026\n",
      "training 0.0005910047912038863 relative L2 0.0011935929069295526\n",
      "training 0.000590943789575249 relative L2 0.0011935323709622025\n",
      "training 0.0005908830207772553 relative L2 0.0011934717185795307\n",
      "training 0.0005908221355639398 relative L2 0.0011934112990275025\n",
      "training 0.0005907614249736071 relative L2 0.001193350413814187\n",
      "training 0.0005907005397602916 relative L2 0.0011932899942621589\n",
      "training 0.0005906397709622979 relative L2 0.0011932295747101307\n",
      "training 0.0005905792932026088 relative L2 0.0011931693879887462\n",
      "training 0.0005905185826122761 relative L2 0.0011931088520213962\n",
      "training 0.0005904579302296042 relative L2 0.0011930483160540462\n",
      "training 0.0005903972196392715 relative L2 0.0011929880129173398\n",
      "training 0.0005903365672565997 relative L2 0.001192927360534668\n",
      "training 0.0005902760894969106 relative L2 0.001192867406643927\n",
      "training 0.0005902156117372215 relative L2 0.001192806870676577\n",
      "training 0.0005901550757698715 relative L2 0.0011927466839551926\n",
      "training 0.0005900945398025215 relative L2 0.0011926861479878426\n",
      "training 0.0005900341784581542 relative L2 0.0011926260776817799\n",
      "training 0.0005899737006984651 relative L2 0.0011925658909603953\n",
      "training 0.0005899131065234542 relative L2 0.0011925054714083672\n",
      "training 0.0005898527451790869 relative L2 0.0011924454011023045\n",
      "training 0.0005897923256270587 relative L2 0.0011923846323043108\n",
      "training 0.0005897319642826915 relative L2 0.0011923247948288918\n",
      "training 0.0005896716029383242 relative L2 0.0011922642588615417\n",
      "training 0.000589611241593957 relative L2 0.0011922040721401572\n",
      "training 0.0005895509384572506 relative L2 0.0011921441182494164\n",
      "training 0.0005894906935282052 relative L2 0.00119208381511271\n",
      "training 0.0005894303903914988 relative L2 0.001192023977637291\n",
      "training 0.0005893702036701143 relative L2 0.001191963441669941\n",
      "training 0.0005893099587410688 relative L2 0.0011919037206098437\n",
      "training 0.0005892497720196843 relative L2 0.0011918435338884592\n",
      "training 0.0005891896435059607 relative L2 0.0011917835799977183\n",
      "training 0.0005891295149922371 relative L2 0.001191723975352943\n",
      "training 0.0005890694446861744 relative L2 0.001191664021462202\n",
      "training 0.0005890092579647899 relative L2 0.0011916039511561394\n",
      "training 0.0005889491294510663 relative L2 0.0011915439972653985\n",
      "training 0.0005888891755603254 relative L2 0.001191483810544014\n",
      "training 0.0005888290470466018 relative L2 0.0011914244387298822\n",
      "training 0.0005887691513635218 relative L2 0.0011913643684238195\n",
      "training 0.0005887092556804419 relative L2 0.0011913045309484005\n",
      "training 0.0005886492435820401 relative L2 0.0011912452755495906\n",
      "training 0.0005885896389372647 relative L2 0.0011911853216588497\n",
      "training 0.0005885295104235411 relative L2 0.0011911256005987525\n",
      "training 0.0005884696147404611 relative L2 0.0011910656467080116\n",
      "training 0.0005884094862267375 relative L2 0.0011910058092325926\n",
      "training 0.0005883496487513185 relative L2 0.0011909462045878172\n",
      "training 0.0005882899858988822 relative L2 0.0011908865999430418\n",
      "training 0.0005882301484234631 relative L2 0.0011908268788829446\n",
      "training 0.000588170369155705 relative L2 0.0011907671578228474\n",
      "training 0.0005881105898879468 relative L2 0.0011907076695933938\n",
      "training 0.0005880509852431715 relative L2 0.001190647715702653\n",
      "training 0.0005879912641830742 relative L2 0.0011905879946425557\n",
      "training 0.0005879313684999943 relative L2 0.001190528622828424\n",
      "training 0.0005878715310245752 relative L2 0.0011904689017683268\n",
      "training 0.0005878118681721389 relative L2 0.0011904092971235514\n",
      "training 0.0005877521471120417 relative L2 0.001190349692478776\n",
      "training 0.0005876923096366227 relative L2 0.001190289855003357\n",
      "training 0.0005876329378224909 relative L2 0.001190230599604547\n",
      "training 0.0005875732167623937 relative L2 0.0011901708785444498\n",
      "training 0.0005875136121176183 relative L2 0.0011901110410690308\n",
      "training 0.0005874540074728429 relative L2 0.0011900519020855427\n",
      "training 0.0005873943446204066 relative L2 0.0011899915989488363\n",
      "training 0.0005873347399756312 relative L2 0.001189932692795992\n",
      "training 0.0005872751353308558 relative L2 0.0011898723896592855\n",
      "training 0.0005872155888937414 relative L2 0.001189813599921763\n",
      "training 0.0005871559260413051 relative L2 0.0011897535296157002\n",
      "training 0.0005870964378118515 relative L2 0.001189694507047534\n",
      "training 0.0005870370077900589 relative L2 0.0011896344367414713\n",
      "training 0.0005869773449376225 relative L2 0.0011895751813426614\n",
      "training 0.0005869176238775253 relative L2 0.001189515576697886\n",
      "training 0.0005868586013093591 relative L2 0.001189456321299076\n",
      "training 0.0005867989384569228 relative L2 0.0011893966002389789\n",
      "training 0.000586739566642791 relative L2 0.001189337344840169\n",
      "training 0.0005866801948286593 relative L2 0.0011892779730260372\n",
      "training 0.0005866208230145276 relative L2 0.0011892186012119055\n",
      "training 0.000586561334785074 relative L2 0.0011891594622284174\n",
      "training 0.0005865018465556204 relative L2 0.0011890996247529984\n",
      "training 0.0005864424165338278 relative L2 0.001189040718600154\n",
      "training 0.0005863832193426788 relative L2 0.0011889811139553785\n",
      "training 0.000586323847528547 relative L2 0.0011889217421412468\n",
      "training 0.0005862644175067544 relative L2 0.0011888624867424369\n",
      "training 0.0005862049874849617 relative L2 0.0011888029985129833\n",
      "training 0.0005861457320861518 relative L2 0.0011887437431141734\n",
      "training 0.000586086418479681 relative L2 0.0011886840220540762\n",
      "training 0.0005860269884578884 relative L2 0.00118862499948591\n",
      "training 0.0005859676748514175 relative L2 0.001188565162010491\n",
      "training 0.0005859084194526076 relative L2 0.0011885061394423246\n",
      "training 0.000585848989430815 relative L2 0.0011884463019669056\n",
      "training 0.0005857896758243442 relative L2 0.0011883872793987393\n",
      "training 0.0005857304786331952 relative L2 0.001188327674753964\n",
      "training 0.0005856710486114025 relative L2 0.0011882685357704759\n",
      "training 0.0005856117932125926 relative L2 0.0011882089311257005\n",
      "training 0.0005855524213984609 relative L2 0.0011881499085575342\n",
      "training 0.0005854933988302946 relative L2 0.0011880905367434025\n",
      "training 0.0005854340852238238 relative L2 0.0011880313977599144\n",
      "training 0.0005853748298250139 relative L2 0.001187971793115139\n",
      "training 0.000585315516218543 relative L2 0.001187912654131651\n",
      "training 0.0005852562608197331 relative L2 0.0011878532823175192\n",
      "training 0.0005851971218362451 relative L2 0.001187794259749353\n",
      "training 0.0005851380410604179 relative L2 0.0011877346551045775\n",
      "training 0.000585078785661608 relative L2 0.0011876756325364113\n",
      "training 0.0005850195302627981 relative L2 0.001187615911476314\n",
      "training 0.0005849604494869709 relative L2 0.0011875577038154006\n",
      "training 0.0005849013687111437 relative L2 0.0011874971678480506\n",
      "training 0.0005848423461429775 relative L2 0.0011874402407556772\n",
      "training 0.0005847839056514204 relative L2 0.0011873793555423617\n",
      "training 0.0005847254651598632 relative L2 0.0011873248731717467\n",
      "training 0.0005846675485372543 relative L2 0.0011872636387124658\n",
      "training 0.0005846110871061683 relative L2 0.0011872166069224477\n",
      "training 0.0005845577688887715 relative L2 0.0011871596798300743\n",
      "training 0.0005845106206834316 relative L2 0.0011871411697939038\n",
      "training 0.0005844786064699292 relative L2 0.0011871237074956298\n",
      "training 0.0005844824481755495 relative L2 0.0011872448958456516\n",
      "training 0.0005845738342031837 relative L2 0.0011874980991706252\n",
      "training 0.0005848777364008129 relative L2 0.0011883946135640144\n",
      "training 0.0005857068463228643 relative L2 0.0011903776321560144\n",
      "training 0.0005878253141418099 relative L2 0.001195811084471643\n",
      "training 0.0005931444466114044 relative L2 0.001208422938361764\n",
      "training 0.0006062955362722278 relative L2 0.0012406238820403814\n",
      "training 0.0006393013172782958 relative L2 0.0013144847471266985\n",
      "training 0.000720068346709013 relative L2 0.0014854409964755177\n",
      "training 0.0009219787898473442 relative L2 0.0018206839449703693\n",
      "training 0.0013938971096649766 relative L2 0.002416483825072646\n",
      "training 0.0024617649614810944 relative L2 0.003192669479176402\n",
      "training 0.004312782548367977 relative L2 0.003909478895366192\n",
      "training 0.006466573104262352 relative L2 0.0037223563995212317\n",
      "training 0.0058669280260801315 relative L2 0.0023996783420443535\n",
      "training 0.0024274589959532022 relative L2 0.001210339367389679\n",
      "training 0.0006079107988625765 relative L2 0.0025138461496680975\n",
      "training 0.0026690373197197914 relative L2 0.0030543054454028606\n",
      "training 0.003941493574529886 relative L2 0.001959478948265314\n",
      "training 0.0016166004352271557 relative L2 0.001317500602453947\n",
      "training 0.0007234454387798905 relative L2 0.002408406464383006\n",
      "training 0.0024455066304653883 relative L2 0.002272832440212369\n",
      "training 0.0021794699132442474 relative L2 0.00125498219858855\n",
      "training 0.000654468429274857 relative L2 0.0017466162098571658\n",
      "training 0.001279784250073135 relative L2 0.0021471958607435226\n",
      "training 0.0019437687005847692 relative L2 0.0014709876850247383\n",
      "training 0.0009039153810590506 relative L2 0.0013648418243974447\n",
      "training 0.0007763822213746607 relative L2 0.0019141024677082896\n",
      "training 0.0015419042902067304 relative L2 0.0015725631965324283\n",
      "training 0.0010349861113354564 relative L2 0.0012208448024466634\n",
      "training 0.0006187519175000489 relative L2 0.001696738530881703\n",
      "training 0.0012087456416338682 relative L2 0.0015805383445695043\n",
      "training 0.0010456913150846958 relative L2 0.0011915458599105477\n",
      "training 0.0005889619933441281 relative L2 0.0015261671505868435\n",
      "training 0.0009753892081789672 relative L2 0.0015413225628435612\n",
      "training 0.0009938159491866827 relative L2 0.0011992186773568392\n",
      "training 0.0005968601326458156 relative L2 0.001402638852596283\n",
      "training 0.0008217893191613257 relative L2 0.0014852855820208788\n",
      "training 0.0009219229104928672 relative L2 0.0012131253024563193\n",
      "training 0.0006111591937951744 relative L2 0.0013170952443033457\n",
      "training 0.0007229800103232265 relative L2 0.0014266667421907187\n",
      "training 0.0008495595538988709 relative L2 0.0012244287645444274\n",
      "training 0.000622885359916836 relative L2 0.001259954646229744\n",
      "training 0.0006604124209843576 relative L2 0.0013720865827053785\n",
      "training 0.0007848115055821836 relative L2 0.0012314309133216739\n",
      "training 0.0006302002584561706 relative L2 0.0012236606562510133\n",
      "training 0.0006220866343937814 relative L2 0.0013244075234979391\n",
      "training 0.0007303348975256085 relative L2 0.0012344338465481997\n",
      "training 0.0006333533092401922 relative L2 0.0012022472219541669\n",
      "training 0.0005999762797728181 relative L2 0.0012845873134210706\n",
      "training 0.0006863305461592972 relative L2 0.001233986928127706\n",
      "training 0.0006328928866423666 relative L2 0.0011912327026948333\n",
      "training 0.0005887330626137555 relative L2 0.0012523806653916836\n",
      "training 0.0006517420988529921 relative L2 0.0012307165889069438\n",
      "training 0.0006294732447713614 relative L2 0.0011870220769196749\n",
      "training 0.0005844344850629568 relative L2 0.0012276332126930356\n",
      "training 0.0006257732165977359 relative L2 0.0012254513567313552\n",
      "training 0.0006239783251658082 relative L2 0.0011868178844451904\n",
      "training 0.0005841856473125517 relative L2 0.0012095222482457757\n",
      "training 0.0006071089883334935 relative L2 0.001218654215335846\n",
      "training 0.0006169153493829072 relative L2 0.0011885097483173013\n",
      "training 0.0005858546937815845 relative L2 0.0011971982894465327\n",
      "training 0.0005945804878138006 relative L2 0.0012111765099689364\n",
      "training 0.0006091857212595642 relative L2 0.0011906154686585069\n",
      "training 0.0005879534292034805 relative L2 0.001189565286040306\n",
      "training 0.0005869006854481995 relative L2 0.001203725696541369\n",
      "training 0.0006015277467668056 relative L2 0.0011921774130314589\n",
      "training 0.0005895163048990071 relative L2 0.0011855695629492402\n",
      "training 0.0005829180008731782 relative L2 0.001196937751956284\n",
      "training 0.0005945865996181965 relative L2 0.0011926479637622833\n",
      "training 0.000589987263083458 relative L2 0.0011840998195111752\n",
      "training 0.0005814781761728227 relative L2 0.001191412564367056\n",
      "training 0.0005889592575840652 relative L2 0.001192042138427496\n",
      "training 0.0005893783527426422 relative L2 0.0011840836377814412\n",
      "training 0.0005814919131807983 relative L2 0.0011872894829139113\n",
      "training 0.0005847690626978874 relative L2 0.0011904735583811998\n",
      "training 0.0005878035444766283 relative L2 0.0011846332345157862\n",
      "training 0.000582064560148865 relative L2 0.0011847191490232944\n",
      "training 0.0005821539089083672 relative L2 0.0011884847190231085\n",
      "training 0.0005858112708665431 relative L2 0.0011851480230689049\n",
      "training 0.0005825938424095511 relative L2 0.0011833918979391456\n",
      "training 0.0005807947018183768 relative L2 0.0011864285916090012\n",
      "training 0.0005837580538354814 relative L2 0.001185301342047751\n",
      "training 0.0005827531567774713 relative L2 0.0011829499853774905\n",
      "training 0.0005803298554383218 relative L2 0.0011846624547615647\n",
      "training 0.0005820011720061302 relative L2 0.001184998662211001\n",
      "training 0.0005824480904266238 relative L2 0.0011829833965748549\n",
      "training 0.0005803481326438487 relative L2 0.0011833966709673405\n",
      "training 0.0005807497072964907 relative L2 0.0011843630345538259\n",
      "training 0.0005818040226586163 relative L2 0.001183134620077908\n",
      "training 0.0005804905667901039 relative L2 0.0011826495174318552\n",
      "training 0.000580018328037113 relative L2 0.0011835965560749173\n",
      "training 0.0005810256698168814 relative L2 0.0011831833980977535\n",
      "training 0.0005805354448966682 relative L2 0.0011823097011074424\n",
      "training 0.0005796924233436584 relative L2 0.0011828604619950056\n",
      "training 0.000580275955144316 relative L2 0.0011830199509859085\n",
      "training 0.0005803717067465186 relative L2 0.0011822035303339362\n",
      "training 0.0005795973120257258 relative L2 0.001182293752208352\n",
      "training 0.0005796958575956523 relative L2 0.001182694104500115\n",
      "training 0.0005800488288514316 relative L2 0.001182167325168848\n",
      "training 0.0005795688484795392 relative L2 0.0011819361243396997\n",
      "training 0.0005793264717794955 relative L2 0.0011822840897366405\n",
      "training 0.0005796444020234048 relative L2 0.0011820836225524545\n",
      "training 0.0005794889875687659 relative L2 0.0011817466001957655\n",
      "training 0.0005791279836557806 relative L2 0.001181894214823842\n",
      "training 0.0005792618612758815 relative L2 0.0011819127248600125\n",
      "training 0.0005793179152533412 relative L2 0.001181642059236765\n",
      "training 0.0005790174473077059 relative L2 0.0011815817561000586\n",
      "training 0.0005789577844552696 relative L2 0.001181668834760785\n",
      "training 0.0005790712893940508 relative L2 0.0011815407779067755\n",
      "training 0.0005789130227640271 relative L2 0.0011813658056780696\n",
      "training 0.0005787490517832339 relative L2 0.0011814143508672714\n",
      "training 0.0005788123235106468 relative L2 0.001181402476504445\n",
      "training 0.0005787741974927485 relative L2 0.0011812122538685799\n",
      "training 0.0005786015535704792 relative L2 0.0011811826843768358\n",
      "training 0.0005785753601230681 relative L2 0.0011812209850177169\n",
      "training 0.000578594277612865 relative L2 0.0011810807045549154\n",
      "training 0.0005784740787930787 relative L2 0.0011809943243861198\n",
      "training 0.0005783825763501227 relative L2 0.001181017025373876\n",
      "training 0.0005783936940133572 relative L2 0.001180939027108252\n",
      "training 0.00057833461323753 relative L2 0.0011808397248387337\n",
      "training 0.0005782245425507426 relative L2 0.001180814579129219\n",
      "training 0.0005781954387202859 relative L2 0.0011807760456576943\n",
      "training 0.0005781715735793114 relative L2 0.001180698862299323\n",
      "training 0.0005780815845355392 relative L2 0.0011806327383965254\n",
      "training 0.0005780179635621607 relative L2 0.0011806015390902758\n",
      "training 0.0005779959610663354 relative L2 0.0011805526446551085\n",
      "training 0.0005779349012300372 relative L2 0.0011804705718532205\n",
      "training 0.0005778593476861715 relative L2 0.001180429244413972\n",
      "training 0.0005778220947831869 relative L2 0.0011803973466157913\n",
      "training 0.000577780301682651 relative L2 0.0011803183006122708\n",
      "training 0.000577709695789963 relative L2 0.0011802657973021269\n",
      "training 0.0005776571924798191 relative L2 0.0011802364606410265\n",
      "training 0.00057762092910707 relative L2 0.0011801678920164704\n",
      "training 0.0005775613244622946 relative L2 0.0011801095679402351\n",
      "training 0.0005775000900030136 relative L2 0.0011800731299445033\n",
      "training 0.0005774598103016615 relative L2 0.001180014805868268\n",
      "training 0.0005774098681285977 relative L2 0.001179957645945251\n",
      "training 0.0005773477605544031 relative L2 0.0011799103813245893\n",
      "training 0.0005772996228188276 relative L2 0.0011798591585829854\n",
      "training 0.0005772548029199243 relative L2 0.0011798073537647724\n",
      "training 0.0005771975847892463 relative L2 0.0011797519400715828\n",
      "training 0.0005771438591182232 relative L2 0.0011797023471444845\n",
      "training 0.0005770982825197279 relative L2 0.0011796550825238228\n",
      "training 0.0005770461866632104 relative L2 0.0011795972241088748\n",
      "training 0.000576991296838969 relative L2 0.001179547281935811\n",
      "training 0.0005769434501416981 relative L2 0.0011795019963756204\n",
      "training 0.0005768939154222608 relative L2 0.0011794442543759942\n",
      "training 0.0005768398405052722 relative L2 0.0011793933808803558\n",
      "training 0.0005767896655015647 relative L2 0.0011793479789048433\n",
      "training 0.0005767412949353456 relative L2 0.001179291750304401\n",
      "training 0.000576688558794558 relative L2 0.0011792402947321534\n",
      "training 0.0005766369285993278 relative L2 0.0011791933793574572\n",
      "training 0.0005765883252024651 relative L2 0.0011791388969868422\n",
      "training 0.0005765374517068267 relative L2 0.0011790877906605601\n",
      "training 0.0005764851230196655 relative L2 0.0011790385469794273\n",
      "training 0.0005764355300925672 relative L2 0.001178985578007996\n",
      "training 0.0005763856461271644 relative L2 0.0011789352865889668\n",
      "training 0.0005763338995166123 relative L2 0.0011788845295086503\n",
      "training 0.0005762837245129049 relative L2 0.0011788326082751155\n",
      "training 0.000576233898755163 relative L2 0.0011787828989326954\n",
      "training 0.0005761828506365418 relative L2 0.0011787315597757697\n",
      "training 0.0005761322099715471 relative L2 0.0011786801042035222\n",
      "training 0.000576082500629127 relative L2 0.0011786306276917458\n",
      "training 0.0005760318599641323 relative L2 0.0011785790557041764\n",
      "training 0.0005759813939221203 relative L2 0.0011785280657932162\n",
      "training 0.0005759315099567175 relative L2 0.0011784785892814398\n",
      "training 0.0005758810439147055 relative L2 0.0011784265516325831\n",
      "training 0.0005758304032497108 relative L2 0.001178375561721623\n",
      "training 0.0005757801700383425 relative L2 0.0011783259687945247\n",
      "training 0.0005757299368269742 relative L2 0.0011782740475609899\n",
      "training 0.0005756795289926231 relative L2 0.0011782234068959951\n",
      "training 0.0005756292375735939 relative L2 0.0011781733483076096\n",
      "training 0.0005755792371928692 relative L2 0.0011781217763200402\n",
      "training 0.00057552894577384 relative L2 0.0011780713684856892\n",
      "training 0.000575478479731828 relative L2 0.001178020960651338\n",
      "training 0.0005754285375587642 relative L2 0.0011779691558331251\n",
      "training 0.000575378246139735 relative L2 0.001177919446490705\n",
      "training 0.0005753278965130448 relative L2 0.0011778685729950666\n",
      "training 0.0005752776050940156 relative L2 0.0011778174666687846\n",
      "training 0.0005752274300903082 relative L2 0.0011777672916650772\n",
      "training 0.0005751773132942617 relative L2 0.0011777161853387952\n",
      "training 0.0005751270218752325 relative L2 0.0011776653118431568\n",
      "training 0.0005750767886638641 relative L2 0.0011776152532547712\n",
      "training 0.0005750266718678176 relative L2 0.0011775640305131674\n",
      "training 0.0005749763222411275 relative L2 0.001177513157017529\n",
      "training 0.0005749260308220983 relative L2 0.0011774629820138216\n",
      "training 0.0005748760304413736 relative L2 0.0011774117592722178\n",
      "training 0.0005748258554376662 relative L2 0.0011773612350225449\n",
      "training 0.0005747756222262979 relative L2 0.0011773105943575501\n",
      "training 0.0005747254472225904 relative L2 0.00117725960444659\n",
      "training 0.0005746752140112221 relative L2 0.0011772089637815952\n",
      "training 0.0005746249807998538 relative L2 0.0011771583231166005\n",
      "training 0.0005745748057961464 relative L2 0.0011771073332056403\n",
      "training 0.0005745245725847781 relative L2 0.001177056459710002\n",
      "training 0.0005744742811657488 relative L2 0.0011770057026296854\n",
      "training 0.0005744241643697023 relative L2 0.0011769549455493689\n",
      "training 0.0005743738147430122 relative L2 0.0011769041884690523\n",
      "training 0.0005743236979469657 relative L2 0.0011768534313887358\n",
      "training 0.0005742735229432583 relative L2 0.0011768023250624537\n",
      "training 0.000574223231524229 relative L2 0.0011767520336434245\n",
      "training 0.0005741730565205216 relative L2 0.0011767011601477861\n",
      "training 0.0005741227651014924 relative L2 0.0011766502866521478\n",
      "training 0.0005740727647207677 relative L2 0.0011765992967411876\n",
      "training 0.0005740223568864167 relative L2 0.001176548539660871\n",
      "training 0.0005739722400903702 relative L2 0.001176497433334589\n",
      "training 0.0005739219486713409 relative L2 0.0011764466762542725\n",
      "training 0.0005738715990446508 relative L2 0.0011763962684199214\n",
      "training 0.0005738215404562652 relative L2 0.001176345394924283\n",
      "training 0.0005737713072448969 relative L2 0.0011762945214286447\n",
      "training 0.0005737210740335286 relative L2 0.0011762437643483281\n",
      "training 0.0005736707826144993 relative L2 0.001176192658022046\n",
      "training 0.000573620549403131 relative L2 0.0011761420173570514\n",
      "training 0.0005735703743994236 relative L2 0.0011760912602767348\n",
      "training 0.0005735200829803944 relative L2 0.0011760401539504528\n",
      "training 0.000573469849769026 relative L2 0.0011759893968701363\n",
      "training 0.0005734196165576577 relative L2 0.0011759385233744979\n",
      "training 0.0005733693833462894 relative L2 0.0011758875334635377\n",
      "training 0.000573319208342582 relative L2 0.0011758367763832211\n",
      "training 0.0005732689751312137 relative L2 0.0011757859028875828\n",
      "training 0.0005732187419198453 relative L2 0.0011757349129766226\n",
      "training 0.000573168508708477 relative L2 0.001175684155896306\n",
      "training 0.0005731182172894478 relative L2 0.0011756331659853458\n",
      "training 0.0005730679840780795 relative L2 0.0011755822924897075\n",
      "training 0.0005730176926590502 relative L2 0.001175531418994069\n",
      "training 0.0005729675176553428 relative L2 0.0011754806619137526\n",
      "training 0.0005729173426516354 relative L2 0.0011754296720027924\n",
      "training 0.0005728670512326062 relative L2 0.0011753789149224758\n",
      "training 0.0005728168762288988 relative L2 0.0011753281578421593\n",
      "training 0.0005727666430175304 relative L2 0.0011752774007618427\n",
      "training 0.000572716526221484 relative L2 0.0011752264108508825\n",
      "training 0.0005726662348024547 relative L2 0.0011751755373552442\n",
      "training 0.0005726160015910864 relative L2 0.0011751246638596058\n",
      "training 0.000572565826587379 relative L2 0.0011750739067792892\n",
      "training 0.0005725154769606888 relative L2 0.0011750230332836509\n",
      "training 0.0005724652437493205 relative L2 0.0011749719269573689\n",
      "training 0.0005724150687456131 relative L2 0.0011749211698770523\n",
      "training 0.0005723648355342448 relative L2 0.001174870296381414\n",
      "training 0.0005723144859075546 relative L2 0.0011748194228857756\n",
      "training 0.0005722643691115081 relative L2 0.0011747684329748154\n",
      "training 0.000572214019484818 relative L2 0.0011747173266485333\n",
      "training 0.0005721639026887715 relative L2 0.0011746663367375731\n",
      "training 0.0005721134948544204 relative L2 0.0011746152304112911\n",
      "training 0.0005720632616430521 relative L2 0.0011745643569156528\n",
      "training 0.0005720130284316838 relative L2 0.0011745133670046926\n",
      "training 0.0005719626788049936 relative L2 0.0011744623770937324\n",
      "training 0.0005719123873859644 relative L2 0.0011744112707674503\n",
      "training 0.0005718620959669352 relative L2 0.0011743601644411683\n",
      "training 0.0005718118045479059 relative L2 0.0011743091745302081\n",
      "training 0.0005717615131288767 relative L2 0.0011742584174498916\n",
      "training 0.0005717112799175084 relative L2 0.0011742071947082877\n",
      "training 0.0005716609884984791 relative L2 0.0011741563212126493\n",
      "training 0.0005716106970794499 relative L2 0.0011741052148863673\n",
      "training 0.0005715604056604207 relative L2 0.001174054341390729\n",
      "training 0.0005715100560337305 relative L2 0.001174003235064447\n",
      "training 0.0005714598228223622 relative L2 0.0011739523615688086\n",
      "training 0.000571409531403333 relative L2 0.0011739012552425265\n",
      "training 0.0005713592399843037 relative L2 0.00117385049816221\n",
      "training 0.0005713089485652745 relative L2 0.0011737992754206061\n",
      "training 0.0005712586571462452 relative L2 0.0011737486347556114\n",
      "training 0.0005712084239348769 relative L2 0.0011736974120140076\n",
      "training 0.0005711581325158477 relative L2 0.0011736463056877255\n",
      "training 0.0005711077828891575 relative L2 0.0011735954321920872\n",
      "training 0.0005710574914701283 relative L2 0.0011735445586964488\n",
      "training 0.0005710072000510991 relative L2 0.0011734935687854886\n",
      "training 0.0005709569668397307 relative L2 0.0011734425788745284\n",
      "training 0.0005709067918360233 relative L2 0.0011733915889635682\n",
      "training 0.0005708564422093332 relative L2 0.0011733407154679298\n",
      "training 0.0005708062089979649 relative L2 0.001173289492726326\n",
      "training 0.0005707559175789356 relative L2 0.001173238386400044\n",
      "training 0.0005707055679522455 relative L2 0.001173187280073762\n",
      "training 0.0005706553347408772 relative L2 0.0011731364065781236\n",
      "training 0.000570604985114187 relative L2 0.0011730854166671634\n",
      "training 0.0005705546936951578 relative L2 0.0011730343103408813\n",
      "training 0.0005705044604837894 relative L2 0.0011729830875992775\n",
      "training 0.0005704541108570993 relative L2 0.0011729322141036391\n",
      "training 0.0005704038194380701 relative L2 0.0011728809913620353\n",
      "training 0.0005703535862267017 relative L2 0.001172830001451075\n",
      "training 0.0005703030037693679 relative L2 0.0011727786622941494\n",
      "training 0.0005702529451809824 relative L2 0.0011727276723831892\n",
      "training 0.0005702024791389704 relative L2 0.0011726767988875508\n",
      "training 0.000570152245927602 relative L2 0.0011726254597306252\n",
      "training 0.0005701018963009119 relative L2 0.0011725745862349868\n",
      "training 0.0005700516630895436 relative L2 0.001172523247078061\n",
      "training 0.0005700011970475316 relative L2 0.0011724724899977446\n",
      "training 0.0005699510802514851 relative L2 0.0011724213836714625\n",
      "training 0.0005699006142094731 relative L2 0.0011723701609298587\n",
      "training 0.0005698503227904439 relative L2 0.0011723192874342203\n",
      "training 0.0005697999731637537 relative L2 0.0011722681811079383\n",
      "training 0.0005697495653294027 relative L2 0.0011722173076123\n",
      "training 0.0005696993321180344 relative L2 0.001172166201286018\n",
      "training 0.0005696488660760224 relative L2 0.0011721150949597359\n",
      "training 0.0005695985164493322 relative L2 0.0011720642214640975\n",
      "training 0.0005695481086149812 relative L2 0.0011720132315531373\n",
      "training 0.000569497758988291 relative L2 0.0011719621252268553\n",
      "training 0.0005694474093616009 relative L2 0.0011719110189005733\n",
      "training 0.0005693970597349107 relative L2 0.0011718599125742912\n",
      "training 0.0005693467101082206 relative L2 0.001171808922663331\n",
      "training 0.0005692962440662086 relative L2 0.0011717579327523708\n",
      "training 0.0005692457780241966 relative L2 0.001171706710010767\n",
      "training 0.0005691954866051674 relative L2 0.0011716554872691631\n",
      "training 0.0005691448459401727 relative L2 0.0011716041481122375\n",
      "training 0.0005690943216904998 relative L2 0.0011715531582012773\n",
      "training 0.0005690439720638096 relative L2 0.001171502168290317\n",
      "training 0.0005689935060217977 relative L2 0.0011714508291333914\n",
      "training 0.0005689430981874466 relative L2 0.0011713998392224312\n",
      "training 0.0005688926903530955 relative L2 0.001171348849311471\n",
      "training 0.0005688421661034226 relative L2 0.0011712978594005108\n",
      "training 0.0005687917000614107 relative L2 0.001171246636658907\n",
      "training 0.0005687412340193987 relative L2 0.0011711952975019813\n",
      "training 0.0005686907097697258 relative L2 0.0011711441911756992\n",
      "training 0.0005686402437277138 relative L2 0.0011710929684340954\n",
      "training 0.0005685897194780409 relative L2 0.0011710417456924915\n",
      "training 0.0005685393116436899 relative L2 0.0011709907557815313\n",
      "training 0.0005684887291863561 relative L2 0.0011709394166246057\n",
      "training 0.0005684382631443441 relative L2 0.0011708885431289673\n",
      "training 0.0005683879135176539 relative L2 0.0011708372039720416\n",
      "training 0.0005683372146449983 relative L2 0.0011707860976457596\n",
      "training 0.0005682868650183082 relative L2 0.0011707348749041557\n",
      "training 0.0005682361661456525 relative L2 0.0011706837685778737\n",
      "training 0.0005681858747266233 relative L2 0.0011706323130056262\n",
      "training 0.0005681352340616286 relative L2 0.001170581323094666\n",
      "training 0.0005680847680196166 relative L2 0.0011705299839377403\n",
      "training 0.0005680341273546219 relative L2 0.001170479110442102\n",
      "training 0.0005679837777279317 relative L2 0.0011704276548698545\n",
      "training 0.0005679330206476152 relative L2 0.0011703764321282506\n",
      "training 0.0005678826128132641 relative L2 0.001170325092971325\n",
      "training 0.0005678319139406085 relative L2 0.001170273986645043\n",
      "training 0.0005677815061062574 relative L2 0.0011702225310727954\n",
      "training 0.0005677309818565845 relative L2 0.0011701713083311915\n",
      "training 0.0005676805158145726 relative L2 0.0011701199691742659\n",
      "training 0.0005676298751495779 relative L2 0.001170068746432662\n",
      "training 0.000567579292692244 relative L2 0.0011700174072757363\n",
      "training 0.0005675287684425712 relative L2 0.0011699663009494543\n",
      "training 0.0005674782441928983 relative L2 0.001169914728961885\n",
      "training 0.0005674276617355645 relative L2 0.0011698633898049593\n",
      "training 0.0005673771956935525 relative L2 0.0011698120506480336\n",
      "training 0.0005673265550285578 relative L2 0.0011697605950757861\n",
      "training 0.0005672760889865458 relative L2 0.001169709605164826\n",
      "training 0.0005672254483215511 relative L2 0.0011696581495925784\n",
      "training 0.0005671749240718782 relative L2 0.001169606694020331\n",
      "training 0.0005671243998222053 relative L2 0.0011695552384480834\n",
      "training 0.0005670737591572106 relative L2 0.0011695038992911577\n",
      "training 0.0005670231766998768 relative L2 0.0011694526765495539\n",
      "training 0.000566972594242543 relative L2 0.0011694013373926282\n",
      "training 0.0005669220117852092 relative L2 0.0011693498818203807\n",
      "training 0.0005668714293278754 relative L2 0.0011692987754940987\n",
      "training 0.0005668207886628807 relative L2 0.0011692473199218512\n",
      "training 0.0005667701479978859 relative L2 0.0011691957479342818\n",
      "training 0.0005667195073328912 relative L2 0.0011691444087773561\n",
      "training 0.0005666689248755574 relative L2 0.001169092720374465\n",
      "training 0.0005666184006258845 relative L2 0.0011690413812175393\n",
      "training 0.0005665677599608898 relative L2 0.0011689896928146482\n",
      "training 0.0005665170610882342 relative L2 0.0011689382372424006\n",
      "training 0.0005664663622155786 relative L2 0.0011688866652548313\n",
      "training 0.0005664157215505838 relative L2 0.0011688353260979056\n",
      "training 0.0005663652555085719 relative L2 0.0011687835212796926\n",
      "training 0.0005663144402205944 relative L2 0.0011687324149534106\n",
      "training 0.0005662637995555997 relative L2 0.0011686807265505195\n",
      "training 0.000566213158890605 relative L2 0.0011686296202242374\n",
      "training 0.0005661626346409321 relative L2 0.0011685780482366681\n",
      "training 0.0005661117611452937 relative L2 0.0011685264762490988\n",
      "training 0.0005660611786879599 relative L2 0.0011684750206768513\n",
      "training 0.0005660103633999825 relative L2 0.0011684237979352474\n",
      "training 0.0005659599555656314 relative L2 0.0011683717602863908\n",
      "training 0.0005659089656546712 relative L2 0.001168320537544787\n",
      "training 0.0005658583831973374 relative L2 0.001168268732726574\n",
      "training 0.0005658075679093599 relative L2 0.0011682171607390046\n",
      "training 0.0005657569272443652 relative L2 0.001168165705166757\n",
      "training 0.0005657062283717096 relative L2 0.001168114016763866\n",
      "training 0.0005656554130837321 relative L2 0.0011680624447762966\n",
      "training 0.0005656047142110765 relative L2 0.0011680106399580836\n",
      "training 0.00056555395713076 relative L2 0.0011679590679705143\n",
      "training 0.0005655032000504434 relative L2 0.0011679073795676231\n",
      "training 0.0005654524429701269 relative L2 0.0011678558075800538\n",
      "training 0.0005654015694744885 relative L2 0.0011678041191771626\n",
      "training 0.0005653509288094938 relative L2 0.0011677523143589497\n",
      "training 0.0005653002299368382 relative L2 0.0011677007423713803\n",
      "training 0.0005652494146488607 relative L2 0.0011676489375531673\n",
      "training 0.0005651985993608832 relative L2 0.001167597365565598\n",
      "training 0.0005651478422805667 relative L2 0.0011675456771627069\n",
      "training 0.0005650970269925892 relative L2 0.0011674938723444939\n",
      "training 0.0005650462699122727 relative L2 0.0011674420675262809\n",
      "training 0.0005649954546242952 relative L2 0.0011673903791233897\n",
      "training 0.0005649446975439787 relative L2 0.0011673385743051767\n",
      "training 0.0005648938822560012 relative L2 0.0011672868859022856\n",
      "training 0.0005648430669680238 relative L2 0.0011672349646687508\n",
      "training 0.0005647922516800463 relative L2 0.0011671833926811814\n",
      "training 0.0005647414363920689 relative L2 0.0011671315878629684\n",
      "training 0.0005646906793117523 relative L2 0.0011670796666294336\n",
      "training 0.0005646398640237749 relative L2 0.0011670279782265425\n",
      "training 0.0005645889905281365 relative L2 0.0011669762898236513\n",
      "training 0.000564538175240159 relative L2 0.0011669243685901165\n",
      "training 0.0005644873599521816 relative L2 0.0011668726801872253\n",
      "training 0.0005644364864565432 relative L2 0.0011668206425383687\n",
      "training 0.000564385496545583 relative L2 0.0011667689541354775\n",
      "training 0.0005643346812576056 relative L2 0.001166716916486621\n",
      "training 0.0005642838077619672 relative L2 0.001166665111668408\n",
      "training 0.0005642328760586679 relative L2 0.0011666130740195513\n",
      "training 0.0005641820607706904 relative L2 0.0011665610363706946\n",
      "training 0.0005641311290673912 relative L2 0.001166508998721838\n",
      "training 0.0005640804301947355 relative L2 0.0011664573103189468\n",
      "training 0.0005640293820761144 relative L2 0.0011664055055007339\n",
      "training 0.000563978566788137 relative L2 0.0011663534678518772\n",
      "training 0.0005639276932924986 relative L2 0.0011663018958643079\n",
      "training 0.0005638767615891993 relative L2 0.0011662497418001294\n",
      "training 0.0005638258880935609 relative L2 0.0011661978205665946\n",
      "training 0.0005637750145979226 relative L2 0.0011661458993330598\n",
      "training 0.0005637240246869624 relative L2 0.0011660938616842031\n",
      "training 0.000563673151191324 relative L2 0.0011660418240353465\n",
      "training 0.0005636222194880247 relative L2 0.0011659900192171335\n",
      "training 0.0005635711713694036 relative L2 0.0011659377487376332\n",
      "training 0.0005635202396661043 relative L2 0.0011658858275040984\n",
      "training 0.0005634692497551441 relative L2 0.00116583367343992\n",
      "training 0.0005634183180518448 relative L2 0.0011657816357910633\n",
      "training 0.0005633672699332237 relative L2 0.0011657294817268848\n",
      "training 0.0005633163964375854 relative L2 0.0011656773276627064\n",
      "training 0.0005632653483189642 relative L2 0.0011656254064291716\n",
      "training 0.000563214416615665 relative L2 0.0011655731359496713\n",
      "training 0.0005631634267047048 relative L2 0.0011655212147161365\n",
      "training 0.0005631123785860837 relative L2 0.0011654692934826016\n",
      "training 0.0005630613304674625 relative L2 0.0011654171394184232\n",
      "training 0.0005630102241411805 relative L2 0.0011653649853542447\n",
      "training 0.0005629591760225594 relative L2 0.0011653128312900662\n",
      "training 0.0005629081861115992 relative L2 0.0011652609100565314\n",
      "training 0.0005628570797853172 relative L2 0.0011652085231617093\n",
      "training 0.0005628059734590352 relative L2 0.0011651563690975308\n",
      "training 0.000562754925340414 relative L2 0.0011651042150333524\n",
      "training 0.0005627039936371148 relative L2 0.0011650521773844957\n",
      "training 0.0005626528873108327 relative L2 0.0011649997904896736\n",
      "training 0.0005626017809845507 relative L2 0.0011649476364254951\n",
      "training 0.0005625507328659296 relative L2 0.0011648954823613167\n",
      "training 0.0005624996265396476 relative L2 0.0011648430954664946\n",
      "training 0.0005624485784210265 relative L2 0.001164790941402316\n",
      "training 0.0005623974138870835 relative L2 0.0011647386709228158\n",
      "training 0.0005623463657684624 relative L2 0.0011646865168586373\n",
      "training 0.0005622952012345195 relative L2 0.0011646341299638152\n",
      "training 0.0005622440367005765 relative L2 0.0011645819758996367\n",
      "training 0.0005621929885819554 relative L2 0.0011645295890048146\n",
      "training 0.0005621416494250298 relative L2 0.0011644774349406362\n",
      "training 0.0005620906013064086 relative L2 0.0011644251644611359\n",
      "training 0.0005620394949801266 relative L2 0.0011643731268122792\n",
      "training 0.0005619883886538446 relative L2 0.0011643209727481008\n",
      "training 0.0005619371659122407 relative L2 0.0011642687022686005\n",
      "training 0.0005618860595859587 relative L2 0.0011642163153737783\n",
      "training 0.000561834720429033 relative L2 0.001164164044894278\n",
      "training 0.0005617835558950901 relative L2 0.0011641117744147778\n",
      "training 0.0005617323331534863 relative L2 0.0011640593875199556\n",
      "training 0.0005616811686195433 relative L2 0.0011640071170404553\n",
      "training 0.0005616298294626176 relative L2 0.001163954846560955\n",
      "training 0.0005615786067210138 relative L2 0.0011639025760814548\n",
      "training 0.0005615273839794099 relative L2 0.0011638501891866326\n",
      "training 0.000561476219445467 relative L2 0.0011637978022918105\n",
      "training 0.0005614249384962022 relative L2 0.0011637454153969884\n",
      "training 0.0005613736575469375 relative L2 0.0011636930285021663\n",
      "training 0.000561322143767029 relative L2 0.001163640758022666\n",
      "training 0.0005612709792330861 relative L2 0.0011635884875431657\n",
      "training 0.0005612198729068041 relative L2 0.001163535751402378\n",
      "training 0.0005611683009192348 relative L2 0.001163483364507556\n",
      "training 0.0005611169617623091 relative L2 0.001163430861197412\n",
      "training 0.0005610657972283661 relative L2 0.0011633787071332335\n",
      "training 0.0005610143998637795 relative L2 0.0011633260874077678\n",
      "training 0.0005609630607068539 relative L2 0.001163273467682302\n",
      "training 0.0005609116051346064 relative L2 0.0011632210807874799\n",
      "training 0.0005608602659776807 relative L2 0.0011631686938926578\n",
      "training 0.0005608089850284159 relative L2 0.0011631159577518702\n",
      "training 0.0005607573548331857 relative L2 0.0011630636872723699\n",
      "training 0.0005607061320915818 relative L2 0.0011630109511315823\n",
      "training 0.0005606545601040125 relative L2 0.0011629585642367601\n",
      "training 0.0005606031627394259 relative L2 0.0011629057116806507\n",
      "training 0.0005605518235825002 relative L2 0.0011628533247858286\n",
      "training 0.0005605004262179136 relative L2 0.0011628007050603628\n",
      "training 0.0005604488542303443 relative L2 0.001162748085334897\n",
      "training 0.0005603974568657577 relative L2 0.0011626954656094313\n",
      "training 0.0005603460012935102 relative L2 0.0011626429622992873\n",
      "training 0.0005602944293059409 relative L2 0.0011625903425738215\n",
      "training 0.0005602429737336934 relative L2 0.0011625378392636776\n",
      "training 0.0005601915763691068 relative L2 0.00116248510312289\n",
      "training 0.0005601401207968593 relative L2 0.0011624324833974242\n",
      "training 0.0005600885488092899 relative L2 0.0011623797472566366\n",
      "training 0.0005600371514447033 relative L2 0.0011623273603618145\n",
      "training 0.0005599855212494731 relative L2 0.0011622742749750614\n",
      "training 0.0005599340656772256 relative L2 0.0011622217716649175\n",
      "training 0.0005598825518973172 relative L2 0.0011621690355241299\n",
      "training 0.0005598309799097478 relative L2 0.0011621162993833423\n",
      "training 0.0005597795243375003 relative L2 0.0011620635632425547\n",
      "training 0.0005597280105575919 relative L2 0.0011620109435170889\n",
      "training 0.0005596763221547008 relative L2 0.0011619579745456576\n",
      "training 0.0005596247501671314 relative L2 0.0011619054712355137\n",
      "training 0.0005595733528025448 relative L2 0.0011618523858487606\n",
      "training 0.0005595216061919928 relative L2 0.001161799649707973\n",
      "training 0.0005594701506197453 relative L2 0.0011617470299825072\n",
      "training 0.0005594185786321759 relative L2 0.0011616944102570415\n",
      "training 0.0005593669484369457 relative L2 0.0011616412084549665\n",
      "training 0.0005593153764493763 relative L2 0.0011615887051448226\n",
      "training 0.0005592637462541461 relative L2 0.0011615357361733913\n",
      "training 0.0005592121742665768 relative L2 0.0011614830000326037\n",
      "training 0.0005591604858636856 relative L2 0.0011614300310611725\n",
      "training 0.0005591087974607944 relative L2 0.001161377178505063\n",
      "training 0.0005590572254732251 relative L2 0.00116132409311831\n",
      "training 0.0005590055952779949 relative L2 0.0011612713569775224\n",
      "training 0.0005589539068751037 relative L2 0.0011612183880060911\n",
      "training 0.0005589022184722126 relative L2 0.0011611654190346599\n",
      "training 0.0005588504718616605 relative L2 0.0011611123336479068\n",
      "training 0.0005587988416664302 relative L2 0.0011610595975071192\n",
      "training 0.0005587471532635391 relative L2 0.001161006628535688\n",
      "training 0.000558695406652987 relative L2 0.0011609536595642567\n",
      "training 0.0005586436600424349 relative L2 0.0011609006905928254\n",
      "training 0.0005585919716395438 relative L2 0.0011608476052060723\n",
      "training 0.0005585401668213308 relative L2 0.0011607948690652847\n",
      "training 0.0005584884784184396 relative L2 0.001160741550847888\n",
      "training 0.0005584364989772439 relative L2 0.001160688465461135\n",
      "training 0.0005583849269896746 relative L2 0.00116063526365906\n",
      "training 0.0005583329475484788 relative L2 0.0011605822946876287\n",
      "training 0.0005582811427302659 relative L2 0.0011605293257161975\n",
      "training 0.0005582293379120529 relative L2 0.0011604762403294444\n",
      "training 0.0005581775330938399 relative L2 0.0011604230385273695\n",
      "training 0.000558125670067966 relative L2 0.0011603699531406164\n",
      "training 0.0005580738070420921 relative L2 0.0011603167513385415\n",
      "training 0.0005580218276008964 relative L2 0.0011602635495364666\n",
      "training 0.0005579699645750225 relative L2 0.0011602106969803572\n",
      "training 0.0005579181597568095 relative L2 0.0011601574951782823\n",
      "training 0.0005578661803156137 relative L2 0.0011601042933762074\n",
      "training 0.0005578142590820789 relative L2 0.0011600508587434888\n",
      "training 0.0005577622796408832 relative L2 0.0011599981226027012\n",
      "training 0.0005577105330303311 relative L2 0.0011599446879699826\n",
      "training 0.0005576585535891354 relative L2 0.0011598913697525859\n",
      "training 0.0005576065741479397 relative L2 0.001159838167950511\n",
      "training 0.000557554536499083 relative L2 0.0011597848497331142\n",
      "training 0.0005575025570578873 relative L2 0.0011597316479310393\n",
      "training 0.0005574506358243525 relative L2 0.0011596784461289644\n",
      "training 0.0005573985981754959 relative L2 0.0011596250114962459\n",
      "training 0.000557346676941961 relative L2 0.0011595713440328836\n",
      "training 0.0005572945810854435 relative L2 0.0011595182586461306\n",
      "training 0.0005572425434365869 relative L2 0.0011594647075980902\n",
      "training 0.0005571904475800693 relative L2 0.0011594112729653716\n",
      "training 0.0005571384099312127 relative L2 0.0011593579547479749\n",
      "training 0.000557086372282356 relative L2 0.0011593046365305781\n",
      "training 0.0005570342182181776 relative L2 0.0011592512018978596\n",
      "training 0.00055698212236166 relative L2 0.001159197767265141\n",
      "training 0.0005569300265051425 relative L2 0.0011591442162171006\n",
      "training 0.0005568779306486249 relative L2 0.001159090781584382\n",
      "training 0.0005568258929997683 relative L2 0.0011590373469516635\n",
      "training 0.0005567737971432507 relative L2 0.001158983795903623\n",
      "training 0.0005567216430790722 relative L2 0.0011589303612709045\n",
      "training 0.0005566696054302156 relative L2 0.0011588766938075423\n",
      "training 0.0005566173349507153 relative L2 0.0011588233755901456\n",
      "training 0.0005565653555095196 relative L2 0.001158769940957427\n",
      "training 0.0005565130850300193 relative L2 0.001158716157078743\n",
      "training 0.0005564609309658408 relative L2 0.0011586626060307026\n",
      "training 0.0005564087769016623 relative L2 0.0011586090549826622\n",
      "training 0.000556356564629823 relative L2 0.0011585556203499436\n",
      "training 0.0005563044105656445 relative L2 0.001158502185717225\n",
      "training 0.000556252256501466 relative L2 0.0011584487510845065\n",
      "training 0.0005562001024372876 relative L2 0.0011583948507905006\n",
      "training 0.0005561476573348045 relative L2 0.0011583417654037476\n",
      "training 0.0005560955032706261 relative L2 0.0011582879815250635\n",
      "training 0.0005560432909987867 relative L2 0.0011582344304770231\n",
      "training 0.0005559911369346082 relative L2 0.0011581809958443046\n",
      "training 0.0005559388664551079 relative L2 0.0011581272119656205\n",
      "training 0.0005558865959756076 relative L2 0.0011580731952562928\n",
      "training 0.0005558343254961073 relative L2 0.0011580195277929306\n",
      "training 0.000555782055016607 relative L2 0.0011579658603295684\n",
      "training 0.0005557297263294458 relative L2 0.0011579121928662062\n",
      "training 0.0005556773394346237 relative L2 0.0011578582925722003\n",
      "training 0.0005556251271627843 relative L2 0.0011578045086935163\n",
      "training 0.0005555727984756231 relative L2 0.0011577506083995104\n",
      "training 0.0005555205279961228 relative L2 0.0011576968245208263\n",
      "training 0.0005554680246859789 relative L2 0.0011576429242268205\n",
      "training 0.0005554158706218004 relative L2 0.0011575891403481364\n",
      "training 0.0005553634255193174 relative L2 0.0011575352400541306\n",
      "training 0.000555311213247478 relative L2 0.0011574813397601247\n",
      "training 0.000555258768144995 relative L2 0.0011574274394661188\n",
      "training 0.0005552064394578338 relative L2 0.0011573736555874348\n",
      "training 0.0005551539943553507 relative L2 0.0011573198717087507\n",
      "training 0.0005551015492528677 relative L2 0.001157265854999423\n",
      "training 0.0005550492205657065 relative L2 0.0011572118382900953\n",
      "training 0.0005549967172555625 relative L2 0.0011571577051654458\n",
      "training 0.0005549443303607404 relative L2 0.00115710380487144\n",
      "training 0.0005548920016735792 relative L2 0.001157049904577434\n",
      "training 0.0005548394983634353 relative L2 0.0011569960042834282\n",
      "training 0.0005547871114686131 relative L2 0.0011569421039894223\n",
      "training 0.0005547346081584692 relative L2 0.0011568880872800946\n",
      "training 0.0005546822212636471 relative L2 0.001156833954155445\n",
      "training 0.0005546296015381813 relative L2 0.001156780170276761\n",
      "training 0.000554577331058681 relative L2 0.0011567258043214679\n",
      "training 0.0005545247695408762 relative L2 0.001156671904027462\n",
      "training 0.0005544721498154104 relative L2 0.001156618120148778\n",
      "training 0.0005544198211282492 relative L2 0.0011565638706088066\n",
      "training 0.0005543672014027834 relative L2 0.0011565099703148007\n",
      "training 0.0005543147563003004 relative L2 0.0011564560700207949\n",
      "training 0.0005542621947824955 relative L2 0.001156402169726789\n",
      "training 0.0005542095750570297 relative L2 0.0011563479201868176\n",
      "training 0.0005541570717468858 relative L2 0.00115629390347749\n",
      "training 0.0005541046266444027 relative L2 0.0011562398867681623\n",
      "training 0.000554052006918937 relative L2 0.0011561857536435127\n",
      "training 0.0005539994454011321 relative L2 0.0011561313876882195\n",
      "training 0.0005539467674680054 relative L2 0.00115607725456357\n",
      "training 0.0005538941477425396 relative L2 0.0011560230050235987\n",
      "training 0.0005538415862247348 relative L2 0.0011559687554836273\n",
      "training 0.0005537890247069299 relative L2 0.0011559143895283341\n",
      "training 0.0005537364049814641 relative L2 0.0011558601399883628\n",
      "training 0.0005536837270483375 relative L2 0.0011558058904483914\n",
      "training 0.0005536310491152108 relative L2 0.00115575164090842\n",
      "training 0.000553578429389745 relative L2 0.001155697274953127\n",
      "training 0.0005535256932489574 relative L2 0.0011556430254131556\n",
      "training 0.0005534730153158307 relative L2 0.0011555887758731842\n",
      "training 0.0005534203955903649 relative L2 0.001155534409917891\n",
      "training 0.0005533676594495773 relative L2 0.0011554801603779197\n",
      "training 0.0005533148651011288 relative L2 0.0011554257944226265\n",
      "training 0.0005532621871680021 relative L2 0.0011553713120520115\n",
      "training 0.0005532094510272145 relative L2 0.0011553172953426838\n",
      "training 0.000553156656678766 relative L2 0.0011552628129720688\n",
      "training 0.0005531038623303175 relative L2 0.0011552084470167756\n",
      "training 0.0005530512426048517 relative L2 0.0011551541974768043\n",
      "training 0.0005529985064640641 relative L2 0.0011550994822755456\n",
      "training 0.0005529455374926329 relative L2 0.0011550452327355742\n",
      "training 0.0005528929177671671 relative L2 0.0011549907503649592\n",
      "training 0.0005528399487957358 relative L2 0.0011549362679943442\n",
      "training 0.0005527869798243046 relative L2 0.0011548817856237292\n",
      "training 0.0005527341854758561 relative L2 0.0011548273032531142\n",
      "training 0.0005526813329197466 relative L2 0.001154772937297821\n",
      "training 0.0005526285385712981 relative L2 0.0011547183385118842\n",
      "training 0.0005525755695998669 relative L2 0.0011546638561412692\n",
      "training 0.0005525228334590793 relative L2 0.0011546090245246887\n",
      "training 0.000552469864487648 relative L2 0.0011545546585693955\n",
      "training 0.0005524169537238777 relative L2 0.0011544999433681369\n",
      "training 0.0005523641011677682 relative L2 0.0011544451117515564\n",
      "training 0.0005523112486116588 relative L2 0.0011543906293809414\n",
      "training 0.0005522582796402276 relative L2 0.0011543361470103264\n",
      "training 0.0005522054270841181 relative L2 0.0011542814318090677\n",
      "training 0.0005521524581126869 relative L2 0.001154226716607809\n",
      "training 0.0005520994891412556 relative L2 0.0011541718849912286\n",
      "training 0.0005520465783774853 relative L2 0.001154117053374648\n",
      "training 0.000551993609406054 relative L2 0.0011540623381733894\n",
      "training 0.0005519406986422837 relative L2 0.0011540073901414871\n",
      "training 0.0005518876714631915 relative L2 0.0011539526749402285\n",
      "training 0.0005518347606994212 relative L2 0.0011538979597389698\n",
      "training 0.000551781733520329 relative L2 0.0011538431281223893\n",
      "training 0.0005517287063412368 relative L2 0.0011537885293364525\n",
      "training 0.0005516756791621447 relative L2 0.0011537334648892283\n",
      "training 0.0005516226519830525 relative L2 0.0011536787496879697\n",
      "training 0.0005515695083886385 relative L2 0.0011536239180713892\n",
      "training 0.0005515165394172072 relative L2 0.0011535690864548087\n",
      "training 0.0005514635122381151 relative L2 0.0011535141384229064\n",
      "training 0.0005514103686437011 relative L2 0.001153459306806326\n",
      "training 0.000551357283256948 relative L2 0.0011534044751897454\n",
      "training 0.0005513043142855167 relative L2 0.0011533495271578431\n",
      "training 0.0005512511124834418 relative L2 0.0011532945791259408\n",
      "training 0.0005511979688890278 relative L2 0.0011532398639246821\n",
      "training 0.0005511448252946138 relative L2 0.0011531846830621362\n",
      "training 0.000551091565284878 relative L2 0.0011531299678608775\n",
      "training 0.0005510386545211077 relative L2 0.0011530747869983315\n",
      "training 0.0005509853363037109 relative L2 0.0011530200717970729\n",
      "training 0.0005509321927092969 relative L2 0.0011529650073498487\n",
      "training 0.0005508791073225439 relative L2 0.0011529101757332683\n",
      "training 0.0005508257891051471 relative L2 0.0011528549948707223\n",
      "training 0.000550772703718394 relative L2 0.00115280004683882\n",
      "training 0.0005507194437086582 relative L2 0.0011527449823915958\n",
      "training 0.0005506662419065833 relative L2 0.0011526901507750154\n",
      "training 0.0005506130983121693 relative L2 0.0011526350863277912\n",
      "training 0.0005505598383024335 relative L2 0.001152580021880567\n",
      "training 0.0005505066365003586 relative L2 0.001152524841018021\n",
      "training 0.0005504534929059446 relative L2 0.0011524696601554751\n",
      "training 0.000550400058273226 relative L2 0.001152414595708251\n",
      "training 0.0005503467982634902 relative L2 0.0011523592984303832\n",
      "training 0.0005502935382537544 relative L2 0.0011523040011525154\n",
      "training 0.0005502401036210358 relative L2 0.0011522487038746476\n",
      "training 0.0005501867271959782 relative L2 0.0011521934065967798\n",
      "training 0.0005501334089785814 relative L2 0.0011521383421495557\n",
      "training 0.0005500800907611847 relative L2 0.0011520830448716879\n",
      "training 0.0005500268307514489 relative L2 0.001152027864009142\n",
      "training 0.0005499734543263912 relative L2 0.0011519723339006305\n",
      "training 0.0005499201361089945 relative L2 0.0011519170366227627\n",
      "training 0.000549866643268615 relative L2 0.001151861622929573\n",
      "training 0.0005498132668435574 relative L2 0.0011518063256517053\n",
      "training 0.0005497599486261606 relative L2 0.0011517510283738375\n",
      "training 0.000549706572201103 relative L2 0.0011516957310959697\n",
      "training 0.0005496532539837062 relative L2 0.0011516404338181019\n",
      "training 0.0005495997029356658 relative L2 0.0011515849037095904\n",
      "training 0.0005495463265106082 relative L2 0.0011515297228470445\n",
      "training 0.0005494928918778896 relative L2 0.0011514740763232112\n",
      "training 0.0005494394572451711 relative L2 0.0011514186626300216\n",
      "training 0.0005493859644047916 relative L2 0.0011513628996908665\n",
      "training 0.0005493324715644121 relative L2 0.0011513077188283205\n",
      "training 0.0005492790951393545 relative L2 0.001151252188719809\n",
      "training 0.0005492254858836532 relative L2 0.0011511965421959758\n",
      "training 0.0005491721094585955 relative L2 0.0011511411285027862\n",
      "training 0.0005491185584105551 relative L2 0.0011510855983942747\n",
      "training 0.0005490650655701756 relative L2 0.0011510298354551196\n",
      "training 0.0005490115145221353 relative L2 0.0011509743053466082\n",
      "training 0.000548957905266434 relative L2 0.0011509186588227749\n",
      "training 0.0005489044706337154 relative L2 0.0011508632451295853\n",
      "training 0.000548850919585675 relative L2 0.001150807598605752\n",
      "training 0.0005487973103299737 relative L2 0.0011507521849125624\n",
      "training 0.0005487438174895942 relative L2 0.001150696538388729\n",
      "training 0.000548690150026232 relative L2 0.0011506408918648958\n",
      "training 0.0005486367153935134 relative L2 0.001150585012510419\n",
      "training 0.0005485829897224903 relative L2 0.0011505293659865856\n",
      "training 0.0005485293222591281 relative L2 0.0011504737194627523\n",
      "training 0.0005484757130034268 relative L2 0.0011504179565235972\n",
      "training 0.0005484221037477255 relative L2 0.001150362309999764\n",
      "training 0.0005483684944920242 relative L2 0.0011503065470606089\n",
      "training 0.0005483148852363229 relative L2 0.00115025055129081\n",
      "training 0.0005482612759806216 relative L2 0.0011501949047669768\n",
      "training 0.0005482076667249203 relative L2 0.0011501390254125\n",
      "training 0.0005481538828462362 relative L2 0.0011500834953039885\n",
      "training 0.000548100215382874 relative L2 0.0011500277323648334\n",
      "training 0.0005480464897118509 relative L2 0.001149972085841\n",
      "training 0.0005479928222484887 relative L2 0.0011499165557324886\n",
      "training 0.0005479392129927874 relative L2 0.0011498606763780117\n",
      "training 0.0005478854873217642 relative L2 0.0011498049134388566\n",
      "training 0.0005478317616507411 relative L2 0.0011497489176690578\n",
      "training 0.0005477780941873789 relative L2 0.0011496931547299027\n",
      "training 0.0005477243103086948 relative L2 0.0011496370425447822\n",
      "training 0.0005476704682223499 relative L2 0.001149581279605627\n",
      "training 0.0005476167425513268 relative L2 0.0011495252838358283\n",
      "training 0.0005475630168803036 relative L2 0.0011494694044813514\n",
      "training 0.0005475090583786368 relative L2 0.0011494134087115526\n",
      "training 0.0005474553327076137 relative L2 0.0011493571801111102\n",
      "training 0.0005474014324136078 relative L2 0.0011493013007566333\n",
      "training 0.0005473476485349238 relative L2 0.0011492453049868345\n",
      "training 0.0005472938646562397 relative L2 0.0011491894256323576\n",
      "training 0.000547239906154573 relative L2 0.0011491331970319152\n",
      "training 0.0005471861222758889 relative L2 0.0011490773176774383\n",
      "training 0.000547132221981883 relative L2 0.0011490210890769958\n",
      "training 0.0005470783216878772 relative L2 0.001148965209722519\n",
      "training 0.0005470245378091931 relative L2 0.001148909330368042\n",
      "training 0.0005469706375151873 relative L2 0.0011488533345982432\n",
      "training 0.0005469167372211814 relative L2 0.0011487972224131227\n",
      "training 0.0005468627787195146 relative L2 0.0011487409938126802\n",
      "training 0.0005468087620101869 relative L2 0.0011486848816275597\n",
      "training 0.0005467547453008592 relative L2 0.001148628769442439\n",
      "training 0.0005467007285915315 relative L2 0.0011485725408419967\n",
      "training 0.0005466468282975256 relative L2 0.001148516428656876\n",
      "training 0.0005465929280035198 relative L2 0.0011484602000564337\n",
      "training 0.0005465388530865312 relative L2 0.0011484039714559913\n",
      "training 0.0005464847199618816 relative L2 0.0011483480921015143\n",
      "training 0.000546430645044893 relative L2 0.0011482915142551064\n",
      "training 0.0005463764537125826 relative L2 0.001148235285654664\n",
      "training 0.0005463224952109158 relative L2 0.0011481791734695435\n",
      "training 0.0005462684202939272 relative L2 0.0011481227120384574\n",
      "training 0.0005462144035845995 relative L2 0.0011480663670226932\n",
      "training 0.00054616027045995 relative L2 0.001148009905591607\n",
      "training 0.0005461060791276395 relative L2 0.0011479537934064865\n",
      "training 0.0005460520624183118 relative L2 0.0011478972155600786\n",
      "training 0.0005459979292936623 relative L2 0.0011478406377136707\n",
      "training 0.0005459437379613519 relative L2 0.0011477841762825847\n",
      "training 0.0005458894884213805 relative L2 0.0011477277148514986\n",
      "training 0.000545835355296731 relative L2 0.0011476713698357344\n",
      "training 0.0005457812221720815 relative L2 0.0011476147919893265\n",
      "training 0.0005457269726321101 relative L2 0.0011475583305582404\n",
      "training 0.0005456727812997997 relative L2 0.0011475016362965107\n",
      "training 0.0005456184735521674 relative L2 0.0011474450584501028\n",
      "training 0.0005455641658045352 relative L2 0.0011473885970190167\n",
      "training 0.0005455099744722247 relative L2 0.0011473320191726089\n",
      "training 0.0005454555503092706 relative L2 0.001147275441326201\n",
      "training 0.0005454013589769602 relative L2 0.0011472186306491494\n",
      "training 0.0005453471094369888 relative L2 0.0011471619363874197\n",
      "training 0.0005452927434816957 relative L2 0.0011471053585410118\n",
      "training 0.0005452383775264025 relative L2 0.0011470488971099257\n",
      "training 0.0005451840697787702 relative L2 0.001146992202848196\n",
      "training 0.000545129703823477 relative L2 0.0011469352757558227\n",
      "training 0.0005450752796605229 relative L2 0.001146878581494093\n",
      "training 0.0005450208554975688 relative L2 0.001146822003647685\n",
      "training 0.0005449664313346148 relative L2 0.0011467654258012772\n",
      "training 0.0005449120071716607 relative L2 0.0011467086151242256\n",
      "training 0.0005448576412163675 relative L2 0.001146651920862496\n",
      "training 0.0005448031588457525 relative L2 0.0011465954594314098\n",
      "training 0.0005447487346827984 relative L2 0.0011465385323390365\n",
      "training 0.0005446940776892006 relative L2 0.001146481721661985\n",
      "training 0.0005446395953185856 relative L2 0.0011464249109849334\n",
      "training 0.0005445851711556315 relative L2 0.00114636798389256\n",
      "training 0.0005445306305773556 relative L2 0.0011463109403848648\n",
      "training 0.0005444760899990797 relative L2 0.0011462540132924914\n",
      "training 0.0005444215494208038 relative L2 0.00114619720261544\n",
      "training 0.000544366950634867 relative L2 0.0011461402755230665\n",
      "training 0.000544312410056591 relative L2 0.0011460833484306931\n",
      "training 0.000544257927685976 relative L2 0.0011460264213383198\n",
      "training 0.0005442032706923783 relative L2 0.0011459694942459464\n",
      "training 0.0005441486136987805 relative L2 0.0011459127999842167\n",
      "training 0.0005440941313281655 relative L2 0.0011458557564765215\n",
      "training 0.0005440394161269069 relative L2 0.0011457987129688263\n",
      "training 0.00054398481734097 relative L2 0.001145741669461131\n",
      "training 0.0005439302185550332 relative L2 0.0011456848587840796\n",
      "training 0.0005438755615614355 relative L2 0.0011456278152763844\n",
      "training 0.0005438209045678377 relative L2 0.0011455710045993328\n",
      "training 0.0005437660729512572 relative L2 0.0011455138446763158\n",
      "training 0.0005437114741653204 relative L2 0.001145456568337977\n",
      "training 0.0005436568171717227 relative L2 0.0011453996412456036\n",
      "training 0.0005436020437628031 relative L2 0.0011453424813225865\n",
      "training 0.0005435473867692053 relative L2 0.0011452854378148913\n",
      "training 0.0005434926715679467 relative L2 0.0011452281614765525\n",
      "training 0.000543437956366688 relative L2 0.0011451712343841791\n",
      "training 0.0005433832411654294 relative L2 0.0011451138416305184\n",
      "training 0.000543328351341188 relative L2 0.0011450566817075014\n",
      "training 0.0005432736361399293 relative L2 0.0011449995217844844\n",
      "training 0.0005432188045233488 relative L2 0.0011449423618614674\n",
      "training 0.0005431639729067683 relative L2 0.0011448853183537722\n",
      "training 0.000543109024874866 relative L2 0.0011448278091847897\n",
      "training 0.0005430542514659464 relative L2 0.0011447707656770945\n",
      "training 0.0005429995362646878 relative L2 0.001144713256508112\n",
      "training 0.0005429445300251245 relative L2 0.0011446562130004168\n",
      "training 0.0005428896402008832 relative L2 0.0011445984710007906\n",
      "training 0.0005428347503766418 relative L2 0.001144541660323739\n",
      "training 0.0005427798605524004 relative L2 0.001144483918324113\n",
      "training 0.000542724912520498 relative L2 0.0011444269912317395\n",
      "training 0.0005426700226962566 relative L2 0.0011443692492321134\n",
      "training 0.0005426151910796762 relative L2 0.0011443120893090963\n",
      "training 0.000542560126632452 relative L2 0.0011442545801401138\n",
      "training 0.0005425051786005497 relative L2 0.0011441971873864532\n",
      "training 0.0005424501723609865 relative L2 0.0011441395618021488\n",
      "training 0.0005423951079137623 relative L2 0.0011440824018791318\n",
      "training 0.0005423402180895209 relative L2 0.0011440245434641838\n",
      "training 0.000542285037226975 relative L2 0.0011439671507105231\n",
      "training 0.0005422300309874117 relative L2 0.0011439097579568624\n",
      "training 0.0005421750247478485 relative L2 0.0011438521323725581\n",
      "training 0.0005421199603006244 relative L2 0.001143794390372932\n",
      "training 0.0005420648376457393 relative L2 0.001143737230449915\n",
      "training 0.000542009889613837 relative L2 0.001143679372034967\n",
      "training 0.000541954708751291 relative L2 0.0011436218628659844\n",
      "training 0.0005418995278887451 relative L2 0.00114356423728168\n",
      "training 0.0005418445216491818 relative L2 0.0011435068445280194\n",
      "training 0.0005417893407866359 relative L2 0.0011434491025283933\n",
      "training 0.0005417342181317508 relative L2 0.0011433917097747326\n",
      "training 0.0005416790954768658 relative L2 0.0011433339677751064\n",
      "training 0.0005416239728219807 relative L2 0.0011432759929448366\n",
      "training 0.0005415686755441129 relative L2 0.0011432183673605323\n",
      "training 0.000541513494681567 relative L2 0.0011431605089455843\n",
      "training 0.0005414582556113601 relative L2 0.0011431026505306363\n",
      "training 0.000541403132956475 relative L2 0.001143045024946332\n",
      "training 0.00054134801030159 relative L2 0.0011429873993620276\n",
      "training 0.0005412927130237222 relative L2 0.0011429296573624015\n",
      "training 0.0005412374157458544 relative L2 0.0011428717989474535\n",
      "training 0.0005411821766756475 relative L2 0.0011428139405325055\n",
      "training 0.0005411269958131015 relative L2 0.0011427561985328794\n",
      "training 0.0005410715821199119 relative L2 0.0011426982237026095\n",
      "training 0.000541016343049705 relative L2 0.0011426405981183052\n",
      "training 0.0005409611621871591 relative L2 0.001142582157626748\n",
      "training 0.0005409058067016304 relative L2 0.0011425254633650184\n",
      "training 0.0005408508004620671 relative L2 0.001142466557212174\n",
      "training 0.0005407959688454866 relative L2 0.0011424121912568808\n",
      "training 0.000540741893928498 relative L2 0.0011423546820878983\n",
      "training 0.0005406909040175378 relative L2 0.001142324530519545\n",
      "training 0.0005406542913988233 relative L2 0.001142330700531602\n",
      "training 0.000540677341632545 relative L2 0.001142660854384303\n",
      "training 0.0005409648874774575 relative L2 0.0011440855450928211\n",
      "training 0.000542416877578944 relative L2 0.0011509957257658243\n",
      "training 0.0005490004550665617 relative L2 0.0011801079381257296\n",
      "training 0.000578099163249135 relative L2 0.0013033355353400111\n",
      "training 0.0007072595180943608 relative L2 0.001736030331812799\n",
      "training 0.0012664853129535913 relative L2 0.0029331697151064873\n",
      "training 0.0036341252271085978 relative L2 0.005234516691416502\n",
      "training 0.011613412760198116 relative L2 0.007844464853405952\n",
      "training 0.026086675003170967 relative L2 0.006385694723576307\n",
      "training 0.017288463190197945 relative L2 0.0011620266595855355\n",
      "training 0.0005598025163635612 relative L2 0.005499150604009628\n",
      "training 0.012812457047402859 relative L2 0.004674842581152916\n",
      "training 0.009260599501430988 relative L2 0.0016705850139260292\n",
      "training 0.001171923358924687 relative L2 0.005004455801099539\n",
      "training 0.010609091259539127 relative L2 0.002160019939765334\n",
      "training 0.001967585412785411 relative L2 0.003488796064630151\n",
      "training 0.005152611061930656 relative L2 0.0036940129939466715\n",
      "training 0.005774117074906826 relative L2 0.0016744164749979973\n",
      "training 0.0011758952168747783 relative L2 0.003927850630134344\n",
      "training 0.006533791311085224 relative L2 0.00139313202816993\n",
      "training 0.0008100871345959604 relative L2 0.003242617007344961\n",
      "training 0.0044459751807153225 relative L2 0.0022049443796277046\n",
      "training 0.0020504952408373356 relative L2 0.0022826590575277805\n",
      "training 0.0021984970662742853 relative L2 0.002712634624913335\n",
      "training 0.0031072921119630337 relative L2 0.0014797498006373644\n",
      "training 0.0009154995786957443 relative L2 0.002784345531836152\n",
      "training 0.003277125768363476 relative L2 0.0012047144118696451\n",
      "training 0.0006027281633578241 relative L2 0.002507132710888982\n",
      "training 0.002652327762916684 relative L2 0.0014627937925979495\n",
      "training 0.0008953931974247098 relative L2 0.0020823590457439423\n",
      "training 0.001827637548558414 relative L2 0.0017891470342874527\n",
      "training 0.0013443093048408628 relative L2 0.0016316227847710252\n",
      "training 0.001115861814469099 relative L2 0.001954363426193595\n",
      "training 0.0016084102680906653 relative L2 0.0012881933944299817\n",
      "training 0.0006914898985996842 relative L2 0.0019520232453942299\n",
      "training 0.0016027612145990133 relative L2 0.0011876086937263608\n",
      "training 0.0005855947965756059 relative L2 0.001824235892854631\n",
      "training 0.0013997366186231375 relative L2 0.0012766321888193488\n",
      "training 0.0006783583085052669 relative L2 0.0016101475339382887\n",
      "training 0.001086427946574986 relative L2 0.0014082585694268346\n",
      "training 0.0008290015393868089 relative L2 0.0013858694583177567\n",
      "training 0.0008024385897442698 relative L2 0.0014998832484707236\n",
      "training 0.000941081962082535 relative L2 0.0012248139828443527\n",
      "training 0.0006234757020138204 relative L2 0.0015128637896850705\n",
      "training 0.0009586924570612609 relative L2 0.001163512235507369\n",
      "training 0.0005615694099105895 relative L2 0.0014515039511024952\n",
      "training 0.0008805643883533776 relative L2 0.0011914134956896305\n",
      "training 0.0005895796930417418 relative L2 0.0013484782539308071\n",
      "training 0.0007590313325636089 relative L2 0.0012534121051430702\n",
      "training 0.0006534368149004877 relative L2 0.0012422701111063361\n",
      "training 0.000641638645902276 relative L2 0.0012963941553607583\n",
      "training 0.0007005496299825609 relative L2 0.0011692270636558533\n",
      "training 0.0005673142150044441 relative L2 0.0013008358655497432\n",
      "training 0.0007046828977763653 relative L2 0.0011490207398310304\n",
      "training 0.0005473211058415473 relative L2 0.0012681030202656984\n",
      "training 0.0006697331555187702 relative L2 0.0011675111018121243\n",
      "training 0.0005653210100717843 relative L2 0.0012169822584837675\n",
      "training 0.0006152232526801527 relative L2 0.001195400720462203\n",
      "training 0.0005936366505920887 relative L2 0.0011700158938765526\n",
      "training 0.0005680990288965404 relative L2 0.0012119063176214695\n",
      "training 0.0006099756574258208 relative L2 0.0011461456306278706\n",
      "training 0.0005444253911264241 relative L2 0.0012065672781318426\n",
      "training 0.0006050404044799507 relative L2 0.0011459984816610813\n",
      "training 0.0005442656693048775 relative L2 0.0011857966892421246\n",
      "training 0.000583451590500772 relative L2 0.001158452476374805\n",
      "training 0.0005566337495110929 relative L2 0.0011610102374106646\n",
      "training 0.0005591616500169039 relative L2 0.0011702184565365314\n",
      "training 0.0005679171881638467 relative L2 0.0011450931197032332\n",
      "training 0.0005433671176433563 relative L2 0.0011716400040313601\n",
      "training 0.0005697172600775957 relative L2 0.0011412410531193018\n",
      "training 0.0005396806518547237 relative L2 0.001163991168141365\n",
      "training 0.0005617660353891551 relative L2 0.0011459855595603585\n",
      "training 0.0005443841218948364 relative L2 0.0011518980609253049\n",
      "training 0.0005501853302121162 relative L2 0.0011523970169946551\n",
      "training 0.0005504246219061315 relative L2 0.001143027562648058\n",
      "training 0.0005413683829829097 relative L2 0.001154242199845612\n",
      "training 0.0005524958833120763 relative L2 0.00113986327778548\n",
      "training 0.000538366031832993 relative L2 0.001151484320871532\n",
      "training 0.000549535674508661 relative L2 0.0011415625922381878\n",
      "training 0.0005400640075094998 relative L2 0.0011456318898126483\n",
      "training 0.0005440515233203769 relative L2 0.0011447900906205177\n",
      "training 0.0005430575693026185 relative L2 0.0011410247534513474\n",
      "training 0.0005394440959207714 relative L2 0.0011458983644843102\n",
      "training 0.0005443154368549585 relative L2 0.001139121362939477\n",
      "training 0.0005376533954404294 relative L2 0.0011447886936366558\n",
      "training 0.0005430516903288662 relative L2 0.0011397786438465118\n",
      "training 0.0005383192910812795 relative L2 0.001141872606240213\n",
      "training 0.0005403757095336914 relative L2 0.0011413583997637033\n",
      "training 0.0005397527129389346 relative L2 0.001139577478170395\n",
      "training 0.0005380542716011405 relative L2 0.0011418585199862719\n",
      "training 0.0005403621471486986 relative L2 0.0011385884135961533\n",
      "training 0.0005371319712139666 relative L2 0.0011412884341552854\n",
      "training 0.00053967977873981 relative L2 0.0011388856219127774\n",
      "training 0.0005374418688006699 relative L2 0.0011397643247619271\n",
      "training 0.0005383101524785161 relative L2 0.0011396461632102728\n",
      "training 0.0005381086375564337 relative L2 0.0011386268306523561\n",
      "training 0.0005371439037844539 relative L2 0.0011397743364796042\n",
      "training 0.0005383214447647333 relative L2 0.0011381525546312332\n",
      "training 0.0005367055418901145 relative L2 0.0011394035536795855\n",
      "training 0.0005378735950216651 relative L2 0.0011383009841665626\n",
      "training 0.000536870036739856 relative L2 0.001138543477281928\n",
      "training 0.0005371129373088479 relative L2 0.0011386382393538952\n",
      "training 0.0005371448933146894 relative L2 0.0011379806092008948\n",
      "training 0.0005365285906009376 relative L2 0.0011385679244995117\n",
      "training 0.0005371405277401209 relative L2 0.0011377695482224226\n",
      "training 0.000536332547198981 relative L2 0.0011382935335859656\n",
      "training 0.0005368146230466664 relative L2 0.0011378198396414518\n",
      "training 0.0005364012322388589 relative L2 0.0011378051713109016\n",
      "training 0.0005363886011764407 relative L2 0.001137937419116497\n",
      "training 0.0005364759126678109 relative L2 0.0011375185567885637\n",
      "training 0.0005360882496461272 relative L2 0.0011378012131899595\n",
      "training 0.0005363901145756245 relative L2 0.0011374132009223104\n",
      "training 0.0005359870265237987 relative L2 0.0011375978356227279\n",
      "training 0.0005361528019420803 relative L2 0.0011374019086360931\n",
      "training 0.0005359952338039875 relative L2 0.0011373019078746438\n",
      "training 0.0005358958151191473 relative L2 0.0011374109890311956\n",
      "training 0.0005359745118767023 relative L2 0.0011371386935934424\n",
      "training 0.000535727187525481 relative L2 0.001137255341745913\n",
      "training 0.0005358575726859272 relative L2 0.0011370740830898285\n",
      "training 0.0005356608307920396 relative L2 0.001137089915573597\n",
      "training 0.0005356718902476132 relative L2 0.0011370168067514896\n",
      "training 0.0005356241599656641 relative L2 0.001136902254074812\n",
      "training 0.0005355073953978717 relative L2 0.0011369626736268401\n",
      "training 0.0005355497705750167 relative L2 0.0011367946863174438\n",
      "training 0.0005354017484933138 relative L2 0.0011368124978616834\n",
      "training 0.0005354282329790294 relative L2 0.0011367332190275192\n",
      "training 0.0005353358574211597 relative L2 0.0011366844410076737\n",
      "training 0.0005352895823307335 relative L2 0.0011366454418748617\n",
      "training 0.0005352669395506382 relative L2 0.0011365560349076986\n",
      "training 0.0005351741565391421 relative L2 0.0011365649988874793\n",
      "training 0.0005351739237084985 relative L2 0.0011364632518962026\n",
      "training 0.0005350870196707547 relative L2 0.0011364337988197803\n",
      "training 0.0005350621649995446 relative L2 0.0011363913072273135\n",
      "training 0.0005350108258426189 relative L2 0.001136329723522067\n",
      "training 0.0005349540151655674 relative L2 0.0011362956138327718\n",
      "training 0.0005349304410628974 relative L2 0.001136226812377572\n",
      "training 0.0005348582053557038 relative L2 0.001136205974034965\n",
      "training 0.0005348337581381202 relative L2 0.001136135426349938\n",
      "training 0.0005347743281163275 relative L2 0.0011360920034348965\n",
      "training 0.0005347340484149754 relative L2 0.0011360583594068885\n",
      "training 0.0005346941761672497 relative L2 0.0011359944473952055\n",
      "training 0.0005346365505829453 relative L2 0.0011359569616615772\n",
      "training 0.0005346064572222531 relative L2 0.0011359016643837094\n",
      "training 0.000534547958523035 relative L2 0.0011358654592186213\n",
      "training 0.0005345114041119814 relative L2 0.0011358088813722134\n",
      "training 0.0005344637902453542 relative L2 0.0011357604525983334\n",
      "training 0.0005344167002476752 relative L2 0.00113572645932436\n",
      "training 0.0005343788652680814 relative L2 0.001135666505433619\n",
      "training 0.0005343252560123801 relative L2 0.001135624828748405\n",
      "training 0.000534288992639631 relative L2 0.0011355761671438813\n",
      "training 0.0005342381773516536 relative L2 0.0011355330934748054\n",
      "training 0.0005341964424587786 relative L2 0.0011354817543178797\n",
      "training 0.0005341516807675362 relative L2 0.0011354340240359306\n",
      "training 0.0005341054638847709 relative L2 0.0011353958398103714\n",
      "training 0.0005340653005987406 relative L2 0.0011353413574397564\n",
      "training 0.0005340161151252687 relative L2 0.0011352966539561749\n",
      "training 0.0005339760100468993 relative L2 0.0011352506699040532\n",
      "training 0.0005339289200492203 relative L2 0.0011352055007591844\n",
      "training 0.0005338854971341789 relative L2 0.0011351561406627297\n",
      "training 0.000533841666765511 relative L2 0.0011351094581186771\n",
      "training 0.0005337964394129813 relative L2 0.001135067781433463\n",
      "training 0.0005337544716894627 relative L2 0.0011350172571837902\n",
      "training 0.0005337080801837146 relative L2 0.0011349712731316686\n",
      "training 0.0005336658796295524 relative L2 0.0011349257547408342\n",
      "training 0.000533620419446379 relative L2 0.0011348804691806436\n",
      "training 0.0005335770547389984 relative L2 0.0011348321568220854\n",
      "training 0.0005335327587090433 relative L2 0.0011347857071086764\n",
      "training 0.0005334888119250536 relative L2 0.0011347420513629913\n",
      "training 0.0005334454472176731 relative L2 0.0011346940882503986\n",
      "training 0.0005334005109034479 relative L2 0.0011346472892910242\n",
      "training 0.0005333576118573546 relative L2 0.0011346013052389026\n",
      "training 0.0005333127919584513 relative L2 0.0011345570674166083\n",
      "training 0.0005332697182893753 relative L2 0.0011345087550580502\n",
      "training 0.0005332251312211156 relative L2 0.0011344626545906067\n",
      "training 0.0005331818829290569 relative L2 0.0011344177182763815\n",
      "training 0.0005331375868991017 relative L2 0.001134371617808938\n",
      "training 0.000533093698322773 relative L2 0.0011343241203576326\n",
      "training 0.0005330500425770879 relative L2 0.0011342782527208328\n",
      "training 0.0005330060957930982 relative L2 0.0011342336656525731\n",
      "training 0.0005329625564627349 relative L2 0.0011341864010319114\n",
      "training 0.0005329184350557625 relative L2 0.0011341398349031806\n",
      "training 0.0005328748957253993 relative L2 0.0011340943165123463\n",
      "training 0.0005328308325260878 relative L2 0.0011340485652908683\n",
      "training 0.0005327871185727417 relative L2 0.0011340015335008502\n",
      "training 0.0005327433464117348 relative L2 0.0011339555494487286\n",
      "training 0.0005326995742507279 relative L2 0.0011339103803038597\n",
      "training 0.000532655802089721 relative L2 0.001133863814175129\n",
      "training 0.0005326118553057313 relative L2 0.0011338172480463982\n",
      "training 0.0005325681995600462 relative L2 0.0011337714968249202\n",
      "training 0.0005325244273990393 relative L2 0.0011337253963574767\n",
      "training 0.0005324806552380323 relative L2 0.0011336789466440678\n",
      "training 0.0005324368248693645 relative L2 0.001133633079007268\n",
      "training 0.0005323931109160185 relative L2 0.0011335875606164336\n",
      "training 0.0005323495133779943 relative L2 0.0011335411109030247\n",
      "training 0.0005323057412169874 relative L2 0.0011334948940202594\n",
      "training 0.0005322622018866241 relative L2 0.0011334492592141032\n",
      "training 0.000532218546140939 relative L2 0.0011334032751619816\n",
      "training 0.000532174832187593 relative L2 0.0011333569418638945\n",
      "training 0.0005321311764419079 relative L2 0.0011333109578117728\n",
      "training 0.0005320875206962228 relative L2 0.0011332652065902948\n",
      "training 0.0005320438067428768 relative L2 0.0011332191061228514\n",
      "training 0.0005320000345818698 relative L2 0.001133173005655408\n",
      "training 0.0005319564952515066 relative L2 0.00113312725443393\n",
      "training 0.0005319127230904996 relative L2 0.00113308138679713\n",
      "training 0.0005318691255524755 relative L2 0.0011330351699143648\n",
      "training 0.0005318255280144513 relative L2 0.0011329891858622432\n",
      "training 0.0005317819304764271 relative L2 0.0011329434346407652\n",
      "training 0.0005317384493537247 relative L2 0.0011328973341733217\n",
      "training 0.0005316946771927178 relative L2 0.0011328513501212\n",
      "training 0.0005316509632393718 relative L2 0.0011328054824844003\n",
      "training 0.0005316074239090085 relative L2 0.0011327593820169568\n",
      "training 0.0005315637681633234 relative L2 0.0011327133979648352\n",
      "training 0.0005315201706252992 relative L2 0.0011326676467433572\n",
      "training 0.0005314763984642923 relative L2 0.0011326214298605919\n",
      "training 0.000531432859133929 relative L2 0.0011325752129778266\n",
      "training 0.000531389145180583 relative L2 0.0011325294617563486\n",
      "training 0.0005313454894348979 relative L2 0.0011324833612889051\n",
      "training 0.0005313018918968737 relative L2 0.0011324371444061399\n",
      "training 0.0005312580615282059 relative L2 0.0011323909275233746\n",
      "training 0.0005312144057825208 relative L2 0.0011323450598865747\n",
      "training 0.0005311706918291748 relative L2 0.0011322989594191313\n",
      "training 0.0005311270360834897 relative L2 0.0011322532081976533\n",
      "training 0.0005310832639224827 relative L2 0.001132206991314888\n",
      "training 0.0005310396663844585 relative L2 0.0011321607744321227\n",
      "training 0.0005309959524311125 relative L2 0.0011321146739646792\n",
      "training 0.0005309522966854274 relative L2 0.001132069039158523\n",
      "training 0.0005309086409397423 relative L2 0.0011320225894451141\n",
      "training 0.0005308648687787354 relative L2 0.0011319766053929925\n",
      "training 0.0005308210966177285 relative L2 0.0011319302720949054\n",
      "training 0.0005307773244567215 relative L2 0.00113188405521214\n",
      "training 0.0005307336687110364 relative L2 0.0011318379547446966\n",
      "training 0.0005306897801347077 relative L2 0.0011317917378619313\n",
      "training 0.0005306460661813617 relative L2 0.001131745520979166\n",
      "training 0.0005306021776050329 relative L2 0.0011316993040964007\n",
      "training 0.0005305583472363651 relative L2 0.0011316530872136354\n",
      "training 0.0005305145750753582 relative L2 0.0011316071031615138\n",
      "training 0.0005304706282913685 relative L2 0.0011315606534481049\n",
      "training 0.0005304270307533443 relative L2 0.0011315144365653396\n",
      "training 0.0005303830839693546 relative L2 0.001131468336097896\n",
      "training 0.0005303393700160086 relative L2 0.0011314218863844872\n",
      "training 0.000530295423232019 relative L2 0.0011313757859170437\n",
      "training 0.0005302517092786729 relative L2 0.0011313295690342784\n",
      "training 0.000530207937117666 relative L2 0.0011312832357361913\n",
      "training 0.0005301641067489982 relative L2 0.0011312369024381042\n",
      "training 0.0005301202763803303 relative L2 0.0011311908019706607\n",
      "training 0.0005300763878040016 relative L2 0.0011311444686725736\n",
      "training 0.0005300326738506556 relative L2 0.0011310982517898083\n",
      "training 0.0005299887852743268 relative L2 0.0011310519184917212\n",
      "training 0.0005299448384903371 relative L2 0.001131005585193634\n",
      "training 0.0005299011245369911 relative L2 0.001130959251895547\n",
      "training 0.0005298571777530015 relative L2 0.001130912802182138\n",
      "training 0.0005298132309690118 relative L2 0.0011308665852993727\n",
      "training 0.000529769342392683 relative L2 0.0011308201355859637\n",
      "training 0.0005297255702316761 relative L2 0.0011307738022878766\n",
      "training 0.0005296816234476864 relative L2 0.0011307274689897895\n",
      "training 0.0005296376766636968 relative L2 0.0011306812521070242\n",
      "training 0.000529593788087368 relative L2 0.0011306348023936152\n",
      "training 0.0005295498413033783 relative L2 0.0011305887019261718\n",
      "training 0.0005295060109347105 relative L2 0.0011305422522127628\n",
      "training 0.0005294620641507208 relative L2 0.0011304960353299975\n",
      "training 0.000529418233782053 relative L2 0.0011304494692012668\n",
      "training 0.0005293742287904024 relative L2 0.0011304030194878578\n",
      "training 0.0005293303402140737 relative L2 0.0011303565697744489\n",
      "training 0.000529286393430084 relative L2 0.00113031012006104\n",
      "training 0.0005292423302307725 relative L2 0.0011302639031782746\n",
      "training 0.0005291984998621047 relative L2 0.0011302173370495439\n",
      "training 0.000529154553078115 relative L2 0.0011301706545054913\n",
      "training 0.0005291104898788035 relative L2 0.0011301242047920823\n",
      "training 0.000529066426679492 relative L2 0.001130077987909317\n",
      "training 0.0005290225381031632 relative L2 0.00113003165461123\n",
      "training 0.0005289785913191736 relative L2 0.0011299849720671773\n",
      "training 0.0005289344699122012 relative L2 0.0011299385223537683\n",
      "training 0.0005288905231282115 relative L2 0.0011298919562250376\n",
      "training 0.0005288466345518827 relative L2 0.0011298453900963068\n",
      "training 0.0005288025131449103 relative L2 0.0011297987075522542\n",
      "training 0.0005287585081532598 relative L2 0.0011297522578388453\n",
      "training 0.0005287145031616092 relative L2 0.0011297055752947927\n",
      "training 0.0005286703235469759 relative L2 0.00112965889275074\n",
      "training 0.0005286263767629862 relative L2 0.001129612559452653\n",
      "training 0.0005285823135636747 relative L2 0.0011295658769086003\n",
      "training 0.0005285381921567023 relative L2 0.0011295193107798696\n",
      "training 0.0005284941871650517 relative L2 0.001129472628235817\n",
      "training 0.0005284501239657402 relative L2 0.001129426178522408\n",
      "training 0.0005284060607664287 relative L2 0.0011293792631477118\n",
      "training 0.0005283619393594563 relative L2 0.0011293329298496246\n",
      "training 0.0005283179343678057 relative L2 0.001129286247305572\n",
      "training 0.0005282738129608333 relative L2 0.0011292396811768413\n",
      "training 0.0005282297497615218 relative L2 0.0011291931150481105\n",
      "training 0.0005281856283545494 relative L2 0.0011291461996734142\n",
      "training 0.0005281415651552379 relative L2 0.0011290995171293616\n",
      "training 0.0005280975019559264 relative L2 0.0011290529510006309\n",
      "training 0.000528053380548954 relative L2 0.0011290062684565783\n",
      "training 0.0005280092591419816 relative L2 0.0011289595859125257\n",
      "training 0.0005279650213196874 relative L2 0.001128912903368473\n",
      "training 0.0005279209581203759 relative L2 0.0011288661044090986\n",
      "training 0.0005278767785057425 relative L2 0.001128819421865046\n",
      "training 0.0005278326570987701 relative L2 0.0011287727393209934\n",
      "training 0.0005277884774841368 relative L2 0.0011287258239462972\n",
      "training 0.0005277444142848253 relative L2 0.0011286790249869227\n",
      "training 0.0005277001764625311 relative L2 0.0011286323424428701\n",
      "training 0.0005276559968478978 relative L2 0.0011285854270681739\n",
      "training 0.0005276117590256035 relative L2 0.0011285386281087995\n",
      "training 0.0005275675794109702 relative L2 0.001128491829149425\n",
      "training 0.0005275235744193196 relative L2 0.0011284451466053724\n",
      "training 0.0005274793365970254 relative L2 0.001128398347645998\n",
      "training 0.0005274350405670702 relative L2 0.0011283515486866236\n",
      "training 0.0005273909773677588 relative L2 0.0011283045168966055\n",
      "training 0.0005273467977531254 relative L2 0.0011282578343525529\n",
      "training 0.0005273026181384921 relative L2 0.0011282109189778566\n",
      "training 0.000527258322108537 relative L2 0.0011281641200184822\n",
      "training 0.0005272142007015646 relative L2 0.0011281173210591078\n",
      "training 0.0005271699046716094 relative L2 0.0011280702892690897\n",
      "training 0.0005271256668493152 relative L2 0.0011280233738943934\n",
      "training 0.0005270814872346818 relative L2 0.001127976574935019\n",
      "training 0.0005270371912047267 relative L2 0.0011279296595603228\n",
      "training 0.0005269930115900934 relative L2 0.0011278826277703047\n",
      "training 0.0005269487737677991 relative L2 0.0011278355959802866\n",
      "training 0.000526904477737844 relative L2 0.0011277886806055903\n",
      "training 0.0005268602399155498 relative L2 0.0011277416488155723\n",
      "training 0.0005268158856779337 relative L2 0.0011276948498561978\n",
      "training 0.0005267715896479785 relative L2 0.0011276480508968234\n",
      "training 0.0005267272936180234 relative L2 0.0011276009026914835\n",
      "training 0.0005266830557957292 relative L2 0.001127554103732109\n",
      "training 0.000526638759765774 relative L2 0.001127507071942091\n",
      "training 0.000526594405528158 relative L2 0.001127460040152073\n",
      "training 0.0005265501094982028 relative L2 0.0011274131247773767\n",
      "training 0.0005265057552605867 relative L2 0.0011273660929873586\n",
      "training 0.0005264612846076488 relative L2 0.0011273190611973405\n",
      "training 0.0005264171049930155 relative L2 0.0011272720294073224\n",
      "training 0.0005263728089630604 relative L2 0.0011272247647866607\n",
      "training 0.0005263282801024616 relative L2 0.0011271776165813208\n",
      "training 0.0005262839258648455 relative L2 0.0011271307012066245\n",
      "training 0.0005262397462502122 relative L2 0.0011270834365859628\n",
      "training 0.0005261951009742916 relative L2 0.0011270364047959447\n",
      "training 0.0005261508049443364 relative L2 0.0011269892565906048\n",
      "training 0.0005261064507067204 relative L2 0.0011269423412159085\n",
      "training 0.0005260620964691043 relative L2 0.0011268951930105686\n",
      "training 0.0005260175676085055 relative L2 0.0011268481612205505\n",
      "training 0.0005259732715785503 relative L2 0.0011268010130152106\n",
      "training 0.0005259289173409343 relative L2 0.001126753748394549\n",
      "training 0.0005258844466879964 relative L2 0.001126706600189209\n",
      "training 0.0005258399178273976 relative L2 0.0011266593355685472\n",
      "training 0.0005257955635897815 relative L2 0.0011266121873632073\n",
      "training 0.0005257510929368436 relative L2 0.0011265650391578674\n",
      "training 0.0005257066804915667 relative L2 0.0011265176581218839\n",
      "training 0.0005256621516309679 relative L2 0.0011264703935012221\n",
      "training 0.0005256175645627081 relative L2 0.0011264231288805604\n",
      "training 0.0005255730939097703 relative L2 0.0011263757478445768\n",
      "training 0.0005255286232568324 relative L2 0.0011263282503932714\n",
      "training 0.0005254840361885726 relative L2 0.0011262809857726097\n",
      "training 0.0005254394491203129 relative L2 0.001126233721151948\n",
      "training 0.000525394978467375 relative L2 0.001126186572946608\n",
      "training 0.0005253503913991153 relative L2 0.0011261390754953027\n",
      "training 0.0005253058625385165 relative L2 0.0011260916944593191\n",
      "training 0.0005252612754702568 relative L2 0.0011260443134233356\n",
      "training 0.0005252166884019971 relative L2 0.001125996932387352\n",
      "training 0.0005251721013337374 relative L2 0.0011259495513513684\n",
      "training 0.0005251274560578167 relative L2 0.0011259024031460285\n",
      "training 0.000525082868989557 relative L2 0.0011258547892794013\n",
      "training 0.0005250381655059755 relative L2 0.0011258074082434177\n",
      "training 0.0005249935784377158 relative L2 0.0011257599107921124\n",
      "training 0.000524948991369456 relative L2 0.001125712413340807\n",
      "training 0.0005249041714705527 relative L2 0.0011256649158895016\n",
      "training 0.000524859526194632 relative L2 0.001125617534853518\n",
      "training 0.0005248150555416942 relative L2 0.0011255699209868908\n",
      "training 0.0005247702938504517 relative L2 0.001125522656366229\n",
      "training 0.0005247256485745311 relative L2 0.0011254751589149237\n",
      "training 0.0005246810032986104 relative L2 0.0011254276614636183\n",
      "training 0.0005246362998150289 relative L2 0.0011253802804276347\n",
      "training 0.0005245916545391083 relative L2 0.0011253328993916512\n",
      "training 0.0005245469510555267 relative L2 0.0011252854019403458\n",
      "training 0.0005245022475719452 relative L2 0.0011252379044890404\n",
      "training 0.0005244575440883636 relative L2 0.0011251902906224132\n",
      "training 0.0005244127823971212 relative L2 0.0011251429095864296\n",
      "training 0.0005243680207058787 relative L2 0.0011250952957198024\n",
      "training 0.0005243233172222972 relative L2 0.0011250480310991406\n",
      "training 0.0005242785555310547 relative L2 0.0011250004172325134\n",
      "training 0.0005242337938398123 relative L2 0.0011249528033658862\n",
      "training 0.0005241888575255871 relative L2 0.0011249049566686153\n",
      "training 0.0005241442122496665 relative L2 0.00112485745921731\n",
      "training 0.0005240993341431022 relative L2 0.0011248099617660046\n",
      "training 0.0005240545724518597 relative L2 0.0011247619986534119\n",
      "training 0.0005240096943452954 relative L2 0.0011247143847867846\n",
      "training 0.0005239648162387311 relative L2 0.0011246666545048356\n",
      "training 0.0005239199381321669 relative L2 0.0011246191570535302\n",
      "training 0.0005238751182332635 relative L2 0.0011245714267715812\n",
      "training 0.0005238302401266992 relative L2 0.0011245236964896321\n",
      "training 0.0005237853620201349 relative L2 0.0011244758497923613\n",
      "training 0.0005237406003288925 relative L2 0.0011244283523410559\n",
      "training 0.00052369583863765 relative L2 0.0011243802728131413\n",
      "training 0.0005236508441157639 relative L2 0.001124332775361836\n",
      "training 0.0005236060824245214 relative L2 0.001124285045079887\n",
      "training 0.0005235612625256181 relative L2 0.0011242370819672942\n",
      "training 0.0005235160933807492 relative L2 0.0011241893516853452\n",
      "training 0.0005234712734818459 relative L2 0.0011241418542340398\n",
      "training 0.0005234263953752816 relative L2 0.0011240937747061253\n",
      "training 0.0005233814590610564 relative L2 0.0011240463936701417\n",
      "training 0.0005233365227468312 relative L2 0.0011239985469728708\n",
      "training 0.0005232917028479278 relative L2 0.0011239507002756\n",
      "training 0.0005232467665337026 relative L2 0.0011239030864089727\n",
      "training 0.0005232018302194774 relative L2 0.0011238553561270237\n",
      "training 0.000523157010320574 relative L2 0.0011238076258450747\n",
      "training 0.0005231120740063488 relative L2 0.0011237598955631256\n",
      "training 0.0005230671376921237 relative L2 0.0011237120488658547\n",
      "training 0.0005230220849625766 relative L2 0.0011236642021685839\n",
      "training 0.0005229771486483514 relative L2 0.001123616355471313\n",
      "training 0.0005229322123341262 relative L2 0.0011235685087740421\n",
      "training 0.0005228872178122401 relative L2 0.001123520778492093\n",
      "training 0.000522842223290354 relative L2 0.0011234725825488567\n",
      "training 0.0005227971123531461 relative L2 0.0011234247358515859\n",
      "training 0.00052275211783126 relative L2 0.001123376889154315\n",
      "training 0.0005227070651017129 relative L2 0.0011233288096264005\n",
      "training 0.0005226620123721659 relative L2 0.0011232810793444514\n",
      "training 0.0005226171342656016 relative L2 0.001123232883401215\n",
      "training 0.000522571848705411 relative L2 0.0011231850367039442\n",
      "training 0.0005225268541835248 relative L2 0.0011231369571760297\n",
      "training 0.0005224818014539778 relative L2 0.001123088994063437\n",
      "training 0.0005224367487244308 relative L2 0.0011230409145355225\n",
      "training 0.000522391579579562 relative L2 0.0011229930678382516\n",
      "training 0.0005223465268500149 relative L2 0.001122945104725659\n",
      "training 0.000522301415912807 relative L2 0.0011228971416130662\n",
      "training 0.000522256304975599 relative L2 0.0011228489456698298\n",
      "training 0.0005222111940383911 relative L2 0.0011228009825572371\n",
      "training 0.0005221661413088441 relative L2 0.0011227529030293226\n",
      "training 0.0005221208557486534 relative L2 0.001122704823501408\n",
      "training 0.0005220758030191064 relative L2 0.0011226568603888154\n",
      "training 0.0005220306920818985 relative L2 0.001122608664445579\n",
      "training 0.0005219854647293687 relative L2 0.0011225605849176645\n",
      "training 0.0005219403537921607 relative L2 0.00112251250538975\n",
      "training 0.0005218951846472919 relative L2 0.0011224644258618355\n",
      "training 0.0005218499572947621 relative L2 0.001122416346333921\n",
      "training 0.0005218048463575542 relative L2 0.0011223681503906846\n",
      "training 0.0005217597354203463 relative L2 0.0011223199544474483\n",
      "training 0.0005217145662754774 relative L2 0.0011222718749195337\n",
      "training 0.0005216693971306086 relative L2 0.0011222237953916192\n",
      "training 0.0005216242279857397 relative L2 0.001122175483033061\n",
      "training 0.000521578942425549 relative L2 0.0011221272870898247\n",
      "training 0.0005215337732806802 relative L2 0.0011220792075619102\n",
      "training 0.0005214886041358113 relative L2 0.001122030895203352\n",
      "training 0.0005214433767832816 relative L2 0.0011219826992601156\n",
      "training 0.0005213979748077691 relative L2 0.0011219343869015574\n",
      "training 0.0005213527474552393 relative L2 0.0011218859581276774\n",
      "training 0.0005213074618950486 relative L2 0.001121837762184441\n",
      "training 0.0005212621763348579 relative L2 0.001121789333410561\n",
      "training 0.0005212168325670063 relative L2 0.0011217411374673247\n",
      "training 0.0005211715470068157 relative L2 0.001121692592278123\n",
      "training 0.000521126261446625 relative L2 0.0011216443963348866\n",
      "training 0.0005210809758864343 relative L2 0.0011215959675610065\n",
      "training 0.0005210354574956 relative L2 0.001121547888033092\n",
      "training 0.0005209902883507311 relative L2 0.0011214996920898557\n",
      "training 0.0005209448863752186 relative L2 0.001121451030485332\n",
      "training 0.0005208994261920452 relative L2 0.0011214027181267738\n",
      "training 0.0005208540824241936 relative L2 0.0011213545221835375\n",
      "training 0.0005208089132793248 relative L2 0.0011213059769943357\n",
      "training 0.0005207633948884904 relative L2 0.0011212576646357775\n",
      "training 0.0005207181093282998 relative L2 0.0011212094686925411\n",
      "training 0.0005206728237681091 relative L2 0.0011211608070880175\n",
      "training 0.0005206272471696138 relative L2 0.0011211123783141375\n",
      "training 0.0005205819034017622 relative L2 0.0011210640659555793\n",
      "training 0.0005205365596339107 relative L2 0.0011210155207663774\n",
      "training 0.0005204909248277545 relative L2 0.0011209669755771756\n",
      "training 0.0005204456392675638 relative L2 0.0011209184303879738\n",
      "training 0.0005204000626690686 relative L2 0.0011208701180294156\n",
      "training 0.0005203547771088779 relative L2 0.001120821456424892\n",
      "training 0.0005203093169257045 relative L2 0.0011207729112356901\n",
      "training 0.0005202637985348701 relative L2 0.0011207241332158446\n",
      "training 0.0005202183383516967 relative L2 0.0011206757044419646\n",
      "training 0.0005201728199608624 relative L2 0.0011206272756680846\n",
      "training 0.0005201272433623672 relative L2 0.0011205788468942046\n",
      "training 0.000520081608556211 relative L2 0.0011205300688743591\n",
      "training 0.0005200360901653767 relative L2 0.0011204816401004791\n",
      "training 0.0005199905135668814 relative L2 0.0011204330949112773\n",
      "training 0.0005199449369683862 relative L2 0.0011203843168914318\n",
      "training 0.0005198993603698909 relative L2 0.0011203356552869081\n",
      "training 0.0005198538419790566 relative L2 0.0011202871100977063\n",
      "training 0.0005198082653805614 relative L2 0.0011202383320778608\n",
      "training 0.0005197626305744052 relative L2 0.0011201896704733372\n",
      "training 0.00051971705397591 relative L2 0.0011201410088688135\n",
      "training 0.0005196713609620929 relative L2 0.001120092230848968\n",
      "training 0.0005196257843635976 relative L2 0.0011200435692444444\n",
      "training 0.0005195801495574415 relative L2 0.0011199949076399207\n",
      "training 0.0005195344565436244 relative L2 0.0011199461296200752\n",
      "training 0.0005194887635298073 relative L2 0.0011198973516002297\n",
      "training 0.0005194431287236512 relative L2 0.001119848689995706\n",
      "training 0.000519397493917495 relative L2 0.0011198000283911824\n",
      "training 0.000519351742696017 relative L2 0.0011197511339560151\n",
      "training 0.0005193061078898609 relative L2 0.0011197024723514915\n",
      "training 0.0005192604148760438 relative L2 0.0011196539271622896\n",
      "training 0.0005192147218622267 relative L2 0.0011196049163118005\n",
      "training 0.0005191690288484097 relative L2 0.001119556138291955\n",
      "training 0.0005191232194192708 relative L2 0.0011195071274414659\n",
      "training 0.0005190775264054537 relative L2 0.0011194583494216204\n",
      "training 0.0005190317169763148 relative L2 0.0011194095714017749\n",
      "training 0.0005189859657548368 relative L2 0.0011193606769666076\n",
      "training 0.0005189402145333588 relative L2 0.0011193117825314403\n",
      "training 0.0005188945215195417 relative L2 0.0011192627716809511\n",
      "training 0.0005188487702980638 relative L2 0.0011192138772457838\n",
      "training 0.0005188029608689249 relative L2 0.0011191650992259383\n",
      "training 0.000518757151439786 relative L2 0.001119116204790771\n",
      "training 0.000518711400218308 relative L2 0.00111906707752496\n",
      "training 0.0005186655907891691 relative L2 0.0011190182995051146\n",
      "training 0.0005186197813600302 relative L2 0.0011189692886546254\n",
      "training 0.0005185738555155694 relative L2 0.0011189202778041363\n",
      "training 0.0005185281042940915 relative L2 0.0011188712669536471\n",
      "training 0.0005184821784496307 relative L2 0.001118822256103158\n",
      "training 0.0005184363690204918 relative L2 0.0011187732452526689\n",
      "training 0.0005183906177990139 relative L2 0.0011187242344021797\n",
      "training 0.0005183444591239095 relative L2 0.001118674990721047\n",
      "training 0.0005182986496947706 relative L2 0.0011186262127012014\n",
      "training 0.0005182528402656317 relative L2 0.0011185769690200686\n",
      "training 0.0005182066815905273 relative L2 0.0011185277253389359\n",
      "training 0.0005181607557460666 relative L2 0.0011184787144884467\n",
      "training 0.0005181149463169277 relative L2 0.0011184295872226357\n",
      "training 0.0005180689040571451 relative L2 0.0011183805763721466\n",
      "training 0.0005180229782126844 relative L2 0.0011183316819369793\n",
      "training 0.0005179771105758846 relative L2 0.0011182823218405247\n",
      "training 0.0005179309519007802 relative L2 0.0011182333109900355\n",
      "training 0.0005178850842639804 relative L2 0.0011181841837242246\n",
      "training 0.0005178390420041978 relative L2 0.0011181349400430918\n",
      "training 0.0005177929415367544 relative L2 0.0011180858127772808\n",
      "training 0.0005177470156922936 relative L2 0.0011180364526808262\n",
      "training 0.0005177009734325111 relative L2 0.0011179873254150152\n",
      "training 0.0005176548147574067 relative L2 0.0011179379653185606\n",
      "training 0.0005176087142899632 relative L2 0.001117888605222106\n",
      "training 0.0005175626720301807 relative L2 0.0011178393615409732\n",
      "training 0.0005175166297703981 relative L2 0.0011177900014445186\n",
      "training 0.0005174704710952938 relative L2 0.0011177407577633858\n",
      "training 0.0005174244288355112 relative L2 0.0011176912812516093\n",
      "training 0.0005173784447833896 relative L2 0.0011176421539857984\n",
      "training 0.0005173322861082852 relative L2 0.0011175927938893437\n",
      "training 0.0005172861856408417 relative L2 0.001117543550208211\n",
      "training 0.0005172401433810592 relative L2 0.0011174941901117563\n",
      "training 0.000517193810082972 relative L2 0.001117444597184658\n",
      "training 0.0005171477096155286 relative L2 0.001117395469918847\n",
      "training 0.0005171015509404242 relative L2 0.0011173462262377143\n",
      "training 0.0005170554504729807 relative L2 0.001117296633310616\n",
      "training 0.0005170091171748936 relative L2 0.0011172471567988396\n",
      "training 0.0005169630167074502 relative L2 0.001117197796702385\n",
      "training 0.0005169168580323458 relative L2 0.0011171483201906085\n",
      "training 0.0005168705247342587 relative L2 0.0011170989600941539\n",
      "training 0.0005168241914361715 relative L2 0.0011170492507517338\n",
      "training 0.000516778149176389 relative L2 0.001117000007070601\n",
      "training 0.0005167318740859628 relative L2 0.0011169504141435027\n",
      "training 0.0005166855989955366 relative L2 0.0011169008212164044\n",
      "training 0.0005166393821127713 relative L2 0.001116851344704628\n",
      "training 0.0005165931070223451 relative L2 0.0011168018681928515\n",
      "training 0.0005165468319319189 relative L2 0.001116752391681075\n",
      "training 0.0005165004986338317 relative L2 0.0011167027987539768\n",
      "training 0.0005164541653357446 relative L2 0.0011166533222422004\n",
      "training 0.0005164078902453184 relative L2 0.0011166039621457458\n",
      "training 0.0005163615569472313 relative L2 0.0011165543692186475\n",
      "training 0.0005163152818568051 relative L2 0.001116504892706871\n",
      "training 0.0005162690067663789 relative L2 0.0011164552997797728\n",
      "training 0.0005162226734682918 relative L2 0.0011164058232679963\n",
      "training 0.0005161762819625437 relative L2 0.0011163561139255762\n",
      "training 0.0005161299486644566 relative L2 0.0011163066374137998\n",
      "training 0.0005160836735740304 relative L2 0.0011162570444867015\n",
      "training 0.0005160371656529605 relative L2 0.0011162073351442814\n",
      "training 0.0005159909487701952 relative L2 0.0011161575093865395\n",
      "training 0.0005159444408491254 relative L2 0.001116108032874763\n",
      "training 0.0005158982239663601 relative L2 0.001116058207117021\n",
      "training 0.0005158517742529511 relative L2 0.001116008497774601\n",
      "training 0.0005158053245395422 relative L2 0.0011159590212628245\n",
      "training 0.000515759049449116 relative L2 0.0011159093119204044\n",
      "training 0.0005157124833203852 relative L2 0.0011158597189933062\n",
      "training 0.0005156660918146372 relative L2 0.0011158101260662079\n",
      "training 0.0005156196421012282 relative L2 0.0011157601838931441\n",
      "training 0.0005155731341801584 relative L2 0.0011157105909660459\n",
      "training 0.0005155266844667494 relative L2 0.0011156608816236258\n",
      "training 0.0005154803511686623 relative L2 0.001115610939450562\n",
      "training 0.0005154337850399315 relative L2 0.0011155609972774982\n",
      "training 0.0005153872189112008 relative L2 0.0011155112879350781\n",
      "training 0.0005153407691977918 relative L2 0.0011154612293466926\n",
      "training 0.0005152940866537392 relative L2 0.0011154116364195943\n",
      "training 0.0005152476369403303 relative L2 0.0011153614614158869\n",
      "training 0.0005152009543962777 relative L2 0.001115311635658145\n",
      "training 0.0005151544464752078 relative L2 0.001115261809900403\n",
      "training 0.0005151079385541379 relative L2 0.001115211984142661\n",
      "training 0.0005150613142177463 relative L2 0.0011151618091389537\n",
      "training 0.0005150148062966764 relative L2 0.0011151120997965336\n",
      "training 0.0005149681237526238 relative L2 0.0011150619247928262\n",
      "training 0.0005149214994162321 relative L2 0.0011150119826197624\n",
      "training 0.0005148749332875013 relative L2 0.0011149620404466987\n",
      "training 0.0005148283089511096 relative L2 0.001114912098273635\n",
      "training 0.0005147815099917352 relative L2 0.0011148623889312148\n",
      "training 0.0005147347692400217 relative L2 0.0011148122139275074\n",
      "training 0.0005146882613189518 relative L2 0.0011147621553391218\n",
      "training 0.0005146415205672383 relative L2 0.0011147120967507362\n",
      "training 0.0005145947798155248 relative L2 0.0011146620381623507\n",
      "training 0.0005145479226484895 relative L2 0.001114611979573965\n",
      "training 0.0005145013565197587 relative L2 0.0011145619209855795\n",
      "training 0.0005144546157680452 relative L2 0.0011145116295665503\n",
      "training 0.0005144079332239926 relative L2 0.0011144618038088083\n",
      "training 0.0005143611924722791 relative L2 0.0011144117452204227\n",
      "training 0.0005143143935129046 relative L2 0.0011143613373860717\n",
      "training 0.0005142676527611911 relative L2 0.001114311278797686\n",
      "training 0.0005142209702171385 relative L2 0.0011142612202093005\n",
      "training 0.0005141741712577641 relative L2 0.0011142109287902713\n",
      "training 0.0005141272558830678 relative L2 0.0011141607537865639\n",
      "training 0.0005140805733390152 relative L2 0.0011141103459522128\n",
      "training 0.0005140337743796408 relative L2 0.0011140602873638272\n",
      "training 0.0005139868590049446 relative L2 0.0011140102287754416\n",
      "training 0.000513940118253231 relative L2 0.0011139599373564124\n",
      "training 0.0005138932028785348 relative L2 0.0011139096459373832\n",
      "training 0.0005138462292961776 relative L2 0.001113859354518354\n",
      "training 0.0005137994303368032 relative L2 0.0011138089466840029\n",
      "training 0.0005137525149621069 relative L2 0.0011137587716802955\n",
      "training 0.0005137054831720889 relative L2 0.0011137087130919099\n",
      "training 0.0005136586260050535 relative L2 0.001113658188842237\n",
      "training 0.0005136117688380182 relative L2 0.0011136080138385296\n",
      "training 0.0005135648534633219 relative L2 0.0011135576060041785\n",
      "training 0.0005135179380886257 relative L2 0.0011135073145851493\n",
      "training 0.0005134709645062685 relative L2 0.0011134567903354764\n",
      "training 0.0005134240491315722 relative L2 0.0011134064989164472\n",
      "training 0.0005133770755492151 relative L2 0.0011133559746667743\n",
      "training 0.0005133299855515361 relative L2 0.001113305683247745\n",
      "training 0.0005132830701768398 relative L2 0.0011132550425827503\n",
      "training 0.0005132361548021436 relative L2 0.0011132046347483993\n",
      "training 0.0005131890648044646 relative L2 0.00111315434332937\n",
      "training 0.0005131421494297683 relative L2 0.0011131038190796971\n",
      "training 0.0005130950012244284 relative L2 0.0011130532948300242\n",
      "training 0.0005130480276420712 relative L2 0.0011130033526569605\n",
      "training 0.000513001112267375 relative L2 0.001112952595576644\n",
      "training 0.0005129539640620351 relative L2 0.0011129019549116492\n",
      "training 0.0005129068740643561 relative L2 0.00111285166349262\n",
      "training 0.0005128599586896598 relative L2 0.0011128010228276253\n",
      "training 0.0005128126940689981 relative L2 0.0011127504985779524\n",
      "training 0.00051276566227898 relative L2 0.0011126997414976358\n",
      "training 0.0005127183976583183 relative L2 0.0011126493336632848\n",
      "training 0.0005126712494529784 relative L2 0.0011125988094136119\n",
      "training 0.0005126240430399776 relative L2 0.0011125480523332953\n",
      "training 0.0005125769530422986 relative L2 0.0011124975280836225\n",
      "training 0.0005125298048369586 relative L2 0.0011124468874186277\n",
      "training 0.0005124825984239578 relative L2 0.001112396246753633\n",
      "training 0.0005124354502186179 relative L2 0.0011123454896733165\n",
      "training 0.0005123882438056171 relative L2 0.001112294732593\n",
      "training 0.0005123409791849554 relative L2 0.0011122440919280052\n",
      "training 0.0005122937145642936 relative L2 0.0011121934512630105\n",
      "training 0.0005122464499436319 relative L2 0.0011121425777673721\n",
      "training 0.0005121991271153092 relative L2 0.0011120919371023774\n",
      "training 0.0005121519207023084 relative L2 0.0011120411800220609\n",
      "training 0.0005121047142893076 relative L2 0.001111990655772388\n",
      "training 0.0005120573914609849 relative L2 0.0011119397822767496\n",
      "training 0.000512010243255645 relative L2 0.001111889025196433\n",
      "training 0.0005119628040120006 relative L2 0.0011118380352854729\n",
      "training 0.0005119154811836779 relative L2 0.0011117871617898345\n",
      "training 0.0005118680419400334 relative L2 0.001111736404709518\n",
      "training 0.0005118208355270326 relative L2 0.0011116854147985578\n",
      "training 0.0005117733380757272 relative L2 0.0011116343084722757\n",
      "training 0.0005117259570397437 relative L2 0.0011115832021459937\n",
      "training 0.0005116785760037601 relative L2 0.0011115323286503553\n",
      "training 0.0005116312531754375 relative L2 0.0011114815715700388\n",
      "training 0.0005115838721394539 relative L2 0.0011114304652437568\n",
      "training 0.0005115363164804876 relative L2 0.0011113797081634402\n",
      "training 0.0005114890518598258 relative L2 0.0011113284854218364\n",
      "training 0.0005114416708238423 relative L2 0.001111277611926198\n",
      "training 0.0005113941733725369 relative L2 0.001111226505599916\n",
      "training 0.0005113466759212315 relative L2 0.0011111756321042776\n",
      "training 0.0005112991784699261 relative L2 0.0011111247586086392\n",
      "training 0.0005112517392262816 relative L2 0.0011110735358670354\n",
      "training 0.0005112042999826372 relative L2 0.0011110224295407534\n",
      "training 0.0005111568025313318 relative L2 0.0011109714396297932\n",
      "training 0.0005111093050800264 relative L2 0.001110920449718833\n",
      "training 0.0005110617494210601 relative L2 0.0011108692269772291\n",
      "training 0.0005110142519697547 relative L2 0.001110818120650947\n",
      "training 0.0005109667545184493 relative L2 0.001110767130739987\n",
      "training 0.0005109192570671439 relative L2 0.001110715907998383\n",
      "training 0.0005108715849928558 relative L2 0.0011106646852567792\n",
      "training 0.0005108240875415504 relative L2 0.0011106133460998535\n",
      "training 0.0005107764154672623 relative L2 0.0011105623561888933\n",
      "training 0.0005107289180159569 relative L2 0.001110511482693255\n",
      "training 0.0005106813623569906 relative L2 0.0011104599107056856\n",
      "training 0.0005106336320750415 relative L2 0.0011104088043794036\n",
      "training 0.0005105860764160752 relative L2 0.0011103576980531216\n",
      "training 0.000510538462549448 relative L2 0.001110306242480874\n",
      "training 0.0005104907904751599 relative L2 0.001110255136154592\n",
      "training 0.0005104431184008718 relative L2 0.0011102037969976664\n",
      "training 0.0005103954463265836 relative L2 0.0011101528070867062\n",
      "training 0.0005103477160446346 relative L2 0.0011101013515144587\n",
      "training 0.0005103001021780074 relative L2 0.0011100501287728548\n",
      "training 0.0005102523136883974 relative L2 0.0011099987896159291\n",
      "training 0.0005102046416141093 relative L2 0.0011099473340436816\n",
      "training 0.0005101569113321602 relative L2 0.0011098958784714341\n",
      "training 0.0005101091810502112 relative L2 0.0011098445393145084\n",
      "training 0.0005100615089759231 relative L2 0.001109793083742261\n",
      "training 0.0005100137204863131 relative L2 0.001109741861000657\n",
      "training 0.000509966048412025 relative L2 0.001109690172597766\n",
      "training 0.000509918259922415 relative L2 0.0011096388334408402\n",
      "training 0.0005098704714328051 relative L2 0.001109587261453271\n",
      "training 0.000509822741150856 relative L2 0.0011095359222963452\n",
      "training 0.0005097749526612461 relative L2 0.0011094844667240977\n",
      "training 0.0005097271641716361 relative L2 0.0011094328947365284\n",
      "training 0.0005096792592667043 relative L2 0.0011093814391642809\n",
      "training 0.0005096313543617725 relative L2 0.0011093297507613897\n",
      "training 0.0005095835076645017 relative L2 0.0011092782951891422\n",
      "training 0.0005095357773825526 relative L2 0.001109226723201573\n",
      "training 0.0005094879306852818 relative L2 0.00110917491838336\n",
      "training 0.0005094399093650281 relative L2 0.0011091232299804688\n",
      "training 0.0005093920626677573 relative L2 0.0011090717744082212\n",
      "training 0.0005093442159704864 relative L2 0.001109020202420652\n",
      "training 0.0005092962528578937 relative L2 0.001108968281187117\n",
      "training 0.0005092481733299792 relative L2 0.0011089164763689041\n",
      "training 0.0005092002684250474 relative L2 0.0011088649043813348\n",
      "training 0.0005091523635201156 relative L2 0.0011088130995631218\n",
      "training 0.0005091044004075229 relative L2 0.0011087612947449088\n",
      "training 0.0005090564372949302 relative L2 0.0011087097227573395\n",
      "training 0.0005090084741823375 relative L2 0.0011086579179391265\n",
      "training 0.000508960394654423 relative L2 0.0011086063459515572\n",
      "training 0.0005089124315418303 relative L2 0.0011085545411333442\n",
      "training 0.0005088644102215767 relative L2 0.0011085027363151312\n",
      "training 0.0005088163306936622 relative L2 0.0011084511643275619\n",
      "training 0.0005087683093734086 relative L2 0.0011083993595093489\n",
      "training 0.0005087202880531549 relative L2 0.001108347438275814\n",
      "training 0.0005086721503175795 relative L2 0.001108295749872923\n",
      "training 0.0005086241872049868 relative L2 0.0011082437122240663\n",
      "training 0.0005085761076770723 relative L2 0.0011081919074058533\n",
      "training 0.0005085281445644796 relative L2 0.0011081399861723185\n",
      "training 0.0005084798904135823 relative L2 0.0011080881813541055\n",
      "training 0.0005084319273009896 relative L2 0.0011080362601205707\n",
      "training 0.0005083837895654142 relative L2 0.0011079843388870358\n",
      "training 0.0005083355936221778 relative L2 0.0011079326504841447\n",
      "training 0.0005082876305095851 relative L2 0.0011078804964199662\n",
      "training 0.0005082393763586879 relative L2 0.0011078286916017532\n",
      "training 0.0005081912968307734 relative L2 0.0011077766539528966\n",
      "training 0.0005081432173028588 relative L2 0.00110772461630404\n",
      "training 0.0005080949049443007 relative L2 0.001107672811485827\n",
      "training 0.0005080467672087252 relative L2 0.0011076206574216485\n",
      "training 0.000507998513057828 relative L2 0.0011075686197727919\n",
      "training 0.0005079502589069307 relative L2 0.0011075165821239352\n",
      "training 0.0005079020629636943 relative L2 0.0011074644280597568\n",
      "training 0.0005078536923974752 relative L2 0.0011074123904109001\n",
      "training 0.0005078053800389171 relative L2 0.0011073602363467216\n",
      "training 0.0005077571840956807 relative L2 0.0011073080822825432\n",
      "training 0.0005077089299447834 relative L2 0.0011072558118030429\n",
      "training 0.0005076605011709034 relative L2 0.0011072037741541862\n",
      "training 0.0005076122470200062 relative L2 0.0011071517365053296\n",
      "training 0.000507563934661448 relative L2 0.0011070995824411511\n",
      "training 0.000507515505887568 relative L2 0.0011070474283769727\n",
      "training 0.0005074671353213489 relative L2 0.0011069952743127942\n",
      "training 0.0005074188811704516 relative L2 0.0011069431202486157\n",
      "training 0.0005073705106042325 relative L2 0.0011068909661844373\n",
      "training 0.0005073223146609962 relative L2 0.001106838695704937\n",
      "training 0.0005072739440947771 relative L2 0.0011067863088101149\n",
      "training 0.0005072254571132362 relative L2 0.0011067341547459364\n",
      "training 0.0005071770283393562 relative L2 0.0011066817678511143\n",
      "training 0.0005071284831501544 relative L2 0.0011066293809562922\n",
      "training 0.0005070801125839353 relative L2 0.0011065772268921137\n",
      "training 0.0005070317420177162 relative L2 0.001106525189243257\n",
      "training 0.0005069834878668189 relative L2 0.001106472802348435\n",
      "training 0.0005069351173005998 relative L2 0.0011064206482842565\n",
      "training 0.0005068867467343807 relative L2 0.0011063682613894343\n",
      "training 0.0005068383761681616 relative L2 0.0011063156416639686\n",
      "training 0.0005067898309789598 relative L2 0.0011062633711844683\n",
      "training 0.0005067411693744361 relative L2 0.001106211100704968\n",
      "training 0.000506692798808217 relative L2 0.0011061587138101459\n",
      "training 0.000506644370034337 relative L2 0.0011061063269153237\n",
      "training 0.0005065958248451352 relative L2 0.0011060538236051798\n",
      "training 0.0005065473960712552 relative L2 0.0011060014367103577\n",
      "training 0.0005064988508820534 relative L2 0.0011059491662308574\n",
      "training 0.0005064503639005125 relative L2 0.0011058967793360353\n",
      "training 0.0005064017605036497 relative L2 0.0011058441596105695\n",
      "training 0.000506353098899126 relative L2 0.0011057918891310692\n",
      "training 0.0005063044372946024 relative L2 0.001105739502236247\n",
      "training 0.0005062559503130615 relative L2 0.0011056867660954595\n",
      "training 0.0005062072887085378 relative L2 0.0011056343792006373\n",
      "training 0.0005061588017269969 relative L2 0.0011055819923058152\n",
      "training 0.0005061100819148123 relative L2 0.0011055291397497058\n",
      "training 0.0005060613621026278 relative L2 0.0011054767528548837\n",
      "training 0.000506012758705765 relative L2 0.0011054237838834524\n",
      "training 0.0005059640971012414 relative L2 0.0011053713969886303\n",
      "training 0.0005059153772890568 relative L2 0.0011053186608478427\n",
      "training 0.0005058667156845331 relative L2 0.0011052661575376987\n",
      "training 0.0005058180540800095 relative L2 0.0011052134213969111\n",
      "training 0.0005057695088908076 relative L2 0.0011051606852561235\n",
      "training 0.0005057205562479794 relative L2 0.0011051080655306578\n",
      "training 0.0005056718946434557 relative L2 0.0011050549801439047\n",
      "training 0.0005056229420006275 relative L2 0.0011050021275877953\n",
      "training 0.000505574163980782 relative L2 0.0011049496242776513\n",
      "training 0.0005055253277532756 relative L2 0.00110489665530622\n",
      "training 0.0005054764333181083 relative L2 0.001104844268411398\n",
      "training 0.0005054277135059237 relative L2 0.0011047912994399667\n",
      "training 0.0005053789936937392 relative L2 0.0011047387961298227\n",
      "training 0.0005053300992585719 relative L2 0.0011046858271583915\n",
      "training 0.0005052812630310655 relative L2 0.001104632974602282\n",
      "training 0.0005052323685958982 relative L2 0.001104579889215529\n",
      "training 0.0005051834159530699 relative L2 0.0011045272694900632\n",
      "training 0.0005051345797255635 relative L2 0.001104474300518632\n",
      "training 0.0005050856270827353 relative L2 0.0011044215643778443\n",
      "training 0.000505036732647568 relative L2 0.0011043683625757694\n",
      "training 0.0005049878382124007 relative L2 0.00110431551001966\n",
      "training 0.0005049390019848943 relative L2 0.0011042625410482287\n",
      "training 0.000504890107549727 relative L2 0.0011042093392461538\n",
      "training 0.000504840980283916 relative L2 0.001104156137444079\n",
      "training 0.0005047919112257659 relative L2 0.0011041032848879695\n",
      "training 0.0005047429003752768 relative L2 0.0011040503159165382\n",
      "training 0.0005046939477324486 relative L2 0.0011039971141144633\n",
      "training 0.0005046448204666376 relative L2 0.001103944145143032\n",
      "training 0.0005045959260314703 relative L2 0.0011038911761716008\n",
      "training 0.0005045469733886421 relative L2 0.0011038380907848477\n",
      "training 0.0005044977879151702 relative L2 0.0011037851218134165\n",
      "training 0.000504448835272342 relative L2 0.0011037320364266634\n",
      "training 0.0005043996497988701 relative L2 0.0011036790674552321\n",
      "training 0.000504350638948381 relative L2 0.001103625982068479\n",
      "training 0.0005043013952672482 relative L2 0.0011035727802664042\n",
      "training 0.000504252384416759 relative L2 0.001103519694879651\n",
      "training 0.000504203315358609 relative L2 0.001103466609492898\n",
      "training 0.0005041542463004589 relative L2 0.0011034131748601794\n",
      "training 0.0005041051190346479 relative L2 0.0011033599730581045\n",
      "training 0.0005040559335611761 relative L2 0.0011033067712560296\n",
      "training 0.0005040066898800433 relative L2 0.0011032535694539547\n",
      "training 0.0005039575044065714 relative L2 0.0011032001348212361\n",
      "training 0.0005039083771407604 relative L2 0.0011031469330191612\n",
      "training 0.0005038592498749495 relative L2 0.0011030936148017645\n",
      "training 0.0005038100061938167 relative L2 0.0011030406458303332\n",
      "training 0.0005037608789280057 relative L2 0.0011029870947822928\n",
      "training 0.0005037116934545338 relative L2 0.001102933892980218\n",
      "training 0.0005036625079810619 relative L2 0.0011028804583474994\n",
      "training 0.0005036130896769464 relative L2 0.0011028270237147808\n",
      "training 0.0005035638459958136 relative L2 0.001102773705497384\n",
      "training 0.0005035146023146808 relative L2 0.0011027202708646655\n",
      "training 0.0005034653004258871 relative L2 0.001102666836231947\n",
      "training 0.0005034161149524152 relative L2 0.0011026131687685847\n",
      "training 0.0005033666384406388 relative L2 0.0011025601997971535\n",
      "training 0.0005033174529671669 relative L2 0.0011025065323337913\n",
      "training 0.0005032678600400686 relative L2 0.001102452864870429\n",
      "training 0.0005032184417359531 relative L2 0.0011023995466530323\n",
      "training 0.0005031692562624812 relative L2 0.0011023457627743483\n",
      "training 0.0005031196633353829 relative L2 0.001102292095310986\n",
      "training 0.0005030703032389283 relative L2 0.0011022385442629457\n",
      "training 0.0005030210595577955 relative L2 0.0011021849932149053\n",
      "training 0.0005029714666306973 relative L2 0.0011021314421668649\n",
      "training 0.0005029219901189208 relative L2 0.001102077541872859\n",
      "training 0.0005028724553994834 relative L2 0.0011020241072401404\n",
      "training 0.0005028230370953679 relative L2 0.0011019703233614564\n",
      "training 0.0005027735023759305 relative L2 0.001101916772313416\n",
      "training 0.0005027239676564932 relative L2 0.0011018632212653756\n",
      "training 0.0005026744911447167 relative L2 0.0011018095538020134\n",
      "training 0.0005026251310482621 relative L2 0.0011017557699233294\n",
      "training 0.0005025754799135029 relative L2 0.0011017021024599671\n",
      "training 0.0005025260034017265 relative L2 0.001101648434996605\n",
      "training 0.0005024763522669673 relative L2 0.0011015943018719554\n",
      "training 0.0005024266429245472 relative L2 0.0011015406344085932\n",
      "training 0.0005023771082051098 relative L2 0.001101486966945231\n",
      "training 0.0005023276899009943 relative L2 0.001101433066651225\n",
      "training 0.000502278096973896 relative L2 0.0011013791663572192\n",
      "training 0.0005022285040467978 relative L2 0.0011013253824785352\n",
      "training 0.0005021789111196995 relative L2 0.0011012715985998511\n",
      "training 0.0005021293764002621 relative L2 0.001101217931136489\n",
      "training 0.000502079667057842 relative L2 0.0011011637980118394\n",
      "training 0.0005020299577154219 relative L2 0.0011011100141331553\n",
      "training 0.0005019802483730018 relative L2 0.0011010561138391495\n",
      "training 0.0005019306554459035 relative L2 0.0011010024463757873\n",
      "training 0.0005018810625188053 relative L2 0.001100948778912425\n",
      "training 0.0005018315860070288 relative L2 0.0011008946457877755\n",
      "training 0.0005017818184569478 relative L2 0.0011008408619090915\n",
      "training 0.0005017321673221886 relative L2 0.0011007868451997638\n",
      "training 0.0005016824579797685 relative L2 0.0011007327120751143\n",
      "training 0.0005016326322220266 relative L2 0.0011006788117811084\n",
      "training 0.0005015828646719456 relative L2 0.001100624562241137\n",
      "training 0.0005015329807065427 relative L2 0.0011005705455318093\n",
      "training 0.0005014832713641226 relative L2 0.0011005165288224816\n",
      "training 0.0005014335620217025 relative L2 0.001100462512113154\n",
      "training 0.0005013836780562997 relative L2 0.0011004083789885044\n",
      "training 0.0005013338522985578 relative L2 0.0011003543622791767\n",
      "training 0.0005012840265408158 relative L2 0.0011002999963238835\n",
      "training 0.0005012342007830739 relative L2 0.0011002459796145558\n",
      "training 0.0005011843168176711 relative L2 0.0011001918464899063\n",
      "training 0.0005011344328522682 relative L2 0.001100137596949935\n",
      "training 0.0005010846070945263 relative L2 0.0011000835802406073\n",
      "training 0.0005010347231291234 relative L2 0.0011000294471159577\n",
      "training 0.0005009850719943643 relative L2 0.0010999751975759864\n",
      "training 0.0005009349551983178 relative L2 0.001099920948036015\n",
      "training 0.0005008851294405758 relative L2 0.0010998665820807219\n",
      "training 0.0005008348962292075 relative L2 0.0010998124489560723\n",
      "training 0.0005007850122638047 relative L2 0.0010997584322467446\n",
      "training 0.0005007351282984018 relative L2 0.0010997038334608078\n",
      "training 0.0005006851861253381 relative L2 0.0010996495839208364\n",
      "training 0.0005006353021599352 relative L2 0.0010995952179655433\n",
      "training 0.0005005852435715497 relative L2 0.0010995406191796064\n",
      "training 0.0005005351849831641 relative L2 0.001099486369639635\n",
      "training 0.0005004853010177612 relative L2 0.00109943188726902\n",
      "training 0.000500435009598732 relative L2 0.001099377404898405\n",
      "training 0.0005003850674256682 relative L2 0.001099323038943112\n",
      "training 0.0005003350088372827 relative L2 0.0010992686729878187\n",
      "training 0.0005002848920412362 relative L2 0.0010992140742018819\n",
      "training 0.0005002347752451897 relative L2 0.0010991595918312669\n",
      "training 0.000500184774864465 relative L2 0.0010991052258759737\n",
      "training 0.0005001347162760794 relative L2 0.0010990507435053587\n",
      "training 0.000500084541272372 relative L2 0.0010989960283041\n",
      "training 0.0005000343080610037 relative L2 0.001098941545933485\n",
      "training 0.0004999840748496354 relative L2 0.0010988869471475482\n",
      "training 0.000499933899845928 relative L2 0.0010988323483616114\n",
      "training 0.0004998837248422205 relative L2 0.0010987776331603527\n",
      "training 0.000499833608046174 relative L2 0.001098722917959094\n",
      "training 0.0004997832584194839 relative L2 0.0010986683191731572\n",
      "training 0.0004997327923774719 relative L2 0.0010986137203872204\n",
      "training 0.0004996826173737645 relative L2 0.0010985590051859617\n",
      "training 0.0004996323841623962 relative L2 0.001098504289984703\n",
      "training 0.0004995822673663497 relative L2 0.0010984495747834444\n",
      "training 0.0004995319759473205 relative L2 0.0010983948595821857\n",
      "training 0.0004994816263206303 relative L2 0.001098340144380927\n",
      "training 0.000499431393109262 relative L2 0.0010982853127643466\n",
      "training 0.0004993810434825718 relative L2 0.001098230597563088\n",
      "training 0.0004993306356482208 relative L2 0.0010981755331158638\n",
      "training 0.0004992804024368525 relative L2 0.0010981207014992833\n",
      "training 0.0004992299363948405 relative L2 0.0010980655206367373\n",
      "training 0.0004991795285604894 relative L2 0.0010980109218508005\n",
      "training 0.0004991291207261384 relative L2 0.00109795609023422\n",
      "training 0.0004990788293071091 relative L2 0.0010979011422023177\n",
      "training 0.0004990284214727581 relative L2 0.001097846427001059\n",
      "training 0.0004989780718460679 relative L2 0.001097791246138513\n",
      "training 0.0004989276640117168 relative L2 0.0010977362981066108\n",
      "training 0.0004988771979697049 relative L2 0.0010976811172440648\n",
      "training 0.0004988267319276929 relative L2 0.0010976264020428061\n",
      "training 0.00049877620767802 relative L2 0.0010975712211802602\n",
      "training 0.0004987258580513299 relative L2 0.0010975162731483579\n",
      "training 0.0004986755084246397 relative L2 0.001097461092285812\n",
      "training 0.000498624867759645 relative L2 0.001097406493499875\n",
      "training 0.0004985745763406157 relative L2 0.0010973511962220073\n",
      "training 0.000498523935675621 relative L2 0.0010972961317747831\n",
      "training 0.0004984734114259481 relative L2 0.001097241067327559\n",
      "training 0.0004984227707609534 relative L2 0.0010971857700496912\n",
      "training 0.0004983722465112805 relative L2 0.0010971309384331107\n",
      "training 0.0004983216640539467 relative L2 0.001097075524739921\n",
      "training 0.000498271023388952 relative L2 0.0010970205767080188\n",
      "training 0.0004982204991392791 relative L2 0.001096965279430151\n",
      "training 0.0004981698002666235 relative L2 0.0010969099821522832\n",
      "training 0.0004981192760169506 relative L2 0.001096854917705059\n",
      "training 0.000498068577144295 relative L2 0.0010967996204271913\n",
      "training 0.0004980179364793003 relative L2 0.0010967444395646453\n",
      "training 0.0004979672376066446 relative L2 0.0010966892587020993\n",
      "training 0.0004979165969416499 relative L2 0.0010966339614242315\n",
      "training 0.0004978659562766552 relative L2 0.0010965788969770074\n",
      "training 0.0004978153738193214 relative L2 0.0010965237161144614\n",
      "training 0.0004977646749466658 relative L2 0.00109646818600595\n",
      "training 0.0004977139178663492 relative L2 0.001096413005143404\n",
      "training 0.0004976631607860327 relative L2 0.0010963573586195707\n",
      "training 0.0004976123454980552 relative L2 0.0010963018285110593\n",
      "training 0.0004975616466253996 relative L2 0.0010962465312331915\n",
      "training 0.000497510947752744 relative L2 0.0010961911175400019\n",
      "training 0.0004974602488800883 relative L2 0.0010961357038468122\n",
      "training 0.0004974095500074327 relative L2 0.0010960801737383008\n",
      "training 0.0004973587929271162 relative L2 0.0010960246436297894\n",
      "training 0.0004973079776391387 relative L2 0.001095969113521278\n",
      "training 0.0004972571623511612 relative L2 0.0010959133505821228\n",
      "training 0.000497206230647862 relative L2 0.0010958577040582895\n",
      "training 0.0004971552989445627 relative L2 0.001095802173949778\n",
      "training 0.000497104600071907 relative L2 0.0010957467602565885\n",
      "training 0.0004970537847839296 relative L2 0.001095691230148077\n",
      "training 0.000497003027703613 relative L2 0.0010956357000395656\n",
      "training 0.0004969519795849919 relative L2 0.001095580286346376\n",
      "training 0.0004969011060893536 relative L2 0.0010955247562378645\n",
      "training 0.0004968502325937152 relative L2 0.0010954689932987094\n",
      "training 0.000496799242682755 relative L2 0.0010954135796055198\n",
      "training 0.0004967482527717948 relative L2 0.0010953578166663647\n",
      "training 0.0004966973792761564 relative L2 0.0010953022865578532\n",
      "training 0.0004966463893651962 relative L2 0.0010952465236186981\n",
      "training 0.0004965954576618969 relative L2 0.0010951909935101867\n",
      "training 0.0004965443513356149 relative L2 0.0010951351141557097\n",
      "training 0.0004964934196323156 relative L2 0.0010950794676318765\n",
      "training 0.0004964423133060336 relative L2 0.0010950235882773995\n",
      "training 0.0004963912651874125 relative L2 0.001094968058168888\n",
      "training 0.0004963403334841132 relative L2 0.0010949124116450548\n",
      "training 0.0004962894599884748 relative L2 0.001094856415875256\n",
      "training 0.0004962381790392101 relative L2 0.0010948007693514228\n",
      "training 0.0004961872473359108 relative L2 0.0010947448899969459\n",
      "training 0.0004961361410096288 relative L2 0.001094689010642469\n",
      "training 0.0004960850928910077 relative L2 0.0010946333641186357\n",
      "training 0.0004960339865647256 relative L2 0.001094577251933515\n",
      "training 0.0004959828220307827 relative L2 0.0010945213725790381\n",
      "training 0.0004959317739121616 relative L2 0.0010944654932245612\n",
      "training 0.0004958804929628968 relative L2 0.0010944097302854061\n",
      "training 0.0004958295030519366 relative L2 0.0010943538509309292\n",
      "training 0.0004957783385179937 relative L2 0.001094298087991774\n",
      "training 0.0004957272903993726 relative L2 0.0010942418593913317\n",
      "training 0.0004956761258654296 relative L2 0.0010941859800368547\n",
      "training 0.0004956249613314867 relative L2 0.0010941298678517342\n",
      "training 0.000495573622174561 relative L2 0.0010940738720819354\n",
      "training 0.0004955224576406181 relative L2 0.0010940179927274585\n",
      "training 0.0004954712348990142 relative L2 0.0010939622297883034\n",
      "training 0.0004954200121574104 relative L2 0.0010939061176031828\n",
      "training 0.0004953688476234674 relative L2 0.0010938503546640277\n",
      "training 0.0004953176248818636 relative L2 0.001093794242478907\n",
      "training 0.0004952664021402597 relative L2 0.0010937383631244302\n",
      "training 0.0004952150629833341 relative L2 0.0010936822509393096\n",
      "training 0.0004951637820340693 relative L2 0.001093626138754189\n",
      "training 0.0004951123846694827 relative L2 0.001093569677323103\n",
      "training 0.000495061045512557 relative L2 0.0010935134487226605\n",
      "training 0.0004950097063556314 relative L2 0.0010934574529528618\n",
      "training 0.0004949584836140275 relative L2 0.0010934013407677412\n",
      "training 0.0004949072026647627 relative L2 0.001093344995751977\n",
      "training 0.0004948558053001761 relative L2 0.0010932889999821782\n",
      "training 0.0004948044079355896 relative L2 0.0010932327713817358\n",
      "training 0.000494752952363342 relative L2 0.0010931766591966152\n",
      "training 0.0004947016132064164 relative L2 0.0010931200813502073\n",
      "training 0.0004946501576341689 relative L2 0.0010930639691650867\n",
      "training 0.0004945986438542604 relative L2 0.0010930078569799662\n",
      "training 0.0004945472464896739 relative L2 0.001092951511964202\n",
      "training 0.0004944959073327482 relative L2 0.0010928951669484377\n",
      "training 0.0004944443935528398 relative L2 0.0010928387055173516\n",
      "training 0.0004943929379805923 relative L2 0.0010927822440862656\n",
      "training 0.0004943413659930229 relative L2 0.0010927260154858232\n",
      "training 0.0004942898522131145 relative L2 0.001092669670470059\n",
      "training 0.000494238396640867 relative L2 0.0010926133254542947\n",
      "training 0.0004941868246532977 relative L2 0.0010925568640232086\n",
      "training 0.0004941353690810502 relative L2 0.0010925005190074444\n",
      "training 0.0004940838553011417 relative L2 0.0010924440575763583\n",
      "training 0.0004940323415212333 relative L2 0.0010923873633146286\n",
      "training 0.0004939805367030203 relative L2 0.0010923307854682207\n",
      "training 0.0004939290811307728 relative L2 0.0010922743240371346\n",
      "training 0.0004938774509355426 relative L2 0.0010922178626060486\n",
      "training 0.0004938259371556342 relative L2 0.0010921611683443189\n",
      "training 0.000493774248752743 relative L2 0.0010921049397438765\n",
      "training 0.0004937227349728346 relative L2 0.0010920483618974686\n",
      "training 0.0004936711047776043 relative L2 0.0010919917840510607\n",
      "training 0.0004936194163747132 relative L2 0.001091935089789331\n",
      "training 0.0004935677861794829 relative L2 0.0010918782791122794\n",
      "training 0.0004935160395689309 relative L2 0.0010918215848505497\n",
      "training 0.0004934644093737006 relative L2 0.00109176489058882\n",
      "training 0.0004934127209708095 relative L2 0.0010917081963270903\n",
      "training 0.0004933609161525965 relative L2 0.001091651269234717\n",
      "training 0.0004933091113343835 relative L2 0.0010915945749729872\n",
      "training 0.0004932574811391532 relative L2 0.0010915377642959356\n",
      "training 0.0004932056763209403 relative L2 0.001091480953618884\n",
      "training 0.0004931539297103882 relative L2 0.0010914242593571544\n",
      "training 0.000493102241307497 relative L2 0.001091367332264781\n",
      "training 0.0004930505529046059 relative L2 0.0010913106380030513\n",
      "training 0.0004929988645017147 relative L2 0.0010912538273259997\n",
      "training 0.0004929470596835017 relative L2 0.0010911972494795918\n",
      "training 0.0004928953130729496 relative L2 0.0010911405552178621\n",
      "training 0.0004928434500470757 relative L2 0.0010910838609561324\n",
      "training 0.0004927916452288628 relative L2 0.001091026933863759\n",
      "training 0.0004927400150336325 relative L2 0.0010909700067713857\n",
      "training 0.0004926879773847759 relative L2 0.0010909131960943341\n",
      "training 0.0004926359979435802 relative L2 0.0010908562690019608\n",
      "training 0.0004925840767100453 relative L2 0.0010907993419095874\n",
      "training 0.0004925322718918324 relative L2 0.0010907426476478577\n",
      "training 0.0004924804670736194 relative L2 0.0010906857205554843\n",
      "training 0.0004924286622554064 relative L2 0.0010906285606324673\n",
      "training 0.0004923768574371934 relative L2 0.0010905717499554157\n",
      "training 0.0004923247615806758 relative L2 0.0010905144736170769\n",
      "training 0.0004922727821394801 relative L2 0.0010904574301093817\n",
      "training 0.0004922209191136062 relative L2 0.0010904001537710428\n",
      "training 0.0004921688814647496 relative L2 0.0010903432266786695\n",
      "training 0.0004921169602312148 relative L2 0.0010902861831709743\n",
      "training 0.0004920650972053409 relative L2 0.0010902292560786009\n",
      "training 0.0004920130595564842 relative L2 0.0010901722125709057\n",
      "training 0.0004919610801152885 relative L2 0.0010901149362325668\n",
      "training 0.0004919090424664319 relative L2 0.001090057659894228\n",
      "training 0.0004918568884022534 relative L2 0.001090000499971211\n",
      "training 0.0004918048507533967 relative L2 0.001089943340048194\n",
      "training 0.0004917526966892183 relative L2 0.001089886180125177\n",
      "training 0.0004917006590403616 relative L2 0.0010898286709561944\n",
      "training 0.0004916485049761832 relative L2 0.001089771743863821\n",
      "training 0.0004915965255349874 relative L2 0.0010897143511101604\n",
      "training 0.0004915443132631481 relative L2 0.0010896569583564997\n",
      "training 0.0004914923338219523 relative L2 0.001089599565602839\n",
      "training 0.000491440063342452 relative L2 0.0010895427549257874\n",
      "training 0.0004913879092782736 relative L2 0.0010894853621721268\n",
      "training 0.0004913359298370779 relative L2 0.001089428085833788\n",
      "training 0.0004912836593575776 relative L2 0.0010893710423260927\n",
      "training 0.00049123156350106 relative L2 0.0010893138824030757\n",
      "training 0.0004911792348138988 relative L2 0.0010892567224800587\n",
      "training 0.0004911271971650422 relative L2 0.0010891990968957543\n",
      "training 0.000491074868477881 relative L2 0.0010891417041420937\n",
      "training 0.0004910225979983807 relative L2 0.001089084311388433\n",
      "training 0.0004909702693112195 relative L2 0.0010890270350500941\n",
      "training 0.0004909180570393801 relative L2 0.0010889696422964334\n",
      "training 0.0004908657865598798 relative L2 0.0010889119002968073\n",
      "training 0.0004908135742880404 relative L2 0.0010888545075431466\n",
      "training 0.0004907613038085401 relative L2 0.0010887966491281986\n",
      "training 0.0004907091497443616 relative L2 0.0010887393727898598\n",
      "training 0.0004906568210572004 relative L2 0.0010886815143749118\n",
      "training 0.0004906043759547174 relative L2 0.001088624238036573\n",
      "training 0.0004905522218905389 relative L2 0.0010885666124522686\n",
      "training 0.0004904997767880559 relative L2 0.0010885087540373206\n",
      "training 0.0004904473898932338 relative L2 0.00108845136128366\n",
      "training 0.0004903952940367162 relative L2 0.0010883936192840338\n",
      "training 0.0004903427907265723 relative L2 0.0010883361101150513\n",
      "training 0.0004902904620394111 relative L2 0.0010882782517001033\n",
      "training 0.0004902379587292671 relative L2 0.001088220626115799\n",
      "training 0.0004901856300421059 relative L2 0.0010881630005314946\n",
      "training 0.0004901331849396229 relative L2 0.0010881052585318685\n",
      "training 0.0004900806816294789 relative L2 0.0010880475165322423\n",
      "training 0.0004900282365269959 relative L2 0.0010879897745326161\n",
      "training 0.0004899757914245129 relative L2 0.0010879319161176682\n",
      "training 0.0004899232881143689 relative L2 0.0010878738248720765\n",
      "training 0.0004898707265965641 relative L2 0.0010878160828724504\n",
      "training 0.0004898181650787592 relative L2 0.0010877582244575024\n",
      "training 0.0004897656035609543 relative L2 0.001087700598873198\n",
      "training 0.0004897131584584713 relative L2 0.00108764274045825\n",
      "training 0.0004896605387330055 relative L2 0.001087584882043302\n",
      "training 0.0004896080936305225 relative L2 0.001087527140043676\n",
      "training 0.0004895556485280395 relative L2 0.0010874693980440497\n",
      "training 0.0004895030288025737 relative L2 0.0010874115396291018\n",
      "training 0.0004894505254924297 relative L2 0.0010873534483835101\n",
      "training 0.0004893977893516421 relative L2 0.0010872954735532403\n",
      "training 0.0004893451696261764 relative L2 0.0010872376151382923\n",
      "training 0.0004892924916930497 relative L2 0.0010871796403080225\n",
      "training 0.0004892401047982275 relative L2 0.0010871216654777527\n",
      "training 0.00048918736865744 relative L2 0.0010870636906474829\n",
      "training 0.0004891349817626178 relative L2 0.0010870055994018912\n",
      "training 0.0004890823038294911 relative L2 0.0010869475081562996\n",
      "training 0.0004890296841040254 relative L2 0.001086889416910708\n",
      "training 0.0004889769479632378 relative L2 0.0010868312092497945\n",
      "training 0.0004889242700301111 relative L2 0.0010867731180042028\n",
      "training 0.0004888715338893235 relative L2 0.0010867149103432894\n",
      "training 0.0004888189723715186 relative L2 0.0010866570519283414\n",
      "training 0.000488766236230731 relative L2 0.001086598844267428\n",
      "training 0.0004887136747129261 relative L2 0.0010865407530218363\n",
      "training 0.0004886608803644776 relative L2 0.001086482428945601\n",
      "training 0.0004886080860160291 relative L2 0.001086424570530653\n",
      "training 0.0004885554662905633 relative L2 0.0010863661300390959\n",
      "training 0.0004885027301497757 relative L2 0.0010863080387935042\n",
      "training 0.0004884497611783445 relative L2 0.0010862494818866253\n",
      "training 0.0004883969086222351 relative L2 0.0010861915070563555\n",
      "training 0.0004883442306891084 relative L2 0.0010861330665647984\n",
      "training 0.0004882914363406599 relative L2 0.0010860748589038849\n",
      "training 0.00048823864199221134 relative L2 0.0010860164184123278\n",
      "training 0.0004881857894361019 relative L2 0.0010859579779207706\n",
      "training 0.00048813290777616203 relative L2 0.0010858996538445354\n",
      "training 0.00048807996790856123 relative L2 0.0010858413297683\n",
      "training 0.0004880271153524518 relative L2 0.001085782889276743\n",
      "training 0.00048797408817335963 relative L2 0.001085724332369864\n",
      "training 0.00048792106099426746 relative L2 0.0010856665903702378\n",
      "training 0.0004878684994764626 relative L2 0.0010856080334633589\n",
      "training 0.0004878156178165227 relative L2 0.0010855497093871236\n",
      "training 0.0004877625033259392 relative L2 0.0010854911524802446\n",
      "training 0.00048770950525067747 relative L2 0.0010854327119886875\n",
      "training 0.00048765650717541575 relative L2 0.001085374504327774\n",
      "training 0.00048760356730781496 relative L2 0.001085315947420895\n",
      "training 0.0004875505401287228 relative L2 0.0010852578561753035\n",
      "training 0.0004874975129496306 relative L2 0.0010851990664377809\n",
      "training 0.0004874446603935212 relative L2 0.001085141091607511\n",
      "training 0.0004873916623182595 relative L2 0.0010850818362087011\n",
      "training 0.00048733846051618457 relative L2 0.0010850236285477877\n",
      "training 0.00048728546244092286 relative L2 0.0010849646059796214\n",
      "training 0.00048723231884650886 relative L2 0.0010849059326574206\n",
      "training 0.00048717932077124715 relative L2 0.0010848472593352199\n",
      "training 0.0004871261480730027 relative L2 0.0010847884695976973\n",
      "training 0.00048707312089391053 relative L2 0.0010847297962754965\n",
      "training 0.00048702009371481836 relative L2 0.0010846707737073302\n",
      "training 0.0004869670083280653 relative L2 0.0010846122168004513\n",
      "training 0.00048691389383748174 relative L2 0.0010845533106476068\n",
      "training 0.0004868607211392373 relative L2 0.0010844948701560497\n",
      "training 0.0004868075193371624 relative L2 0.0010844356147572398\n",
      "training 0.00048675425932742655 relative L2 0.0010843770578503609\n",
      "training 0.000486701144836843 relative L2 0.0010843181516975164\n",
      "training 0.00048664785572327673 relative L2 0.001084259245544672\n",
      "training 0.0004865946830250323 relative L2 0.001084200688637793\n",
      "training 0.00048654148122295737 relative L2 0.0010841413168236613\n",
      "training 0.000486488250317052 relative L2 0.0010840828763321042\n",
      "training 0.0004864349903073162 relative L2 0.0010840235045179725\n",
      "training 0.0004863817594014108 relative L2 0.0010839648311957717\n",
      "training 0.0004863284120801836 relative L2 0.0010839058086276054\n",
      "training 0.0004862750938627869 relative L2 0.0010838470188900828\n",
      "training 0.0004862217465415597 relative L2 0.0010837881127372384\n",
      "training 0.00048616863205097616 relative L2 0.0010837294394150376\n",
      "training 0.0004861152556259185 relative L2 0.0010836701840162277\n",
      "training 0.0004860619956161827 relative L2 0.001083611510694027\n",
      "training 0.00048600867739878595 relative L2 0.0010835521388798952\n",
      "training 0.00048595527186989784 relative L2 0.0010834933491423726\n",
      "training 0.00048590192454867065 relative L2 0.001083433860912919\n",
      "training 0.00048584846081212163 relative L2 0.0010833751875907183\n",
      "training 0.00048579511349089444 relative L2 0.0010833156993612647\n",
      "training 0.0004857416788581759 relative L2 0.001083256909623742\n",
      "training 0.00048568841884844005 relative L2 0.0010831973049789667\n",
      "training 0.00048563486780039966 relative L2 0.0010831381659954786\n",
      "training 0.0004855815495830029 relative L2 0.001083078794181347\n",
      "training 0.000485528027638793 relative L2 0.0010830196551978588\n",
      "training 0.00048547450569458306 relative L2 0.0010829605162143707\n",
      "training 0.0004854210128542036 relative L2 0.0010829010279849172\n",
      "training 0.0004853677237406373 relative L2 0.0010828424710780382\n",
      "training 0.0004853143182117492 relative L2 0.0010827826336026192\n",
      "training 0.00048526079626753926 relative L2 0.0010827239602804184\n",
      "training 0.00048520724521949887 relative L2 0.0010826641228049994\n",
      "training 0.0004851537523791194 relative L2 0.001082605100236833\n",
      "training 0.0004851000849157572 relative L2 0.0010825456120073795\n",
      "training 0.00048504656297154725 relative L2 0.0010824863566085696\n",
      "training 0.0004849929828196764 relative L2 0.001082426868379116\n",
      "training 0.00048493940266780555 relative L2 0.0010823678458109498\n",
      "training 0.00048488599713891745 relative L2 0.0010823081247508526\n",
      "training 0.00048483232967555523 relative L2 0.0010822488693520427\n",
      "training 0.00048477863310836256 relative L2 0.0010821890318766236\n",
      "training 0.0004847249365411699 relative L2 0.0010821298928931355\n",
      "training 0.0004846713854931295 relative L2 0.0010820700554177165\n",
      "training 0.00048461774713359773 relative L2 0.0010820108000189066\n",
      "training 0.0004845639632549137 relative L2 0.0010819511953741312\n",
      "training 0.000484510266687721 relative L2 0.0010818920563906431\n",
      "training 0.0004844569193664938 relative L2 0.0010818325681611896\n",
      "training 0.00048440322279930115 relative L2 0.0010817729635164142\n",
      "training 0.00048434946802444756 relative L2 0.0010817133588716388\n",
      "training 0.00048429580056108534 relative L2 0.0010816538706421852\n",
      "training 0.0004842420748900622 relative L2 0.001081594149582088\n",
      "training 0.00048418829101137817 relative L2 0.001081534312106669\n",
      "training 0.0004841343325097114 relative L2 0.0010814749402925372\n",
      "training 0.0004840805777348578 relative L2 0.0010814149864017963\n",
      "training 0.00048402699758298695 relative L2 0.0010813557310029864\n",
      "training 0.0004839732137043029 relative L2 0.00108129542786628\n",
      "training 0.00048391937161795795 relative L2 0.0010812360560521483\n",
      "training 0.00048386550042778254 relative L2 0.0010811758693307638\n",
      "training 0.0004838117165490985 relative L2 0.0010811167303472757\n",
      "training 0.00048375799087807536 relative L2 0.0010810565436258912\n",
      "training 0.0004837040323764086 relative L2 0.001080996822565794\n",
      "training 0.0004836501320824027 relative L2 0.001080936985090375\n",
      "training 0.0004835963190998882 relative L2 0.001080877729691565\n",
      "training 0.00048354247701354325 relative L2 0.0010808174265548587\n",
      "training 0.00048348846030421555 relative L2 0.001080758054740727\n",
      "training 0.0004834346764255315 relative L2 0.0010806977516040206\n",
      "training 0.0004833807179238647 relative L2 0.0010806383797898889\n",
      "training 0.00048332675942219794 relative L2 0.0010805780766531825\n",
      "training 0.0004832726845052093 relative L2 0.0010805188212543726\n",
      "training 0.00048321866779588163 relative L2 0.00108045875094831\n",
      "training 0.0004831648839171976 relative L2 0.0010803989134728909\n",
      "training 0.00048311083810403943 relative L2 0.0010803387267515063\n",
      "training 0.0004830567922908813 relative L2 0.0010802788892760873\n",
      "training 0.00048300260095857084 relative L2 0.0010802187025547028\n",
      "training 0.0004829486715607345 relative L2 0.0010801589814946055\n",
      "training 0.0004828947421628982 relative L2 0.0010800983291119337\n",
      "training 0.0004828404635190964 relative L2 0.001080038957297802\n",
      "training 0.00048278647591359913 relative L2 0.0010799787705764174\n",
      "training 0.00048273251741193235 relative L2 0.0010799190495163202\n",
      "training 0.00048267835518345237 relative L2 0.0010798585135489702\n",
      "training 0.00048262433847412467 relative L2 0.0010797986760735512\n",
      "training 0.0004825700889341533 relative L2 0.0010797381401062012\n",
      "training 0.0004825160722248256 relative L2 0.0010796785354614258\n",
      "training 0.0004824618808925152 relative L2 0.001079617883078754\n",
      "training 0.00048240774776786566 relative L2 0.0010795583948493004\n",
      "training 0.00048235352733172476 relative L2 0.0010794976260513067\n",
      "training 0.0004822994233109057 relative L2 0.0010794379049912095\n",
      "training 0.0004822450573556125 relative L2 0.0010793772526085377\n",
      "training 0.00048219089512713253 relative L2 0.0010793172987177968\n",
      "training 0.00048213658737950027 relative L2 0.0010792565299198031\n",
      "training 0.00048208239604718983 relative L2 0.001079196692444384\n",
      "training 0.0004820280591957271 relative L2 0.0010791360400617123\n",
      "training 0.00048197386786341667 relative L2 0.0010790762025862932\n",
      "training 0.0004819196183234453 relative L2 0.0010790154337882996\n",
      "training 0.00048186545609496534 relative L2 0.0010789555963128805\n",
      "training 0.0004818111192435026 relative L2 0.0010788948275148869\n",
      "training 0.00048175681149587035 relative L2 0.001078834873624146\n",
      "training 0.0004817024164367467 relative L2 0.0010787741048261523\n",
      "training 0.000481648079585284 relative L2 0.0010787140345200896\n",
      "training 0.0004815938591491431 relative L2 0.001078653265722096\n",
      "training 0.0004815393767785281 relative L2 0.0010785930790007114\n",
      "training 0.00048148503992706537 relative L2 0.0010785323102027178\n",
      "training 0.00048143064486794174 relative L2 0.0010784721234813333\n",
      "training 0.0004813762498088181 relative L2 0.001078411121852696\n",
      "training 0.0004813217674382031 relative L2 0.001078351167961955\n",
      "training 0.00048126725596375763 relative L2 0.0010782901663333178\n",
      "training 0.0004812129191122949 relative L2 0.0010782299796119332\n",
      "training 0.00048115834943018854 relative L2 0.001078168861567974\n",
      "training 0.00048110392526723444 relative L2 0.001078109024092555\n",
      "training 0.00048104950110428035 relative L2 0.0010780481388792396\n",
      "training 0.0004809951351489872 relative L2 0.001077987952157855\n",
      "training 0.0004809406818822026 relative L2 0.001077926717698574\n",
      "training 0.0004808861413039267 relative L2 0.0010778664145618677\n",
      "training 0.0004808316007256508 relative L2 0.001077805063687265\n",
      "training 0.0004807770310435444 relative L2 0.0010777447605505586\n",
      "training 0.0004807224031537771 relative L2 0.0010776836425065994\n",
      "training 0.00048066789167933166 relative L2 0.0010776235722005367\n",
      "training 0.0004806133802048862 relative L2 0.0010775626869872212\n",
      "training 0.00048055898514576256 relative L2 0.001077502267435193\n",
      "training 0.000480504211736843 relative L2 0.0010774414986371994\n",
      "training 0.000480449729366228 relative L2 0.0010773810790851712\n",
      "training 0.0004803952178917825 relative L2 0.001077320077456534\n",
      "training 0.00048034059000201523 relative L2 0.001077259425073862\n",
      "training 0.0004802857874892652 relative L2 0.0010771984234452248\n",
      "training 0.00048023139243014157 relative L2 0.0010771378874778748\n",
      "training 0.000480176619021222 relative L2 0.0010770764201879501\n",
      "training 0.00048012202023528516 relative L2 0.001077016000635922\n",
      "training 0.00048006727593019605 relative L2 0.0010769545333459973\n",
      "training 0.00048001258983276784 relative L2 0.001076894230209291\n",
      "training 0.00047995796194300056 relative L2 0.0010768326465040445\n",
      "training 0.00047990313032642007 relative L2 0.0010767725761979818\n",
      "training 0.00047984838602133095 relative L2 0.00107671064324677\n",
      "training 0.0004797936126124114 relative L2 0.0010766509221866727\n",
      "training 0.00047973907203413546 relative L2 0.001076588872820139\n",
      "training 0.0004796845023520291 relative L2 0.0010765301994979382\n",
      "training 0.000479630078189075 relative L2 0.0010764674516394734\n",
      "training 0.00047957635251805186 relative L2 0.001076413900591433\n",
      "training 0.00047952437307685614 relative L2 0.0010763563914224505\n",
      "training 0.0004794793203473091 relative L2 0.0010763452155515552\n",
      "training 0.00047945877304300666 relative L2 0.0010763925965875387\n",
      "training 0.0004795236745849252 relative L2 0.001076837652362883\n",
      "training 0.0004798919253516942 relative L2 0.001078350585885346\n",
      "training 0.00048134938697330654 relative L2 0.0010843714699149132\n",
      "training 0.0004867356037721038 relative L2 0.0011049786116927862\n",
      "training 0.0005061411065980792 relative L2 0.001178150181658566\n",
      "training 0.000576500257011503 relative L2 0.0014053171034902334\n",
      "training 0.0008264196803793311 relative L2 0.002023105276748538\n",
      "training 0.001722759916447103 relative L2 0.003319299779832363\n",
      "training 0.0046637882478535175 relative L2 0.0055127814412117004\n",
      "training 0.012875204905867577 relative L2 0.007404838223010302\n",
      "training 0.0232514888048172 relative L2 0.006618361920118332\n",
      "training 0.01856514997780323 relative L2 0.0016403963090851903\n",
      "training 0.0011303969658911228 relative L2 0.004660009406507015\n",
      "training 0.009202627465128899 relative L2 0.005892585497349501\n",
      "training 0.01471487432718277 relative L2 0.001483449130319059\n",
      "training 0.0009222480584867299 relative L2 0.004519349429756403\n",
      "training 0.008654867298901081 relative L2 0.004566304385662079\n",
      "training 0.008831383660435677 relative L2 0.0013673617504537106\n",
      "training 0.0007808585069142282 relative L2 0.004706751089543104\n",
      "training 0.009387892670929432 relative L2 0.0025615801569074392\n",
      "training 0.0027705568354576826 relative L2 0.0029363189823925495\n",
      "training 0.0036443411372601986 relative L2 0.0037510753609240055\n",
      "training 0.005958176217973232 relative L2 0.0011848506983369589\n",
      "training 0.0005838475772179663 relative L2 0.0035849278792738914\n",
      "training 0.005438221152871847 relative L2 0.001700386987067759\n",
      "training 0.0012152285780757666 relative L2 0.002694083610549569\n",
      "training 0.003068122547119856 relative L2 0.0025913084391504526\n",
      "training 0.002835343824699521 relative L2 0.0016205841675400734\n",
      "training 0.0011015485506504774 relative L2 0.002852150471881032\n",
      "training 0.003440149826928973 relative L2 0.0011547120520845056\n",
      "training 0.0005536124808713794 relative L2 0.002545422175899148\n",
      "training 0.002735323505476117 relative L2 0.001570749212987721\n",
      "training 0.0010353499092161655 relative L2 0.001958586508408189\n",
      "training 0.0016162708634510636 relative L2 0.0020034622866660357\n",
      "training 0.0016899044858291745 relative L2 0.001373911160044372\n",
      "training 0.000788457109592855 relative L2 0.002127454150468111\n",
      "training 0.0019091239664703608 relative L2 0.001138772233389318\n",
      "training 0.0005382492090575397 relative L2 0.0019494288135319948\n",
      "training 0.0015993948327377439 relative L2 0.0013258261606097221\n",
      "training 0.0007342843455262482 relative L2 0.0016063412185758352\n",
      "training 0.00108344538602978 relative L2 0.0015723793767392635\n",
      "training 0.0010363846085965633 relative L2 0.0012652988079935312\n",
      "training 0.000667029176838696 relative L2 0.001662815804593265\n",
      "training 0.001161809079349041 relative L2 0.0011136668035760522\n",
      "training 0.0005143624730408192 relative L2 0.00157207902520895\n",
      "training 0.0010359802981838584 relative L2 0.001193227362819016\n",
      "training 0.0005924692377448082 relative L2 0.0013731442159041762\n",
      "training 0.000788525678217411 relative L2 0.001332924934104085\n",
      "training 0.0007414850406348705 relative L2 0.0011772068683058023\n",
      "training 0.000575868645682931 relative L2 0.001390756224282086\n",
      "training 0.0008091966155916452 relative L2 0.0010944297537207603\n",
      "training 0.000496344524435699 relative L2 0.0013390136882662773\n",
      "training 0.0007483490626327693 relative L2 0.0011378953931853175\n",
      "training 0.0005376836052164435 relative L2 0.0012217285111546516\n",
      "training 0.0006217244663275778 relative L2 0.0012140601174905896\n",
      "training 0.000613129697740078 relative L2 0.001117080682888627\n",
      "training 0.0005173966055735946 relative L2 0.0012408066540956497\n",
      "training 0.0006416654214262962 relative L2 0.0010838942835107446\n",
      "training 0.0004865390947088599 relative L2 0.001203589141368866\n",
      "training 0.000602345506194979 relative L2 0.0011147198965772986\n",
      "training 0.0005154635291546583 relative L2 0.001135174767114222\n",
      "training 0.0005350231658667326 relative L2 0.0011542633874341846\n",
      "training 0.0005530638736672699 relative L2 0.0010863392380997539\n",
      "training 0.0004886893439106643 relative L2 0.001159294624812901\n",
      "training 0.0005585310864262283 relative L2 0.0010814536362886429\n",
      "training 0.00048420627717860043 relative L2 0.001130353775806725\n",
      "training 0.0005299019976519048 relative L2 0.001103410730138421\n",
      "training 0.0005047705490142107 relative L2 0.0010931827127933502\n",
      "training 0.0004952053423039615 relative L2 0.0011201092274859548\n",
      "training 0.0005201255553402007 relative L2 0.0010759640717878938\n",
      "training 0.0004792094405274838 relative L2 0.0011138289701193571\n",
      "training 0.0005146006587892771 relative L2 0.0010820975294336677\n",
      "training 0.00048474830691702664 relative L2 0.0010938828345388174\n",
      "training 0.000495564250741154 relative L2 0.0010946238180622458\n",
      "training 0.0004965529660694301 relative L2 0.0010772281093522906\n",
      "training 0.0004804334894288331 relative L2 0.0010981896193698049\n",
      "training 0.0004995451308786869 relative L2 0.001074851374141872\n",
      "training 0.00047823667409829795 relative L2 0.0010893018916249275\n",
      "training 0.0004916043835692108 relative L2 0.001081745489500463\n",
      "training 0.0004844123322982341 relative L2 0.0010782352183014154\n",
      "training 0.000481219234643504 relative L2 0.00108636065851897\n",
      "training 0.0004888793919235468 relative L2 0.0010732951341196895\n",
      "training 0.0004767877981066704 relative L2 0.001084179151803255\n",
      "training 0.0004866280360147357 relative L2 0.001075567677617073\n",
      "training 0.00047891176654957235 relative L2 0.0010774877155199647\n",
      "training 0.00048068450996652246 relative L2 0.0010794737609103322\n",
      "training 0.00048233382403850555 relative L2 0.0010731472866609693\n",
      "training 0.00047662618453614414 relative L2 0.001079432899132371\n",
      "training 0.00048248106031678617 relative L2 0.0010732532246038318\n",
      "training 0.000476715067634359 relative L2 0.00107632577419281\n",
      "training 0.00047947585699148476 relative L2 0.0010753965470939875\n",
      "training 0.00047876089229248464 relative L2 0.0010729605564847589\n",
      "training 0.0004765077028423548 relative L2 0.0010763577884063125\n",
      "training 0.0004795029235538095 relative L2 0.0010722511215135455\n",
      "training 0.00047584439744241536 relative L2 0.0010746960761025548\n",
      "training 0.00047811857075430453 relative L2 0.0010734155075624585\n",
      "training 0.0004768509534187615 relative L2 0.0010726983891800046\n",
      "training 0.00047620973782613873 relative L2 0.0010740814032033086\n",
      "training 0.0004775538109242916 relative L2 0.0010718046687543392\n",
      "training 0.00047542585525661707 relative L2 0.0010735628893598914\n",
      "training 0.0004769795050378889 relative L2 0.0010722167789936066\n",
      "training 0.0004758289142046124 relative L2 0.0010721940780058503\n",
      "training 0.000475809327326715 relative L2 0.0010728316847234964\n",
      "training 0.00047632111818529665 relative L2 0.0010714889504015446\n",
      "training 0.0004751361848320812 relative L2 0.0010724954772740602\n",
      "training 0.000476092507597059 relative L2 0.001071613747626543\n",
      "training 0.00047523551620543003 relative L2 0.0010717815021052957\n",
      "training 0.0004753813846036792 relative L2 0.0010718696285039186\n",
      "training 0.0004755150876007974 relative L2 0.0010711910435929894\n",
      "training 0.00047487483243457973 relative L2 0.0010718238772824407\n",
      "training 0.0004754155525006354 relative L2 0.0010711371432989836\n",
      "training 0.0004748314677271992 relative L2 0.0010712983785197139\n",
      "training 0.00047498763888143003 relative L2 0.0010713125811889768\n",
      "training 0.00047496004845015705 relative L2 0.001070921658538282\n",
      "training 0.0004746199701912701 relative L2 0.0010712055955082178\n",
      "training 0.00047490705037489533 relative L2 0.001070812693797052\n",
      "training 0.0004745216283481568 relative L2 0.0010709468042477965\n",
      "training 0.00047463460941798985 relative L2 0.0010708261979743838\n",
      "training 0.0004745572223328054 relative L2 0.0010706244502216578\n",
      "training 0.00047436560271307826 relative L2 0.00107080047018826\n",
      "training 0.00047450276906602085 relative L2 0.001070500584319234\n",
      "training 0.0004742525634355843 relative L2 0.0010705573949962854\n",
      "training 0.00047431353596039116 relative L2 0.001070493832230568\n",
      "training 0.00047423236537724733 relative L2 0.0010703577427193522\n",
      "training 0.0004741154261864722 relative L2 0.0010704006999731064\n",
      "training 0.00047417348832823336 relative L2 0.0010702309664338827\n",
      "training 0.00047400392941199243 relative L2 0.001070263679139316\n",
      "training 0.00047402692143805325 relative L2 0.001070164144039154\n",
      "training 0.00047395608271472156 relative L2 0.0010700715938583016\n",
      "training 0.0004738685383927077 relative L2 0.001070105587132275\n",
      "training 0.0004738854186143726 relative L2 0.001069957623258233\n",
      "training 0.00047376437578350306 relative L2 0.0010699450504034758\n",
      "training 0.0004737592244055122 relative L2 0.00106989371124655\n",
      "training 0.0004736986302305013 relative L2 0.0010698105907067657\n",
      "training 0.00047362735494971275 relative L2 0.0010697883553802967\n",
      "training 0.00047361914766952395 relative L2 0.0010696982499212027\n",
      "training 0.00047352834371849895 relative L2 0.0010696770623326302\n",
      "training 0.0004735055554192513 relative L2 0.0010696051176637411\n",
      "training 0.00047345220809802413 relative L2 0.001069538644514978\n",
      "training 0.0004733899550046772 relative L2 0.001069521764293313\n",
      "training 0.0004733666137326509 relative L2 0.0010694325901567936\n",
      "training 0.0004732942907139659 relative L2 0.0010693924268707633\n",
      "training 0.0004732615780085325 relative L2 0.0010693459771573544\n",
      "training 0.00047321157762780786 relative L2 0.0010692817158997059\n",
      "training 0.0004731560475192964 relative L2 0.00106923864223063\n",
      "training 0.0004731242952402681 relative L2 0.0010691756615415215\n",
      "training 0.0004730618966277689 relative L2 0.0010691359639167786\n",
      "training 0.0004730242071673274 relative L2 0.0010690740309655666\n",
      "training 0.0004729755164589733 relative L2 0.0010690185008570552\n",
      "training 0.00047292388626374304 relative L2 0.0010689825285226107\n",
      "training 0.0004728866333607584 relative L2 0.001068914425559342\n",
      "training 0.0004728300264105201 relative L2 0.001068866578862071\n",
      "training 0.0004727894556708634 relative L2 0.0010688180336728692\n",
      "training 0.00047274091048166156 relative L2 0.0010687626199796796\n",
      "training 0.00047269233618862927 relative L2 0.0010687115136533976\n",
      "training 0.0004726506595034152 relative L2 0.0010686576133593917\n",
      "training 0.0004725992330349982 relative L2 0.00106861034873873\n",
      "training 0.00047255560639314353 relative L2 0.0010685538873076439\n",
      "training 0.000472508545499295 relative L2 0.001068502082489431\n",
      "training 0.00047246183385141194 relative L2 0.001068456214852631\n",
      "training 0.0004724181198980659 relative L2 0.0010683995205909014\n",
      "training 0.0004723692836705595 relative L2 0.0010683489963412285\n",
      "training 0.00047232588985934854 relative L2 0.001068298821337521\n",
      "training 0.0004722784215118736 relative L2 0.0010682475985959172\n",
      "training 0.0004722324665635824 relative L2 0.0010681945132091641\n",
      "training 0.00047218703548423946 relative L2 0.0010681429412215948\n",
      "training 0.00047214049845933914 relative L2 0.0010680945124477148\n",
      "training 0.00047209570766426623 relative L2 0.001068040612153709\n",
      "training 0.0004720487049780786 relative L2 0.001067989389412105\n",
      "training 0.0004720037686638534 relative L2 0.0010679392144083977\n",
      "training 0.00047195758088491857 relative L2 0.0010678882244974375\n",
      "training 0.0004719120042864233 relative L2 0.0010678358376026154\n",
      "training 0.00047186645679175854 relative L2 0.0010677848476916552\n",
      "training 0.0004718206182587892 relative L2 0.001067735138349235\n",
      "training 0.00047177536180242896 relative L2 0.0010676828678697348\n",
      "training 0.00047172934864647686 relative L2 0.001067631528712809\n",
      "training 0.0004716842668130994 relative L2 0.0010675812372937799\n",
      "training 0.0004716383700724691 relative L2 0.0010675304802134633\n",
      "training 0.000471593055408448 relative L2 0.001067478908225894\n",
      "training 0.00047154768253676593 relative L2 0.0010674279183149338\n",
      "training 0.00047150207683444023 relative L2 0.0010673780925571918\n",
      "training 0.00047145682037808 relative L2 0.0010673260549083352\n",
      "training 0.00047141104005277157 relative L2 0.0010672747157514095\n",
      "training 0.0004713658709079027 relative L2 0.0010672243079170585\n",
      "training 0.000471320265205577 relative L2 0.00106717343442142\n",
      "training 0.00047127471771091223 relative L2 0.0010671220952644944\n",
      "training 0.000471229461254552 relative L2 0.0010670714545994997\n",
      "training 0.0004711839137598872 relative L2 0.0010670211631804705\n",
      "training 0.0004711385990958661 relative L2 0.0010669698240235448\n",
      "training 0.00047109313891269267 relative L2 0.0010669184848666191\n",
      "training 0.0004710477951448411 relative L2 0.0010668681934475899\n",
      "training 0.00047100233496166766 relative L2 0.0010668174363672733\n",
      "training 0.0004709570202976465 relative L2 0.0010667662136256695\n",
      "training 0.000470911618322134 relative L2 0.0010667153401300311\n",
      "training 0.00047086639096960425 relative L2 0.0010666651651263237\n",
      "training 0.0004708210763055831 relative L2 0.001066613825969398\n",
      "training 0.00047077558701857924 relative L2 0.0010665628360584378\n",
      "training 0.0004707302723545581 relative L2 0.001066512195393443\n",
      "training 0.0004706848703790456 relative L2 0.0010664615547284484\n",
      "training 0.00047063955571502447 relative L2 0.0010664102155715227\n",
      "training 0.00047059415373951197 relative L2 0.0010663593420758843\n",
      "training 0.00047054883907549083 relative L2 0.0010663085849955678\n",
      "training 0.0004705033788923174 relative L2 0.0010662578279152513\n",
      "training 0.0004704580642282963 relative L2 0.0010662066051736474\n",
      "training 0.0004704125749412924 relative L2 0.0010661558480933309\n",
      "training 0.00047036726027727127 relative L2 0.0010661050910130143\n",
      "training 0.00047032194561325014 relative L2 0.0010660539846867323\n",
      "training 0.00047027654363773763 relative L2 0.0010660028783604503\n",
      "training 0.0004702312289737165 relative L2 0.0010659523541107774\n",
      "training 0.00047018600162118673 relative L2 0.0010659012477844954\n",
      "training 0.00047014051233418286 relative L2 0.0010658502578735352\n",
      "training 0.0004700951976701617 relative L2 0.0010657989187166095\n",
      "training 0.0004700496792793274 relative L2 0.001065748161636293\n",
      "training 0.00047000442282296717 relative L2 0.001065696938894689\n",
      "training 0.0004699588753283024 relative L2 0.001065645832568407\n",
      "training 0.00046991361887194216 relative L2 0.0010655949590727687\n",
      "training 0.00046986815868876874 relative L2 0.0010655440855771303\n",
      "training 0.00046982287312857807 relative L2 0.0010654929792508483\n",
      "training 0.00046977735473774374 relative L2 0.001065441989339888\n",
      "training 0.0004697319818660617 relative L2 0.001065390999428928\n",
      "training 0.00046968646347522736 relative L2 0.0010653401259332895\n",
      "training 0.0004696410906035453 relative L2 0.0010652889031916857\n",
      "training 0.0004695956304203719 relative L2 0.0010652379132807255\n",
      "training 0.00046955031575635076 relative L2 0.0010651869233697653\n",
      "training 0.00046950491378083825 relative L2 0.001065136049874127\n",
      "training 0.00046945957001298666 relative L2 0.0010650849435478449\n",
      "training 0.0004694141389336437 relative L2 0.0010650336043909192\n",
      "training 0.0004693686496466398 relative L2 0.001064982614479959\n",
      "training 0.00046932310215197504 relative L2 0.001064931508153677\n",
      "training 0.0004692776710726321 relative L2 0.0010648802854120731\n",
      "training 0.00046923215268179774 relative L2 0.0010648291790857911\n",
      "training 0.0004691867798101157 relative L2 0.001064778072759509\n",
      "training 0.0004691413778346032 relative L2 0.001064726966433227\n",
      "training 0.00046909580123610795 relative L2 0.0010646757436916232\n",
      "training 0.0004690502828452736 relative L2 0.0010646245209500194\n",
      "training 0.00046900470624677837 relative L2 0.0010645735310390592\n",
      "training 0.0004689592751674354 relative L2 0.0010645224247127771\n",
      "training 0.0004689137276727706 relative L2 0.0010644709691405296\n",
      "training 0.00046886818017810583 relative L2 0.0010644198628142476\n",
      "training 0.00046882269089110196 relative L2 0.0010643688729032874\n",
      "training 0.00046877723070792854 relative L2 0.0010643175337463617\n",
      "training 0.00046873182873241603 relative L2 0.0010642664274200797\n",
      "training 0.00046868628123775125 relative L2 0.0010642149718478322\n",
      "training 0.0004686405882239342 relative L2 0.0010641636326909065\n",
      "training 0.00046859506983309984 relative L2 0.0010641122935339808\n",
      "training 0.0004685496096499264 relative L2 0.0010640607215464115\n",
      "training 0.00046850385842844844 relative L2 0.0010640096152201295\n",
      "training 0.00046845831093378365 relative L2 0.001063958159647882\n",
      "training 0.0004684127925429493 relative L2 0.0010639069369062781\n",
      "training 0.0004683672159444541 relative L2 0.0010638551320880651\n",
      "training 0.00046832143561914563 relative L2 0.0010638039093464613\n",
      "training 0.00046827588812448084 relative L2 0.0010637524537742138\n",
      "training 0.00046823019511066377 relative L2 0.00106370123103261\n",
      "training 0.00046818467671982944 relative L2 0.001063650008291006\n",
      "training 0.0004681389546021819 relative L2 0.0010635984363034368\n",
      "training 0.0004680932906921953 relative L2 0.001063547213561833\n",
      "training 0.00046804785961285233 relative L2 0.001063495990820229\n",
      "training 0.0004680022830143571 relative L2 0.0010634444188326597\n",
      "training 0.00046795656089670956 relative L2 0.0010633931960910559\n",
      "training 0.0004679108678828925 relative L2 0.001063341973349452\n",
      "training 0.00046786529128439724 relative L2 0.0010632905177772045\n",
      "training 0.0004678196564782411 relative L2 0.001063239062204957\n",
      "training 0.00046777387615293264 relative L2 0.0010631876066327095\n",
      "training 0.00046772838686592877 relative L2 0.0010631362674757838\n",
      "training 0.0004676826356444508 relative L2 0.0010630846954882145\n",
      "training 0.0004676369426306337 relative L2 0.0010630331235006452\n",
      "training 0.00046759130782447755 relative L2 0.0010629817843437195\n",
      "training 0.0004675454692915082 relative L2 0.0010629302123561502\n",
      "training 0.00046749983448535204 relative L2 0.001062878523953259\n",
      "training 0.00046745393774472177 relative L2 0.0010628268355503678\n",
      "training 0.0004674080410040915 relative L2 0.0010627754963934422\n",
      "training 0.00046736231888644397 relative L2 0.001062723807990551\n",
      "training 0.0004673166258726269 relative L2 0.0010626723524183035\n",
      "training 0.0004672707582358271 relative L2 0.0010626207804307342\n",
      "training 0.0004672249488066882 relative L2 0.0010625689756125212\n",
      "training 0.00046717916848137975 relative L2 0.0010625174036249518\n",
      "training 0.000467133242636919 relative L2 0.0010624659480527043\n",
      "training 0.0004670874332077801 relative L2 0.0010624139104038477\n",
      "training 0.0004670416237786412 relative L2 0.001062362571246922\n",
      "training 0.0004669959016609937 relative L2 0.0010623105335980654\n",
      "training 0.00046694991760887206 relative L2 0.001062258961610496\n",
      "training 0.00046690390445291996 relative L2 0.001062207156792283\n",
      "training 0.00046685803681612015 relative L2 0.0010621552355587482\n",
      "training 0.0004668121982831508 relative L2 0.001062103547155857\n",
      "training 0.0004667663015425205 relative L2 0.0010620516259223223\n",
      "training 0.0004667202301789075 relative L2 0.0010619998211041093\n",
      "training 0.0004666742461267859 relative L2 0.00106194824911654\n",
      "training 0.0004666284075938165 relative L2 0.0010618965607136488\n",
      "training 0.0004665826272685081 relative L2 0.001061844639480114\n",
      "training 0.0004665365850087255 relative L2 0.0010617930674925447\n",
      "training 0.0004664907755795866 relative L2 0.0010617412626743317\n",
      "training 0.0004664447042159736 relative L2 0.0010616893414407969\n",
      "training 0.00046639860374853015 relative L2 0.0010616375366225839\n",
      "training 0.0004663527652155608 relative L2 0.0010615857318043709\n",
      "training 0.00046630683937110007 relative L2 0.001061533926986158\n",
      "training 0.00046626082621514797 relative L2 0.001061482005752623\n",
      "training 0.00046621475485153496 relative L2 0.0010614300845190883\n",
      "training 0.00046616874169558287 relative L2 0.0010613781632855535\n",
      "training 0.0004661226412281394 relative L2 0.0010613261256366968\n",
      "training 0.0004660765698645264 relative L2 0.0010612743208184838\n",
      "training 0.00046603052760474384 relative L2 0.0010612220503389835\n",
      "training 0.0004659843980334699 relative L2 0.0010611702455207705\n",
      "training 0.0004659383266698569 relative L2 0.0010611184407025576\n",
      "training 0.0004658923135139048 relative L2 0.0010610661702230573\n",
      "training 0.00046584615483880043 relative L2 0.001061014598235488\n",
      "training 0.0004658002289943397 relative L2 0.0010609626770019531\n",
      "training 0.0004657542158383876 relative L2 0.0010609107557684183\n",
      "training 0.00046570811537094414 relative L2 0.0010608586017042398\n",
      "training 0.0004656619857996702 relative L2 0.001060806680470705\n",
      "training 0.0004656159144360572 relative L2 0.0010607545264065266\n",
      "training 0.0004655697848647833 relative L2 0.0010607027215883136\n",
      "training 0.000465523567982018 relative L2 0.0010606503346934915\n",
      "training 0.00046547732199542224 relative L2 0.0010605982970446348\n",
      "training 0.00046543125063180923 relative L2 0.0010605463758111\n",
      "training 0.0004653851210605353 relative L2 0.0010604942217469215\n",
      "training 0.00046533881686627865 relative L2 0.001060442067682743\n",
      "training 0.00046529254177585244 relative L2 0.001060389680787921\n",
      "training 0.0004652462375815958 relative L2 0.001060337875969708\n",
      "training 0.0004652001953218132 relative L2 0.0010602854890748858\n",
      "training 0.000465153920231387 relative L2 0.0010602332185953856\n",
      "training 0.00046510776155628264 relative L2 0.0010601812973618507\n",
      "training 0.0004650616319850087 relative L2 0.0010601289104670286\n",
      "training 0.0004650153568945825 relative L2 0.0010600764071568847\n",
      "training 0.0004649691400118172 relative L2 0.0010600241366773844\n",
      "training 0.00046492277760989964 relative L2 0.001059971982613206\n",
      "training 0.0004648765898309648 relative L2 0.0010599197121337056\n",
      "training 0.00046483034384436905 relative L2 0.0010598673252388835\n",
      "training 0.00046478406875394285 relative L2 0.0010598149383440614\n",
      "training 0.0004647376772481948 relative L2 0.001059762667864561\n",
      "training 0.00046469137305393815 relative L2 0.001059710280969739\n",
      "training 0.0004646449815481901 relative L2 0.0010596580104902387\n",
      "training 0.000464598648250103 relative L2 0.0010596057400107384\n",
      "training 0.0004645522858481854 relative L2 0.0010595533531159163\n",
      "training 0.0004645058943424374 relative L2 0.0010595009662210941\n",
      "training 0.00046445944462902844 relative L2 0.001059448579326272\n",
      "training 0.0004644131404347718 relative L2 0.0010593963088467717\n",
      "training 0.0004643668362405151 relative L2 0.0010593439219519496\n",
      "training 0.00046432038652710617 relative L2 0.0010592914186418056\n",
      "training 0.00046427390770986676 relative L2 0.0010592391481623054\n",
      "training 0.00046422751620411873 relative L2 0.0010591864120215178\n",
      "training 0.00046418100828304887 relative L2 0.0010591339087113738\n",
      "training 0.0004641345585696399 relative L2 0.0010590815218165517\n",
      "training 0.00046408819616772234 relative L2 0.0010590291349217296\n",
      "training 0.0004640418919734657 relative L2 0.0010589765151962638\n",
      "training 0.000463995267637074 relative L2 0.001058923895470798\n",
      "training 0.0004639488470274955 relative L2 0.0010588716249912977\n",
      "training 0.000463902426417917 relative L2 0.0010588191216811538\n",
      "training 0.0004638558020815253 relative L2 0.0010587663855403662\n",
      "training 0.0004638093523681164 relative L2 0.0010587136493995786\n",
      "training 0.00046376261161640286 relative L2 0.0010586610296741128\n",
      "training 0.000463716103695333 relative L2 0.001058608409948647\n",
      "training 0.0004636695666704327 relative L2 0.001058555906638503\n",
      "training 0.00046362311695702374 relative L2 0.001058503519743681\n",
      "training 0.0004635765217244625 relative L2 0.0010584507836028934\n",
      "training 0.0004635298391804099 relative L2 0.0010583976982161403\n",
      "training 0.0004634832148440182 relative L2 0.0010583450784906745\n",
      "training 0.000463436619611457 relative L2 0.0010582925751805305\n",
      "training 0.00046339011169038713 relative L2 0.0010582399554550648\n",
      "training 0.000463343458250165 relative L2 0.0010581872193142772\n",
      "training 0.0004632969503290951 relative L2 0.0010581343667581677\n",
      "training 0.00046325018047355115 relative L2 0.0010580813977867365\n",
      "training 0.00046320364344865084 relative L2 0.0010580286616459489\n",
      "training 0.00046315681538544595 relative L2 0.0010579758090898395\n",
      "training 0.0004631101037375629 relative L2 0.0010579228401184082\n",
      "training 0.0004630634211935103 relative L2 0.001057869871146977\n",
      "training 0.0004630166513379663 relative L2 0.0010578172514215112\n",
      "training 0.0004629699687939137 relative L2 0.001057764166034758\n",
      "training 0.00046292319893836975 relative L2 0.0010577116627246141\n",
      "training 0.0004628766037058085 relative L2 0.0010576586937531829\n",
      "training 0.00046282989205792546 relative L2 0.0010576057247817516\n",
      "training 0.0004627831222023815 relative L2 0.0010575527558103204\n",
      "training 0.00046273632324300706 relative L2 0.0010574997868388891\n",
      "training 0.0004626894660759717 relative L2 0.001057446701452136\n",
      "training 0.0004626427253242582 relative L2 0.0010573937324807048\n",
      "training 0.00046259595546871424 relative L2 0.0010573408799245954\n",
      "training 0.0004625491565093398 relative L2 0.001057287910953164\n",
      "training 0.00046250224113464355 relative L2 0.0010572347091510892\n",
      "training 0.0004624553839676082 relative L2 0.0010571816237643361\n",
      "training 0.00046240835217759013 relative L2 0.0010571286547929049\n",
      "training 0.00046236152411438525 relative L2 0.0010570756858214736\n",
      "training 0.0004623145505320281 relative L2 0.0010570224840193987\n",
      "training 0.00046226754784584045 relative L2 0.0010569693986326456\n",
      "training 0.0004622206324711442 relative L2 0.0010569165460765362\n",
      "training 0.0004621738044079393 relative L2 0.001056863577105105\n",
      "training 0.0004621268599294126 relative L2 0.0010568102588877082\n",
      "training 0.0004620799154508859 relative L2 0.001056757173500955\n",
      "training 0.00046203291276469827 relative L2 0.0010567039716988802\n",
      "training 0.0004619858809746802 relative L2 0.0010566507698968053\n",
      "training 0.0004619389364961535 relative L2 0.0010565975680947304\n",
      "training 0.0004618920502252877 relative L2 0.0010565445991232991\n",
      "training 0.00046184504753910005 relative L2 0.0010564917465671897\n",
      "training 0.00046179816126823425 relative L2 0.0010564385447651148\n",
      "training 0.0004617511003743857 relative L2 0.001056385226547718\n",
      "training 0.00046170398127287626 relative L2 0.0010563320247456431\n",
      "training 0.00046165689127519727 relative L2 0.0010562787065282464\n",
      "training 0.0004616099176928401 relative L2 0.0010562257375568151\n",
      "training 0.0004615629150066525 relative L2 0.0010561724193394184\n",
      "training 0.00046151570859365165 relative L2 0.0010561192175373435\n",
      "training 0.00046146856038831174 relative L2 0.0010560662485659122\n",
      "training 0.00046142167411744595 relative L2 0.0010560128139331937\n",
      "training 0.00046137458411976695 relative L2 0.0010559596121311188\n",
      "training 0.00046132749412208796 relative L2 0.0010559064103290439\n",
      "training 0.00046128040412440896 relative L2 0.0010558529756963253\n",
      "training 0.00046123319771140814 relative L2 0.0010557995410636067\n",
      "training 0.0004611859912984073 relative L2 0.0010557463392615318\n",
      "training 0.00046113881398923695 relative L2 0.0010556929046288133\n",
      "training 0.00046109186951071024 relative L2 0.0010556394699960947\n",
      "training 0.00046104463399387896 relative L2 0.0010555859189480543\n",
      "training 0.00046099754399619997 relative L2 0.0010555326007306576\n",
      "training 0.0004609503084793687 relative L2 0.0010554790496826172\n",
      "training 0.0004609031311701983 relative L2 0.001055425382219255\n",
      "training 0.0004608558665495366 relative L2 0.0010553720640018582\n",
      "training 0.0004608087183441967 relative L2 0.001055318396538496\n",
      "training 0.0004607612791005522 relative L2 0.0010552649619057775\n",
      "training 0.00046071401447989047 relative L2 0.001055211410857737\n",
      "training 0.00046066686627455056 relative L2 0.0010551580926403403\n",
      "training 0.0004606196889653802 relative L2 0.0010551046580076218\n",
      "training 0.0004605724534485489 relative L2 0.0010550512233749032\n",
      "training 0.0004605251888278872 relative L2 0.0010549979051575065\n",
      "training 0.000460477895103395 relative L2 0.0010549441212788224\n",
      "training 0.0004604306013789028 relative L2 0.0010548906866461039\n",
      "training 0.00046038333675824106 relative L2 0.0010548371355980635\n",
      "training 0.0004603360721375793 relative L2 0.001054783700965345\n",
      "training 0.00046028874930925667 relative L2 0.0010547300335019827\n",
      "training 0.0004602413682732731 relative L2 0.0010546764824539423\n",
      "training 0.0004601940745487809 relative L2 0.0010546226985752583\n",
      "training 0.00046014654799364507 relative L2 0.001054569031111896\n",
      "training 0.00046009919606149197 relative L2 0.0010545153636485338\n",
      "training 0.0004600518732331693 relative L2 0.0010544613469392061\n",
      "training 0.0004600044048856944 relative L2 0.001054407679475844\n",
      "training 0.0004599570238497108 relative L2 0.0010543540120124817\n",
      "training 0.00045990964281372726 relative L2 0.0010543002281337976\n",
      "training 0.0004598623199854046 relative L2 0.0010542466770857573\n",
      "training 0.00045981493894942105 relative L2 0.001054193009622395\n",
      "training 0.0004597674706019461 relative L2 0.0010541393421590328\n",
      "training 0.0004597200604621321 relative L2 0.0010540852090343833\n",
      "training 0.0004596724465955049 relative L2 0.001054031541571021\n",
      "training 0.00045962512376718223 relative L2 0.001053977757692337\n",
      "training 0.0004595777136273682 relative L2 0.0010539242066442966\n",
      "training 0.0004595302452798933 relative L2 0.0010538704227656126\n",
      "training 0.00045948271872475743 relative L2 0.0010538165224716067\n",
      "training 0.0004594351921696216 relative L2 0.001053762505762279\n",
      "training 0.00045938757830299437 relative L2 0.0010537083726376295\n",
      "training 0.0004593400517478585 relative L2 0.00105365423951298\n",
      "training 0.0004592924378812313 relative L2 0.0010536002228036523\n",
      "training 0.0004592447658069432 relative L2 0.0010535463225096464\n",
      "training 0.0004591972101479769 relative L2 0.0010534923058003187\n",
      "training 0.00045914968359284103 relative L2 0.0010534381726756692\n",
      "training 0.00045910218614153564 relative L2 0.0010533842723816633\n",
      "training 0.0004590546013787389 relative L2 0.0010533301392570138\n",
      "training 0.00045900698751211166 relative L2 0.0010532760061323643\n",
      "training 0.0004589591990225017 relative L2 0.0010532218730077147\n",
      "training 0.0004589114978443831 relative L2 0.0010531676234677434\n",
      "training 0.0004588639712892473 relative L2 0.0010531137231737375\n",
      "training 0.0004588163283187896 relative L2 0.0010530598228797317\n",
      "training 0.00045876859803684056 relative L2 0.0010530055733397603\n",
      "training 0.00045872098417021334 relative L2 0.001052951323799789\n",
      "training 0.0004586731956806034 relative L2 0.0010528970742598176\n",
      "training 0.00045862531987950206 relative L2 0.0010528431739658117\n",
      "training 0.00045857776422053576 relative L2 0.0010527886915951967\n",
      "training 0.0004585300339385867 relative L2 0.0010527344420552254\n",
      "training 0.00045848204172216356 relative L2 0.001052680192515254\n",
      "training 0.00045843436964787543 relative L2 0.0010526262922212481\n",
      "training 0.00045838661026209593 relative L2 0.001052571926265955\n",
      "training 0.0004583387344609946 relative L2 0.0010525177931413054\n",
      "training 0.000458291033282876 relative L2 0.0010524631943553686\n",
      "training 0.00045824324479326606 relative L2 0.0010524088284000754\n",
      "training 0.00045819522347301245 relative L2 0.001052354578860104\n",
      "training 0.00045814746408723295 relative L2 0.0010523005621507764\n",
      "training 0.0004580996173899621 relative L2 0.0010522461961954832\n",
      "training 0.00045805194531567395 relative L2 0.0010521919466555119\n",
      "training 0.0004580039530992508 relative L2 0.0010521374642848969\n",
      "training 0.0004579561064019799 relative L2 0.0010520833311602473\n",
      "training 0.0004579082306008786 relative L2 0.0010520288487896323\n",
      "training 0.00045786029659211636 relative L2 0.0010519744828343391\n",
      "training 0.0004578124498948455 relative L2 0.0010519202332943678\n",
      "training 0.00045776457409374416 relative L2 0.0010518655180931091\n",
      "training 0.00045771640725433826 relative L2 0.0010518112685531378\n",
      "training 0.000457668473245576 relative L2 0.001051756669767201\n",
      "training 0.0004576205392368138 relative L2 0.001051702187396586\n",
      "training 0.0004575725179165602 relative L2 0.0010516475886106491\n",
      "training 0.0004575244674924761 relative L2 0.001051593106240034\n",
      "training 0.0004574765043798834 relative L2 0.0010515385074540973\n",
      "training 0.0004574284248519689 relative L2 0.0010514837922528386\n",
      "training 0.00045738028711639345 relative L2 0.0010514293098822236\n",
      "training 0.0004573322948999703 relative L2 0.0010513747110962868\n",
      "training 0.0004572842735797167 relative L2 0.0010513199958950281\n",
      "training 0.0004572361649479717 relative L2 0.001051265629939735\n",
      "training 0.00045718802721239626 relative L2 0.0010512109147384763\n",
      "training 0.0004571398894768208 relative L2 0.0010511563159525394\n",
      "training 0.0004570917517412454 relative L2 0.0010511016007512808\n",
      "training 0.0004570435266941786 relative L2 0.0010510472347959876\n",
      "training 0.00045699544716626406 relative L2 0.0010509924031794071\n",
      "training 0.00045694707660004497 relative L2 0.0010509379208087921\n",
      "training 0.00045689893886446953 relative L2 0.0010508830891922116\n",
      "training 0.0004568508011288941 relative L2 0.0010508284904062748\n",
      "training 0.0004568026342894882 relative L2 0.0010507740080356598\n",
      "training 0.00045675443834625185 relative L2 0.0010507190600037575\n",
      "training 0.00045670627150684595 relative L2 0.001050664228387177\n",
      "training 0.0004566579591482878 relative L2 0.0010506092803552747\n",
      "training 0.0004566095594782382 relative L2 0.0010505543323233724\n",
      "training 0.0004565612762235105 relative L2 0.0010504996171221137\n",
      "training 0.00045651313848793507 relative L2 0.0010504447855055332\n",
      "training 0.0004564648261293769 relative L2 0.001050389837473631\n",
      "training 0.0004564164555631578 relative L2 0.0010503347730264068\n",
      "training 0.00045636805589310825 relative L2 0.001050280057825148\n",
      "training 0.0004563197144307196 relative L2 0.0010502252262085676\n",
      "training 0.00045627140207216144 relative L2 0.0010501703945919871\n",
      "training 0.0004562230606097728 relative L2 0.0010501152137294412\n",
      "training 0.0004561744863167405 relative L2 0.0010500604985281825\n",
      "training 0.00045612608664669096 relative L2 0.0010500055504962802\n",
      "training 0.00045607780339196324 relative L2 0.0010499506024643779\n",
      "training 0.00045602943282574415 relative L2 0.0010498955380171537\n",
      "training 0.0004559808876365423 relative L2 0.0010498403571546078\n",
      "training 0.00045593248796649277 relative L2 0.0010497852927073836\n",
      "training 0.00045588400098495185 relative L2 0.0010497302282601595\n",
      "training 0.00045583551400341094 relative L2 0.0010496750473976135\n",
      "training 0.00045578708522953093 relative L2 0.0010496197501197457\n",
      "training 0.00045573859824799 relative L2 0.0010495646856725216\n",
      "training 0.0004556900530587882 relative L2 0.0010495095048099756\n",
      "training 0.0004556414787657559 relative L2 0.0010494544403627515\n",
      "training 0.0004555930499918759 relative L2 0.00104939891025424\n",
      "training 0.0004555442719720304 relative L2 0.0010493439622223377\n",
      "training 0.00045549587230198085 relative L2 0.0010492887813597918\n",
      "training 0.00045544729800894856 relative L2 0.0010492336004972458\n",
      "training 0.0004553986946120858 relative L2 0.0010491781868040562\n",
      "training 0.00045535003300756216 relative L2 0.0010491230059415102\n",
      "training 0.00045530148781836033 relative L2 0.0010490675922483206\n",
      "training 0.00045525291352532804 relative L2 0.0010490122949704528\n",
      "training 0.0004552042519208044 relative L2 0.0010489568812772632\n",
      "training 0.0004551556194201112 relative L2 0.0010489015839993954\n",
      "training 0.000455106986919418 relative L2 0.001048845937475562\n",
      "training 0.00045505809248425066 relative L2 0.0010487907566130161\n",
      "training 0.0004550095181912184 relative L2 0.0010487353429198265\n",
      "training 0.00045496097300201654 relative L2 0.0010486799292266369\n",
      "training 0.0004549121658783406 relative L2 0.0010486241662874818\n",
      "training 0.0004548633878584951 relative L2 0.0010485686361789703\n",
      "training 0.0004548147553578019 relative L2 0.0010485133389011025\n",
      "training 0.00045476594823412597 relative L2 0.0010484576923772693\n",
      "training 0.00045471719931811094 relative L2 0.001048402045853436\n",
      "training 0.00045466842129826546 relative L2 0.0010483463993296027\n",
      "training 0.00045461958507075906 relative L2 0.001048290403559804\n",
      "training 0.00045457069063559175 relative L2 0.0010482351062819362\n",
      "training 0.00045452199992723763 relative L2 0.0010481792269274592\n",
      "training 0.0004544731928035617 relative L2 0.0010481239296495914\n",
      "training 0.0004544243565760553 relative L2 0.0010480679338797927\n",
      "training 0.00045437560766004026 relative L2 0.0010480121709406376\n",
      "training 0.00045432665501721203 relative L2 0.0010479564080014825\n",
      "training 0.0004542777023743838 relative L2 0.0010479007614776492\n",
      "training 0.0004542289534583688 relative L2 0.0010478449985384941\n",
      "training 0.00045418014633469284 relative L2 0.0010477893520146608\n",
      "training 0.0004541311936918646 relative L2 0.0010477335890755057\n",
      "training 0.0004540822410490364 relative L2 0.0010476777097210288\n",
      "training 0.0004540334630291909 relative L2 0.0010476219467818737\n",
      "training 0.00045398439397104084 relative L2 0.0010475661838427186\n",
      "training 0.000453935528639704 relative L2 0.0010475104209035635\n",
      "training 0.0004538866051007062 relative L2 0.001047454308718443\n",
      "training 0.0004538375069387257 relative L2 0.0010473985457792878\n",
      "training 0.00045378843788057566 relative L2 0.0010473427828401327\n",
      "training 0.00045373954344540834 relative L2 0.0010472869034856558\n",
      "training 0.00045369056169874966 relative L2 0.0010472310241311789\n",
      "training 0.00045364172547124326 relative L2 0.0010471752611920238\n",
      "training 0.0004535926564130932 relative L2 0.0010471190325915813\n",
      "training 0.000453543703770265 relative L2 0.0010470632696524262\n",
      "training 0.00045349454740062356 relative L2 0.0010470073902979493\n",
      "training 0.0004534455656539649 relative L2 0.0010469513945281506\n",
      "training 0.0004533965839073062 relative L2 0.0010468955151736736\n",
      "training 0.00045334745664149523 relative L2 0.001046839402988553\n",
      "training 0.00045329832937568426 relative L2 0.0010467835236340761\n",
      "training 0.00045324923121370375 relative L2 0.0010467275278642774\n",
      "training 0.00045319998753257096 relative L2 0.0010466715320944786\n",
      "training 0.0004531509184744209 relative L2 0.0010466155363246799\n",
      "training 0.00045310179120860994 relative L2 0.0010465594241395593\n",
      "training 0.0004530525184236467 relative L2 0.0010465033119544387\n",
      "training 0.00045300336205400527 relative L2 0.0010464474325999618\n",
      "training 0.0004529541765805334 relative L2 0.0010463913204148412\n",
      "training 0.0004529049911070615 relative L2 0.0010463353246450424\n",
      "training 0.00045285571832209826 relative L2 0.0010462792124599218\n",
      "training 0.0004528065037447959 relative L2 0.0010462231002748013\n",
      "training 0.0004527572600636631 relative L2 0.001046166755259037\n",
      "training 0.00045270807459019125 relative L2 0.0010461107594892383\n",
      "training 0.0004526589182205498 relative L2 0.0010460547637194395\n",
      "training 0.0004526097036432475 relative L2 0.001045998651534319\n",
      "training 0.00045256048906594515 relative L2 0.0010459426557645202\n",
      "training 0.00045251112896949053 relative L2 0.0010458866599947214\n",
      "training 0.00045246179797686636 relative L2 0.001045830431394279\n",
      "training 0.0004524124669842422 relative L2 0.0010457742027938366\n",
      "training 0.0004523632233031094 relative L2 0.0010457179741933942\n",
      "training 0.0004523139214143157 relative L2 0.0010456617455929518\n",
      "training 0.00045226450311020017 relative L2 0.0010456055169925094\n",
      "training 0.00045221514301374555 relative L2 0.001045549288392067\n",
      "training 0.0004521658120211214 relative L2 0.0010454929433763027\n",
      "training 0.00045211639371700585 relative L2 0.0010454365983605385\n",
      "training 0.0004520671791397035 relative L2 0.0010453800205141306\n",
      "training 0.0004520176153164357 relative L2 0.0010453234426677227\n",
      "training 0.0004519681679084897 relative L2 0.0010452669812366366\n",
      "training 0.0004519187787082046 relative L2 0.001045210869051516\n",
      "training 0.00045186944771558046 relative L2 0.0010451542912051082\n",
      "training 0.000451819971203804 relative L2 0.0010450980626046658\n",
      "training 0.00045177069841884077 relative L2 0.0010450414847582579\n",
      "training 0.00045172119280323386 relative L2 0.0010449851397424936\n",
      "training 0.0004516716580837965 relative L2 0.0010449286783114076\n",
      "training 0.0004516221524681896 relative L2 0.001044872566126287\n",
      "training 0.000451572792371735 relative L2 0.0010448162211105227\n",
      "training 0.0004515231994446367 relative L2 0.0010447596432641149\n",
      "training 0.00045147351920604706 relative L2 0.0010447034146636724\n",
      "training 0.00045142415910959244 relative L2 0.0010446467204019427\n",
      "training 0.00045137456618249416 relative L2 0.0010445904918015003\n",
      "training 0.00045132514787837863 relative L2 0.0010445340303704143\n",
      "training 0.00045127561315894127 relative L2 0.0010444772196933627\n",
      "training 0.000451226020231843 relative L2 0.001044420525431633\n",
      "training 0.0004511764273047447 relative L2 0.001044363947585225\n",
      "training 0.00045112689258530736 relative L2 0.0010443072533234954\n",
      "training 0.00045107718324288726 relative L2 0.0010442507918924093\n",
      "training 0.000451027590315789 relative L2 0.0010441939812153578\n",
      "training 0.0004509780847001821 relative L2 0.001044137286953628\n",
      "training 0.00045092840446159244 relative L2 0.0010440807091072202\n",
      "training 0.0004508787824306637 relative L2 0.0010440240148454905\n",
      "training 0.00045082910219207406 relative L2 0.001043967087753117\n",
      "training 0.00045077948016114533 relative L2 0.0010439103934913874\n",
      "training 0.00045072982902638614 relative L2 0.0010438535828143358\n",
      "training 0.00045068011968396604 relative L2 0.001043796888552606\n",
      "training 0.0004506303812377155 relative L2 0.0010437401942908764\n",
      "training 0.0004505807301029563 relative L2 0.0010436831507831812\n",
      "training 0.0004505310207605362 relative L2 0.0010436263401061296\n",
      "training 0.00045048139872960746 relative L2 0.0010435694130137563\n",
      "training 0.00045043168938718736 relative L2 0.0010435126023367047\n",
      "training 0.00045038192183710635 relative L2 0.0010434554424136877\n",
      "training 0.0004503320960793644 relative L2 0.0010433986317366362\n",
      "training 0.0004502824158407748 relative L2 0.001043341588228941\n",
      "training 0.0004502326773945242 relative L2 0.0010432845447212458\n",
      "training 0.00045018302625976503 relative L2 0.0010432273847982287\n",
      "training 0.00045013311319053173 relative L2 0.0010431701084598899\n",
      "training 0.0004500832292251289 relative L2 0.0010431132977828383\n",
      "training 0.0004500333743635565 relative L2 0.001043056370690465\n",
      "training 0.0004499837523326278 relative L2 0.001042999210767448\n",
      "training 0.0004499338392633945 relative L2 0.001042941934429109\n",
      "training 0.00044988386798650026 relative L2 0.0010428850073367357\n",
      "training 0.0004498340131249279 relative L2 0.0010428274981677532\n",
      "training 0.0004497841582633555 relative L2 0.0010427706874907017\n",
      "training 0.00044973421609029174 relative L2 0.0010427134111523628\n",
      "training 0.00044968436122871935 relative L2 0.001042656134814024\n",
      "training 0.00044963438995182514 relative L2 0.0010425987420603633\n",
      "training 0.0004495844477787614 relative L2 0.0010425414657220244\n",
      "training 0.00044953441829420626 relative L2 0.0010424844222143292\n",
      "training 0.0004494845343288034 relative L2 0.0010424272622913122\n",
      "training 0.0004494346503634006 relative L2 0.0010423699859529734\n",
      "training 0.000449384591775015 relative L2 0.0010423125931993127\n",
      "training 0.00044933470780961215 relative L2 0.0010422554332762957\n",
      "training 0.00044928473653271794 relative L2 0.001042198040522635\n",
      "training 0.00044923461973667145 relative L2 0.0010421406477689743\n",
      "training 0.00044918470666743815 relative L2 0.0010420832550153136\n",
      "training 0.0004491346189752221 relative L2 0.0010420259786769748\n",
      "training 0.000449084589490667 relative L2 0.0010419684695079923\n",
      "training 0.00044903456000611186 relative L2 0.0010419111931696534\n",
      "training 0.00044898458872921765 relative L2 0.001041853684000671\n",
      "training 0.00044893453014083207 relative L2 0.001041796407662332\n",
      "training 0.00044888450065627694 relative L2 0.0010417388984933496\n",
      "training 0.0004488344129640609 relative L2 0.0010416811564937234\n",
      "training 0.0004487842379603535 relative L2 0.0010416234144940972\n",
      "training 0.00044873420847579837 relative L2 0.0010415659053251147\n",
      "training 0.0004486842663027346 relative L2 0.0010415080469101667\n",
      "training 0.0004486341786105186 relative L2 0.0010414504213258624\n",
      "training 0.0004485840618144721 relative L2 0.0010413925629109144\n",
      "training 0.00044853391591459513 relative L2 0.0010413345880806446\n",
      "training 0.0004484836827032268 relative L2 0.0010412767296656966\n",
      "training 0.00044843347859568894 relative L2 0.0010412192204967141\n",
      "training 0.00044838347821496427 relative L2 0.0010411613620817661\n",
      "training 0.0004483332741074264 relative L2 0.0010411033872514963\n",
      "training 0.000448283099103719 relative L2 0.001041045761667192\n",
      "training 0.00044823295320384204 relative L2 0.001040987903252244\n",
      "training 0.00044818269088864326 relative L2 0.0010409302776679397\n",
      "training 0.00044813266140408814 relative L2 0.0010408725356683135\n",
      "training 0.0004480824281927198 relative L2 0.0010408149100840092\n",
      "training 0.00044803222408518195 relative L2 0.0010407568188384175\n",
      "training 0.00044798196176998317 relative L2 0.0010406988440081477\n",
      "training 0.00044793178676627576 relative L2 0.001040640869177878\n",
      "training 0.00044788149534724653 relative L2 0.0010405827779322863\n",
      "training 0.0004478312621358782 relative L2 0.0010405252687633038\n",
      "training 0.00044778105802834034 relative L2 0.0010404670611023903\n",
      "training 0.0004477308539208025 relative L2 0.0010404093191027641\n",
      "training 0.0004476805333979428 relative L2 0.0010403511114418507\n",
      "training 0.000447630271082744 relative L2 0.001040293020196259\n",
      "training 0.0004475799505598843 relative L2 0.0010402348125353456\n",
      "training 0.0004475296300370246 relative L2 0.0010401768377050757\n",
      "training 0.0004474793968256563 relative L2 0.0010401183972135186\n",
      "training 0.0004474289307836443 relative L2 0.0010400604223832488\n",
      "training 0.00044737852294929326 relative L2 0.0010400023311376572\n",
      "training 0.0004473282606340945 relative L2 0.0010399441234767437\n",
      "training 0.0004472778527997434 relative L2 0.0010398857994005084\n",
      "training 0.0004472274740692228 relative L2 0.0010398279409855604\n",
      "training 0.0004471770371310413 relative L2 0.001039769733324647\n",
      "training 0.0004471266584005207 relative L2 0.0010397112928330898\n",
      "training 0.00044707616325467825 relative L2 0.00103965331800282\n",
      "training 0.0004470257554203272 relative L2 0.001039594877511263\n",
      "training 0.00044697546400129795 relative L2 0.0010395366698503494\n",
      "training 0.0004469249688554555 relative L2 0.0010394782293587923\n",
      "training 0.00044687444460578263 relative L2 0.001039419905282557\n",
      "training 0.00044682403677143157 relative L2 0.0010393615812063217\n",
      "training 0.00044677359983325005 relative L2 0.0010393031407147646\n",
      "training 0.00044672307558357716 relative L2 0.0010392448166385293\n",
      "training 0.00044667249312624335 relative L2 0.0010391860269010067\n",
      "training 0.0004466219979804009 relative L2 0.0010391274699941278\n",
      "training 0.00044657153193838894 relative L2 0.0010390692623332143\n",
      "training 0.0004465209785848856 relative L2 0.0010390104725956917\n",
      "training 0.0004464704543352127 relative L2 0.0010389521485194564\n",
      "training 0.0004464200173970312 relative L2 0.0010388935916125774\n",
      "training 0.000446369347628206 relative L2 0.0010388348018750548\n",
      "training 0.00044631861965171993 relative L2 0.0010387763613834977\n",
      "training 0.0004462680662982166 relative L2 0.0010387178044766188\n",
      "training 0.0004462174547370523 relative L2 0.0010386592475697398\n",
      "training 0.000446166901383549 relative L2 0.0010386004578322172\n",
      "training 0.0004461160860955715 relative L2 0.0010385419009253383\n",
      "training 0.0004460654454305768 relative L2 0.0010384832276031375\n",
      "training 0.000446014862973243 relative L2 0.0010384244378656149\n",
      "training 0.00044596410589292645 relative L2 0.0010383655317127705\n",
      "training 0.0004459134070202708 relative L2 0.0010383068583905697\n",
      "training 0.00044586288277059793 relative L2 0.0010382478358224034\n",
      "training 0.0004458119219634682 relative L2 0.0010381892789155245\n",
      "training 0.0004457613395061344 relative L2 0.00103813037276268\n",
      "training 0.00044571064063347876 relative L2 0.0010380715830251575\n",
      "training 0.00044565985444933176 relative L2 0.001038012676872313\n",
      "training 0.0004456090391613543 relative L2 0.0010379536543041468\n",
      "training 0.0004455583111848682 relative L2 0.001037894980981946\n",
      "training 0.0004455074085853994 relative L2 0.001037835841998458\n",
      "training 0.0004454566806089133 relative L2 0.0010377771686762571\n",
      "training 0.00044540592352859676 relative L2 0.001037718029692769\n",
      "training 0.0004453551082406193 relative L2 0.0010376587742939591\n",
      "training 0.00044530417653732 relative L2 0.0010375997517257929\n",
      "training 0.0004452532739378512 relative L2 0.0010375406127423048\n",
      "training 0.0004452024004422128 relative L2 0.0010374814737588167\n",
      "training 0.00044515173067338765 relative L2 0.0010374225676059723\n",
      "training 0.0004451009153854102 relative L2 0.0010373631957918406\n",
      "training 0.00044504995457828045 relative L2 0.0010373039403930306\n",
      "training 0.0004449990810826421 relative L2 0.0010372450342401862\n",
      "training 0.0004449482948984951 relative L2 0.0010371857788413763\n",
      "training 0.00044489733409136534 relative L2 0.00103712675627321\n",
      "training 0.00044484640238806605 relative L2 0.0010370677337050438\n",
      "training 0.0004447957035154104 relative L2 0.0010370087111368775\n",
      "training 0.00044474462629295886 relative L2 0.001036949222907424\n",
      "training 0.0004446937527973205 relative L2 0.0010368902003392577\n",
      "training 0.00044464293750934303 relative L2 0.0010368312941864133\n",
      "training 0.0004445920349098742 relative L2 0.0010367720387876034\n",
      "training 0.00044454116141423583 relative L2 0.001036713132634759\n",
      "training 0.0004444902006071061 relative L2 0.001036653877235949\n",
      "training 0.00044443923979997635 relative L2 0.0010365946218371391\n",
      "training 0.00044438824988901615 relative L2 0.0010365353664383292\n",
      "training 0.0004443372308742255 relative L2 0.0010364762274548411\n",
      "training 0.00044428632827475667 relative L2 0.001036417088471353\n",
      "training 0.00044423533836379647 relative L2 0.0010363577166572213\n",
      "training 0.000444184202933684 relative L2 0.001036298694089055\n",
      "training 0.00044413330033421516 relative L2 0.0010362392058596015\n",
      "training 0.0004440823686309159 relative L2 0.0010361799504607916\n",
      "training 0.0004440312332008034 relative L2 0.0010361206950619817\n",
      "training 0.0004439802432898432 relative L2 0.0010360615560784936\n",
      "training 0.0004439290496520698 relative L2 0.0010360023006796837\n",
      "training 0.0004438780597411096 relative L2 0.0010359430452808738\n",
      "training 0.0004438270698301494 relative L2 0.0010358834406360984\n",
      "training 0.0004437760799191892 relative L2 0.0010358241852372885\n",
      "training 0.0004437249735929072 relative L2 0.001035764580592513\n",
      "training 0.0004436738381627947 relative L2 0.0010357050923630595\n",
      "training 0.00044362270273268223 relative L2 0.0010356458369642496\n",
      "training 0.00044357162551023066 relative L2 0.0010355864651501179\n",
      "training 0.00044352057739160955 relative L2 0.0010355269769206643\n",
      "training 0.00044346944196149707 relative L2 0.001035467372275889\n",
      "training 0.00044341819011606276 relative L2 0.0010354078840464354\n",
      "training 0.00044336708378978074 relative L2 0.0010353486286476254\n",
      "training 0.00044331594835966825 relative L2 0.00103528902400285\n",
      "training 0.00044326484203338623 relative L2 0.0010352295357733965\n",
      "training 0.00044321356108412147 relative L2 0.001035169931128621\n",
      "training 0.00044316257117316127 relative L2 0.0010351103264838457\n",
      "training 0.00044311126112006605 relative L2 0.0010350507218390703\n",
      "training 0.00044306006748229265 relative L2 0.001034991117194295\n",
      "training 0.0004430089029483497 relative L2 0.0010349316289648414\n",
      "training 0.00044295782572589815 relative L2 0.0010348721407353878\n",
      "training 0.0004429066611919552 relative L2 0.001034812186844647\n",
      "training 0.0004428552929311991 relative L2 0.0010347528150305152\n",
      "training 0.0004428040992934257 relative L2 0.0010346934432163835\n",
      "training 0.0004427529056556523 relative L2 0.0010346337221562862\n",
      "training 0.00044270147918723524 relative L2 0.001034574001096189\n",
      "training 0.00044265028554946184 relative L2 0.0010345145128667355\n",
      "training 0.000442599062807858 relative L2 0.0010344545589759946\n",
      "training 0.0004425476072356105 relative L2 0.0010343949543312192\n",
      "training 0.0004424963262863457 relative L2 0.0010343351168558002\n",
      "training 0.0004424450162332505 relative L2 0.001034275279380381\n",
      "training 0.00044239358976483345 relative L2 0.001034215441904962\n",
      "training 0.00044234225060790777 relative L2 0.0010341553715988994\n",
      "training 0.00044229079503566027 relative L2 0.0010340955341234803\n",
      "training 0.0004422394558787346 relative L2 0.0010340356966480613\n",
      "training 0.00044218814582563937 relative L2 0.0010339756263419986\n",
      "training 0.00044213669025339186 relative L2 0.0010339157888665795\n",
      "training 0.0004420853219926357 relative L2 0.0010338558349758387\n",
      "training 0.00044203383731655777 relative L2 0.001033795764669776\n",
      "training 0.0004419822071213275 relative L2 0.001033735810779035\n",
      "training 0.0004419308388605714 relative L2 0.001033675973303616\n",
      "training 0.00044187941239215434 relative L2 0.0010336160194128752\n",
      "training 0.0004418279277160764 relative L2 0.0010335560655221343\n",
      "training 0.00044177641393616796 relative L2 0.0010334958788007498\n",
      "training 0.0004417248710524291 relative L2 0.0010334356920793653\n",
      "training 0.0004416735318955034 relative L2 0.0010333756217733026\n",
      "training 0.00044162195990793407 relative L2 0.0010333155514672399\n",
      "training 0.0004415703588165343 relative L2 0.0010332554811611772\n",
      "training 0.0004415190778672695 relative L2 0.0010331951780244708\n",
      "training 0.00044146738946437836 relative L2 0.0010331347584724426\n",
      "training 0.00044141578837297857 relative L2 0.0010330744553357363\n",
      "training 0.0004413641872815788 relative L2 0.0010330143850296736\n",
      "training 0.0004413127026055008 relative L2 0.0010329546639695764\n",
      "training 0.00044126115972176194 relative L2 0.00103289436083287\n",
      "training 0.00044120950042270124 relative L2 0.0010328344069421291\n",
      "training 0.0004411578120198101 relative L2 0.0010327741038054228\n",
      "training 0.00044110624003224075 relative L2 0.0010327138006687164\n",
      "training 0.0004410546680446714 relative L2 0.00103265349753201\n",
      "training 0.0004410029505379498 relative L2 0.0010325934272259474\n",
      "training 0.0004409514076542109 relative L2 0.0010325327748432755\n",
      "training 0.00044089980656281114 relative L2 0.0010324727045372128\n",
      "training 0.00044084805995225906 relative L2 0.0010324122849851847\n",
      "training 0.00044079648796468973 relative L2 0.0010323517490178347\n",
      "training 0.0004407448577694595 relative L2 0.0010322914458811283\n",
      "training 0.0004406931984703988 relative L2 0.0010322309099137783\n",
      "training 0.00044064148096367717 relative L2 0.0010321703739464283\n",
      "training 0.0004405897343531251 relative L2 0.0010321099543944001\n",
      "training 0.0004405381332617253 relative L2 0.001032049534842372\n",
      "training 0.0004404863575473428 relative L2 0.0010319891152903438\n",
      "training 0.00044043478555977345 relative L2 0.0010319285793229938\n",
      "training 0.0004403830098453909 relative L2 0.0010318680433556437\n",
      "training 0.0004403311759233475 relative L2 0.0010318075073882937\n",
      "training 0.00044027951662428677 relative L2 0.001031746855005622\n",
      "training 0.0004402278864290565 relative L2 0.0010316860862076283\n",
      "training 0.00044017608161084354 relative L2 0.0010316254338249564\n",
      "training 0.00044012407306581736 relative L2 0.0010315647814422846\n",
      "training 0.0004400722391437739 relative L2 0.001031503896228969\n",
      "training 0.0004400205216370523 relative L2 0.001031443360261619\n",
      "training 0.0004399687168188393 relative L2 0.0010313824750483036\n",
      "training 0.00043991676648147404 relative L2 0.0010313218226656318\n",
      "training 0.0004398651362862438 relative L2 0.0010312610538676381\n",
      "training 0.0004398131859488785 relative L2 0.0010312000522390008\n",
      "training 0.0004397612647153437 relative L2 0.0010311395162716508\n",
      "training 0.00043970943079330027 relative L2 0.0010310787474736571\n",
      "training 0.00043965750955976546 relative L2 0.0010310179786756635\n",
      "training 0.0004396056174300611 relative L2 0.0010309573262929916\n",
      "training 0.0004395538126118481 relative L2 0.0010308966739103198\n",
      "training 0.0004395017749629915 relative L2 0.0010308359051123261\n",
      "training 0.00043945005745626986 relative L2 0.0010307751363143325\n",
      "training 0.0004393979033920914 relative L2 0.0010307143675163388\n",
      "training 0.0004393460985738784 relative L2 0.0010306538315489888\n",
      "training 0.0004392940900288522 relative L2 0.0010305927135050297\n",
      "training 0.00043924208148382604 relative L2 0.0010305318282917142\n",
      "training 0.00043919016025029123 relative L2 0.0010304711759090424\n",
      "training 0.0004391381226014346 relative L2 0.0010304102906957269\n",
      "training 0.0004390861722640693 relative L2 0.0010303492890670896\n",
      "training 0.0004390342510305345 relative L2 0.0010302881710231304\n",
      "training 0.0004389820387586951 relative L2 0.0010302270529791713\n",
      "training 0.00043892997200600803 relative L2 0.001030166051350534\n",
      "training 0.0004388779343571514 relative L2 0.0010301050497218966\n",
      "training 0.0004388259840197861 relative L2 0.0010300440480932593\n",
      "training 0.000438773917267099 relative L2 0.0010299829300493002\n",
      "training 0.0004387218796182424 relative L2 0.0010299220448359847\n",
      "training 0.0004386696091387421 relative L2 0.0010298610432073474\n",
      "training 0.00043861762969754636 relative L2 0.00102980004157871\n",
      "training 0.0004385655338410288 relative L2 0.0010297391563653946\n",
      "training 0.0004385134088806808 relative L2 0.0010296778054907918\n",
      "training 0.00043846110929735005 relative L2 0.0010296166874468327\n",
      "training 0.00043840904254466295 relative L2 0.0010295556858181953\n",
      "training 0.00043835685937665403 relative L2 0.0010294945677742362\n",
      "training 0.000438304734416306 relative L2 0.0010294333333149552\n",
      "training 0.00043825243483297527 relative L2 0.0010293720988556743\n",
      "training 0.0004382003389764577 relative L2 0.0010293108643963933\n",
      "training 0.00043814803939312696 relative L2 0.0010292495135217905\n",
      "training 0.00043809579801745713 relative L2 0.0010291881626471877\n",
      "training 0.00043804358574561775 relative L2 0.0010291269281879067\n",
      "training 0.00043799125705845654 relative L2 0.001029065577313304\n",
      "training 0.0004379390738904476 relative L2 0.0010290044592693448\n",
      "training 0.00043788671609945595 relative L2 0.001028943108394742\n",
      "training 0.0004378343583084643 relative L2 0.001028881873935461\n",
      "training 0.0004377820005174726 relative L2 0.0010288205230608582\n",
      "training 0.0004377297009341419 relative L2 0.0010287594050168991\n",
      "training 0.00043767745955847204 relative L2 0.0010286984033882618\n",
      "training 0.0004376251017674804 relative L2 0.0010286369360983372\n",
      "training 0.0004375726857688278 relative L2 0.0010285755852237344\n",
      "training 0.00043752038618549705 relative L2 0.0010285143507644534\n",
      "training 0.00043746791197918355 relative L2 0.001028452767059207\n",
      "training 0.0004374154668767005 relative L2 0.0010283912997692823\n",
      "training 0.00043736305087804794 relative L2 0.0010283299488946795\n",
      "training 0.0004373104602564126 relative L2 0.0010282681323587894\n",
      "training 0.0004372580733615905 relative L2 0.001028206548653543\n",
      "training 0.00043720557005144656 relative L2 0.0010281450813636184\n",
      "training 0.000437153154052794 relative L2 0.001028083497658372\n",
      "training 0.0004371007380541414 relative L2 0.0010280220303684473\n",
      "training 0.00043704817653633654 relative L2 0.0010279604466632009\n",
      "training 0.0004369957314338535 relative L2 0.0010278988629579544\n",
      "training 0.0004369432863313705 relative L2 0.0010278373956680298\n",
      "training 0.0004368906666059047 relative L2 0.0010277756955474615\n",
      "training 0.0004368381341919303 relative L2 0.001027714111842215\n",
      "training 0.00043678542715497315 relative L2 0.0010276525281369686\n",
      "training 0.0004367328656371683 relative L2 0.001027591060847044\n",
      "training 0.0004366803332231939 relative L2 0.0010275293607264757\n",
      "training 0.0004366276552900672 relative L2 0.001027467893436551\n",
      "training 0.0004365751228760928 relative L2 0.001027406076900661\n",
      "training 0.00043652261956594884 relative L2 0.0010273442603647709\n",
      "training 0.0004364700289443135 relative L2 0.001027282327413559\n",
      "training 0.0004364173801150173 relative L2 0.001027220394462347\n",
      "training 0.00043636481859721243 relative L2 0.001027158461511135\n",
      "training 0.00043631208245642483 relative L2 0.0010270967613905668\n",
      "training 0.00043625960825011134 relative L2 0.0010270349448546767\n",
      "training 0.00043620693031698465 relative L2 0.0010269730119034648\n",
      "training 0.00043615425238385797 relative L2 0.0010269115446135402\n",
      "training 0.00043610160355456173 relative L2 0.00102684972807765\n",
      "training 0.00043604892562143505 relative L2 0.00102678791154176\n",
      "training 0.00043599624768830836 relative L2 0.0010267262114211917\n",
      "training 0.0004359434824436903 relative L2 0.0010266642784699798\n",
      "training 0.00043589071719907224 relative L2 0.0010266026947647333\n",
      "training 0.00043583824299275875 relative L2 0.0010265407618135214\n",
      "training 0.0004357852740213275 relative L2 0.0010264785960316658\n",
      "training 0.000435732479672879 relative L2 0.0010264168959110975\n",
      "training 0.0004356799472589046 relative L2 0.0010263546137139201\n",
      "training 0.0004356269200798124 relative L2 0.0010262925643473864\n",
      "training 0.0004355741839390248 relative L2 0.0010262307478114963\n",
      "training 0.0004355213895905763 relative L2 0.0010261686984449625\n",
      "training 0.0004354685079306364 relative L2 0.001026106532663107\n",
      "training 0.0004354158299975097 relative L2 0.0010260443668812513\n",
      "training 0.0004353629192337394 relative L2 0.0010259823175147176\n",
      "training 0.0004353100375737995 relative L2 0.0010259203845635056\n",
      "training 0.00043525733053684235 relative L2 0.001025858335196972\n",
      "training 0.0004352045070845634 relative L2 0.00102579640224576\n",
      "training 0.00043515159632079303 relative L2 0.0010257341200485826\n",
      "training 0.0004350985982455313 relative L2 0.001025671954266727\n",
      "training 0.0004350456583779305 relative L2 0.0010256096720695496\n",
      "training 0.00043499277671799064 relative L2 0.0010255477391183376\n",
      "training 0.0004349400114733726 relative L2 0.0010254854569211602\n",
      "training 0.0004348869842942804 relative L2 0.0010254231747239828\n",
      "training 0.00043483407353051007 relative L2 0.00102536054328084\n",
      "training 0.0004347811045590788 relative L2 0.0010252982610836625\n",
      "training 0.000434728164691478 relative L2 0.001025235978886485\n",
      "training 0.00043467513751238585 relative L2 0.0010251738131046295\n",
      "training 0.0004346221103332937 relative L2 0.0010251114144921303\n",
      "training 0.00043456919956952333 relative L2 0.0010250486666336656\n",
      "training 0.0004345162014942616 relative L2 0.0010249863844364882\n",
      "training 0.000434463145211339 relative L2 0.001024923869408667\n",
      "training 0.0004344102053437382 relative L2 0.001024861354380846\n",
      "training 0.00043435714906081557 relative L2 0.0010247990721836686\n",
      "training 0.0004343041800893843 relative L2 0.0010247365571558475\n",
      "training 0.00043425115291029215 relative L2 0.0010246739257127047\n",
      "training 0.0004341979220043868 relative L2 0.0010246114106848836\n",
      "training 0.00043414492392912507 relative L2 0.0010245488956570625\n",
      "training 0.0004340917803347111 relative L2 0.0010244861477985978\n",
      "training 0.00043403857853263617 relative L2 0.0010244236327707767\n",
      "training 0.0004339856677688658 relative L2 0.0010243611177429557\n",
      "training 0.00043393243686296046 relative L2 0.0010242982534691691\n",
      "training 0.00043387929326854646 relative L2 0.001024235738441348\n",
      "training 0.00043382623698562384 relative L2 0.0010241728741675615\n",
      "training 0.00043377294787205756 relative L2 0.0010241102427244186\n",
      "training 0.00043371968786232173 relative L2 0.0010240472620353103\n",
      "training 0.00043366660247556865 relative L2 0.0010239843977615237\n",
      "training 0.000433613226050511 relative L2 0.0010239219991490245\n",
      "training 0.0004335601697675884 relative L2 0.001023859134875238\n",
      "training 0.00043350696796551347 relative L2 0.001023796503432095\n",
      "training 0.00043345376616343856 relative L2 0.0010237336391583085\n",
      "training 0.00043340041884221137 relative L2 0.0010236706584692001\n",
      "training 0.00043334707152098417 relative L2 0.0010236076777800918\n",
      "training 0.0004332938406150788 relative L2 0.001023544929921627\n",
      "training 0.000433240580605343 relative L2 0.001023481716401875\n",
      "training 0.0004331871750764549 relative L2 0.0010234190849587321\n",
      "training 0.0004331340314820409 relative L2 0.0010233561042696238\n",
      "training 0.0004330806259531528 relative L2 0.00102329277433455\n",
      "training 0.0004330271331127733 relative L2 0.0010232299100607634\n",
      "training 0.00043297381489537656 relative L2 0.0010231671622022986\n",
      "training 0.00043292040936648846 relative L2 0.0010231041815131903\n",
      "training 0.0004328671784605831 relative L2 0.0010230414336547256\n",
      "training 0.00043281386024318635 relative L2 0.0010229784529656172\n",
      "training 0.0004327605420257896 relative L2 0.001022915355861187\n",
      "training 0.00043270716560073197 relative L2 0.0010228523751720786\n",
      "training 0.0004326537891756743 relative L2 0.0010227891616523266\n",
      "training 0.00043260035454295576 relative L2 0.0010227259481325746\n",
      "training 0.0004325469199102372 relative L2 0.0010226626181975007\n",
      "training 0.0004324934270698577 relative L2 0.0010225997539237142\n",
      "training 0.0004324400215409696 relative L2 0.001022536656819284\n",
      "training 0.00043238652870059013 relative L2 0.001022473443299532\n",
      "training 0.00043233291944488883 relative L2 0.00102241022977978\n",
      "training 0.0004322794557083398 relative L2 0.0010223470162600279\n",
      "training 0.0004322259337641299 relative L2 0.0010222839191555977\n",
      "training 0.00043217247002758086 relative L2 0.0010222207056358457\n",
      "training 0.0004321189771872014 relative L2 0.0010221576085314155\n",
      "training 0.000432065426139161 relative L2 0.0010220942785963416\n",
      "training 0.0004320119332987815 relative L2 0.0010220311814919114\n",
      "training 0.00043195829493924975 relative L2 0.0010219678515568376\n",
      "training 0.000431904656579718 relative L2 0.0010219044052064419\n",
      "training 0.000431851192843169 relative L2 0.001021841075271368\n",
      "training 0.00043179761269129813 relative L2 0.0010217777453362942\n",
      "training 0.0004317439452279359 relative L2 0.0010217141825705767\n",
      "training 0.00043169024866074324 relative L2 0.0010216509690508246\n",
      "training 0.0004316367267165333 relative L2 0.001021587522700429\n",
      "training 0.0004315831756684929 relative L2 0.0010215241927653551\n",
      "training 0.0004315294499974698 relative L2 0.0010214608628302813\n",
      "training 0.000431475811637938 relative L2 0.0010213974164798856\n",
      "training 0.00043142211507074535 relative L2 0.00102133397012949\n",
      "training 0.0004313682147767395 relative L2 0.001021270640194416\n",
      "training 0.0004313144600018859 relative L2 0.0010212070774286985\n",
      "training 0.0004312607052270323 relative L2 0.0010211438639089465\n",
      "training 0.0004312068922445178 relative L2 0.0010210806503891945\n",
      "training 0.00043115325388498604 relative L2 0.0010210168547928333\n",
      "training 0.0004310994117986411 relative L2 0.0010209535248577595\n",
      "training 0.0004310455988161266 relative L2 0.0010208901949226856\n",
      "training 0.00043099181493744254 relative L2 0.0010208261664956808\n",
      "training 0.00043093791464343667 relative L2 0.0010207630693912506\n",
      "training 0.00043088424718007445 relative L2 0.0010206991573795676\n",
      "training 0.00043083023047074676 relative L2 0.0010206353617832065\n",
      "training 0.0004307763301767409 relative L2 0.0010205719154328108\n",
      "training 0.0004307225754018873 relative L2 0.0010205082362517715\n",
      "training 0.0004306685586925596 relative L2 0.0010204440914094448\n",
      "training 0.00043061465839855373 relative L2 0.0010203806450590491\n",
      "training 0.00043056075810454786 relative L2 0.0010203163838014007\n",
      "training 0.0004305066540837288 relative L2 0.0010202527046203613\n",
      "training 0.00043045287020504475 relative L2 0.0010201891418546438\n",
      "training 0.00043039885349571705 relative L2 0.0010201252298429608\n",
      "training 0.00043034498230554163 relative L2 0.0010200614342465997\n",
      "training 0.00043029096559621394 relative L2 0.0010199974058195949\n",
      "training 0.0004302368906792253 relative L2 0.00101993337739259\n",
      "training 0.0004301829030737281 relative L2 0.001019869465380907\n",
      "training 0.00043012891546823084 relative L2 0.0010198054369539022\n",
      "training 0.0004300748696550727 relative L2 0.0010197415249422193\n",
      "training 0.00043002082384191453 relative L2 0.0010196773800998926\n",
      "training 0.00042996671982109547 relative L2 0.0010196133516728878\n",
      "training 0.00042991279042325914 relative L2 0.0010195495560765266\n",
      "training 0.00042985877371393144 relative L2 0.0010194854112342\n",
      "training 0.0004298046696931124 relative L2 0.0010194217320531607\n",
      "training 0.0004297505074646324 relative L2 0.001019357587210834\n",
      "training 0.0004296964034438133 relative L2 0.0010192934423685074\n",
      "training 0.00042964209569618106 relative L2 0.0010192292975261807\n",
      "training 0.0004295881080906838 relative L2 0.0010191649198532104\n",
      "training 0.000429533829446882 relative L2 0.0010191005421802402\n",
      "training 0.00042947957990691066 relative L2 0.0010190365137532353\n",
      "training 0.00042942544678226113 relative L2 0.0010189722524955869\n",
      "training 0.0004293713136576116 relative L2 0.0010189079912379384\n",
      "training 0.0004293171514291316 relative L2 0.0010188439628109336\n",
      "training 0.0004292629018891603 relative L2 0.001018779817968607\n",
      "training 0.00042920850683003664 relative L2 0.0010187155567109585\n",
      "training 0.0004291542572900653 relative L2 0.00101865129545331\n",
      "training 0.0004291000368539244 relative L2 0.0010185871506109834\n",
      "training 0.0004290456708986312 relative L2 0.001018522889353335\n",
      "training 0.0004289914504624903 relative L2 0.0010184585116803646\n",
      "training 0.0004289370554033667 relative L2 0.001018394366838038\n",
      "training 0.0004288827476557344 relative L2 0.0010183301055803895\n",
      "training 0.00042882849811576307 relative L2 0.0010182656114920974\n",
      "training 0.00042877401574514806 relative L2 0.001018201233819127\n",
      "training 0.0004287197662051767 relative L2 0.0010181368561461568\n",
      "training 0.000428665429353714 relative L2 0.001018072129227221\n",
      "training 0.000428610946983099 relative L2 0.0010180079843848944\n",
      "training 0.0004285566683392972 relative L2 0.0010179434902966022\n",
      "training 0.00042850218596868217 relative L2 0.0010178789962083101\n",
      "training 0.00042844770359806716 relative L2 0.0010178149677813053\n",
      "training 0.00042839330853894353 relative L2 0.0010177504736930132\n",
      "training 0.0004283387679606676 relative L2 0.001017685979604721\n",
      "training 0.00042828431469388306 relative L2 0.0010176216019317508\n",
      "training 0.00042822989053092897 relative L2 0.0010175569914281368\n",
      "training 0.0004281752335373312 relative L2 0.0010174927301704884\n",
      "training 0.0004281208966858685 relative L2 0.0010174281196668744\n",
      "training 0.0004280663270037621 relative L2 0.0010173635091632605\n",
      "training 0.0004280118446331471 relative L2 0.0010172987822443247\n",
      "training 0.000427957100328058 relative L2 0.001017233938910067\n",
      "training 0.0004279023560229689 relative L2 0.001017169444821775\n",
      "training 0.0004278477863408625 relative L2 0.0010171047179028392\n",
      "training 0.00042779321665875614 relative L2 0.0010170399909839034\n",
      "training 0.000427738472353667 relative L2 0.0010169751476496458\n",
      "training 0.0004276839317753911 relative L2 0.0010169101879000664\n",
      "training 0.0004276292456779629 relative L2 0.0010168456938117743\n",
      "training 0.00042757464689202607 relative L2 0.0010167809668928385\n",
      "training 0.00042752004810608923 relative L2 0.0010167158907279372\n",
      "training 0.0004274651873856783 relative L2 0.001016651396639645\n",
      "training 0.0004274106177035719 relative L2 0.0010165864368900657\n",
      "training 0.00042735584429465234 relative L2 0.0010165214771404862\n",
      "training 0.0004273009253665805 relative L2 0.0010164567502215505\n",
      "training 0.0004272462101653218 relative L2 0.001016391790471971\n",
      "training 0.0004271914658602327 relative L2 0.0010163269471377134\n",
      "training 0.0004271367215551436 relative L2 0.0010162618709728122\n",
      "training 0.0004270817735232413 relative L2 0.001016196678392589\n",
      "training 0.0004270269419066608 relative L2 0.0010161313693970442\n",
      "training 0.0004269721102900803 relative L2 0.001016066293232143\n",
      "training 0.0004269172204658389 relative L2 0.0010160012170672417\n",
      "training 0.0004268623306415975 relative L2 0.0010159361409023404\n",
      "training 0.00042680746992118657 relative L2 0.001015871181152761\n",
      "training 0.00042675246368162334 relative L2 0.001015805988572538\n",
      "training 0.0004266976029612124 relative L2 0.0010157411452382803\n",
      "training 0.00042664268403314054 relative L2 0.0010156759526580572\n",
      "training 0.00042658770689740777 relative L2 0.0010156109929084778\n",
      "training 0.000426532729761675 relative L2 0.0010155458003282547\n",
      "training 0.0004264776944182813 relative L2 0.0010154804913327098\n",
      "training 0.00042642271728254855 relative L2 0.0010154154151678085\n",
      "training 0.0004263677110429853 relative L2 0.00101534987334162\n",
      "training 0.0004263127048034221 relative L2 0.001015284564346075\n",
      "training 0.00042625769856385887 relative L2 0.0010152192553505301\n",
      "training 0.00042620260501280427 relative L2 0.0010151537135243416\n",
      "training 0.0004261473659425974 relative L2 0.0010150882881134748\n",
      "training 0.00042609230149537325 relative L2 0.0010150226298719645\n",
      "training 0.00042603706242516637 relative L2 0.0010149574372917414\n",
      "training 0.0004259818815626204 relative L2 0.0010148916626349092\n",
      "training 0.0004259268753230572 relative L2 0.001014826470054686\n",
      "training 0.0004258716944605112 relative L2 0.0010147609282284975\n",
      "training 0.0004258166009094566 relative L2 0.0010146958520635962\n",
      "training 0.00042576156556606293 relative L2 0.001014630077406764\n",
      "training 0.0004257060936652124 relative L2 0.0010145644191652536\n",
      "training 0.00042565108742564917 relative L2 0.001014498877339065\n",
      "training 0.0004255957610439509 relative L2 0.0010144332190975547\n",
      "training 0.00042554037645459175 relative L2 0.0010143679101020098\n",
      "training 0.00042548528290353715 relative L2 0.0010143021354451776\n",
      "training 0.0004254298401065171 relative L2 0.0010142364772036672\n",
      "training 0.0004253745428286493 relative L2 0.0010141710517928004\n",
      "training 0.00042531927465461195 relative L2 0.0010141049278900027\n",
      "training 0.0004252638027537614 relative L2 0.0010140393860638142\n",
      "training 0.0004252085054758936 relative L2 0.001013973611406982\n",
      "training 0.0004251530335750431 relative L2 0.001013907720334828\n",
      "training 0.0004250976489856839 relative L2 0.0010138419456779957\n",
      "training 0.00042504232260398567 relative L2 0.001013775821775198\n",
      "training 0.00042498670518398285 relative L2 0.0010137098142877221\n",
      "training 0.0004249311750754714 relative L2 0.001013643923215568\n",
      "training 0.00042487570317462087 relative L2 0.0010135777993127704\n",
      "training 0.00042482023127377033 relative L2 0.0010135119082406163\n",
      "training 0.0004247647011652589 relative L2 0.0010134459007531404\n",
      "training 0.00042470908374525607 relative L2 0.0010133798932656646\n",
      "training 0.00042465361184440553 relative L2 0.0010133137693628669\n",
      "training 0.0004245979944244027 relative L2 0.001013247761875391\n",
      "training 0.00042454246431589127 relative L2 0.0010131815215572715\n",
      "training 0.0004244868759997189 relative L2 0.001013115281239152\n",
      "training 0.000424430996645242 relative L2 0.0010130489245057106\n",
      "training 0.0004243752046022564 relative L2 0.001012982800602913\n",
      "training 0.00042431941255927086 relative L2 0.0010129164438694715\n",
      "training 0.0004242636787239462 relative L2 0.0010128504363819957\n",
      "training 0.00042420782847329974 relative L2 0.0010127841960638762\n",
      "training 0.00042415212374180555 relative L2 0.0010127178393304348\n",
      "training 0.00042409633169882 relative L2 0.001012651715427637\n",
      "training 0.0004240406269673258 relative L2 0.001012585242278874\n",
      "training 0.0004239846603013575 relative L2 0.001012518652714789\n",
      "training 0.0004239287518430501 relative L2 0.0010124522959813476\n",
      "training 0.0004238727851770818 relative L2 0.0010123855900019407\n",
      "training 0.00042381678940728307 relative L2 0.0010123191168531775\n",
      "training 0.0004237606772221625 relative L2 0.0010122524108737707\n",
      "training 0.00042370468145236373 relative L2 0.001012185588479042\n",
      "training 0.0004236484819557518 relative L2 0.0010121187660843134\n",
      "training 0.0004235923697706312 relative L2 0.0010120522929355502\n",
      "training 0.0004235361993778497 relative L2 0.001011985237710178\n",
      "training 0.0004234797670505941 relative L2 0.0010119180660694838\n",
      "training 0.00042342336382716894 relative L2 0.0010118510108441114\n",
      "training 0.0004233668150845915 relative L2 0.0010117838392034173\n",
      "training 0.0004233103827573359 relative L2 0.0010117166675627232\n",
      "training 0.00042325377580709755 relative L2 0.0010116493795067072\n",
      "training 0.000423196965130046 relative L2 0.0010115823242813349\n",
      "training 0.00042314035817980766 relative L2 0.0010115151526406407\n",
      "training 0.00042308386764489114 relative L2 0.0010114478645846248\n",
      "training 0.00042302702786400914 relative L2 0.001011380460113287\n",
      "training 0.0004229702753946185 relative L2 0.0010113129392266273\n",
      "training 0.0004229135811328888 relative L2 0.0010112450690940022\n",
      "training 0.00042285682866349816 relative L2 0.0010111776646226645\n",
      "training 0.0004227999015711248 relative L2 0.001011110027320683\n",
      "training 0.000422743265517056 relative L2 0.0010110423900187016\n",
      "training 0.00042268630932085216 relative L2 0.0010109746363013983\n",
      "training 0.0004226293822284788 relative L2 0.0010109066497534513\n",
      "training 0.00042257230961695313 relative L2 0.0010108387796208262\n",
      "training 0.0004225152952130884 relative L2 0.001010771025903523\n",
      "training 0.00042245848453603685 relative L2 0.001010703039355576\n",
      "training 0.0004224014119245112 relative L2 0.001010635169222951\n",
      "training 0.0004223445721436292 relative L2 0.0010105674155056477\n",
      "training 0.00042228755773976445 relative L2 0.0010104996617883444\n",
      "training 0.0004222307470627129 relative L2 0.0010104315588250756\n",
      "training 0.0004221736453473568 relative L2 0.0010103636886924505\n",
      "training 0.0004221165436320007 relative L2 0.0010102958185598254\n",
      "training 0.0004220596165396273 relative L2 0.0010102278320118785\n",
      "training 0.00042200248572044075 relative L2 0.0010101599618792534\n",
      "training 0.0004219453257974237 relative L2 0.001010092324577272\n",
      "training 0.00042188845691271126 relative L2 0.0010100244544446468\n",
      "training 0.00042183135519735515 relative L2 0.0010099568171426654\n",
      "training 0.0004217743990011513 relative L2 0.0010098887141793966\n",
      "training 0.0004217174428049475 relative L2 0.0010098210768774152\n",
      "training 0.00042166031198576093 relative L2 0.0010097529739141464\n",
      "training 0.00042160318116657436 relative L2 0.0010096849873661995\n",
      "training 0.00042154607945121825 relative L2 0.001009617350064218\n",
      "training 0.0004214890068396926 relative L2 0.0010095492471009493\n",
      "training 0.0004214319633319974 relative L2 0.0010094812605530024\n",
      "training 0.00042137480340898037 relative L2 0.0010094133904203773\n",
      "training 0.00042131778900511563 relative L2 0.0010093451710417867\n",
      "training 0.00042126065818592906 relative L2 0.0010092774173244834\n",
      "training 0.0004212035273667425 relative L2 0.0010092095471918583\n",
      "training 0.00042114645475521684 relative L2 0.0010091414442285895\n",
      "training 0.0004210894403513521 relative L2 0.0010090733412653208\n",
      "training 0.00042103230953216553 relative L2 0.0010090058203786612\n",
      "training 0.0004209752951283008 relative L2 0.001008937950246036\n",
      "training 0.0004209181061014533 relative L2 0.0010088701965287328\n",
      "training 0.00042086103348992765 relative L2 0.001008802093565464\n",
      "training 0.0004208038153592497 relative L2 0.0010087337577715516\n",
      "training 0.00042074653902091086 relative L2 0.0010086661204695702\n",
      "training 0.00042068937909789383 relative L2 0.0010085981339216232\n",
      "training 0.0004206322773825377 relative L2 0.0010085300309583545\n",
      "training 0.00042057529208250344 relative L2 0.0010084619279950857\n",
      "training 0.0004205182194709778 relative L2 0.001008393825031817\n",
      "training 0.00042046111775562167 relative L2 0.0010083260713145137\n",
      "training 0.000420404045144096 relative L2 0.001008257851935923\n",
      "training 0.0004203467979095876 relative L2 0.0010081895161420107\n",
      "training 0.00042028960888274014 relative L2 0.0010081216460093856\n",
      "training 0.00042023256537504494 relative L2 0.0010080535430461168\n",
      "training 0.00042017546365968883 relative L2 0.00100798555649817\n",
      "training 0.0004201183619443327 relative L2 0.0010079178027808666\n",
      "training 0.00042006149305962026 relative L2 0.0010078496998175979\n",
      "training 0.0004200045659672469 relative L2 0.0010077819461002946\n",
      "training 0.0004199475806672126 relative L2 0.001007713726721704\n",
      "training 0.0004198907408863306 relative L2 0.0010076456237584352\n",
      "training 0.00041983366827480495 relative L2 0.0010075776372104883\n",
      "training 0.00041977682849392295 relative L2 0.0010075099999085069\n",
      "training 0.0004197202215436846 relative L2 0.0010074418969452381\n",
      "training 0.00041966346907429397 relative L2 0.0010073741432279348\n",
      "training 0.00041960683302022517 relative L2 0.0010073058074340224\n",
      "training 0.00041954999323934317 relative L2 0.0010072382865473628\n",
      "training 0.0004194934736005962 relative L2 0.0010071705328300595\n",
      "training 0.0004194368375465274 relative L2 0.0010071025462821126\n",
      "training 0.0004193801723886281 relative L2 0.0010070347925648093\n",
      "training 0.00041932344902306795 relative L2 0.0010069668060168624\n",
      "training 0.0004192666383460164 relative L2 0.0010068990522995591\n",
      "training 0.000419210089603439 relative L2 0.0010068315314128995\n",
      "training 0.00041915354086086154 relative L2 0.0010067636612802744\n",
      "training 0.00041909696301445365 relative L2 0.0010066957911476493\n",
      "training 0.0004190402396488935 relative L2 0.001006628037430346\n",
      "training 0.0004189834580756724 relative L2 0.0010065598180517554\n",
      "training 0.0004189265600871295 relative L2 0.0010064915986731648\n",
      "training 0.000418869691202417 relative L2 0.0010064240777865052\n",
      "training 0.00041881296783685684 relative L2 0.0010063565568998456\n",
      "training 0.0004187563608866185 relative L2 0.0010062886867672205\n",
      "training 0.0004186997830402106 relative L2 0.001006221049465239\n",
      "training 0.00041864303057082 relative L2 0.0010061535285785794\n",
      "training 0.00041858648182824254 relative L2 0.001006085891276598\n",
      "training 0.0004185298166703433 relative L2 0.0010060183703899384\n",
      "training 0.00041847291868180037 relative L2 0.001005950616672635\n",
      "training 0.0004184161371085793 relative L2 0.0010058828629553318\n",
      "training 0.00041835944284684956 relative L2 0.0010058151092380285\n",
      "training 0.0004183025739621371 relative L2 0.001005747471936047\n",
      "training 0.0004182459961157292 relative L2 0.001005679601803422\n",
      "training 0.0004181892145425081 relative L2 0.0010056119645014405\n",
      "training 0.00041813254938460886 relative L2 0.0010055440943688154\n",
      "training 0.00041807565139606595 relative L2 0.0010054758749902248\n",
      "training 0.0004180188407190144 relative L2 0.0010054082376882434\n",
      "training 0.0004179620009381324 relative L2 0.0010053401347249746\n",
      "training 0.0004179052484687418 relative L2 0.0010052720317617059\n",
      "training 0.00041784849599935114 relative L2 0.0010052041616290808\n",
      "training 0.00041779151069931686 relative L2 0.0010051361750811338\n",
      "training 0.00041773467091843486 relative L2 0.0010050683049485087\n",
      "training 0.0004176779475528747 relative L2 0.0010050004348158836\n",
      "training 0.0004176210786681622 relative L2 0.0010049323318526149\n",
      "training 0.0004175642388872802 relative L2 0.0010048646945506334\n",
      "training 0.0004175074282102287 relative L2 0.0010047967080026865\n",
      "training 0.0004174505011178553 relative L2 0.0010047286050394177\n",
      "training 0.00041739363223314285 relative L2 0.0010046606184914708\n",
      "training 0.00041733679245226085 relative L2 0.0010045927483588457\n",
      "training 0.00041727995267137885 relative L2 0.0010045245289802551\n",
      "training 0.00041722296737134457 relative L2 0.0010044563096016645\n",
      "training 0.00041716592386364937 relative L2 0.0010043876245617867\n",
      "training 0.00041710870573297143 relative L2 0.0010043196380138397\n",
      "training 0.00041705186595208943 relative L2 0.0010042514186352491\n",
      "training 0.0004169949097558856 relative L2 0.001004182966426015\n",
      "training 0.0004169379244558513 relative L2 0.0010041150962933898\n",
      "training 0.0004168810264673084 relative L2 0.0010040468769147992\n",
      "training 0.00041682409937493503 relative L2 0.0010039790067821741\n",
      "training 0.00041676711407490075 relative L2 0.0010039107874035835\n",
      "training 0.0004167099541518837 relative L2 0.0010038423351943493\n",
      "training 0.0004166529106441885 relative L2 0.0010037742322310805\n",
      "training 0.00041659598355181515 relative L2 0.0010037057800218463\n",
      "training 0.0004165388527326286 relative L2 0.001003637327812612\n",
      "training 0.00041648169280961156 relative L2 0.0010035688756033778\n",
      "training 0.0004164247657172382 relative L2 0.0010035003069788218\n",
      "training 0.0004163677804172039 relative L2 0.0010034319711849093\n",
      "training 0.00041631064959801733 relative L2 0.0010033634025603533\n",
      "training 0.00041625366429798305 relative L2 0.0010032946011051536\n",
      "training 0.00041619641706347466 relative L2 0.001003226381726563\n",
      "training 0.0004161394026596099 relative L2 0.001003157696686685\n",
      "training 0.00041608233004808426 relative L2 0.0010030895937234163\n",
      "training 0.0004160253156442195 relative L2 0.001003021141514182\n",
      "training 0.0004159682721365243 relative L2 0.0010029524564743042\n",
      "training 0.000415910966694355 relative L2 0.0010028842370957136\n",
      "training 0.0004158538649789989 relative L2 0.0010028155520558357\n",
      "training 0.00041579679236747324 relative L2 0.0010027469834312797\n",
      "training 0.0004157395742367953 relative L2 0.0010026786476373672\n",
      "training 0.0004156824143137783 relative L2 0.0010026097297668457\n",
      "training 0.00041562525439076126 relative L2 0.0010025411611422896\n",
      "training 0.00041556815267540514 relative L2 0.0010024725925177336\n",
      "training 0.00041551096364855766 relative L2 0.0010024037910625339\n",
      "training 0.0004154537455178797 relative L2 0.0010023352224379778\n",
      "training 0.0004153965273872018 relative L2 0.0010022660717368126\n",
      "training 0.000415339192841202 relative L2 0.001002197153866291\n",
      "training 0.00041528174187988043 relative L2 0.0010021281195804477\n",
      "training 0.0004152245237492025 relative L2 0.0010020595509558916\n",
      "training 0.0004151672183070332 relative L2 0.0010019909823313355\n",
      "training 0.00041511000017635524 relative L2 0.001001922064460814\n",
      "training 0.0004150527820456773 relative L2 0.0010018532630056143\n",
      "training 0.0004149955348111689 relative L2 0.0010017843451350927\n",
      "training 0.00041493820026516914 relative L2 0.0010017160093411803\n",
      "training 0.0004148808657191694 relative L2 0.0010016468586400151\n",
      "training 0.0004148235311731696 relative L2 0.0010015784064307809\n",
      "training 0.00041476625483483076 relative L2 0.0010015089064836502\n",
      "training 0.0004147087747696787 relative L2 0.0010014402214437723\n",
      "training 0.0004146513238083571 relative L2 0.0010013713035732508\n",
      "training 0.0004145941056776792 relative L2 0.0010013022692874074\n",
      "training 0.00041453674202784896 relative L2 0.0010012335842475295\n",
      "training 0.00041447929106652737 relative L2 0.0010011642007157207\n",
      "training 0.0004144218983128667 relative L2 0.0010010956320911646\n",
      "training 0.0004143644473515451 relative L2 0.001001026015728712\n",
      "training 0.00041430711280554533 relative L2 0.0010009575635194778\n",
      "training 0.0004142497491557151 relative L2 0.0010008877143263817\n",
      "training 0.0004141923855058849 relative L2 0.001000820193439722\n",
      "training 0.00041413516737520695 relative L2 0.0010007498785853386\n",
      "training 0.00041407838580198586 relative L2 0.0010006881784647703\n",
      "training 0.00041402404895052314 relative L2 0.001000628573819995\n",
      "training 0.00041398039320483804 relative L2 0.0010006517404690385\n",
      "training 0.0004139888333156705 relative L2 0.0010009227553382516\n",
      "training 0.0004142476536799222 relative L2 0.0010027022799476981\n",
      "training 0.0004157008952461183 relative L2 0.0010109215509146452\n",
      "training 0.0004228566540405154 relative L2 0.0010508026462048292\n",
      "training 0.00045744565431959927 relative L2 0.0012213523732498288\n",
      "training 0.000622482446487993 relative L2 0.001835157279856503\n",
      "training 0.0014166778419166803 relative L2 0.0034470614045858383\n",
      "training 0.005031592678278685 relative L2 0.006755273789167404\n",
      "training 0.019342243671417236 relative L2 0.01044050045311451\n",
      "training 0.046232376247644424 relative L2 0.009352386929094791\n",
      "training 0.03708936274051666 relative L2 0.001002943841740489\n",
      "training 0.0004159137897659093 relative L2 0.008581123314797878\n",
      "training 0.031229063868522644 relative L2 0.00624676002189517\n",
      "training 0.016540298238396645 relative L2 0.0041810874827206135\n",
      "training 0.007403326220810413 relative L2 0.007560500875115395\n",
      "training 0.02424030378460884 relative L2 0.0010307609336450696\n",
      "training 0.0004401116166263819 relative L2 0.006742758676409721\n",
      "training 0.0192731786519289 relative L2 0.002061305334791541\n",
      "training 0.00179250817745924 relative L2 0.0056264870800077915\n",
      "training 0.01342009473592043 relative L2 0.003227249952033162\n",
      "training 0.0044059450738132 relative L2 0.004447853658348322\n",
      "training 0.008379029110074043 relative L2 0.0035727142821997404\n",
      "training 0.005405442323535681 relative L2 0.0034648505970835686\n",
      "training 0.00508350832387805 relative L2 0.003699677065014839\n",
      "training 0.005793112795799971 relative L2 0.0027994203846901655\n",
      "training 0.0033117716666311026 relative L2 0.0036013280041515827\n",
      "training 0.005492838099598885 relative L2 0.0022373972460627556\n",
      "training 0.0021137462463229895 relative L2 0.003428193973377347\n",
      "training 0.004971995018422604 relative L2 0.0019210218451917171\n",
      "training 0.0015534698031842709 relative L2 0.003222325351089239\n",
      "training 0.004395593423396349 relative L2 0.0016369130462408066\n",
      "training 0.0011264928616583347 relative L2 0.002996214898303151\n",
      "training 0.0037952098064124584 relative L2 0.0014864798868075013\n",
      "training 0.0009259169455617666 relative L2 0.0027872014325112104\n",
      "training 0.0032861700747162104 relative L2 0.0013393874978646636\n",
      "training 0.000750659906771034 relative L2 0.0025758189149200916\n",
      "training 0.0028022441547363997 relative L2 0.001259259064681828\n",
      "training 0.0006616930477321148 relative L2 0.002391457324847579\n",
      "training 0.0024166652001440525 relative L2 0.0011805967660620809\n",
      "training 0.0005808334099128842 relative L2 0.002211114391684532\n",
      "training 0.0020622513256967068 relative L2 0.001135158585384488\n",
      "training 0.000535916886292398 relative L2 0.0020556000526994467\n",
      "training 0.0017829921562224627 relative L2 0.0010969446739181876\n",
      "training 0.0004999914672225714 relative L2 0.0019064064836129546\n",
      "training 0.0015303659019991755 relative L2 0.0010753400856629014\n",
      "training 0.0004800315364263952 relative L2 0.0017766949022188783\n",
      "training 0.0013294180389493704 relative L2 0.001063443603925407\n",
      "training 0.00046924452180974185 relative L2 0.0016544251702725887\n",
      "training 0.0011499167885631323 relative L2 0.0010586315765976906\n",
      "training 0.0004650430055335164 relative L2 0.0015471847727894783\n",
      "training 0.0010056281462311745 relative L2 0.0010624619899317622\n",
      "training 0.00046828619088046253 relative L2 0.0014476838987320662\n",
      "training 0.0008780141361057758 relative L2 0.0010666752932593226\n",
      "training 0.0004723633755929768 relative L2 0.0013600687962025404\n",
      "training 0.0007747067720629275 relative L2 0.0010757001582533121\n",
      "training 0.00048020860413089395 relative L2 0.0012823689030483365\n",
      "training 0.000686658313497901 relative L2 0.0010818399023264647\n",
      "training 0.0004861986089963466 relative L2 0.0012145591899752617\n",
      "training 0.0006156282033771276 relative L2 0.0010888748802244663\n",
      "training 0.0004922191728837788 relative L2 0.0011580741265788674\n",
      "training 0.0005580612923949957 relative L2 0.0010915456805378199\n",
      "training 0.0004951316514052451 relative L2 0.0011102593271061778\n",
      "training 0.0005126221221871674 relative L2 0.0010934006422758102\n",
      "training 0.0004963513347320259 relative L2 0.0010737370466813445\n",
      "training 0.00047830099356360734 relative L2 0.001090788166038692\n",
      "training 0.0004943864187225699 relative L2 0.0010448185494169593\n",
      "training 0.00045266631059348583 relative L2 0.001086362637579441\n",
      "training 0.000489823694806546 relative L2 0.00102540897205472\n",
      "training 0.0004353416443336755 relative L2 0.0010787734063342214\n",
      "training 0.0004833053972106427 relative L2 0.0010119141079485416\n",
      "training 0.0004238683613948524 relative L2 0.0010700252605602145\n",
      "training 0.0004748837964143604 relative L2 0.0010046656243503094\n",
      "training 0.0004175446229055524 relative L2 0.00105938536580652\n",
      "training 0.0004656935343518853 relative L2 0.0010012127459049225\n",
      "training 0.00041465743561275303 relative L2 0.001049015554599464\n",
      "training 0.0004560215165838599 relative L2 0.0010006147203966975\n",
      "training 0.00041415044688619673 relative L2 0.001038089976646006\n",
      "training 0.0004467083199415356 relative L2 0.0010017239255830646\n",
      "training 0.00041503398097120225 relative L2 0.0010286406613886356\n",
      "training 0.0004380994359962642 relative L2 0.001003343379124999\n",
      "training 0.00041650325874798 relative L2 0.0010196289513260126\n",
      "training 0.0004305439651943743 relative L2 0.0010052957804873586\n",
      "training 0.0004180316173005849 relative L2 0.0010125442640855908\n",
      "training 0.0004241980204824358 relative L2 0.0010063769295811653\n",
      "training 0.00041909978608600795 relative L2 0.0010066659888252616\n",
      "training 0.000419345626141876 relative L2 0.0010072389850392938\n",
      "training 0.00041966346907429397 relative L2 0.0010026617674157023\n",
      "training 0.0004157823568675667 relative L2 0.0010069804266095161\n",
      "training 0.00041960974340327084 relative L2 0.00099971704185009\n",
      "training 0.00041337969014421105 relative L2 0.0010065118549391627\n",
      "training 0.00041902888915501535 relative L2 0.0009980935137718916\n",
      "training 0.0004119309887755662 relative L2 0.0010051517747342587\n",
      "training 0.00041803374188020825 relative L2 0.0009971875697374344\n",
      "training 0.0004111936141271144 relative L2 0.0010038787731900811\n",
      "training 0.00041678460547700524 relative L2 0.0009969102684408426\n",
      "training 0.0004109507135581225 relative L2 0.001002080854959786\n",
      "training 0.00041539676021784544 relative L2 0.0009969992097467184\n",
      "training 0.0004110008885618299 relative L2 0.0010006341617554426\n",
      "training 0.000414035253925249 relative L2 0.0009971546242013574\n",
      "training 0.0004111699527129531 relative L2 0.000999080017209053\n",
      "training 0.0004128236323595047 relative L2 0.0009974444983527064\n",
      "training 0.00041135548963211477 relative L2 0.0009979649912565947\n",
      "training 0.00041178701212629676 relative L2 0.0009974776767194271\n",
      "training 0.00041144812712445855 relative L2 0.0009969390230253339\n",
      "training 0.0004109850851818919 relative L2 0.0009975446155294776\n",
      "training 0.0004114299372304231 relative L2 0.0009963087504729629\n",
      "training 0.00041040239739231765 relative L2 0.000997271854430437\n",
      "training 0.0004112734168302268 relative L2 0.0009958258597180247\n",
      "training 0.000410023785661906 relative L2 0.0009970577666535974\n",
      "training 0.0004110184672754258 relative L2 0.0009955762652680278\n",
      "training 0.00040979948244057596 relative L2 0.000996603281237185\n",
      "training 0.0004107036511413753 relative L2 0.0009954312117770314\n",
      "training 0.00040967727545648813 relative L2 0.0009962607873603702\n",
      "training 0.0004103538522031158 relative L2 0.0009953348198905587\n",
      "training 0.0004096100456081331 relative L2 0.0009957971051335335\n",
      "training 0.0004100168007425964 relative L2 0.0009953011758625507\n",
      "training 0.00040956243174150586 relative L2 0.0009954776614904404\n",
      "training 0.00040970565169118345 relative L2 0.000995202804915607\n",
      "training 0.000409508851589635 relative L2 0.000995122012682259\n",
      "training 0.0004094414762221277 relative L2 0.0009951477404683828\n",
      "training 0.00040943350177258253 relative L2 0.0009948932565748692\n",
      "training 0.0004092262824997306 relative L2 0.0009949853410944343\n",
      "training 0.0004093318711966276 relative L2 0.0009946664795279503\n",
      "training 0.00040905302739702165 relative L2 0.0009948741644620895\n",
      "training 0.00040920759784057736 relative L2 0.0009945116471499205\n",
      "training 0.0004089173744432628 relative L2 0.0009946675272658467\n",
      "training 0.0004090669099241495 relative L2 0.000994380796328187\n",
      "training 0.00040880931192077696 relative L2 0.0009945111814886332\n",
      "training 0.00040890913805924356 relative L2 0.0009942620526999235\n",
      "training 0.00040871778037399054 relative L2 0.0009942941833287477\n",
      "training 0.00040875235572457314 relative L2 0.0009941683383658528\n",
      "training 0.0004086306144017726 relative L2 0.0009941362077370286\n",
      "training 0.00040860273293219507 relative L2 0.0009940407471731305\n",
      "training 0.0004085390246473253 relative L2 0.0009939587907865644\n",
      "training 0.0004084697866346687 relative L2 0.000993944238871336\n",
      "training 0.00040844467002898455 relative L2 0.000993821769952774\n",
      "training 0.00040834693936631083 relative L2 0.0009938018629327416\n",
      "training 0.000408343126764521 relative L2 0.000993680558167398\n",
      "training 0.00040823561721481383 relative L2 0.0009936925489455462\n",
      "training 0.00040823768358677626 relative L2 0.000993557507172227\n",
      "training 0.0004081327060703188 relative L2 0.000993542023934424\n",
      "training 0.0004081272054463625 relative L2 0.0009934426052495837\n",
      "training 0.0004080357903148979 relative L2 0.0009934216504916549\n",
      "training 0.00040801503928378224 relative L2 0.0009933224646374583\n",
      "training 0.00040794123196974397 relative L2 0.0009932745015248656\n",
      "training 0.00040790330967865884 relative L2 0.0009932145476341248\n",
      "training 0.00040784559678286314 relative L2 0.000993153778836131\n",
      "training 0.0004077953926753253 relative L2 0.0009930897504091263\n",
      "training 0.00040774940862320364 relative L2 0.0009930221131071448\n",
      "training 0.00040769187035039067 relative L2 0.0009929821826517582\n",
      "training 0.0004076528421137482 relative L2 0.0009929047664627433\n",
      "training 0.000407591083785519 relative L2 0.0009928552899509668\n",
      "training 0.0004075553733855486 relative L2 0.0009927843930199742\n",
      "training 0.00040749245090410113 relative L2 0.0009927445789799094\n",
      "training 0.0004074559547007084 relative L2 0.0009926672792062163\n",
      "training 0.00040739585529081523 relative L2 0.0009926154743880033\n",
      "training 0.00040735502261668444 relative L2 0.000992554472759366\n",
      "training 0.0004073001618962735 relative L2 0.000992500688880682\n",
      "training 0.0004072546144016087 relative L2 0.0009924356127157807\n",
      "training 0.00040720420656725764 relative L2 0.0009923778707161546\n",
      "training 0.00040715609793551266 relative L2 0.000992325134575367\n",
      "training 0.0004071088624186814 relative L2 0.000992263201624155\n",
      "training 0.00040705842548049986 relative L2 0.0009922052267938852\n",
      "training 0.00040701308171264827 relative L2 0.0009921451564878225\n",
      "training 0.00040696177165955305 relative L2 0.0009920946322381496\n",
      "training 0.0004069170681759715 relative L2 0.0009920306038111448\n",
      "training 0.0004068655543960631 relative L2 0.0009919742587953806\n",
      "training 0.00040682058897800744 relative L2 0.0009919150033965707\n",
      "training 0.00040676924982108176 relative L2 0.000991862267255783\n",
      "training 0.00040672378963790834 relative L2 0.0009917995193973184\n",
      "training 0.0004066731780767441 relative L2 0.0009917429415509105\n",
      "training 0.0004066273686476052 relative L2 0.0009916851995512843\n",
      "training 0.0004065775137860328 relative L2 0.0009916297858580947\n",
      "training 0.0004065306857228279 relative L2 0.0009915689006447792\n",
      "training 0.00040648150024935603 relative L2 0.0009915116243064404\n",
      "training 0.00040643446845933795 relative L2 0.0009914548136293888\n",
      "training 0.0004063857486471534 relative L2 0.000991398119367659\n",
      "training 0.000406338251195848 relative L2 0.0009913383983075619\n",
      "training 0.000406289822421968 relative L2 0.0009912808891385794\n",
      "training 0.00040624220855534077 relative L2 0.0009912237292155623\n",
      "training 0.0004061938088852912 relative L2 0.0009911666857078671\n",
      "training 0.0004061459330841899 relative L2 0.0009911070810630918\n",
      "training 0.0004060975625179708 relative L2 0.0009910492226481438\n",
      "training 0.00040604962850920856 relative L2 0.000990991946309805\n",
      "training 0.0004060011706314981 relative L2 0.0009909343207255006\n",
      "training 0.00040595317841507494 relative L2 0.0009908752981573343\n",
      "training 0.00040590474964119494 relative L2 0.0009908172069117427\n",
      "training 0.0004058567574247718 relative L2 0.0009907594649121165\n",
      "training 0.00040580821223556995 relative L2 0.0009907013736665249\n",
      "training 0.0004057600162923336 relative L2 0.0009906425839290023\n",
      "training 0.0004057115875184536 relative L2 0.0009905837941914797\n",
      "training 0.00040566312964074314 relative L2 0.0009905258193612099\n",
      "training 0.0004056145262438804 relative L2 0.0009904676117002964\n",
      "training 0.0004055659519508481 relative L2 0.000990408705547452\n",
      "training 0.00040551729034632444 relative L2 0.0009903499158099294\n",
      "training 0.00040546886157244444 relative L2 0.0009902918245643377\n",
      "training 0.0004054203745909035 relative L2 0.000990233849734068\n",
      "training 0.00040537200402468443 relative L2 0.000990174594335258\n",
      "training 0.00040532328421249986 relative L2 0.0009901158045977354\n",
      "training 0.00040527491364628077 relative L2 0.000990057596936822\n",
      "training 0.00040522657218389213 relative L2 0.0009899992728605866\n",
      "training 0.00040517820161767304 relative L2 0.0009899405995383859\n",
      "training 0.00040512983105145395 relative L2 0.0009898816933855414\n",
      "training 0.0004050815477967262 relative L2 0.0009898233693093061\n",
      "training 0.00040503329364582896 relative L2 0.0009897650452330709\n",
      "training 0.00040498512680642307 relative L2 0.0009897067211568356\n",
      "training 0.000404937076382339 relative L2 0.0009896480478346348\n",
      "training 0.000404888967750594 relative L2 0.0009895897237583995\n",
      "training 0.0004048410046380013 relative L2 0.0009895317489281297\n",
      "training 0.00040479295421391726 relative L2 0.0009894735412672162\n",
      "training 0.00040474504930898547 relative L2 0.000989415100775659\n",
      "training 0.0004046972608193755 relative L2 0.0009893571259453893\n",
      "training 0.0004046494432259351 relative L2 0.0009892990346997976\n",
      "training 0.00040460159652866423 relative L2 0.0009892405942082405\n",
      "training 0.00040455374983139336 relative L2 0.0009891822701320052\n",
      "training 0.00040450599044561386 relative L2 0.0009891241788864136\n",
      "training 0.00040445837657898664 relative L2 0.0009890665533021092\n",
      "training 0.0004044108500238508 relative L2 0.0009890086948871613\n",
      "training 0.00040436338167637587 relative L2 0.000988950952887535\n",
      "training 0.0004043160006403923 relative L2 0.0009888929780572653\n",
      "training 0.00040426853229291737 relative L2 0.0009888355853036046\n",
      "training 0.00040422106394544244 relative L2 0.0009887777268886566\n",
      "training 0.00040417371201328933 relative L2 0.0009887199848890305\n",
      "training 0.0004041265056002885 relative L2 0.0009886623593047261\n",
      "training 0.0004040792118757963 relative L2 0.000988605315797031\n",
      "training 0.0004040321218781173 relative L2 0.0009885478066280484\n",
      "training 0.00040398500277660787 relative L2 0.000988490297459066\n",
      "training 0.0004039379127789289 relative L2 0.0009884327882900834\n",
      "training 0.00040389091009274125 relative L2 0.0009883757447823882\n",
      "training 0.00040384382009506226 relative L2 0.0009883181191980839\n",
      "training 0.00040379667188972235 relative L2 0.0009882607264444232\n",
      "training 0.0004037496983073652 relative L2 0.0009882035665214062\n",
      "training 0.000403702724725008 relative L2 0.0009881464065983891\n",
      "training 0.0004036559257656336 relative L2 0.0009880891302600503\n",
      "training 0.0004036090394947678 relative L2 0.000988032086752355\n",
      "training 0.00040356229874305427 relative L2 0.00098797504324466\n",
      "training 0.00040351541247218847 relative L2 0.0009879181161522865\n",
      "training 0.00040346861351281404 relative L2 0.0009878610726445913\n",
      "training 0.0004034218436572701 relative L2 0.0009878040291368961\n",
      "training 0.000403375132009387 relative L2 0.0009877472184598446\n",
      "training 0.0004033283912576735 relative L2 0.0009876905241981149\n",
      "training 0.0004032817669212818 relative L2 0.0009876337135210633\n",
      "training 0.00040323511348105967 relative L2 0.0009875769028440118\n",
      "training 0.0004031886055599898 relative L2 0.0009875200921669602\n",
      "training 0.0004031419230159372 relative L2 0.0009874631650745869\n",
      "training 0.0004030951240565628 relative L2 0.0009874062379822135\n",
      "training 0.00040304847061634064 relative L2 0.0009873493108898401\n",
      "training 0.00040300184627994895 relative L2 0.0009872925002127886\n",
      "training 0.00040295530925504863 relative L2 0.0009872361551970243\n",
      "training 0.0004029088595416397 relative L2 0.0009871794609352946\n",
      "training 0.00040286240982823074 relative L2 0.000987122766673565\n",
      "training 0.0004028159601148218 relative L2 0.0009870664216578007\n",
      "training 0.00040276945219375193 relative L2 0.0009870098438113928\n",
      "training 0.000402723002480343 relative L2 0.000986953265964985\n",
      "training 0.00040267661097459495 relative L2 0.0009868965717032552\n",
      "training 0.000402630161261186 relative L2 0.0009868398774415255\n",
      "training 0.00040258359513245523 relative L2 0.0009867834160104394\n",
      "training 0.00040253723273053765 relative L2 0.0009867269545793533\n",
      "training 0.0004024908412247896 relative L2 0.0009866702603176236\n",
      "training 0.00040244447882287204 relative L2 0.0009866136824712157\n",
      "training 0.0004023980291094631 relative L2 0.0009865571046248078\n",
      "training 0.00040235163760371506 relative L2 0.0009865005267784\n",
      "training 0.00040230521699413657 relative L2 0.0009864438325166702\n",
      "training 0.00040225873817689717 relative L2 0.0009863873710855842\n",
      "training 0.0004022125212941319 relative L2 0.000986330909654498\n",
      "training 0.0004021661006845534 relative L2 0.0009862742153927684\n",
      "training 0.0004021198255941272 relative L2 0.0009862177539616823\n",
      "training 0.00040207349229604006 relative L2 0.0009861611761152744\n",
      "training 0.0004020271298941225 relative L2 0.0009861048310995102\n",
      "training 0.00040198079659603536 relative L2 0.000986048486083746\n",
      "training 0.0004019345506094396 relative L2 0.0009859921410679817\n",
      "training 0.00040188818820752203 relative L2 0.0009859355632215738\n",
      "training 0.000401841796701774 relative L2 0.0009858793346211314\n",
      "training 0.0004017956380266696 relative L2 0.0009858227567747235\n",
      "training 0.00040174933383241296 relative L2 0.0009857662953436375\n",
      "training 0.0004017030296381563 relative L2 0.0009857099503278732\n",
      "training 0.0004016567545477301 relative L2 0.0009856532560661435\n",
      "training 0.0004016103921458125 relative L2 0.0009855967946350574\n",
      "training 0.0004015640588477254 relative L2 0.000985539983958006\n",
      "training 0.0004015176382381469 relative L2 0.0009854836389422417\n",
      "training 0.0004014714795630425 relative L2 0.0009854270610958338\n",
      "training 0.00040142532088793814 relative L2 0.0009853705996647477\n",
      "training 0.00040137895848602057 relative L2 0.000985313905403018\n",
      "training 0.00040133262518793344 relative L2 0.0009852572111412883\n",
      "training 0.0004012862918898463 relative L2 0.000985200866125524\n",
      "training 0.00040124007500708103 relative L2 0.0009851442882791162\n",
      "training 0.00040119377081282437 relative L2 0.0009850877104327083\n",
      "training 0.0004011474084109068 relative L2 0.0009850312490016222\n",
      "training 0.00040110110421665013 relative L2 0.0009849747875705361\n",
      "training 0.0004010548582300544 relative L2 0.0009849185589700937\n",
      "training 0.00040100869955495 relative L2 0.0009848619811236858\n",
      "training 0.0004009624244645238 relative L2 0.0009848056361079216\n",
      "training 0.0004009162657894194 relative L2 0.0009847492910921574\n",
      "training 0.0004008697287645191 relative L2 0.0009846927132457495\n",
      "training 0.00040082348277792335 relative L2 0.0009846363682299852\n",
      "training 0.00040077720768749714 relative L2 0.0009845801396295428\n",
      "training 0.00040073087438941 relative L2 0.0009845236781984568\n",
      "training 0.00040068457019515336 relative L2 0.0009844673331826925\n",
      "training 0.00040063829510472715 relative L2 0.0009844106389209628\n",
      "training 0.00040059202001430094 relative L2 0.0009843541774898767\n",
      "training 0.0004005457158200443 relative L2 0.0009842977160587907\n",
      "training 0.00040049944072961807 relative L2 0.0009842412546277046\n",
      "training 0.0004004531365353614 relative L2 0.000984184560365975\n",
      "training 0.00040040683234110475 relative L2 0.0009841277496889234\n",
      "training 0.00040036041173152626 relative L2 0.0009840714046731591\n",
      "training 0.0004003140493296087 relative L2 0.0009840148268267512\n",
      "training 0.000400267745135352 relative L2 0.0009839582489803433\n",
      "training 0.0004002214700449258 relative L2 0.000983901903964579\n",
      "training 0.00040017516585066915 relative L2 0.0009838452097028494\n",
      "training 0.00040012868703342974 relative L2 0.0009837885154411197\n",
      "training 0.00040008212090469897 relative L2 0.00098373182117939\n",
      "training 0.0004000357585027814 relative L2 0.0009836753597483039\n",
      "training 0.00039998951251618564 relative L2 0.0009836190147325397\n",
      "training 0.00039994315011426806 relative L2 0.0009835625533014536\n",
      "training 0.00039989675860852003 relative L2 0.000983505742624402\n",
      "training 0.0003998503088951111 relative L2 0.0009834489319473505\n",
      "training 0.0003998038300778717 relative L2 0.0009833925869315863\n",
      "training 0.0003997574094682932 relative L2 0.0009833358926698565\n",
      "training 0.00039971090154722333 relative L2 0.000983279081992805\n",
      "training 0.0003996644227299839 relative L2 0.0009832225041463971\n",
      "training 0.000399617973016575 relative L2 0.0009831658098846674\n",
      "training 0.00039957152330316603 relative L2 0.0009831091156229377\n",
      "training 0.00039952521910890937 relative L2 0.000983052421361208\n",
      "training 0.0003994787984993309 relative L2 0.0009829953778535128\n",
      "training 0.00039943226147443056 relative L2 0.000982938683591783\n",
      "training 0.0003993857535533607 relative L2 0.000982882222160697\n",
      "training 0.00039933936204761267 relative L2 0.0009828254114836454\n",
      "training 0.0003992928541265428 relative L2 0.000982768600806594\n",
      "training 0.0003992463171016425 relative L2 0.0009827117901295424\n",
      "training 0.000399199896492064 relative L2 0.0009826549794524908\n",
      "training 0.0003991534758824855 relative L2 0.0009825981687754393\n",
      "training 0.0003991068806499243 relative L2 0.0009825413580983877\n",
      "training 0.0003990603145211935 relative L2 0.000982484663836658\n",
      "training 0.00039901374839246273 relative L2 0.0009824278531596065\n",
      "training 0.0003989672986790538 relative L2 0.0009823708096519113\n",
      "training 0.00039892076165415347 relative L2 0.0009823138825595379\n",
      "training 0.00039887416642159224 relative L2 0.0009822574211284518\n",
      "training 0.0003988276293966919 relative L2 0.000982200726866722\n",
      "training 0.00039878126699477434 relative L2 0.000982143683359027\n",
      "training 0.00039873470086604357 relative L2 0.0009820869890972972\n",
      "training 0.0003986881929449737 relative L2 0.0009820300620049238\n",
      "training 0.00039864153950475156 relative L2 0.000981972785666585\n",
      "training 0.00039859485696069896 relative L2 0.0009819158585742116\n",
      "training 0.0003985483490396291 relative L2 0.0009818591643124819\n",
      "training 0.00039850184111855924 relative L2 0.0009818023536354303\n",
      "training 0.0003984553040936589 relative L2 0.0009817457757890224\n",
      "training 0.00039840879617258906 relative L2 0.0009816884994506836\n",
      "training 0.00039836211362853646 relative L2 0.000981631688773632\n",
      "training 0.00039831557660363615 relative L2 0.0009815748780965805\n",
      "training 0.0003982690686825663 relative L2 0.0009815178345888853\n",
      "training 0.00039822232793085277 relative L2 0.0009814607910811901\n",
      "training 0.00039817564538680017 relative L2 0.000981403747573495\n",
      "training 0.0003981290792580694 relative L2 0.0009813469368964434\n",
      "training 0.00039808257133699954 relative L2 0.0009812903590500355\n",
      "training 0.0003980361798312515 relative L2 0.0009812331991270185\n",
      "training 0.00039798961370252073 relative L2 0.000981176272034645\n",
      "training 0.0003979429602622986 relative L2 0.000981119112111628\n",
      "training 0.0003978962486144155 relative L2 0.0009810620686039329\n",
      "training 0.0003978497115895152 relative L2 0.0009810050250962377\n",
      "training 0.000397803116356954 relative L2 0.0009809478651732206\n",
      "training 0.00039775634650141 relative L2 0.000980891054496169\n",
      "training 0.0003977098094765097 relative L2 0.000980834010988474\n",
      "training 0.00039766321424394846 relative L2 0.0009807768510654569\n",
      "training 0.0003976165608037263 relative L2 0.0009807199239730835\n",
      "training 0.00039757011109031737 relative L2 0.0009806629968807101\n",
      "training 0.00039752337033860385 relative L2 0.000980605836957693\n",
      "training 0.0003974768042098731 relative L2 0.000980548677034676\n",
      "training 0.0003974301798734814 relative L2 0.0009804914006963372\n",
      "training 0.0003973834391217679 relative L2 0.0009804347064346075\n",
      "training 0.00039733690209686756 relative L2 0.0009803775465115905\n",
      "training 0.00039729042327962816 relative L2 0.000980320037342608\n",
      "training 0.0003972437116317451 relative L2 0.000980262877419591\n",
      "training 0.00039719726191833615 relative L2 0.000980205717496574\n",
      "training 0.0003971504920627922 relative L2 0.0009801483247429132\n",
      "training 0.0003971038095187396 relative L2 0.0009800910484045744\n",
      "training 0.0003970570396631956 relative L2 0.0009800337720662355\n",
      "training 0.00039701038622297347 relative L2 0.0009799768449738622\n",
      "training 0.0003969638200942427 relative L2 0.0009799196850508451\n",
      "training 0.0003969172539655119 relative L2 0.0009798621758818626\n",
      "training 0.00039687048410996795 relative L2 0.0009798050159588456\n",
      "training 0.00039682385977357626 relative L2 0.0009797478560358286\n",
      "training 0.00039677717722952366 relative L2 0.0009796905796974897\n",
      "training 0.0003967306111007929 relative L2 0.000979633186943829\n",
      "training 0.000396683783037588 relative L2 0.0009795757941901684\n",
      "training 0.0003966371004935354 relative L2 0.0009795187506824732\n",
      "training 0.0003965904761571437 relative L2 0.0009794615907594562\n",
      "training 0.00039654402644373477 relative L2 0.0009794043144211173\n",
      "training 0.0003964973730035126 relative L2 0.0009793469216674566\n",
      "training 0.0003964506322517991 relative L2 0.0009792891796678305\n",
      "training 0.0003964037459809333 relative L2 0.000979232252575457\n",
      "training 0.0003963573253713548 relative L2 0.0009791746269911528\n",
      "training 0.0003963106428273022 relative L2 0.000979117234237492\n",
      "training 0.0003962639602832496 relative L2 0.0009790604235604405\n",
      "training 0.00039621724863536656 relative L2 0.0009790031472221017\n",
      "training 0.0003961705951951444 relative L2 0.000978945754468441\n",
      "training 0.0003961239126510918 relative L2 0.0009788883617147803\n",
      "training 0.0003960772883147001 relative L2 0.0009788312017917633\n",
      "training 0.00039603040204383433 relative L2 0.0009787736926227808\n",
      "training 0.0003959838068112731 relative L2 0.0009787160670384765\n",
      "training 0.0003959370369557291 relative L2 0.0009786589071154594\n",
      "training 0.0003958902962040156 relative L2 0.0009786018636077642\n",
      "training 0.0003958437009714544 relative L2 0.0009785445872694254\n",
      "training 0.0003957970184274018 relative L2 0.0009784873109310865\n",
      "training 0.00039575021946802735 relative L2 0.0009784293361008167\n",
      "training 0.0003957033040933311 relative L2 0.0009783718269318342\n",
      "training 0.0003956564178224653 relative L2 0.0009783142013475299\n",
      "training 0.00039560964796692133 relative L2 0.0009782564593479037\n",
      "training 0.00039556287811137736 relative L2 0.0009781989501789212\n",
      "training 0.0003955160500481725 relative L2 0.0009781416738405824\n",
      "training 0.0003954692801926285 relative L2 0.0009780842810869217\n",
      "training 0.00039542262675240636 relative L2 0.000978026888333261\n",
      "training 0.0003953756531700492 relative L2 0.0009779692627489567\n",
      "training 0.00039532894152216613 relative L2 0.0009779116371646523\n",
      "training 0.0003952819388359785 relative L2 0.0009778538951650262\n",
      "training 0.0003952351398766041 relative L2 0.0009777961531654\n",
      "training 0.00039518828270956874 relative L2 0.0009777386439964175\n",
      "training 0.0003951415419578552 relative L2 0.000977681134827435\n",
      "training 0.0003950947429984808 relative L2 0.0009776236256584525\n",
      "training 0.0003950481186620891 relative L2 0.00097756611648947\n",
      "training 0.00039500120328739285 relative L2 0.0009775083744898438\n",
      "training 0.00039495437522418797 relative L2 0.0009774507489055395\n",
      "training 0.0003949074598494917 relative L2 0.0009773933561518788\n",
      "training 0.00039486068999394774 relative L2 0.0009773354977369308\n",
      "training 0.0003948138328269124 relative L2 0.0009772778721526265\n",
      "training 0.00039476697565987706 relative L2 0.0009772202465683222\n",
      "training 0.0003947201475966722 relative L2 0.0009771626209840178\n",
      "training 0.00039467329042963684 relative L2 0.0009771051118150353\n",
      "training 0.0003946263750549406 relative L2 0.0009770476026460528\n",
      "training 0.0003945796051993966 relative L2 0.0009769900934770703\n",
      "training 0.000394532602513209 relative L2 0.0009769322350621223\n",
      "training 0.0003944856580346823 relative L2 0.0009768744930624962\n",
      "training 0.0003944386844523251 relative L2 0.0009768168674781919\n",
      "training 0.00039439176907762885 relative L2 0.0009767592418938875\n",
      "training 0.0003943449992220849 relative L2 0.0009767016163095832\n",
      "training 0.00039429799653589725 relative L2 0.0009766437578946352\n",
      "training 0.000394251081161201 relative L2 0.0009765861323103309\n",
      "training 0.0003942042530979961 relative L2 0.0009765285067260265\n",
      "training 0.00039415713399648666 relative L2 0.000976470357272774\n",
      "training 0.0003941101022064686 relative L2 0.0009764125570654869\n",
      "training 0.00039406309952028096 relative L2 0.0009763548150658607\n",
      "training 0.00039401621324941516 relative L2 0.0009762973641045392\n",
      "training 0.00039396921056322753 relative L2 0.0009762393892742693\n",
      "training 0.0003939222951885313 relative L2 0.0009761815890669823\n",
      "training 0.0003938752633985132 relative L2 0.000976123905275017\n",
      "training 0.0003938281151931733 relative L2 0.0009760662214830518\n",
      "training 0.0003937809669878334 relative L2 0.0009760083630681038\n",
      "training 0.00039373396430164576 relative L2 0.0009759502136148512\n",
      "training 0.00039368681609630585 relative L2 0.0009758925880305469\n",
      "training 0.0003936398134101182 relative L2 0.0009758348460309207\n",
      "training 0.0003935927525162697 relative L2 0.0009757770458236337\n",
      "training 0.0003935458662454039 relative L2 0.0009757190709933639\n",
      "training 0.00039349880535155535 relative L2 0.0009756612125784159\n",
      "training 0.00039345171535387635 relative L2 0.0009756028885021806\n",
      "training 0.0003934044798370451 relative L2 0.000975544739048928\n",
      "training 0.00039335741894319654 relative L2 0.0009754866478033364\n",
      "training 0.00039331038715317845 relative L2 0.0009754287311807275\n",
      "training 0.00039326323894783854 relative L2 0.0009753706399351358\n",
      "training 0.0003932161198463291 relative L2 0.0009753124904818833\n",
      "training 0.00039316900074481964 relative L2 0.0009752544574439526\n",
      "training 0.0003931219398509711 relative L2 0.0009751964244060218\n",
      "training 0.00039307476254180074 relative L2 0.0009751381585374475\n",
      "training 0.00039302741060964763 relative L2 0.0009750800090841949\n",
      "training 0.0003929803497157991 relative L2 0.0009750219760462642\n",
      "training 0.0003929332015104592 relative L2 0.0009749640012159944\n",
      "training 0.0003928859659936279 relative L2 0.0009749060263857245\n",
      "training 0.00039283884689211845 relative L2 0.0009748479933477938\n",
      "training 0.0003927916695829481 relative L2 0.0009747899021022022\n",
      "training 0.0003927445213776082 relative L2 0.0009747316944412887\n",
      "training 0.0003926972858607769 relative L2 0.0009746734285727143\n",
      "training 0.00039264981751330197 relative L2 0.0009746146970428526\n",
      "training 0.0003926023782696575 relative L2 0.0009745564311742783\n",
      "training 0.00039255511364899576 relative L2 0.0009744982235133648\n",
      "training 0.000392507849028334 relative L2 0.0009744399576447904\n",
      "training 0.0003924607008229941 relative L2 0.0009743818081915379\n",
      "training 0.0003924135526176542 relative L2 0.0009743237751536071\n",
      "training 0.000392366258893162 relative L2 0.0009742652764543891\n",
      "training 0.00039231902337633073 relative L2 0.0009742067777551711\n",
      "training 0.00039227152592502534 relative L2 0.0009741485118865967\n",
      "training 0.0003922242613043636 relative L2 0.0009740898385643959\n",
      "training 0.0003921768511645496 relative L2 0.0009740313398651779\n",
      "training 0.00039212944102473557 relative L2 0.0009739731904119253\n",
      "training 0.00039208223461173475 relative L2 0.00097391486633569\n",
      "training 0.0003920350573025644 relative L2 0.0009738565422594547\n",
      "training 0.0003919877053704113 relative L2 0.0009737977525219321\n",
      "training 0.00039194029523059726 relative L2 0.0009737392538227141\n",
      "training 0.0003918927686754614 relative L2 0.0009736806387081742\n",
      "training 0.00039184538763947785 relative L2 0.0009736221400089562\n",
      "training 0.00039179797749966383 relative L2 0.0009735634666867554\n",
      "training 0.00039175048004835844 relative L2 0.0009735049097798765\n",
      "training 0.0003917030990123749 relative L2 0.0009734463528729975\n",
      "training 0.00039165568887256086 relative L2 0.0009733880870044231\n",
      "training 0.0003916083078365773 relative L2 0.0009733294136822224\n",
      "training 0.00039156072307378054 relative L2 0.0009732706239446998\n",
      "training 0.00039151308010332286 relative L2 0.0009732120670378208\n",
      "training 0.0003914657572750002 relative L2 0.0009731536265462637\n",
      "training 0.00039141823071986437 relative L2 0.0009730951278470457\n",
      "training 0.00039137076237238944 relative L2 0.0009730365127325058\n",
      "training 0.0003913231485057622 relative L2 0.0009729778976179659\n",
      "training 0.00039127576746977866 relative L2 0.0009729196899570525\n",
      "training 0.00039122847374528646 relative L2 0.0009728610166348517\n",
      "training 0.000391181034501642 relative L2 0.0009728022268973291\n",
      "training 0.00039113336242735386 relative L2 0.0009727429714985192\n",
      "training 0.00039108560304157436 relative L2 0.0009726839489303529\n",
      "training 0.0003910381346940994 relative L2 0.0009726254502311349\n",
      "training 0.00039099054993130267 relative L2 0.0009725667187012732\n",
      "training 0.0003909431106876582 relative L2 0.0009725079871714115\n",
      "training 0.0003908956714440137 relative L2 0.0009724491974338889\n",
      "training 0.00039084808668121696 relative L2 0.0009723906987346709\n",
      "training 0.00039080053102225065 relative L2 0.0009723316179588437\n",
      "training 0.000390752888051793 relative L2 0.0009722727700136602\n",
      "training 0.0003907052450813353 relative L2 0.0009722139802761376\n",
      "training 0.00039065760211087763 relative L2 0.000972155190538615\n",
      "training 0.0003906099882442504 relative L2 0.0009720962261781096\n",
      "training 0.00039056246168911457 relative L2 0.0009720376692712307\n",
      "training 0.0003905149642378092 relative L2 0.0009719788795337081\n",
      "training 0.0003904675249941647 relative L2 0.0009719202062115073\n",
      "training 0.0003904199111275375 relative L2 0.0009718610672280192\n",
      "training 0.000390372151741758 relative L2 0.0009718021610751748\n",
      "training 0.00039032453787513077 relative L2 0.0009717433131299913\n",
      "training 0.00039027692400850356 relative L2 0.0009716844651848078\n",
      "training 0.0003902292519342154 relative L2 0.0009716252097859979\n",
      "training 0.00039018140523694456 relative L2 0.0009715665364637971\n",
      "training 0.0003901338786818087 relative L2 0.0009715072810649872\n",
      "training 0.00039008623571135104 relative L2 0.0009714484331198037\n",
      "training 0.00039003859274089336 relative L2 0.0009713892941363156\n",
      "training 0.00038999083335511386 relative L2 0.0009713301551528275\n",
      "training 0.000389942986657843 relative L2 0.0009712710743770003\n",
      "training 0.00038989540189504623 relative L2 0.0009712119353935122\n",
      "training 0.00038984755519777536 relative L2 0.0009711527382023633\n",
      "training 0.00038979988312348723 relative L2 0.0009710938320495188\n",
      "training 0.00038975226925686 relative L2 0.0009710348676890135\n",
      "training 0.00038970468449406326 relative L2 0.00097097601974383\n",
      "training 0.0003896570415236056 relative L2 0.0009709164150990546\n",
      "training 0.0003896089910995215 relative L2 0.0009708572179079056\n",
      "training 0.00038956114440225065 relative L2 0.0009707980789244175\n",
      "training 0.0003895134141203016 relative L2 0.0009707387653179467\n",
      "training 0.00038946562563069165 relative L2 0.0009706795099191368\n",
      "training 0.0003894177789334208 relative L2 0.0009706205455586314\n",
      "training 0.00038937010685913265 relative L2 0.0009705612901598215\n",
      "training 0.0003893224347848445 relative L2 0.0009705020929686725\n",
      "training 0.0003892746171914041 relative L2 0.0009704428375698626\n",
      "training 0.00038922668318264186 relative L2 0.000970383349340409\n",
      "training 0.0003891787491738796 relative L2 0.0009703239193186164\n",
      "training 0.0003891309315804392 relative L2 0.0009702648967504501\n",
      "training 0.00038908308488316834 relative L2 0.0009702054085209966\n",
      "training 0.0003890352672897279 relative L2 0.0009701462695375085\n",
      "training 0.0003889875952154398 relative L2 0.0009700869559310377\n",
      "training 0.0003889397776219994 relative L2 0.0009700278751552105\n",
      "training 0.0003888919309247285 relative L2 0.0009699683869257569\n",
      "training 0.0003888439096044749 relative L2 0.0009699087822809815\n",
      "training 0.00038879606290720403 relative L2 0.0009698494686745107\n",
      "training 0.0003887480706907809 relative L2 0.0009697901550680399\n",
      "training 0.00038870019488967955 relative L2 0.0009697305504232645\n",
      "training 0.0003886522608809173 relative L2 0.0009696712950244546\n",
      "training 0.0003886043559759855 relative L2 0.0009696119814179838\n",
      "training 0.0003885565383825451 relative L2 0.0009695527842268348\n",
      "training 0.00038850863347761333 relative L2 0.0009694931795820594\n",
      "training 0.00038846058305352926 relative L2 0.0009694335167296231\n",
      "training 0.00038841261994093657 relative L2 0.0009693740867078304\n",
      "training 0.00038836465682834387 relative L2 0.000969314482063055\n",
      "training 0.00038831663550809026 relative L2 0.0009692549938336015\n",
      "training 0.0003882687306031585 relative L2 0.0009691950981505215\n",
      "training 0.0003882208839058876 relative L2 0.0009691357263363898\n",
      "training 0.000388172862585634 relative L2 0.0009690762963145971\n",
      "training 0.00038812498678453267 relative L2 0.0009690162842161953\n",
      "training 0.00038807676173746586 relative L2 0.0009689565049484372\n",
      "training 0.0003880287113133818 relative L2 0.0009688967256806791\n",
      "training 0.00038798063178546727 relative L2 0.0009688372374512255\n",
      "training 0.0003879326395690441 relative L2 0.0009687773417681456\n",
      "training 0.0003878846182487905 relative L2 0.0009687175625003874\n",
      "training 0.00038783668424002826 relative L2 0.0009686577832326293\n",
      "training 0.000387788750231266 relative L2 0.0009685982950031757\n",
      "training 0.00038774069980718195 relative L2 0.000968538282904774\n",
      "training 0.00038769232924096286 relative L2 0.0009684785618446767\n",
      "training 0.0003876444243360311 relative L2 0.0009684186079539359\n",
      "training 0.00038759619928896427 relative L2 0.0009683588286861777\n",
      "training 0.0003875482361763716 relative L2 0.0009682991658337414\n",
      "training 0.00038750015664845705 relative L2 0.0009682397358119488\n",
      "training 0.000387452106224373 relative L2 0.0009681800729595125\n",
      "training 0.000387404317734763 relative L2 0.0009681201772764325\n",
      "training 0.0003873560926876962 relative L2 0.0009680604562163353\n",
      "training 0.00038730798405595124 relative L2 0.0009680003859102726\n",
      "training 0.00038725967169739306 relative L2 0.0009679404320195317\n",
      "training 0.00038721165037713945 relative L2 0.000967880361713469\n",
      "training 0.00038716354174539447 relative L2 0.0009678206988610327\n",
      "training 0.00038711531669832766 relative L2 0.0009677608613856137\n",
      "training 0.0003870671789627522 relative L2 0.0009677006164565682\n",
      "training 0.00038701898301951587 relative L2 0.0009676407789811492\n",
      "training 0.0003869709325954318 relative L2 0.0009675807086750865\n",
      "training 0.00038692253292538226 relative L2 0.0009675209294073284\n",
      "training 0.0003868744242936373 relative L2 0.0009674603352323174\n",
      "training 0.00038682599551975727 relative L2 0.0009674003813415766\n",
      "training 0.00038677785778418183 relative L2 0.0009673405438661575\n",
      "training 0.00038672969094477594 relative L2 0.0009672805899754167\n",
      "training 0.00038668158231303096 relative L2 0.0009672206942923367\n",
      "training 0.00038663335726596415 relative L2 0.0009671603911556304\n",
      "training 0.0003865851031150669 relative L2 0.0009671003790572286\n",
      "training 0.0003865367325488478 relative L2 0.0009670400177128613\n",
      "training 0.0003864884201902896 relative L2 0.0009669798309914768\n",
      "training 0.00038644010783173144 relative L2 0.0009669194114394486\n",
      "training 0.00038639167905785143 relative L2 0.0009668595157563686\n",
      "training 0.0003863434540107846 relative L2 0.000966799387242645\n",
      "training 0.0003862952289637178 relative L2 0.0009667390841059387\n",
      "training 0.0003862468875013292 relative L2 0.0009666787809692323\n",
      "training 0.0003861983714159578 relative L2 0.0009666183032095432\n",
      "training 0.000386149826226756 relative L2 0.0009665581746958196\n",
      "training 0.00038610148476436734 relative L2 0.0009664978133514524\n",
      "training 0.00038605311419814825 relative L2 0.0009664376848377287\n",
      "training 0.00038600483094342053 relative L2 0.0009663774981163442\n",
      "training 0.00038595646037720144 relative L2 0.0009663169621489942\n",
      "training 0.0003859081189148128 relative L2 0.0009662567754276097\n",
      "training 0.0003858598356600851 relative L2 0.0009661964140832424\n",
      "training 0.00038581149419769645 relative L2 0.0009661357617005706\n",
      "training 0.0003857628908008337 relative L2 0.0009660754585638642\n",
      "training 0.0003857143747154623 relative L2 0.0009660152136348188\n",
      "training 0.0003856660332530737 relative L2 0.000965954561252147\n",
      "training 0.0003856175462715328 relative L2 0.0009658943163231015\n",
      "training 0.00038556905928999186 relative L2 0.0009658342460170388\n",
      "training 0.0003855207469314337 relative L2 0.0009657737100496888\n",
      "training 0.0003854724345728755 relative L2 0.0009657133487053216\n",
      "training 0.0003854239184875041 relative L2 0.0009656526963226497\n",
      "training 0.00038537522777915 relative L2 0.0009655922185629606\n",
      "training 0.0003853266825899482 relative L2 0.000965531449764967\n",
      "training 0.00038527807919308543 relative L2 0.0009654705645516515\n",
      "training 0.0003852294175885618 relative L2 0.0009654102614149451\n",
      "training 0.00038518093060702085 relative L2 0.000965349841862917\n",
      "training 0.00038513250183314085 relative L2 0.000965289247687906\n",
      "training 0.00038508407305926085 relative L2 0.0009652287699282169\n",
      "training 0.00038503555697388947 relative L2 0.0009651680011302233\n",
      "training 0.0003849868953693658 relative L2 0.0009651068830862641\n",
      "training 0.00038493803003802896 relative L2 0.000965046463534236\n",
      "training 0.00038488960126414895 relative L2 0.0009649854036979377\n",
      "training 0.0003848408523481339 relative L2 0.0009649249259382486\n",
      "training 0.0003847923071589321 relative L2 0.000964864157140255\n",
      "training 0.0003847437328658998 relative L2 0.0009648033883422613\n",
      "training 0.0003846950421575457 relative L2 0.0009647425031289458\n",
      "training 0.00038464643876068294 relative L2 0.0009646815015003085\n",
      "training 0.00038459766074083745 relative L2 0.0009646207327023149\n",
      "training 0.0003845489409286529 relative L2 0.0009645598474889994\n",
      "training 0.00038450033753179014 relative L2 0.000964498904068023\n",
      "training 0.0003844515886157751 relative L2 0.0009644379606470466\n",
      "training 0.0003844028396997601 relative L2 0.000964377133641392\n",
      "training 0.00038435423630289733 relative L2 0.0009643163648433983\n",
      "training 0.00038430560380220413 relative L2 0.0009642555960454047\n",
      "training 0.0003842568548861891 relative L2 0.0009641945944167674\n",
      "training 0.0003842081059701741 relative L2 0.00096413359278813\n",
      "training 0.00038415929884649813 relative L2 0.0009640726493671536\n",
      "training 0.0003841104917228222 relative L2 0.0009640117641538382\n",
      "training 0.00038406168459914625 relative L2 0.0009639509371481836\n",
      "training 0.00038401305209845304 relative L2 0.0009638898773118854\n",
      "training 0.00038396427407860756 relative L2 0.000963828933890909\n",
      "training 0.0003839154960587621 relative L2 0.0009637678740546107\n",
      "training 0.00038386668893508613 relative L2 0.0009637067560106516\n",
      "training 0.00038381764898076653 relative L2 0.0009636456379666924\n",
      "training 0.0003837688418570906 relative L2 0.0009635845781303942\n",
      "training 0.00038372009294107556 relative L2 0.0009635234600864351\n",
      "training 0.0003836712276097387 relative L2 0.0009634624002501369\n",
      "training 0.0003836223913822323 relative L2 0.0009634010493755341\n",
      "training 0.00038357358425855637 relative L2 0.0009633400477468967\n",
      "training 0.00038352495175786316 relative L2 0.0009632791043259203\n",
      "training 0.00038347620284184813 relative L2 0.0009632176370359957\n",
      "training 0.000383427191991359 relative L2 0.0009631560533307493\n",
      "training 0.00038337812293320894 relative L2 0.000963094993494451\n",
      "training 0.00038332922849804163 relative L2 0.000963033817242831\n",
      "training 0.0003832802176475525 relative L2 0.0009629727574065328\n",
      "training 0.0003832313814200461 relative L2 0.0009629118139855564\n",
      "training 0.0003831826034002006 relative L2 0.0009628504049032927\n",
      "training 0.00038313367986120284 relative L2 0.0009627891704440117\n",
      "training 0.00038308455259539187 relative L2 0.000962727761361748\n",
      "training 0.0003830355708487332 relative L2 0.0009626662940718234\n",
      "training 0.0003829865891020745 relative L2 0.0009626052342355251\n",
      "training 0.0003829376946669072 relative L2 0.0009625435923226178\n",
      "training 0.00038288862560875714 relative L2 0.0009624822996556759\n",
      "training 0.0003828397311735898 relative L2 0.0009624208905734122\n",
      "training 0.0003827907785307616 relative L2 0.0009623595396988094\n",
      "training 0.00038274185499176383 relative L2 0.0009622982470318675\n",
      "training 0.00038269293145276606 relative L2 0.0009622363140806556\n",
      "training 0.0003826436004601419 relative L2 0.0009621750214137137\n",
      "training 0.0003825944440905005 relative L2 0.00096211361233145\n",
      "training 0.0003825454623438418 relative L2 0.000962052377872169\n",
      "training 0.00038249645149335265 relative L2 0.0009619909105822444\n",
      "training 0.00038244735333137214 relative L2 0.0009619295014999807\n",
      "training 0.0003823982842732221 relative L2 0.0009618683834560215\n",
      "training 0.0003823493898380548 relative L2 0.0009618069743737578\n",
      "training 0.00038230037898756564 relative L2 0.0009617452160455287\n",
      "training 0.0003822510479949415 relative L2 0.0009616836323402822\n",
      "training 0.00038220168789848685 relative L2 0.0009616224560886621\n",
      "training 0.0003821527352556586 relative L2 0.0009615608141757548\n",
      "training 0.0003821036370936781 relative L2 0.0009614992886781693\n",
      "training 0.0003820544807240367 relative L2 0.0009614375303499401\n",
      "training 0.0003820052952505648 relative L2 0.0009613762376829982\n",
      "training 0.00038195637171156704 relative L2 0.0009613145957700908\n",
      "training 0.00038190727354958653 relative L2 0.0009612528374418616\n",
      "training 0.0003818580589722842 relative L2 0.0009611907880753279\n",
      "training 0.00038180864066816866 relative L2 0.0009611290297470987\n",
      "training 0.0003817595134023577 relative L2 0.0009610673296265304\n",
      "training 0.0003817102697212249 relative L2 0.0009610055712983012\n",
      "training 0.0003816610260400921 relative L2 0.0009609439875930548\n",
      "training 0.00038161195698194206 relative L2 0.0009608823456801474\n",
      "training 0.00038156271330080926 relative L2 0.0009608205873519182\n",
      "training 0.00038151349872350693 relative L2 0.000960758829023689\n",
      "training 0.0003814641386270523 relative L2 0.0009606971289031208\n",
      "training 0.00038141486584208906 relative L2 0.0009606354287825525\n",
      "training 0.00038136556395329535 relative L2 0.000960573845077306\n",
      "training 0.00038131632027216256 relative L2 0.0009605120867490768\n",
      "training 0.00038126707659102976 relative L2 0.0009604503284208477\n",
      "training 0.0003812178620137274 relative L2 0.0009603885700926185\n",
      "training 0.00038116873474791646 relative L2 0.0009603266371414065\n",
      "training 0.0003811194037552923 relative L2 0.0009602645877748728\n",
      "training 0.0003810700145550072 relative L2 0.0009602023055776954\n",
      "training 0.0003810206544585526 relative L2 0.0009601404308341444\n",
      "training 0.000380971294362098 relative L2 0.0009600784978829324\n",
      "training 0.00038092187605798244 relative L2 0.0009600167395547032\n",
      "training 0.00038087257416918874 relative L2 0.0009599546319805086\n",
      "training 0.00038082327228039503 relative L2 0.0009598925826139748\n",
      "training 0.0003807739121839404 relative L2 0.000959830533247441\n",
      "training 0.0003807244647759944 relative L2 0.0009597683674655855\n",
      "training 0.0003806750464718789 relative L2 0.000959706143476069\n",
      "training 0.00038062542444095016 relative L2 0.0009596441523171961\n",
      "training 0.0003805759479291737 relative L2 0.0009595823939889669\n",
      "training 0.00038052673335187137 relative L2 0.0009595201117917895\n",
      "training 0.0003804771404247731 relative L2 0.0009594581788405776\n",
      "training 0.0003804278385359794 relative L2 0.0009593963040970266\n",
      "training 0.00038037847843952477 relative L2 0.0009593343711458147\n",
      "training 0.00038032906013540924 relative L2 0.0009592717979103327\n",
      "training 0.0003802795254159719 relative L2 0.0009592092828825116\n",
      "training 0.0003802298742812127 relative L2 0.0009591473499312997\n",
      "training 0.00038018051418475807 relative L2 0.0009590851259417832\n",
      "training 0.0003801309794653207 relative L2 0.0009590230183675885\n",
      "training 0.0003800814738497138 relative L2 0.0009589609690010548\n",
      "training 0.0003800321137532592 relative L2 0.0009588988614268601\n",
      "training 0.00037998254992999136 relative L2 0.0009588365792296827\n",
      "training 0.0003799330734182149 relative L2 0.0009587742388248444\n",
      "training 0.0003798833640757948 relative L2 0.000958711956627667\n",
      "training 0.00037983371294103563 relative L2 0.0009586495580151677\n",
      "training 0.0003797841491177678 relative L2 0.0009585873340256512\n",
      "training 0.0003797346435021609 relative L2 0.0009585252264514565\n",
      "training 0.0003796851960942149 relative L2 0.0009584629442542791\n",
      "training 0.0003796357486862689 relative L2 0.0009584007784724236\n",
      "training 0.0003795863303821534 relative L2 0.0009583383216522634\n",
      "training 0.0003795367374550551 relative L2 0.0009582759230397642\n",
      "training 0.00037948714452795684 relative L2 0.0009582133498042822\n",
      "training 0.0003794374642893672 relative L2 0.0009581508347764611\n",
      "training 0.00037938772584311664 relative L2 0.0009580882033333182\n",
      "training 0.0003793381038121879 relative L2 0.0009580258629284799\n",
      "training 0.000379288598196581 relative L2 0.0009579635807313025\n",
      "training 0.0003792390343733132 relative L2 0.0009579010074958205\n",
      "training 0.00037918935413472354 relative L2 0.0009578384924679995\n",
      "training 0.0003791396738961339 relative L2 0.0009577760938555002\n",
      "training 0.0003790901100728661 relative L2 0.0009577134624123573\n",
      "training 0.00037904013879597187 relative L2 0.0009576509473845363\n",
      "training 0.0003789903421420604 relative L2 0.0009575884905643761\n",
      "training 0.00037894080742262304 relative L2 0.0009575257427059114\n",
      "training 0.0003788911271840334 relative L2 0.0009574632276780903\n",
      "training 0.00037884150515310466 relative L2 0.0009574007708579302\n",
      "training 0.00037879173760302365 relative L2 0.0009573381976224482\n",
      "training 0.00037874217377975583 relative L2 0.0009572751587256789\n",
      "training 0.00037869231891818345 relative L2 0.0009572124690748751\n",
      "training 0.00037864246405661106 relative L2 0.0009571496048010886\n",
      "training 0.0003785926091950387 relative L2 0.0009570867987349629\n",
      "training 0.00037854284164495766 relative L2 0.000957024225499481\n",
      "training 0.00037849307409487665 relative L2 0.0009569615358486772\n",
      "training 0.00037844336475245655 relative L2 0.0009568986715748906\n",
      "training 0.00037839350989088416 relative L2 0.000956835865508765\n",
      "training 0.0003783435095101595 relative L2 0.0009567730594426394\n",
      "training 0.0003782937419600785 relative L2 0.0009567101369611919\n",
      "training 0.0003782438288908452 relative L2 0.0009566475637257099\n",
      "training 0.0003781938867177814 relative L2 0.0009565846994519234\n",
      "training 0.0003781439736485481 relative L2 0.0009565219515934587\n",
      "training 0.00037809423520229757 relative L2 0.0009564594947732985\n",
      "training 0.00037804461317136884 relative L2 0.0009563967469148338\n",
      "training 0.0003779947874136269 relative L2 0.0009563337080180645\n",
      "training 0.00037794478703290224 relative L2 0.0009562706691212952\n",
      "training 0.00037789487396366894 relative L2 0.0009562076302245259\n",
      "training 0.00037784496089443564 relative L2 0.0009561445331200957\n",
      "training 0.00037779504782520235 relative L2 0.0009560816688463092\n",
      "training 0.00037774519296362996 relative L2 0.0009560190956108272\n",
      "training 0.0003776953089982271 relative L2 0.000955955998506397\n",
      "training 0.0003776454832404852 relative L2 0.0009558930178172886\n",
      "training 0.00037759554106742144 relative L2 0.0009558303863741457\n",
      "training 0.00037754562799818814 relative L2 0.0009557670564390719\n",
      "training 0.00037749556940980256 relative L2 0.0009557039593346417\n",
      "training 0.00037744539440609515 relative L2 0.000955641211476177\n",
      "training 0.0003773955104406923 relative L2 0.0009555782307870686\n",
      "training 0.0003773454809561372 relative L2 0.0009555151336826384\n",
      "training 0.00037729559699073434 relative L2 0.0009554522694088519\n",
      "training 0.0003772456257138401 relative L2 0.0009553894633427262\n",
      "training 0.0003771957417484373 relative L2 0.0009553263080306351\n",
      "training 0.0003771457704715431 relative L2 0.0009552630945108831\n",
      "training 0.00037709559546783566 relative L2 0.0009551997063681483\n",
      "training 0.00037704536225646734 relative L2 0.0009551367838867009\n",
      "training 0.0003769955364987254 relative L2 0.000955073453951627\n",
      "training 0.0003769454197026789 relative L2 0.0009550104732625186\n",
      "training 0.0003768953320104629 relative L2 0.0009549474925734103\n",
      "training 0.0003768454189412296 relative L2 0.0009548842790536582\n",
      "training 0.00037679533124901354 relative L2 0.0009548211237415671\n",
      "training 0.0003767453017644584 relative L2 0.000954757968429476\n",
      "training 0.00037669509765692055 relative L2 0.0009546945802867413\n",
      "training 0.0003766450099647045 relative L2 0.0009546311921440065\n",
      "training 0.0003765948349609971 relative L2 0.0009545681532472372\n",
      "training 0.0003765447763726115 relative L2 0.0009545047651045024\n",
      "training 0.00037649463047273457 relative L2 0.000954441842623055\n",
      "training 0.0003764447756111622 relative L2 0.0009543784544803202\n",
      "training 0.0003763946006074548 relative L2 0.0009543151245452464\n",
      "training 0.0003763444547075778 relative L2 0.0009542516781948507\n",
      "training 0.00037629430880770087 relative L2 0.0009541879990138113\n",
      "training 0.0003762438427656889 relative L2 0.0009541247272863984\n",
      "training 0.00037619375507347286 relative L2 0.0009540613973513246\n",
      "training 0.0003761434927582741 relative L2 0.0009539979510009289\n",
      "training 0.0003760933468583971 relative L2 0.0009539347374811769\n",
      "training 0.000376043317373842 relative L2 0.0009538713493384421\n",
      "training 0.00037599302595481277 relative L2 0.0009538078447803855\n",
      "training 0.00037594279274344444 relative L2 0.000953744282014668\n",
      "training 0.00037589261773973703 relative L2 0.0009536807774566114\n",
      "training 0.00037584220990538597 relative L2 0.0009536173893138766\n",
      "training 0.00037579191848635674 relative L2 0.0009535537683404982\n",
      "training 0.00037574165617115796 relative L2 0.0009534901473671198\n",
      "training 0.0003756913647521287 relative L2 0.0009534269920550287\n",
      "training 0.00037564116064459085 relative L2 0.0009533631964586675\n",
      "training 0.0003755908983293921 relative L2 0.00095329963369295\n",
      "training 0.00037554060691036284 relative L2 0.0009532361873425543\n",
      "training 0.00037549034459516406 relative L2 0.0009531722171232104\n",
      "training 0.00037543990765698254 relative L2 0.0009531087707728148\n",
      "training 0.0003753895580302924 relative L2 0.0009530450915917754\n",
      "training 0.00037533915019594133 relative L2 0.0009529815870337188\n",
      "training 0.00037528882967308164 relative L2 0.0009529179660603404\n",
      "training 0.00037523850915022194 relative L2 0.0009528539376333356\n",
      "training 0.0003751882177311927 relative L2 0.0009527903748676181\n",
      "training 0.00037513775168918073 relative L2 0.000952726521063596\n",
      "training 0.00037508734385482967 relative L2 0.0009526623180136085\n",
      "training 0.00037503676139749587 relative L2 0.0009525984642095864\n",
      "training 0.0003749864408746362 relative L2 0.0009525349596515298\n",
      "training 0.00037493606214411557 relative L2 0.0009524712222628295\n",
      "training 0.0003748857416212559 relative L2 0.0009524073102511466\n",
      "training 0.0003748354210983962 relative L2 0.0009523436310701072\n",
      "training 0.00037478498416021466 relative L2 0.0009522797772660851\n",
      "training 0.0003747345763258636 relative L2 0.0009522159234620631\n",
      "training 0.0003746842558030039 relative L2 0.0009521516622044146\n",
      "training 0.00037463349872268736 relative L2 0.0009520879830233753\n",
      "training 0.00037458311999216676 relative L2 0.0009520241292193532\n",
      "training 0.0003745327703654766 relative L2 0.0009519601007923484\n",
      "training 0.0003744823334272951 relative L2 0.0009518961305730045\n",
      "training 0.00037443172186613083 relative L2 0.0009518324513919652\n",
      "training 0.00037438131403177977 relative L2 0.0009517682483419776\n",
      "training 0.00037433081888593733 relative L2 0.0009517042199149728\n",
      "training 0.00037428023642860353 relative L2 0.0009516398422420025\n",
      "training 0.0003742293338291347 relative L2 0.0009515759884379804\n",
      "training 0.00037417892599478364 relative L2 0.0009515123092569411\n",
      "training 0.0003741285181604326 relative L2 0.0009514482808299363\n",
      "training 0.0003740779357030988 relative L2 0.0009513841941952705\n",
      "training 0.00037402723683044314 relative L2 0.0009513203403912485\n",
      "training 0.0003739767998922616 relative L2 0.0009512564283795655\n",
      "training 0.0003739262174349278 relative L2 0.0009511922835372388\n",
      "training 0.00037387560587376356 relative L2 0.0009511279058642685\n",
      "training 0.00037382476148195565 relative L2 0.0009510638774372637\n",
      "training 0.0003737742663361132 relative L2 0.0009509999654255807\n",
      "training 0.00037372359656728804 relative L2 0.0009509358205832541\n",
      "training 0.0003736728976946324 relative L2 0.0009508716175332665\n",
      "training 0.00037362243165262043 relative L2 0.000950807414483279\n",
      "training 0.00037357184919528663 relative L2 0.0009507433278486133\n",
      "training 0.00037352112121880054 relative L2 0.0009506790083833039\n",
      "training 0.000373470364138484 relative L2 0.0009506145725026727\n",
      "training 0.00037341960705816746 relative L2 0.000950550427660346\n",
      "training 0.00037336890818551183 relative L2 0.000950486515648663\n",
      "training 0.00037331823841668665 relative L2 0.0009504223125986755\n",
      "training 0.00037326759775169194 relative L2 0.0009503579349257052\n",
      "training 0.00037321692798286676 relative L2 0.0009502939647063613\n",
      "training 0.00037316622911021113 relative L2 0.0009502297616563737\n",
      "training 0.0003731156175490469 relative L2 0.0009501655586063862\n",
      "training 0.00037306486046873033 relative L2 0.0009501010063104331\n",
      "training 0.00037301387055777013 relative L2 0.0009500366286374629\n",
      "training 0.00037296308437362313 relative L2 0.0009499723091721535\n",
      "training 0.0003729121235664934 relative L2 0.0009499082225374877\n",
      "training 0.0003728614829014987 relative L2 0.0009498437284491956\n",
      "training 0.0003728106967173517 relative L2 0.0009497795253992081\n",
      "training 0.00037275999784469604 relative L2 0.0009497148566879332\n",
      "training 0.00037270894972607493 relative L2 0.0009496501297689974\n",
      "training 0.00037265807623043656 relative L2 0.0009495852864347398\n",
      "training 0.0003726069408003241 relative L2 0.0009495210833847523\n",
      "training 0.00037255630013532937 relative L2 0.0009494562400504947\n",
      "training 0.00037250525201670825 relative L2 0.0009493916877545416\n",
      "training 0.00037245452404022217 relative L2 0.0009493274264968932\n",
      "training 0.00037240367964841425 relative L2 0.000949262990616262\n",
      "training 0.00037235289346426725 relative L2 0.0009491984383203089\n",
      "training 0.0003723019326571375 relative L2 0.0009491337696090341\n",
      "training 0.0003722510882653296 relative L2 0.0009490690426900983\n",
      "training 0.00037220001104287803 relative L2 0.0009490043739788234\n",
      "training 0.0003721489920280874 relative L2 0.0009489398216828704\n",
      "training 0.0003720980603247881 relative L2 0.0009488754440099001\n",
      "training 0.0003720471868291497 relative L2 0.0009488107170909643\n",
      "training 0.00037199637154117227 relative L2 0.0009487455827184021\n",
      "training 0.00037194526521489024 relative L2 0.0009486807393841445\n",
      "training 0.0003718943044077605 relative L2 0.0009486157214269042\n",
      "training 0.0003718431107699871 relative L2 0.0009485508198849857\n",
      "training 0.00037179203354753554 relative L2 0.0009484858601354063\n",
      "training 0.00037174104363657534 relative L2 0.0009484211332164705\n",
      "training 0.00037169005372561514 relative L2 0.000948356173466891\n",
      "training 0.0003716391802299768 relative L2 0.000948291621170938\n",
      "training 0.0003715881903190166 relative L2 0.0009482267778366804\n",
      "training 0.00037153722951188684 relative L2 0.0009481619345024228\n",
      "training 0.00037148615228943527 relative L2 0.0009480966837145388\n",
      "training 0.0003714350168593228 relative L2 0.0009480316657572985\n",
      "training 0.0003713837650138885 relative L2 0.0009479666478000581\n",
      "training 0.0003713326877914369 relative L2 0.0009479016880504787\n",
      "training 0.00037128155236132443 relative L2 0.0009478366118855774\n",
      "training 0.0003712303878273815 relative L2 0.0009477714775130153\n",
      "training 0.0003711793106049299 relative L2 0.0009477062849327922\n",
      "training 0.00037112817517481744 relative L2 0.0009476412669755518\n",
      "training 0.0003710771561600268 relative L2 0.0009475760743953288\n",
      "training 0.0003710259043145925 relative L2 0.0009475108818151057\n",
      "training 0.0003709745651576668 relative L2 0.0009474458638578653\n",
      "training 0.0003709235170390457 relative L2 0.0009473806712776423\n",
      "training 0.00037087241071276367 relative L2 0.0009473153040744364\n",
      "training 0.0003708211297634989 relative L2 0.0009472501697018743\n",
      "training 0.00037077011074870825 relative L2 0.0009471850353293121\n",
      "training 0.0003707189462147653 relative L2 0.000947119842749089\n",
      "training 0.00037066772347316146 relative L2 0.0009470543009229004\n",
      "training 0.0003706162970047444 relative L2 0.0009469887590967119\n",
      "training 0.00037056501605547965 relative L2 0.0009469232754781842\n",
      "training 0.0003705137060023844 relative L2 0.000946858141105622\n",
      "training 0.00037046257057227194 relative L2 0.000946792948525399\n",
      "training 0.0003704112023115158 relative L2 0.0009467277559451759\n",
      "training 0.0003703602124005556 relative L2 0.0009466626797802746\n",
      "training 0.00037030898965895176 relative L2 0.000946597196161747\n",
      "training 0.0003702575631905347 relative L2 0.0009465314215049148\n",
      "training 0.00037020602030679584 relative L2 0.0009464657050557435\n",
      "training 0.0003701547102537006 relative L2 0.0009464005706831813\n",
      "training 0.00037010337109677494 relative L2 0.0009463349706493318\n",
      "training 0.00037005203193984926 relative L2 0.0009462694870308042\n",
      "training 0.0003700007509905845 relative L2 0.0009462041198275983\n",
      "training 0.0003699495573528111 relative L2 0.0009461388690397143\n",
      "training 0.0003698982181958854 relative L2 0.000946073152590543\n",
      "training 0.000369846704415977 relative L2 0.0009460074361413717\n",
      "training 0.00036979521973989904 relative L2 0.0009459417778998613\n",
      "training 0.00036974356044083834 relative L2 0.0009458760032430291\n",
      "training 0.00036969210486859083 relative L2 0.0009458103450015187\n",
      "training 0.00036964056198485196 relative L2 0.0009457449777983129\n",
      "training 0.0003695891355164349 relative L2 0.0009456794359721243\n",
      "training 0.0003695376799441874 relative L2 0.0009456139523535967\n",
      "training 0.00036948619526810944 relative L2 0.0009455482359044254\n",
      "training 0.00036943465238437057 relative L2 0.000945482577662915\n",
      "training 0.00036938319681212306 relative L2 0.0009454167447984219\n",
      "training 0.00036933147930540144 relative L2 0.0009453510865569115\n",
      "training 0.00036927993642166257 relative L2 0.0009452851372770965\n",
      "training 0.0003692282480187714 relative L2 0.000945219537243247\n",
      "training 0.0003691768506541848 relative L2 0.0009451537625864148\n",
      "training 0.00036912530777044594 relative L2 0.0009450879297219217\n",
      "training 0.0003690737939905375 relative L2 0.0009450222132727504\n",
      "training 0.0003690222220029682 relative L2 0.0009449560893699527\n",
      "training 0.0003689706791192293 relative L2 0.000944889965467155\n",
      "training 0.00036891899071633816 relative L2 0.0009448239579796791\n",
      "training 0.00036886733141727746 relative L2 0.0009447584161534905\n",
      "training 0.0003688157594297081 relative L2 0.0009446924086660147\n",
      "training 0.0003687641874421388 relative L2 0.0009446267504245043\n",
      "training 0.00036871267366223037 relative L2 0.0009445608593523502\n",
      "training 0.0003686609852593392 relative L2 0.000944495084695518\n",
      "training 0.00036860929685644805 relative L2 0.0009444289607927203\n",
      "training 0.0003685576084535569 relative L2 0.0009443628950975835\n",
      "training 0.0003685058618430048 relative L2 0.0009442970622330904\n",
      "training 0.00036845405702479184 relative L2 0.0009442312293685973\n",
      "training 0.0003684025432448834 relative L2 0.0009441652218811214\n",
      "training 0.0003683508839458227 relative L2 0.0009440992726013064\n",
      "training 0.0003682993119582534 relative L2 0.0009440333815291524\n",
      "training 0.00036824759445153177 relative L2 0.000943967083003372\n",
      "training 0.0003681957605294883 relative L2 0.0009439009008929133\n",
      "training 0.00036814395571127534 relative L2 0.0009438346023671329\n",
      "training 0.00036809200537391007 relative L2 0.0009437682456336915\n",
      "training 0.00036804022965952754 relative L2 0.0009437022381462157\n",
      "training 0.0003679883957374841 relative L2 0.000943636114243418\n",
      "training 0.00036793662002310157 relative L2 0.0009435697575099766\n",
      "training 0.0003678847278933972 relative L2 0.0009435037500225008\n",
      "training 0.0003678330103866756 relative L2 0.000943437684327364\n",
      "training 0.0003677811473608017 relative L2 0.0009433714440092444\n",
      "training 0.00036772925523109734 relative L2 0.0009433050290681422\n",
      "training 0.00036767730489373207 relative L2 0.0009432388469576836\n",
      "training 0.0003676254127640277 relative L2 0.000943172664847225\n",
      "training 0.00036757346242666245 relative L2 0.0009431064827367663\n",
      "training 0.0003675215120892972 relative L2 0.000943040126003325\n",
      "training 0.0003674697072710842 relative L2 0.0009429738274775445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "n_epochs = 10000\n",
    "net = networks.FeedforwardNeuralNetwork(2, 50, 1, 8)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "loss_hist6 = []\n",
    "loss_hist7 = []\n",
    "logging.info(f'{net}\\n')\n",
    "logging.info(f'Training started at {datetime.datetime.now()}\\n')\n",
    "min_loss = float(\"inf\")  # Initialize with a large value\n",
    "final_model = None\n",
    "start_time = timer()\n",
    "for _ in tqdm.tqdm(range(n_epochs), desc='[Training procedure]', ascii=True, total=n_epochs):\n",
    "    prediction = net(X_train_tensor)\n",
    "    loss = lossFunction(prediction, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist6.append(loss.item())\n",
    "    \n",
    "    # Calculate the relative L2 loss\n",
    "    prediction2 = net(X_test_tensor)\n",
    "    relative_L2 = torch.sqrt(torch.mean((prediction2 - y_test_tensor)**2)) / torch.sqrt(torch.mean(y_test_tensor**2))\n",
    "    loss_hist7.append(relative_L2.item())\n",
    "    \n",
    "    if relative_L2.item() < min_loss:\n",
    "        min_loss = relative_L2.item()\n",
    "        final_model = net.state_dict()\n",
    "    pass\n",
    "\n",
    "# torch.save(final_model, \"default/nn.pth\")  # Save the model's state dictionary\n",
    "# # Save the training loss history as a CSV file\n",
    "# loss_df = pd.DataFrame(loss_hist7)\n",
    "# loss_df.to_csv('default/nn_loss.csv', index=False)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print('training', loss_hist6[i], 'relative L2', loss_hist7[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
