{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from scipy.stats import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(suppress = True)\n",
    "#Clock\n",
    "started = time.time()\n",
    "\n",
    "#Neural network parameters \n",
    "epochs = 100\n",
    "D = 3 #num dimensions incl.time(time, price, input)\n",
    "L = 9 #layers (depth)\n",
    "N = 20 #nodes per layer (width)\n",
    "\n",
    "\n",
    "# constant parameters\n",
    "r = 0.05        #interest rate\n",
    "K = 3.0         #Strike price\n",
    "sigma = 0.15    #Volatility\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "M = 320                 #Num samples\n",
    "learning_rate = 5e-4\n",
    "m = 64                  #mini-batch size\n",
    "reg = 0*1e-5            #L^2 regularisation\n",
    "beta = 0.9              #constant of momentum\n",
    "epsilon = 0.1           #step size of finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameter X(Asset price in Black Scholes equation)\n",
    "X = 7.0 * random.random_sample((M, D)) #data (input)\n",
    "# print(X)\n",
    "\n",
    "# boudary conditions\n",
    "X[ : , 0] = 10 * random.random_sample (M)\n",
    "X[0:10, 0] = 0.0\n",
    "X[10:20, 0] = 10.0\n",
    "X[20:30, 1] = 0.0\n",
    "X[30:40, 1] = 7.0\n",
    "X[40:50, 2] = 0.0\n",
    "X[50:60, 2] = 7.0\n",
    "# print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation (Xavier initialisation)\n",
    "## Weights\n",
    "The initialisation of the weight matrices:\n",
    "- The elements of the weight matrices need to be random numbers to avoid them updating equally at every iteration. \n",
    "- The initial weight should not be too far away from their target value (The gradietn may be too large), so we use a normal distribution with mean zero and small variance so that the expect average weight value is zero. \n",
    "- The variance of neuron's output will increase following by the increasing number of examples. Theirfore, the inital weight is divided by $\\sqrt{n}$, where n is the number of inputs to the specific neuron solves the problem.\n",
    "- One of the assumptions is that the expectation of inputs to a neuron is zero, therefore, the initialisation should divided by $\\sqrt{2/n}$.\n",
    "\n",
    "### Why we need the initialisation?\n",
    "It seemed impossible to have more than two hidden layers without getting the gradient to either blow up or be close to zero(dont know the reason). To compensate for only having two hidden layers we had to use 50 to 100\n",
    "nodes. However, When the above suggested initialisations were implemented we could right away increase hidden layers to 20 and reduce nodes to 10 per layer without any immediate problems.\n",
    "\n",
    "## Bias\n",
    "Biases can be all set to zero, but from experience it seems here to work better\n",
    "with a small positive number at the start to make sure every node is activated.\n",
    "\n",
    "## Monmentum\n",
    "Momentum can help the neural network escape from local optimisation, where a moving average of previous gradients is used in the calculation of the next moemntum. It can also helps in reducing overly stochastic convergence. Some other methods: esterov momentum where the gradient is calculated with the current weighted velocity, or Aadam (Adaptive momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in = random.randn(D, N) * sqrt(2.0/D) #weights layer 1\n",
    "b_in = ones((1, N)) * 0.01 #bias layer 1\n",
    "\n",
    "W = random.randn(L-1, N, N) * sqrt(2.0/N) #weights\n",
    "b = ones((L-1, 1, N)) * 0.01 #biases\n",
    "\n",
    "W_out = random.randn(N, 1) * sqrt(2.0/N) #weights output\n",
    "b_out = 0.01 #biases output\n",
    "\n",
    "hidden_layers = zeros((L, M, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#momentum weight numpy array\n",
    "VdW_in = zeros((D,N)) \n",
    "VdW = zeros((L-1,N,N))\n",
    "VdW_out = zeros((N, 1 ))\n",
    "\n",
    "# momentum bias numpy array\n",
    "Vdb_in = zeros((1, N)) \n",
    "Vdb = zeros((L-1, 1, N))\n",
    "Vdb_out = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "- Target function: Using the Feynman-Kac theorem to calculate the expected/target value and the loss(error).\n",
    "- Activation function: Using ReLU as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(out, dout, ddout, X=X, M=M):\n",
    "    \"\"\"\n",
    "    Loss function\n",
    "    Args:\n",
    "        out (_type_): _description_\n",
    "        dout (_type_): _description_\n",
    "        ddout (_type_): _description_\n",
    "        X (_type_, optional): _description_. Defaults to X.\n",
    "        M (_type_, optional): _description_. Defaults to M.\n",
    "    \"\"\"\n",
    "    t = copy(X[:,0]).reshape(M,1)\n",
    "    Expectation = zeros_like(out)\n",
    "    \n",
    "    # calculate the expectation/target\n",
    "    for m in range(100):\n",
    "        # discretisize\n",
    "        dt = (10 - t)/100\n",
    "        x = copy(X[:, 1:D]).reshape(M, D-1)\n",
    "        \n",
    "        # Feynman-Kac theorem\n",
    "        for n in range(100):\n",
    "            x = x + x * r * dt + sigma * sqrt(dt) * x * random.randn(M, D-1)\n",
    "            pass\n",
    "        I = -r * (10 - t)\n",
    "        psi = mean(maximum(0, x-K), axis=1).reshape(M,1)\n",
    "        Expectation += exp(I)*psi\n",
    "        pass\n",
    "    Expectation /= 100\n",
    "    \n",
    "    # mean squared error\n",
    "    target = (out - Expectation)**2\n",
    "    dtarget = 2*(out - Expectation)\n",
    "    \n",
    "    return target, dtarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectofoed Linear Unit\n",
    "    Args:\n",
    "        x (_type_): _description_\n",
    "    \"\"\"\n",
    "    return maximum(0, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass function \n",
    "The interior function of the node in the neural network:\n",
    "\n",
    "A node in a feedforward neural network takes the sum of a weighted input vector x, adds a bias b and uses an activation function $\\sigma(x)$. \n",
    "\n",
    "The output of this function is then sent to the next layer as input: $\\mathcal{z} = \\sigma \\left(\\sum_{n=1}^{N}x_n w_n + b\\right)$, where N is the width of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(hidden_layers, X, W_in=W_in, W=W, W_out=W_out, b_in = b_in, b=b, b_out=b_out):\n",
    "    \"\"\"\n",
    "    forward pass of neural network\n",
    "    Args:\n",
    "        hidden_layers (_type_): _description_\n",
    "        X (_type_): _description_\n",
    "        W_in (_type_, optional): _description_. Defaults to W_in.\n",
    "        W (_type_, optional): _description_. Defaults to W.\n",
    "        W_out (_type_, optional): _description_. Defaults to W_out.\n",
    "        b_in (_type_, optional): _description_. Defaults to b_in.\n",
    "        b (_type_, optional): _description_. Defaults to b.\n",
    "        b_out (_type_, optional): _description_. Defaults to b_out.\n",
    "    \"\"\"\n",
    "    L,M,N = shape(hidden_layers)\n",
    "    hl = copy(hidden_layers)\n",
    "    \n",
    "    # input layer\n",
    "    hl[0, :, :] = dot(X, W_in) + b_in\n",
    "    z = dot(hl[0, :, :], W[0, :, :])+b[0, :, :]\n",
    "    hl[1, :, :] = ReLU(z)\n",
    "    \n",
    "    # update layers one by one\n",
    "    for j in range (2, L):\n",
    "        z = dot(hl[j-1, :, :],W[j-1, :, :]) + b[j-1, :, :]\n",
    "        hl[j, :, :] = ReLU(z)\n",
    "        pass\n",
    "    \n",
    "    # output layer\n",
    "    output = dot(hl[L-1], W_out) + b_out\n",
    "    return output, hl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Process\n",
    "\n",
    "Create to update parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(step_size, hidden_layers, W_in=W_in,W=W, W_out=W_out, b_in=b_in, b=b, b_out=b_out, VdW_in=VdW_in, VdW=VdW, VdW_out=VdW_out, Vdb_in=Vdb_in, Vdb=Vdb, Vdb_out=Vdb_out):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "\n",
    "    Args:\n",
    "        step_size (_type_): _description_\n",
    "        hidden_layers (_type_): _description_\n",
    "        W_in (_type_, optional): _description_. Defaults to W_in.\n",
    "        W (_type_, optional): _description_. Defaults to W.\n",
    "        W_out (_type_, optional): _description_. Defaults to W_out.\n",
    "        b_in (_type_, optional): _description_. Defaults to b_in.\n",
    "        b (_type_, optional): _description_. Defaults to b.\n",
    "        b_out (_type_, optional): _description_. Defaults to b_out.\n",
    "        VdW_in (_type_, optional): _description_. Defaults to VdW_in.\n",
    "        VdW (_type_, optional): _description_. Defaults to VdW.\n",
    "        VdW_out (_type_, optional): _description_. Defaults to VdW_out.\n",
    "        Vdb_in (_type_, optional): _description_. Defaults to Vdb_in.\n",
    "        Vdb (_type_, optional): _description_. Defaults to Vdb.\n",
    "        Vdb_out (_type_, optional): _description_. Defaults to Vdb_out.\n",
    "    \"\"\"\n",
    "    #Batches\n",
    "    minibatches = []\n",
    "    p = arange (M)\n",
    "    random.shuffle(p)\n",
    "    X_shuffled = X[p]\n",
    "    hl_shuffled = hidden_layers[:, p]\n",
    "    for k in range(0, M, m) :\n",
    "        X_batch = X_shuffled [k:k+m]\n",
    "        hl_batch = hl_shuffled[:, k:k+m]\n",
    "        minibatches.append ((X_batch, hl_batch))\n",
    "        pass\n",
    "    for X_batch, hl_batch in minibatches:\n",
    "        out_batch , hl_batch = forwardPass(hl_batch, X_batch)[0:2]\n",
    "        dout_batch , ddout_batch = 0.0 , 0.0\n",
    "        #dout_batch , ddout_batch = derivative(hl_batch, X_batch)\n",
    "        #gradientat end layer\n",
    "        dLoss_batch = target(out_batch, dout_batch, ddout_batch, X=X_batch, M=m)[1]\n",
    "        dLoss_batch /= m\n",
    "        #Backpropagation to find gradient w.r.t W and b\n",
    "        dW_out = dot(hl_batch[L-1].T, dLoss_batch)\n",
    "        db_out = sum(dLoss_batch, axis=0, keepdims=False)\n",
    "        dhidden = dot(dLoss_batch, W_out.T)\n",
    "        dhidden[hl_batch[L-1] <=0] = 0\n",
    "        dW = zeros((L-1,N,N))\n",
    "        db = zeros((L-1,1,N) )\n",
    "        for j in reversed(range(0, L-1)):\n",
    "            dW[j] = dot(hl_batch[j].T, dhidden)\n",
    "            db[j] = sum(dhidden, axis=0, keepdims=False)\n",
    "            dhidden = dot(dhidden, W[j].T)\n",
    "            dhidden[hl_batch[j] <= 0] = 0\n",
    "            pass\n",
    "        pass\n",
    "            \n",
    "    dW_in = dot(X_batch.T, dhidden)\n",
    "    db_in = sum(dhidden, axis=0, keepdims=True)\n",
    "    \n",
    "    #adding regularization\n",
    "    dW_in += reg*W_in\n",
    "    dW += reg*W\n",
    "    dW_out += reg*W_out\n",
    "    \n",
    "    #momentum\n",
    "    VdW_in = beta * VdW_in - step_size * dW_in\n",
    "    VdW = beta * VdW - step_size * dW\n",
    "    VdW_out = beta * VdW_out - step_size * dW_out\n",
    "    Vdb_in = beta * Vdb_in - step_size * db_in\n",
    "    Vdb = beta * Vdb - step_size * db\n",
    "    Vdb_out = beta * Vdb_out - step_size * db_out\n",
    "    \n",
    "    #parameter update\n",
    "    cap = 5.0\n",
    "    W_in += VdW_in\n",
    "    W_in [W_in > cap] = cap\n",
    "    W_in [W_in < -cap] = -cap\n",
    "    b_in += Vdb_in\n",
    "    W += VdW\n",
    "    W[W > cap] = cap\n",
    "    W[W < -cap] = -cap\n",
    "    b += Vdb\n",
    "    W_out += VdW_out\n",
    "    W_out [W_out > cap] = cap\n",
    "    W_out [W_out < -cap] = -cap\n",
    "    b_out += Vdb_out\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative function: \n",
    "\n",
    "## Test function: \n",
    "Create to test the performance of the trained neural network.\n",
    "\n",
    "## Learning schedule function \n",
    "Create to control the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(hidden_layers ,X=X, order='first'):\n",
    "    \"\"\" Finite differences of NN\"\"\"\n",
    "    h = epsilon\n",
    "    u = forwardPass(hidden_layers ,X)[0]\n",
    "    h1 = zeros_like(X)\n",
    "    h1 [:, 0] += h\n",
    "    h2 = zeros_like(X)\n",
    "    h2 [:, 1 ] += h\n",
    "    u1p = forwardPass(hidden_layers, X+h1)[0]\n",
    "    u1m = forwardPass(hidden_layers, X-h1)[0]\n",
    "    du1 = (u1p - u1m)/(2*h)\n",
    "    u2p = forwardPass(hidden_layers, X+h2)[0]\n",
    "    u2m = forwardPass(hidden_layers, X-h2)[0]\n",
    "    du2 = (u2p - u2m)/(2*h)\n",
    "    if order == 'second':\n",
    "        ddu1 = (u1p - 2*u + u1m) / h**2\n",
    "        ddu2 = (u2p - 2*u + u2m) / h**2\n",
    "    else: \n",
    "        ddu1, ddu2 = zeros_like(u), zeros_like(u)\n",
    "    #du1 , du2 , ddu1 , ddu2 = 0 , 0 , 0 , 0\n",
    "    return [du1, du2], [ddu1, ddu2]\n",
    "    \n",
    "    \n",
    "def test(hidden_layers = hidden_layers):\n",
    "    \"\"\" Find loss of NN on random validation set \"\"\"\n",
    "    test_points= 7.0 * random.random_sample((M,D))\n",
    "    test_points[:, 0] = 10*random.random_sample(M)\n",
    "    \n",
    "    u,_ = forwardPass(hidden_layers, test_points)[0:2]\n",
    "    #du, ddu = derivative(hidden_layers, test_points, 'second')\n",
    "    du = ddu = 0.0\n",
    "    return sum(target(u, du, ddu, X=test_points)[0])/M\n",
    "\n",
    "def learning_rate_schedule(Loss, learning_rate):\n",
    "    \"\"\" Learning rate schedule to reduce\n",
    "    learning rate during training \"\"\"\n",
    "    if Loss <= 0.02:\n",
    "        step_size = learning_rate/2\n",
    "    else: \n",
    "        step_size = learning_rate\n",
    "    return step_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main part\n",
    "test_array = zeros(epochs) #To plot loss\n",
    "loss_array = zeros(epochs) #To plot loss\n",
    "Loss = 1.0\n",
    "i = 0\n",
    "step_size = learning_rate_schedule(Loss, learning_rate)\n",
    "while Loss >= 0.001 and i < epochs:\n",
    "    out, hidden_layers = forwardPass(hidden_layers, X)[0:2]\n",
    "    #dout, ddout = derivative(hidden_layers, X)\n",
    "    #ddout = derivative(hidden_layers, X, 'second')\n",
    "    dout, ddout = 0, 0\n",
    "    \n",
    "    #Loss\n",
    "    dataLoss = sum(target(out, dout, ddout)[0])/M\n",
    "    regLoss = 0.5 * reg * sum(W_in*W_in) + 0.5 * reg *sum(W_out*W_out) + 0.5 * reg * sum(W*W)\n",
    "    Loss = dataLoss + regLoss\n",
    "    \n",
    "    #parameter update\n",
    "    SGD(step_size, hidden_layers)\n",
    "    \n",
    "    #store loss evolution\n",
    "    test_array[i] = test()\n",
    "    loss_array[i] = Loss\n",
    "    \n",
    "    #continously print loss\n",
    "    if(i%10 == 0):\n",
    "        print('Loss at epoch %d = %f, data loss:%f'%(i, Loss, dataLoss))\n",
    "    i += 1\n",
    "    step_size = learning_rate_schedule(Loss, learning_rate)\n",
    "    pass \n",
    "print('Loss at end = %f'%(Loss))\n",
    "print('Data Loss:', dataLoss, 'Reg Loss:' , regLoss)\n",
    "print('Test loss:', test())\n",
    "elapsed = time.time()- started\n",
    "print('Time taken: %.2f seconds'%(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots\n",
    "times = linspace(0, 9.99, M)\n",
    "dt = times[1] - times[0]\n",
    "tau = (10 - times).reshape(M, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first plot\n",
    "X1 = 2.5 * ones((M,D))\n",
    "X1 [:, 0] = times\n",
    "\n",
    "for i in range(M-1):\n",
    "    X1[i+1 ,1:D] = X1[i, 1:D] + X1[i, 1:D] * r * dt + sigma * sqrt(dt) * X1[i, 1:D] * random.randn(D-1)\n",
    "\n",
    "Y1 = forwardPass(hidden_layers, X1)[0]\n",
    "meanX = mean(X1[:, 1:D], axis=1).reshape(M, 1)\n",
    "dp = (log(meanX/K) + (r + 0.5 * sigma**2) * tau) / (sigma * sqrt(tau))\n",
    "dm = dp - sigma * sqrt(tau)\n",
    "C1 = norm.cdf(dp) * meanX - norm.cdf(dm)*K*exp(-r * tau)\n",
    "\n",
    "plot(times, meanX, '-', label = 'Asset Price')\n",
    "plot(times, C1, 'r--', label = 'Analytic', lw =2.0)\n",
    "plot( times, Y1, '-', label = 'Deep Learning', lw =1.0)\n",
    "tick_params (labelsize =16)\n",
    "xlabel ('Time', fontsize =16)\n",
    "ylabel('Value', fontsize =16)\n",
    "legend()\n",
    "show()\n",
    "\n",
    "plot(arange(epochs), test_array, 'r-', label= 'Validation set', lw =1.0)\n",
    "plot(arange(epochs), loss_array, '-', label= 'Training set', lw =1.0)\n",
    "tick_params(labelsize =16)\n",
    "xlabel('Epochs', fontsize =16)\n",
    "ylabel('loss', fontsize =16)\n",
    "axis([0, epochs, 0, 1])\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second plot\n",
    "X2 = 2.5 * ones((M,D))\n",
    "X2 [:, 0] = times\n",
    "\n",
    "for i in range(M-1):\n",
    "    X2[i+1 ,1:D] = X2[i, 1:D] + X2[i, 1:D] * r * dt + sigma * sqrt(dt) * X2[i, 1:D] * random.randn(D-1)\n",
    "\n",
    "Y2 = forwardPass(hidden_layers, X2)[0]\n",
    "meanX = mean(X2[:, 1:D], axis=1).reshape(M, 1)\n",
    "dp = (log(meanX/K) + (r + 0.5 * sigma**2) * tau) / (sigma * sqrt(tau))\n",
    "dm = dp - sigma * sqrt(tau)\n",
    "C2 = norm.cdf(dp) * meanX - norm.cdf(dm)*K*exp(-r * tau)\n",
    "\n",
    "\n",
    "plot(times, meanX, '-', label = 'Asset Price')\n",
    "plot(times, C2, 'r--', label = 'Analytic', lw =2.0)\n",
    "plot( times, Y2, '-', label = 'Deep Learning', lw =1.0)\n",
    "tick_params (labelsize =16)\n",
    "xlabel ('Time', fontsize =16)\n",
    "ylabel('Value', fontsize =16)\n",
    "legend()\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
