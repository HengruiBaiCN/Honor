{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt, gridspec\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as tgrad\n",
    "from tqdm import tqdm, trange\n",
    "from pyDOE import lhs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "seed = 1234\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU:', use_gpu)\n",
    "# device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "device = 'cuda'\n",
    "print('Device:', device)\n",
    "\n",
    "def is_cuda(data):\n",
    "    data = data.to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = np.array([0.0, 0.0])\n",
    "ub = np.array([1.0, 130.0])\n",
    "def random_fun(num):\n",
    "    temp = torch.from_numpy(lb + (ub - lb) * lhs(2, num)).float()\n",
    "    temp = temp.to(device)\n",
    "    return temp\n",
    "\n",
    "def train(num):\n",
    "    # test data\n",
    "    n_st_train = np.concatenate([np.random.uniform(*t_range, (num, 1)), \n",
    "                        np.random.uniform(*S_range, (num, 1))], axis=1)\n",
    "    # n_v_train = np.zeros((num, 1))\n",
    "    temp = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Network and Traning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.iter = 0\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        for i in range(len(layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linear[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linear[i].bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x).to(device)\n",
    "        a = self.activation(self.linear[0](x))\n",
    "        for i in range(1, len(self.layers) - 2):\n",
    "            z = self.linear[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linear[-1](a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, net, x_label, x_labels, x_f_loss_fun,\n",
    "                 x_test, x_test_exact\n",
    "                 ):\n",
    "\n",
    "        self.x_label_s = None\n",
    "        self.x_f_s = None\n",
    "        self.s_collect = []\n",
    "\n",
    "        self.optimizer_LBGFS = None\n",
    "        self.net = net\n",
    "\n",
    "        self.x_label = x_label\n",
    "        self.x_labels = x_labels\n",
    "\n",
    "        self.x_f_N = None\n",
    "        self.x_f_M = None\n",
    "\n",
    "        self.x_f_loss_fun = x_f_loss_fun\n",
    "\n",
    "        self.x_test = x_test\n",
    "        self.x_test_exact = x_test_exact\n",
    "\n",
    "        self.start_loss_collect = False\n",
    "        self.x_label_loss_collect = []\n",
    "        self.x_f_loss_collect = []\n",
    "        self.x_test_estimate_collect = []\n",
    "    \n",
    "    # predict U\n",
    "    def train_U(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def predict_U(self, x):\n",
    "        return self.train_U(x)\n",
    "\n",
    "    def likelihood_loss(self, loss_e, loss_l):\n",
    "        loss = torch.exp(-self.x_f_s) * loss_e.detach() + self.x_f_s \\\n",
    "            + torch.exp(-self.x_label_s) * loss_l.detach() + self.x_label_s\n",
    "        return loss\n",
    "\n",
    "    def true_loss(self, loss_e, loss_l):\n",
    "        return torch.exp(-self.x_f_s.detach()) * loss_e + torch.exp(-self.x_label_s.detach()) * loss_l\n",
    "\n",
    "    # computer backward loss\n",
    "    def epoch_loss(self):\n",
    "        # x_f = torch.cat((self.x_f_N, self.x_f_M), dim=0)\n",
    "        x_f = train(10000)\n",
    "        loss_equation = self.x_f_loss_fun(x_f, self.train_U)\n",
    "\n",
    "        loss_label = torch.mean(\n",
    "            (self.train_U(self.x_label) - self.x_labels) ** 2)\n",
    "\n",
    "        if self.start_loss_collect:\n",
    "            self.x_f_loss_collect.append([self.net.iter, loss_equation.item()])\n",
    "            self.x_label_loss_collect.append(\n",
    "                [self.net.iter, loss_label.item()])\n",
    "        return loss_equation, loss_label\n",
    "\n",
    "    # computer backward loss\n",
    "    def LBGFS_epoch_loss(self):\n",
    "        self.optimizer_LBGFS.zero_grad()\n",
    "        # x_f = torch.cat((self.x_f_N, self.x_f_M), dim=0)\n",
    "        x_f = train(10000)\n",
    "        loss_equation = self.x_f_loss_fun(x_f, self.train_U)\n",
    "        loss_label = torch.mean(\n",
    "            (self.train_U(self.x_label) - self.x_labels) ** 2)\n",
    "\n",
    "        if self.start_loss_collect:\n",
    "            self.x_f_loss_collect.append([self.net.iter, loss_equation.item()])\n",
    "            self.x_label_loss_collect.append(\n",
    "                [self.net.iter, loss_label.item()])\n",
    "\n",
    "        loss = self.true_loss(loss_equation, loss_label)\n",
    "        loss.backward()\n",
    "        self.net.iter += 1\n",
    "        print('Iter:', self.net.iter, 'Loss:', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self):\n",
    "        pred = self.train_U(self.x_test).cuda().detach().numpy()\n",
    "        exact = self.x_test_exact.cuda().detach().numpy()\n",
    "        error = np.linalg.norm(pred - exact, 2) / np.linalg.norm(exact, 2)\n",
    "        return error\n",
    "    \n",
    "    def run_baseline(self):\n",
    "        optimizer_adam = torch.optim.Adam(self.net.parameters(), lr=adam_lr)\n",
    "        self.optimizer_LBGFS = torch.optim.LBFGS(self.net.parameters(), lr=lbgfs_lr,\n",
    "                                                 max_iter=lbgfs_iter)\n",
    "        pbar = trange(adam_iter, ncols=100)\n",
    "        for i in pbar:\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss_e, loss_label = self.epoch_loss()\n",
    "            loss = self.true_loss(loss_e, loss_label)\n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            self.net.iter += 1\n",
    "            pbar.set_postfix({'Iter': self.net.iter,\n",
    "                              'Loss': '{0:.2e}'.format(loss.item())\n",
    "                              })\n",
    "\n",
    "        print('Adam done!')\n",
    "        self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "        print('LBGFS done!')\n",
    "\n",
    "        error = self.evaluate()\n",
    "        print('Test_L2error:', '{0:.2e}'.format(error))\n",
    "\n",
    "    def run_AM_AW1(self):\n",
    "        self.x_f_s = nn.Parameter(self.x_f_s, requires_grad=True)\n",
    "        self.x_label_s = nn.Parameter(self.x_label_s, requires_grad=True)\n",
    "\n",
    "        # for move_count in range(AM_count):\n",
    "            \n",
    "        self.optimizer_LBGFS = torch.optim.LBFGS(self.net.parameters(), lr=lbgfs_lr, max_iter=lbgfs_iter)\n",
    "        optimizer_adam = torch.optim.Adam(self.net.parameters(), lr=adam_lr)\n",
    "        \n",
    "        optimizer_adam_weight = torch.optim.Adam([self.x_f_s] + [self.x_label_s], lr=AW_lr)\n",
    "\n",
    "        pbar = trange(adam_iter, ncols=100)\n",
    "        for i in pbar:\n",
    "            self.s_collect.append([self.net.iter, self.x_f_s.item(), self.x_label_s.item()])\n",
    "\n",
    "            loss_e, loss_label = self.epoch_loss()\n",
    "\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss = self.true_loss(loss_e, loss_label)\n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            self.net.iter += 1\n",
    "            pbar.set_postfix({'Iter': self.net.iter,\n",
    "                                'Loss': '{0:.2e}'.format(loss.item())\n",
    "                                })\n",
    "            \n",
    "            optimizer_adam_weight.zero_grad()\n",
    "            loss = self.likelihood_loss(loss_e, loss_label)\n",
    "            loss.backward()\n",
    "            optimizer_adam_weight.step()\n",
    "\n",
    "        print('Adam done!')\n",
    "        self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "        print('LBGFS done!')\n",
    "\n",
    "            # error = self.evaluate()\n",
    "            # print('change_counts', move_count,\n",
    "            #       'Test_L2error:', '{0:.2e}'.format(error))\n",
    "            # self.x_test_estimate_collect.append(\n",
    "            #     [move_count, '{0:.2e}'.format(error)])\n",
    "\n",
    "            # if AM_type == 0:\n",
    "            #     x_init = random_fun(100000)\n",
    "            #     x_init_residual = abs(self.x_f_loss_fun(x_init, self.train_U))\n",
    "            #     x_init_residual = x_init_residual.cuda.detach().numpy()\n",
    "            #     err_eq = np.power(x_init_residual, AM_K) / \\\n",
    "            #         np.power(x_init_residual, AM_K).mean()\n",
    "            #     err_eq_normalized = (err_eq / sum(err_eq))[:, 0]\n",
    "            #     X_ids = np.random.choice(\n",
    "            #         a=len(x_init), size=M, replace=False, p=err_eq_normalized)\n",
    "            #     self.x_f_M = x_init[X_ids]\n",
    "\n",
    "            # elif AM_type == 1:\n",
    "            #     x_init = random_fun(100000)\n",
    "            #     x = Variable(x_init, requires_grad=True)\n",
    "            #     u = self.train_U(x)\n",
    "            #     dx = torch.autograd.grad(\n",
    "            #         u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "            #     grad_x1 = dx[:, [0]].squeeze()\n",
    "            #     grad_x2 = dx[:, [1]].squeeze()\n",
    "            #     dx = torch.sqrt(1 + grad_x1 ** 2 + grad_x2 **\n",
    "            #                     2).cuda().detach().numpy()\n",
    "            #     err_dx = np.power(dx, AM_K) / np.power(dx, AM_K).mean()\n",
    "            #     p = (err_dx / sum(err_dx))\n",
    "            #     X_ids = np.random.choice(\n",
    "            #         a=len(x_init), size=M, replace=False, p=p)\n",
    "            #     self.x_f_M = x_init[X_ids]\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.x_f_N = random_fun(N)\n",
    "        self.x_f_M = random_fun(M)\n",
    "\n",
    "        # self.x_f_s = is_cuda(-torch.log(torch.tensor(1.).float()))\n",
    "        # self.x_label_s = is_cuda(\n",
    "        #     -torch.log(torch.tensor(100.).float()))  # torch.exp(-self.x_label_s.detach()) = 100\n",
    "\n",
    "        self.x_f_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_label_s = is_cuda(torch.tensor(0.).float())\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.run_AM_AW1()\n",
    "        elapsed = time.time() - start_time\n",
    "        print('Training time: %.2f' % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40 # Strike price\n",
    "r = 0.05 # Risk-free rate\n",
    "sigma = 0.25 # Volatility\n",
    "T = 1 # Time to maturity\n",
    "S_range = [0, 130] # Stock price range\n",
    "Smax = 130\n",
    "t_range = [0, T] # Time range\n",
    "gs = lambda x: np.fmax(x-K, 0) # Payoff function\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 1] # Network structure\n",
    "net = Net(layers).to(device) #  Network initialization\n",
    "\n",
    "N = 1000 \n",
    "M = 1000 \n",
    "Nbc = 100 # number of boundary points\n",
    "\n",
    "adam_iter, lbgfs_iter = 60000, 50000\n",
    "adam_lr, lbgfs_lr = 0.001, 0.5\n",
    "\n",
    "model_type = 2  # 0:baseline  1:AM  2:AM_AW\n",
    "\n",
    "AM_type = 1  # 0:RAM  1:WAM\n",
    "AM_K = 1\n",
    "AM_count = 10\n",
    "\n",
    "AW_lr = 0.001 # Learning rate for the weights\n",
    "\n",
    "lossfunction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "n_st_train = np.concatenate([np.random.uniform(*t_range, (N, 1)), \n",
    "                      np.random.uniform(*S_range, (N, 1))], axis=1)\n",
    "n_v_train = np.zeros((N, 1))\n",
    "\n",
    "# bc data\n",
    "# final condition (t = T, S is randomized)\n",
    "f_st_train = np.concatenate([np.ones((Nbc, 1)),\n",
    "                np.random.uniform(*S_range, (Nbc, 1))], axis=1)\n",
    "f_v_train = gs(f_st_train[:, 1]).reshape(-1, 1)\n",
    "\n",
    "# lower boundary condition (S = 0, t is randomized)\n",
    "lb_st = np.concatenate([np.random.uniform(*t_range, (Nbc, 1)),\n",
    "                    0 * np.ones((Nbc, 1))], axis=1)\n",
    "lb_v = np.zeros((Nbc, 1))\n",
    "\n",
    "# upper boundary condition (S = Smax, t is randomized)\n",
    "ub_st = np.concatenate([np.random.uniform(*t_range, (Nbc, 1)), \n",
    "                    Smax * np.ones((Nbc, 1))], axis=1)\n",
    "ub_v = (Smax - K*np.exp(-r*(T-ub_st[:, 0].reshape(-1)))).reshape(-1, 1)\n",
    "\n",
    "# append boundary condition training points (edge points)\n",
    "bc_st_train = np.vstack([lb_st, ub_st, f_st_train])\n",
    "bc_v_train = np.vstack([lb_v, ub_v, f_v_train])\n",
    "\n",
    "\n",
    "# save training data points to tensor and send to device\n",
    "x_test = torch.from_numpy(n_st_train).float().requires_grad_().to(device)\n",
    "x_test_exact = torch.from_numpy(n_v_train).float().to(device)\n",
    "\n",
    "x_bc = torch.from_numpy(bc_st_train).float().to(device)\n",
    "u_bc = torch.from_numpy(bc_v_train).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# x_test = is_cuda(n_st_train)\n",
    "# x_test_exact = is_cuda(n_v_train)\n",
    "\n",
    "# x_bc = is_cuda(bc_st_train)\n",
    "# u_bc = is_cuda(bc_v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model and Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_f_loss_fun(x, train_U):\n",
    "    if not x.requires_grad:\n",
    "        x = Variable(x, requires_grad=True)\n",
    "        \n",
    "    # PDE Round\n",
    "    y1_hat = train_U(x)\n",
    "    grads = tgrad.grad(y1_hat, x, grad_outputs=torch.ones(y1_hat.shape).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads)\n",
    "    dVdt, dVdS = grads[:, 0].view(-1, 1), grads[:, 1].view(-1, 1)\n",
    "    grads2nd = tgrad.grad(dVdS, x, grad_outputs=torch.ones(dVdS.shape).cuda(), create_graph=True, only_inputs=True)[0]\n",
    "    # print(grads2nd)\n",
    "    d2VdS2 = grads2nd[:, 1].view(-1, 1)\n",
    "    S1 = x[:, 1].view(-1, 1)\n",
    "    pde_loss = lossfunction(-dVdt, 0.5*((sigma*S1)**2)*d2VdS2 + r*S1*dVdS - r*y1_hat)\n",
    "\n",
    "    return pde_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████▊                     | 16116/60000 [01:38<04:28, 163.59it/s, Iter=16116, Loss=3.33e-01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\WAWAM-BS.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Model(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     net\u001b[39m=\u001b[39mnet,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x_label\u001b[39m=\u001b[39mx_bc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     x_test_exact\u001b[39m=\u001b[39mx_test_exact,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mx_test_estimate_collect)\n",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\WAWAM-BS.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_label_s \u001b[39m=\u001b[39m is_cuda(torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_AM_AW1()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining time: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m elapsed)\n",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\WAWAM-BS.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m optimizer_adam\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrue_loss(loss_e, loss_label)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m optimizer_adam\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X15sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39miter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\honor\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    net=net,\n",
    "    x_label=x_bc,\n",
    "    x_labels=u_bc,\n",
    "    x_f_loss_fun=x_f_loss_fun,\n",
    "    x_test=x_test,\n",
    "    x_test_exact=x_test_exact,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "print(model.x_test_estimate_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\WAWAM-BS.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# draw_exact()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# draw_exact_points(model.x_f_M)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# draw_exact_points(model.x_f_M, show_exact=False)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# draw_error()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# draw_some_t()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m draw_epoch_loss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# draw_epoch_w()\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\hengr\\Programs\\Projects\\Honor-Project\\Honor\\WAWAM-BS.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39myscale(\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(x_label_loss_collect[:, \u001b[39m0\u001b[39;49m], x_label_loss_collect[:, \u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLabel_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39m$Epoch$\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hengr/Programs/Projects/Honor-Project/Honor/WAWAM-BS.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39m$Loss$\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_epoch_loss():\n",
    "    x_label_loss_collect = np.array(model.x_label_loss_collect)\n",
    "    x_f_loss_collect = np.array(model.x_f_loss_collect)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_label_loss_collect[:, 0], x_label_loss_collect[:, 1], 'b-', label='Label_loss')\n",
    "    plt.xlabel('$Epoch$')\n",
    "    plt.ylabel('$Loss$')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_f_loss_collect[:, 0], x_f_loss_collect[:, 1], 'r-', label='PDE_loss')\n",
    "    plt.xlabel('$Epoch$')\n",
    "    plt.ylabel('$Loss$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# draw_exact()\n",
    "# draw_exact_points(model.x_f_M)\n",
    "# draw_exact_points(model.x_f_M, show_exact=False)\n",
    "# draw_exact_points(model.x_f_M, N_points=model.x_f_N)\n",
    "# draw_exact_points(model.x_f_M, N_points=model.x_f_N, show_exact=False)\n",
    "# draw_residual()\n",
    "# draw_error()\n",
    "# draw_some_t()\n",
    "draw_epoch_loss()\n",
    "# draw_epoch_w()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
