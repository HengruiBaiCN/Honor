{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from finite_difference_class.ipynb\n",
      "4.072882278148043\n",
      "-1.6291077072251005e+53\n",
      "4.065801939431454\n",
      "4.071594188049893\n",
      "4.072254507998114\n",
      "4.072238354486828\n"
     ]
    }
   ],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%capture\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from py_vollib import black_scholes_merton as bsm\n",
    "from progressbar import ProgressBar\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import uniform\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.sparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "from finite_difference_class import FDExplicitEu, FDImplicitEu, FDCnEu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random data generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S (spot price)\n",
    "# gamma\n",
    "def thisS(q):\n",
    "    return gamma.ppf(q, a = 100, scale = 1)\n",
    "\n",
    "# K (strike price)\n",
    "# uniform (lower = 50, upper = 200)\n",
    "def thisK(q):\n",
    "    return uniform.ppf(q, 50, 200)\n",
    "\n",
    "# (interest rate)\n",
    "# uniform (lower = 0.01, upper = 0.18)\n",
    "def thisR(q):\n",
    "    return uniform.ppf(q, 0.01, 0.18)\n",
    "\n",
    "\n",
    "# D (dividend)\n",
    "# uniform (lower = 0.01, upper = 0.18)\n",
    "def thisD(q):\n",
    "    return 0\n",
    "    # return uniform.ppf(q, 0.01, 0.18)\n",
    "\n",
    "# t (time-to-maturity)\n",
    "# t will be 3, 6, 9, 12 months for all examples (0.25, 0.5, 0.75, 1 year)\n",
    "\n",
    "# sigma (volatility)\n",
    "# beta (add small amount so volatility cannot be zero)\n",
    "def thisSigma(q):\n",
    "    return (beta.ppf(q, a = 2, b = 5) + 0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full DataSet\n",
    "A “as complete as possible” simulated dataset with almost a million observations. Due to constraints on computational resources, this was as a big a dataset we could simulate under reasonable circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.010000\n",
      "1     0.099091\n",
      "2     0.188182\n",
      "3     0.277273\n",
      "4     0.366364\n",
      "5     0.455455\n",
      "6     0.544545\n",
      "7     0.633636\n",
      "8     0.722727\n",
      "9     0.811818\n",
      "10    0.900909\n",
      "11    0.990000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "num_increment = 12\n",
    "percentiles = pd.Series(np.linspace(0.01, 0.99, num_increment))\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sklearn.model_selection._search.ParameterGrid object at 0x00000184AB7180D0>\n",
      "{'S': array([ 78.21598305,  87.37013918,  91.09674336,  93.88453878,\n",
      "        96.29726243,  98.55429076, 100.78778343, 103.11419426,\n",
      "       105.68194588, 108.7585023 , 113.06691449, 124.72256149]), 'K': array([ 52.        ,  69.81818182,  87.63636364, 105.45454545,\n",
      "       123.27272727, 141.09090909, 158.90909091, 176.72727273,\n",
      "       194.54545455, 212.36363636, 230.18181818, 248.        ]), 'q': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), 't': array([0.25, 0.5 , 0.75, 1.  ]), 'r': array([0.0118    , 0.02783636, 0.04387273, 0.05990909, 0.07594545,\n",
      "       0.09198182, 0.10801818, 0.12405455, 0.14009091, 0.15612727,\n",
      "       0.17216364, 0.1882    ]), 'sigma': array([0.02776319, 0.09311179, 0.13570038, 0.17347384, 0.20984291,\n",
      "       0.24654192, 0.28496977, 0.32672565, 0.37420516, 0.43199458,\n",
      "       0.51235236, 0.70668633])}\n"
     ]
    }
   ],
   "source": [
    "S = percentiles.apply(thisS).to_numpy()\n",
    "# print(S)\n",
    "K = percentiles.apply(thisK).to_numpy()\n",
    "q = percentiles.apply(thisD).to_numpy()\n",
    "t = np.array([.25, .5, .75, 1])\n",
    "r = percentiles.apply(thisR).to_numpy()\n",
    "sigma = percentiles.apply(thisSigma).to_numpy()\n",
    "\n",
    "param_grid = {'S': S, 'K': K, 'q': q, 't': t, 'r': r, 'sigma': sigma}\n",
    "grid = ParameterGrid(param_grid)\n",
    "print(grid)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% |###                                                                     |\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m prices\u001b[39m.\u001b[39mappend(bsm\u001b[39m.\u001b[39mblack_scholes_merton(flag \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m, S \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m], K \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m], q \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m], t \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m], r \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m], sigma \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m      9\u001b[0m option \u001b[39m=\u001b[39m FDImplicitEu(S0 \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m], K \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m], r \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m], T \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m], sigma \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m'\u001b[39m], Smax \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m5\u001b[39m\u001b[39m*\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mK\u001b[39m\u001b[39m\"\u001b[39m]), M \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, N \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, is_call\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m fdmprices\u001b[39m.\u001b[39mappend(option\u001b[39m.\u001b[39;49mprice())\n\u001b[0;32m     11\u001b[0m tmp\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mSeries(params)\u001b[39m.\u001b[39mto_frame()\u001b[39m.\u001b[39mT)\n\u001b[0;32m     12\u001b[0m \u001b[39m# fullDF = fullDF.append(pd.Series(params), ignore_index = True)\u001b[39;00m\n",
      "File \u001b[1;32m<string>:42\u001b[0m, in \u001b[0;36mprice\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m<string>:31\u001b[0m, in \u001b[0;36m_traverse_grid_\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\tf\\lib\\site-packages\\scipy\\linalg\\_basic.py:222\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b, sym_pos, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m assume_a \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgen\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    220\u001b[0m     gecon, getrf, getrs \u001b[39m=\u001b[39m get_lapack_funcs((\u001b[39m'\u001b[39m\u001b[39mgecon\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgetrf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgetrs\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    221\u001b[0m                                            (a1, b1))\n\u001b[1;32m--> 222\u001b[0m     lu, ipvt, info \u001b[39m=\u001b[39m getrf(a1, overwrite_a\u001b[39m=\u001b[39;49moverwrite_a)\n\u001b[0;32m    223\u001b[0m     _solve_check(n, info)\n\u001b[0;32m    224\u001b[0m     x, info \u001b[39m=\u001b[39m getrs(lu, ipvt, b1,\n\u001b[0;32m    225\u001b[0m                     trans\u001b[39m=\u001b[39mtrans, overwrite_b\u001b[39m=\u001b[39moverwrite_b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar()\n",
    "fullDF = pd.DataFrame()\n",
    "prices = []\n",
    "fdmprices= []\n",
    "tmp = []\n",
    "for params in pbar(grid):\n",
    "    prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'], q = params['q'], t = params['t'], r = params['r'], sigma = params['sigma']))\n",
    "    \n",
    "    option = FDImplicitEu(S0 = params['S'], K = params['K'], r = params['r'], T = params['t'], sigma = params['sigma'], Smax = int(5*params[\"K\"]), M = 100, N = 100, is_call=False)\n",
    "    fdmprices.append(option.price())\n",
    "    tmp.append(pd.Series(params).to_frame().T)\n",
    "    # fullDF = fullDF.append(pd.Series(params), ignore_index = True)\n",
    "    pass\n",
    "fullDF = pd.concat(tmp, ignore_index=True) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap price to first column\n",
    "fullDF['price'] = prices\n",
    "fullDF['fdm-price'] = fdmprices\n",
    "\n",
    "# output to csv\n",
    "fullDF.to_csv('dataFull.csv', index = False)\n",
    "print(fullDF.head())\n",
    "print(fullDF.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse DataSet\n",
    "A “sparse” version of the full dataset. This set covered the same ranges for each of the parameters, but simply had fewer observations for each. This dataset ended up having 12,500 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_increment = 5\n",
    "percentiles = pd.Series(np.linspace(0.01, 0.99, num_increment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = percentiles.apply(thisS).to_numpy()\n",
    "K = percentiles.apply(thisK).to_numpy()\n",
    "q = percentiles.apply(thisD).to_numpy()\n",
    "t = np.array([0.25, 0.5, 0.75, 1])\n",
    "r = percentiles.apply(thisR).to_numpy()\n",
    "sigma = percentiles.apply(thisSigma).to_numpy()\n",
    "\n",
    "param_grid = {'S': S, 'K' : K, 'q' : q, 't' : t, 'r' : r, 'sigma' : sigma}\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar()\n",
    "sparseDF = pd.DataFrame()\n",
    "prices = []\n",
    "fdmprices= []\n",
    "tmp = []\n",
    "for params in pbar(grid):\n",
    "    prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'], q = params['q'], t = params['t'],r = params['r'], sigma = params['sigma']))\n",
    "    \n",
    "    option = FDImplicitEu(S0 = params['S'], K = params['K'], r = params['r'], T = params['t'], sigma = params['sigma'], Smax = int(5*params[\"K\"]), M = 100, N = 100, is_call=False)\n",
    "    fdmprices.append(option.price())\n",
    "    tmp.append(pd.Series(params).to_frame().T)\n",
    "    # sparseDF = sparseDF.append(pd.Series(params), ignore_index = True)\n",
    "    pass\n",
    "# print(len(prices))\n",
    "# print(len(tmp))\n",
    "# print(tmp)\n",
    "sparseDF = pd.concat(tmp, ignore_index=True) # type: ignore\n",
    "# print(len(sparseDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      K          S    q       r     sigma     t          price     fdm-price\n",
      "0  52.0  78.215983  0.0  0.0118  0.027763  0.25  1.210258e-194  4.762734e-26\n",
      "1  52.0  78.215983  0.0  0.0118  0.027763  0.50  2.595718e-100  8.553145e-23\n",
      "2  52.0  78.215983  0.0  0.0118  0.027763  0.75   9.113920e-69  6.492417e-21\n",
      "3  52.0  78.215983  0.0  0.0118  0.027763  1.00   6.009819e-53  1.350024e-19\n",
      "4  52.0  78.215983  0.0  0.0118  0.164249  0.25   2.693530e-07  5.890810e-06\n",
      "           K           S    q       r     sigma     t       price   fdm-price\n",
      "12495  248.0  124.722561  0.0  0.1882  0.387418  1.00   83.589686   83.704518\n",
      "12496  248.0  124.722561  0.0  0.1882  0.706686  0.25  112.709815  112.748824\n",
      "12497  248.0  124.722561  0.0  0.1882  0.706686  0.50  105.733883  105.766608\n",
      "12498  248.0  124.722561  0.0  0.1882  0.706686  0.75  100.556324  100.540536\n",
      "12499  248.0  124.722561  0.0  0.1882  0.706686  1.00   96.170059   96.014092\n"
     ]
    }
   ],
   "source": [
    "# swap price to first column\n",
    "sparseDF['price'] = prices\n",
    "sparseDF['fdm-price'] = fdmprices\n",
    "\n",
    "# output to csv\n",
    "sparseDF.to_csv('dataSparse.csv', index = False)\n",
    "print(sparseDF.head())\n",
    "print(sparseDF.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme DataSet\n",
    "A “extremes” dataset was similar to the full dataset. The only difference was that the spot prices, instead of being generated from the gamma distribution mentioned above, were instead distributed uniformly from 90 to 110. The purpose of this dataset is to test whether the neural network can generalize from this set of limited data to more “extreme” situations. This set also had almost a million observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_increment = 12\n",
    "percentiles = pd.Series(np.linspace(0.01, 0.99, num_increment))\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_extremes_S (q):\n",
    "    return uniform.ppf(q, 90, 110)\n",
    "S = percentiles.apply(this_extremes_S).to_numpy()\n",
    "K = percentiles.apply(thisK).to_numpy()\n",
    "q = percentiles.apply(thisD).to_numpy()\n",
    "t = np.array([.25, .5, .75, 1])\n",
    "r = percentiles.apply(thisR).to_numpy()\n",
    "sigma = percentiles.apply(thisSigma).to_numpy()\n",
    "\n",
    "param_grid = {'S': S, 'K' : K, 'q' : q, 't' : t, 'r' : r, 'sigma' : sigma}\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "extremesDF = pd.DataFrame()\n",
    "prices = []\n",
    "fdmprices= []\n",
    "tmp = []\n",
    "for params in pbar(grid):\n",
    "    prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'], q = params['q'], t = params['t'],r = params['r'], sigma = params['sigma']))\n",
    "    option = FDImplicitEu(S0 = params['S'], K = params['K'], r = params['r'], T = params['t'], sigma = params['sigma'], Smax = int(5*params[\"K\"]), M = 100, N = 100, is_call=False)\n",
    "    fdmprices.append(option.price())\n",
    "    tmp.append(pd.Series(params).to_frame().T)\n",
    "    pass\n",
    "extremesDF = pd.concat(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap price to first column\n",
    "extremesDF['price'] = prices\n",
    "extremesDF['fdm-price'] = fdmprices\n",
    "\n",
    "# output to csv\n",
    "extremesDF.to_csv('dataExtremes.csv', index = True)\n",
    "print(extremesDF.head())\n",
    "print(extremesDF.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing neural network (full data)\n",
    "fullDF = pd.read_csv(\"dataFull.csv\")\n",
    "\n",
    "# create model \n",
    "def baseline_model():\n",
    "    # layers\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_full = baseline_model()\n",
    "X = fullDF[['S','K','q','r','sigma','t']]\n",
    "y = fullDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_full = model_full.fit(X_train, y_train, batch_size = 64, epochs = 20, verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_full.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "X_test_full = X_test\n",
    "y_test_full = y_test\n",
    "model_full.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing neural network (sparse data)\n",
    "sparseDF = pd.read_csv(\"dataSparse.csv\")\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_sparse = baseline_model()\n",
    "X = sparseDF[['S','K','q','r','sigma','t']]\n",
    "\n",
    "y = sparseDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_sparse = model_sparse.fit(X_train, y_train, batch_size = 64, epochs = 20, verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_sparse.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model_sparse.evaluate(x=X_test_full, y=y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing neural network (extremes data)\n",
    "extremesDF = pd.read_csv(\"dataExtremes.csv\")\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_extremes = baseline_model()\n",
    "X = extremesDF[['S','K','q','r','sigma','t']]\n",
    "y = extremesDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_extremes = model_extremes.fit(X_train, y_train, batch_size = 64, epochs = 20, verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_extremes.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model_extremes.evaluate(x=X_test_full, y=y_test_full)\n",
    "\n",
    "tableOutput = pd.DataFrame({'Full':history_full.history['val_loss'], \\\n",
    "'Sparse':history_sparse.history['val_loss'], \\\n",
    "'Extremes':history_extremes.history['val_loss']}, columns=['Full', 'Sparse', 'Extremes'])\n",
    "tableOutput.to_csv(\"tableResultsValidaton.csv\")\n",
    "\n",
    "print(len(fullDF.index))\n",
    "print(len(sparseDF.index))\n",
    "print(len(extremesDF.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b299ead75a435dd81db5bc5fe55f9447bf2e38f1c54243924f444b1275a07c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
