{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%capture\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from py_vollib import black_scholes_merton as bsm\n",
    "from progressbar import ProgressBar\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import uniform\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S (spot price)\n",
    "# gamma\n",
    "def thisS(q):\n",
    "    return gamma.ppf(q, a = 100, scale = 1)\n",
    "\n",
    "# K (strike price)\n",
    "# uniform (lower = 50, upper = 200)\n",
    "def thisK(q):\n",
    "    return uniform.ppf(q, 50, 200)\n",
    "\n",
    "# (interest rate)\n",
    "# uniform (lower = 0.01, upper = 0.18)\n",
    "def thisR(q):\n",
    "    return uniform.ppf(q, 0.01, 0.18)\n",
    "\n",
    "# D (dividend)\n",
    "# uniform (lower = 0.01, upper = 0.18)\n",
    "def thisD(q):\n",
    "    return uniform.ppf(q, 0.01, 0.18)\n",
    "\n",
    "# t (time-to-maturity)\n",
    "# t will be 3, 6, 9, 12 months for all examples (0.25, 0.5, 0.75, 1 year)\n",
    "\n",
    "# sigma (volatility)\n",
    "# beta (add small amount so volatility cannot be zero)\n",
    "def thisSigma(q):\n",
    "    return (beta.ppf(q, a = 2, b = 5) + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid for parameter 'S' needs to be a list or a numpy array, but got 0       0.000000\n1      86.877620\n2      90.808086\n3      93.675159\n4      96.131942\n5      98.417953\n6     100.672612\n7     103.015743\n8     105.597693\n9     108.687444\n10    113.010524\n11    124.722561\ndtype: float64 (of type Series) instead. Single values need to be wrapped in a list with one element.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m sigma \u001b[39m=\u001b[39m percentiles\u001b[39m.\u001b[39mapply(thisSigma)\n\u001b[0;32m     11\u001b[0m param_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m: S, \u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m: K, \u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m: q, \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m: t, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m: r, \u001b[39m'\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m'\u001b[39m: sigma}\n\u001b[1;32m---> 12\u001b[0m grid \u001b[39m=\u001b[39m ParameterGrid(param_grid)\n\u001b[0;32m     13\u001b[0m pbar \u001b[39m=\u001b[39m ProgressBar()\n\u001b[0;32m     14\u001b[0m fullDF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32mc:\\Users\\hengr\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:117\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[1;34m(self, param_grid)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    111\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter array for \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m should be one-dimensional, got:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m with shape \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    115\u001b[0m     value, (np\u001b[39m.\u001b[39mndarray, Sequence)\n\u001b[0;32m    116\u001b[0m ):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid for parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m needs to be a list or a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m numpy array, but got \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m (of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) instead. Single values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mneed to be wrapped in a list with one element.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid for parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m need \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be a non-empty sequence, got: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter grid for parameter 'S' needs to be a list or a numpy array, but got 0       0.000000\n1      86.877620\n2      90.808086\n3      93.675159\n4      96.131942\n5      98.417953\n6     100.672612\n7     103.015743\n8     105.597693\n9     108.687444\n10    113.010524\n11    124.722561\ndtype: float64 (of type Series) instead. Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "num_increment = 12\n",
    "percentiles = pd.Series(np.linspace(0, 0.99, num_increment))\n",
    "\n",
    "S = percentiles.apply(thisS)\n",
    "K = percentiles.apply(thisK)\n",
    "q = percentiles.apply(thisD)\n",
    "t = np.array([.25, .5, .75, 1])\n",
    "r = percentiles.apply(thisR)\n",
    "sigma = percentiles.apply(thisSigma)\n",
    "\n",
    "param_grid = {'S': S, 'K': K, 'q': q, 't': t, 'r': r, 'sigma': sigma}\n",
    "grid = ParameterGrid(param_grid)\n",
    "pbar = ProgressBar()\n",
    "fullDF = pd.DataFrame()\n",
    "prices = []\n",
    "for params in pbar(grid):\n",
    "    prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'],\\\n",
    "    q = params['q'], t = params['t'],r = params['r'], sigma = params['sigma']))\n",
    "    fullDF = fullDF.append(pd.Series(params), ignore_index = True)\n",
    "    pass\n",
    "\n",
    "# swap price to first column\n",
    "fullDF['price'] = prices\n",
    "\n",
    "# output to csv\n",
    "fullDF.to_csv('dataFull.csv', index = False)\n",
    "print(fullDF.head())\n",
    "print(fullDF.tail())\n",
    "num_increment = 5\n",
    "percentiles = pd.Series(np.linspace(0, 0.99, num_increment))\n",
    "\n",
    "S = percentiles.apply(thisS)\n",
    "K = percentiles.apply(thisK)\n",
    "q = percentiles.apply(thisD)\n",
    "t = np.array([.25, .5, .75, 1])\n",
    "r = percentiles.apply(thisR)\n",
    "sigma = percentiles.apply(thisSigma)\n",
    "\n",
    "param_grid = {'S': S, 'K' : K, 'q' : q, 't' : t, 'r' : r, 'sigma' : sigma}\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "sparseDF = pd.DataFrame()\n",
    "prices = []\n",
    "for params in pbar(grid):\n",
    "    prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'],\\\n",
    "    q = params['q'], t = params['t'],r = params['r'], sigma = params['sigma']))\n",
    "    sparseDF = sparseDF.append(pd.Series(params), ignore_index = True)\n",
    "\n",
    "# swap price to first column\n",
    "sparseDF['price'] = prices\n",
    "\n",
    "# output to csv\n",
    "sparseDF.to_csv('dataSparse.csv', index = False)\n",
    "print(sparseDF.head())\n",
    "print(sparseDF.tail())\n",
    "\n",
    "def this_extremes_S (q):\n",
    "    return uniform.ppf(q, 90, 110)\n",
    "S = percentiles.apply(this_extremes_S)\n",
    "\n",
    "K = percentiles.apply(thisK)\n",
    "q = percentiles.apply(thisD)\n",
    "t = np.array([.25, .5, .75, 1])\n",
    "r = percentiles.apply(thisR)\n",
    "sigma = percentiles.apply(thisSigma)\n",
    "\n",
    "param_grid = {'S': S, 'K' : K, 'q' : q, 't' : t, 'r' : r, 'sigma' : sigma}\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "extremesDF = pd.DataFrame()\n",
    "prices = []\n",
    "for params in pbar(grid):\n",
    "prices.append(bsm.black_scholes_merton(flag = 'p', S = params['S'], K = params['K'],\\\n",
    "q = params['q'], t = params['t'],r = params['r'], sigma = params['sigma']))\n",
    "extremesDF = extremesDF.append(pd.Series(params), ignore_index = True)\n",
    "\n",
    "# swap price to first column\n",
    "extremesDF['price'] = prices\n",
    "\n",
    "# output to csv\n",
    "extremesDF.to_csv('dataExtremes.csv', index = False)\n",
    "print(extremesDF.head())\n",
    "print(extremesDF.tail())\n",
    "\n",
    "# testing neural network (full data)\n",
    "fullDF = pd.read_csv(\"dataFullMain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_full = baseline_model()\n",
    "X = fullDF[['S','K','q','r','sigma','t']]\n",
    "y = fullDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_full = model_full.fit(X_train, y_train, batch_size = 64, epochs = 20, \\\n",
    "verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_full.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "X_test_full = X_test\n",
    "y_test_full = y_test\n",
    "model_full.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "# testing neural network (sparse data)\n",
    "\n",
    "sparseDF = pd.read_csv(\"dataSparseMain.csv\")\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_sparse = baseline_model()\n",
    "X = sparseDF[['S','K','q','r','sigma','t']]\n",
    "y = sparseDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_sparse = model_sparse.fit(X_train, y_train, batch_size = 64, epochs = 20, \\\n",
    "verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_sparse.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sparse.evaluate(x=X_test_full, y=y_test_full)\n",
    "\n",
    "# testing neural network (extremes data)\n",
    "extremesDF = pd.read_csv(\"dataExtremesMain.csv\")\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    i = Input(shape=(6,))\n",
    "    x = Dense(10, activation='relu')(i)\n",
    "    y = Dense(10, activation='relu')(x)\n",
    "    o = Dense(1)(y)\n",
    "    model = Model(i, o)\n",
    "    model.compile(loss=\"mse\", optimizer= \"adam\")\n",
    "    return model\n",
    "\n",
    "model_extremes = baseline_model()\n",
    "X = extremesDF[['S','K','q','r','sigma','t']]\n",
    "y = extremesDF[['price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 7)\n",
    "history_extremes = model_extremes.fit(X_train, y_train, batch_size = 64, epochs = 20, \\\n",
    "verbose = 2, validation_split=0.2) # set batch size to 1, otherwise there are errors when trying to\n",
    "\n",
    "plt.plot(history_extremes.history['val_loss'])\n",
    "plt.title('Model validation loss')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Error', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model_extremes.evaluate(x=X_test_full, y=y_test_full)\n",
    "\n",
    "tableOutput = pd.DataFrame({'Full':history_full.history['val_loss'], \\\n",
    "'Sparse':history_sparse.history['val_loss'], \\\n",
    "'Extremes':history_extremes.history['val_loss']}, columns=['Full', 'Sparse', 'Extremes'])\n",
    "tableOutput.to_csv(\"tableResultsValidaton.csv\")\n",
    "\n",
    "print(len(fullDF.index))\n",
    "print(len(sparseDF.index))\n",
    "print(len(extremesDF.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
